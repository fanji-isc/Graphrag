docid,title,summary,url,authors
0,Bayesian Models and Decision Algorithms for Complex Early Phase Clinical Trials,"An early phase clinical trial is the first step in evaluating the effects in
humans of a potential new anti-disease agent or combination of agents. Usually
called ""phase I"" or ""phase I/II"" trials, these experiments typically have the
nominal scientific goal of determining an acceptable dose, most often based on
adverse event probabilities. This arose from a tradition of phase I trials to
evaluate cytotoxic agents for treating cancer, although some methods may be
applied in other medical settings, such as treatment of stroke or immunological
diseases. Most modern statistical designs for early phase trials include
model-based, outcome-adaptive decision rules that choose doses for successive
patient cohorts based on data from previous patients in the trial. Such designs
have seen limited use in clinical practice, however, due to their complexity,
the requirement of intensive, computer-based data monitoring, and the medical
community's resistance to change. Still, many actual applications of
model-based outcome-adaptive designs have been remarkably successful in terms
of both patient benefit and scientific outcome. In this paper I will review
several Bayesian early phase trial designs that were tailored to accommodate
specific complexities of the treatment regime and patient outcomes in
particular clinical settings.",http://arxiv.org/abs/1011.6494v1,Peter F Thall
1,Deep Historical Borrowing Framework to Prospectively and Simultaneously Synthesize Control Information in Confirmatory Clinical Trials with Multiple Endpoints,"In current clinical trial development, historical information is receiving
more attention as it provides utility beyond sample size calculation.
Meta-analytic-predictive (MAP) priors and robust MAP priors have been proposed
for prospectively borrowing historical data on a single endpoint. To
simultaneously synthesize control information from multiple endpoints in
confirmatory clinical trials, we propose to approximate posterior probabilities
from a Bayesian hierarchical model and estimate critical values by deep
learning to construct pre-specified strategies for hypothesis testing. This
feature is important to ensure study integrity by establishing prospective
decision functions before the trial conduct. Simulations are performed to show
that our method properly controls family-wise error rate (FWER) and preserves
power as compared with a typical practice of choosing constant critical values
given a subset of null space. Satisfactory performance under prior-data
conflict is also demonstrated. We further illustrate our method using a case
study in Immunology.",http://arxiv.org/abs/2008.12774v2,"Tianyu Zhan, Yiwang Zhou, Ziqian Geng, Yihua Gu, Jian Kang, Li Wang, Xiaohong Huang, Elizabeth H Slate"
2,Parametric Resonance May Explain Virologic Failure to HIV Treatment Interruptions,"Pilot studies of structured treatment interruptions (STI) in HIV therapy have
shown that patients can maintain low viral loads whilst benefiting from reduced
treatment toxicity. However, a recent STI clinical trial reported a high degree
of virologic failure. Here we present a novel hypothesis that could explain
virologic failure to STI and provides new insights of great clinical relevance.
We analyze a classic mathematical model of HIV within-host viral dynamics and
find that nonlinear parametric resonance occurs when STI are added to the
model; resonance is observed as virologic failure. We use the model to simulate
clinical trial data and to calculate patient-specific resonant spectra. We gain
two important insights. Firstly, within an STI trial, we determine that
patients who begin with similar viral loads can be expected to show extremely
different virologic responses as a result of resonance. Thus, high
heterogeneity of patient response within a STI clinical trial is to be
expected. Secondly and more importantly, we determine that virologic failure is
not simply due to STI or patient characteristics; rather it is the result of a
complex dynamic interaction between STI and patient viral dynamics. Hence, our
analyses demonstrate that no universal regimen with periodic interruptions will
be effective for all patients. On the basis of our results, we suggest that
immunologic and virologic parameters should be used to design patient-specific
STI regimens.",http://arxiv.org/abs/q-bio/0504031v1,"Romulus Breban, Sally Blower"
3,"Efficient nonparametric inference on the effects of stochastic interventions under two-phase sampling, with applications to vaccine efficacy trials","The advent and subsequent widespread availability of preventive vaccines has
altered the course of public health over the past century. Despite this
success, effective vaccines to prevent many high-burden diseases, including
HIV, have been slow to develop. Vaccine development can be aided by the
identification of immune response markers that serve as effective surrogates
for clinically significant infection or disease endpoints. However, measuring
immune response marker activity is often costly, which has motivated the usage
of two-phase sampling for immune response evaluation in clinical trials of
preventive vaccines. In such trials, the measurement of immunological markers
is performed on a subset of trial participants, where enrollment in this second
phase is potentially contingent on the observed study outcome and other
participant-level information. We propose nonparametric methodology for
efficiently estimating a counterfactual parameter that quantifies the impact of
a given immune response marker on the subsequent probability of infection.
Along the way, we fill in theoretical gaps pertaining to the asymptotic
behavior of nonparametric efficient estimators in the context of two-phase
sampling, including a multiple robustness property enjoyed by our estimators.
Techniques for constructing confidence intervals and hypothesis tests are
presented, and an open source software implementation of the methodology, the
txshift R package, is introduced. We illustrate the proposed techniques using
data from a recent preventive HIV vaccine efficacy trial.",http://arxiv.org/abs/2003.13771v2,"Nima S Hejazi, Mark J van der Laan, Holly E Janes, Peter B Gilbert, David C Benkeser"
4,"DEMO: Dose Exploration, Monitoring, and Optimization Using a Biological Mediator for Clinical Outcomes","Phase 1-2 designs provide a methodological advance over phase 1 designs for
dose finding by using both clinical response and toxicity. A phase 1-2 trial
still may fail to select a truly optimal dose. because early response is not a
perfect surrogate for long term therapeutic success. To address this problem, a
generalized phase 1-2 design first uses a phase 1-2 design's components to
identify a set of candidate doses, adaptively randomizes patients among the
candidates, and after longer follow up selects a dose to maximize long-term
success rate. In this paper, we extend this paradigm by proposing a design that
exploits an early treatment-related, real-valued biological outcome, such as
pharmacodynamic activity or an immunological effect, that may act as a mediator
between dose and clinical outcomes, including tumor response, toxicity, and
survival time. We assume multivariate dose-outcome models that include effects
appearing in causal pathways from dose to the clinical outcomes. Bayesian model
selection is used to identify and eliminate biologically inactive doses. At the
end of the trial, a therapeutically optimal dose is chosen from the set of
doses that are acceptably safe, clinically effective, and biologically active
to maximize restricted mean survival time. Results of a simulation study show
that the proposed design may provide substantial improvements over designs that
ignore the biological variable.",http://arxiv.org/abs/2404.02120v1,"ChengHan Yang, Peter F Thall, Ruitao Lin"
5,Estimating treatment effects with competing intercurrent events in randomized controlled trials,"The analysis of randomized controlled trials is often complicated by
intercurrent events--events that occur after treatment initiation and may
impact outcome assessment. These events may lead to patients discontinuing
their assigned treatment or dropping out of the trial entirely. In an analysis
of data from two recent immunology trials, we categorize intercurrent events
into two broad types: those unrelated to treatment (e.g., withdrawal from the
study due to external factors like pandemics or relocation) and those related
to treatment (e.g., adverse events or lack of efficacy). We adopt distinct
strategies to handle each type, aiming to target a clinically more relevant
estimand. For treatment-related intercurrent events, they often meaningfully
describe the patient's outcome, we employ a composite variable strategy, where
we attribute an outcome value that reflects the lack of treatment success. For
treatment-unrelated intercurrent events, we adopt a hypothetical strategy that
assumes these event times are conditionally independent of the outcome, given
treatment and covariates, and envisions a scenario in which the intercurrent
events do not occur. We establish the nonparametric identification and
semiparametric estimation theory for the causal estimand and introduce doubly
robust estimators. We illustrate our methods through the re-analysis of two
randomized trials on baricitinib for Systemic Lupus Erythematosus. We classify
intercurrent events, apply four estimators, and compare our approach with
common ad-hoc methods, highlighting the robustness and practical implications
of our framework.",http://arxiv.org/abs/2503.03049v1,"Sizhu Lu, Yanyao Yi, Yongming Qu, Huayu Karen Liu, Ting Ye, Peng Ding"
6,Unlocking Historical Clinical Trial Data with ALIGN: A Compositional Large Language Model System for Medical Coding,"The reuse of historical clinical trial data has significant potential to
accelerate medical research and drug development. However, interoperability
challenges, particularly with missing medical codes, hinders effective data
integration across studies. While Large Language Models (LLMs) offer a
promising solution for automated coding without labeled data, current
approaches face challenges on complex coding tasks. We introduce ALIGN, a novel
compositional LLM-based system for automated, zero-shot medical coding. ALIGN
follows a three-step process: (1) diverse candidate code generation; (2)
self-evaluation of codes and (3) confidence scoring and uncertainty estimation
enabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing
medication terms into Anatomical Therapeutic Chemical (ATC) and medical history
terms into Medical Dictionary for Regulatory Activities (MedDRA) codes
extracted from 22 immunology trials. ALIGN outperformed the LLM baselines,
while also providing capabilities for trustworthy deployment. For MedDRA
coding, ALIGN achieved high accuracy across all levels, matching RAG and
excelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN
demonstrated superior performance, particularly at lower hierarchy levels (ATC
Level 4), with 72-73% overall accuracy and 86-89% accuracy for common
medications, outperforming baselines by 7-22%. ALIGN's uncertainty-based
deferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably
enhancing performance on uncommon medications. ALIGN achieves this
cost-efficiently at \$0.0007 and \$0.02 per code for GPT-4o-mini and GPT-4o,
reducing barriers to clinical adoption. ALIGN advances automated medical coding
for clinical trial data, contributing to enhanced data interoperability and
reusability, positioning it as a promising tool to improve clinical research
and accelerate drug development.",http://arxiv.org/abs/2411.13163v1,"Nabeel Seedat, Caterina Tozzi, Andrea Hita Ardiaca, Mihaela van der Schaar, James Weatherall, Adam Taylor"
7,An immunological autobiography: my year as a COVID-19 vaccine trial participant,"I present here longitudinal evaluation of T and B cell immunity to SARS-CoV2
and variants of concern (VOC) from a single subject (me) over an entire year
post vaccination. After enrolling in the Moderna phase III clinical trial, I
collected my own biological samples pre- and post-immunization in the event of
being a recipient of the experimental vaccine. The evidence strongly supports
the conclusion that I did not receive the placebo. The analysis is admittedly
limited to an n of 1, but the results fit well with data taken from published
works and represent one of the more comprehensive longitudinal evaluations of
vaccine-elicited immunity within a single individual yet to be undertaken.
Though the data amount to a well-documented anecdote, given its granularity, it
is not without its insights and may be of further use in directing future
longitudinal studies that have actual statistical significance.",http://arxiv.org/abs/2111.01282v2,Ross M Kedl
8,What Radio Waves Tell Us about Sleep,"The ability to assess sleep at home, capture sleep stages, and detect the
occurrence of apnea (without on-body sensors) simply by analyzing the radio
waves bouncing off people's bodies while they sleep is quite powerful. Such a
capability would allow for longitudinal data collection in patients' homes,
informing our understanding of sleep and its interaction with various diseases
and their therapeutic responses, both in clinical trials and routine care. In
this article, we develop an advanced machine learning algorithm for passively
monitoring sleep and nocturnal breathing from radio waves reflected off people
while asleep. Validation results in comparison with the gold standard (i.e.,
polysomnography) (n=849) demonstrate that the model captures the sleep
hypnogram (with an accuracy of 81% for 30-second epochs categorized into Wake,
Light Sleep, Deep Sleep, or REM), detects sleep apnea (AUROC = 0.88), and
measures the patient's Apnea-Hypopnea Index (ICC=0.95; 95% CI = [0.93, 0.97]).
Notably, the model exhibits equitable performance across race, sex, and age.
Moreover, the model uncovers informative interactions between sleep stages and
a range of diseases including neurological, psychiatric, cardiovascular, and
immunological disorders. These findings not only hold promise for clinical
practice and interventional trials but also underscore the significance of
sleep as a fundamental component in understanding and managing various
diseases.",http://arxiv.org/abs/2405.11739v2,"Hao He, Chao Li, Wolfgang Ganglberger, Kaileigh Gallagher, Rumen Hristov, Michail Ouroutzoglou, Haoqi Sun, Jimeng Sun, Brandon Westover, Dina Katabi"
9,Distinguishing immunological and behavioral effects of vaccination,"The interpretation of vaccine efficacy estimands is subtle, even in
randomized trials designed to quantify immunological effects of vaccination. In
this article, we introduce terminology to distinguish between different vaccine
efficacy estimands and clarify their interpretations. This allows us to
explicitly consider immunological and behavioural effects of vaccination, and
establish that policy-relevant estimands can differ substantially from those
commonly reported in vaccine trials. We further show that a conventional
vaccine trial allows identification and estimation of different vaccine
estimands under plausible conditions, if one additional post-treatment variable
is measured. Specifically, we utilize a ``belief variable'' that indicates the
treatment an individual believed they had received. The belief variable is
similar to ``blinding assessment'' variables that are occasionally collected in
placebo-controlled trials in other fields. We illustrate the relations between
the different estimands, and their practical relevance, in numerical examples
based on an influenza vaccine trial.",http://arxiv.org/abs/2311.08335v1,"Mats Stensrud, Daniel Nevo, Uri Obolski"
10,"Panacea: A foundation model for clinical trial search, summarization, design, and recruitment","Clinical trials are fundamental in developing new drugs, medical devices, and
treatments. However, they are often time-consuming and have low success rates.
Although there have been initial attempts to create large language models
(LLMs) for clinical trial design and patient-trial matching, these models
remain task-specific and not adaptable to diverse clinical trial tasks. To
address this challenge, we propose a clinical trial foundation model named
Panacea, designed to handle multiple tasks, including trial search, trial
summarization, trial design, and patient-trial matching. We also assemble a
large-scale dataset, named TrialAlign, of 793,279 trial documents and 1,113,207
trial-related scientific papers, to infuse clinical knowledge into the model by
pre-training. We further curate TrialInstruct, which has 200,866 of instruction
data for fine-tuning. These resources enable Panacea to be widely applicable
for a range of clinical trial tasks based on user requirements.
  We evaluated Panacea on a new benchmark, named TrialPanorama, which covers
eight clinical trial tasks. Our method performed the best on seven of the eight
tasks compared to six cutting-edge generic or medicine-specific LLMs.
Specifically, Panacea showed great potential to collaborate with human experts
in crafting the design of eligibility criteria, study arms, and outcome
measures, in multi-round conversations. In addition, Panacea achieved 14.42%
improvement in patient-trial matching, 41.78% to 52.02% improvement in trial
search, and consistently ranked at the top for five aspects of trial
summarization. Our approach demonstrates the effectiveness of Panacea in
clinical trials and establishes a comprehensive resource, including training
data, model, and benchmark, for developing clinical trial foundation models,
paving the path for AI-based clinical trial development.",http://arxiv.org/abs/2407.11007v1,"Jiacheng Lin, Hanwen Xu, Zifeng Wang, Sheng Wang, Jimeng Sun"
11,Assessment of Immune Correlates of Protection via Controlled Vaccine Efficacy and Controlled Risk,"Immune correlates of protection (CoPs) are immunologic biomarkers accepted as
a surrogate for an infectious disease clinical endpoint and thus can be used
for traditional or provisional vaccine approval. To study CoPs in randomized,
placebo-controlled trials, correlates of risk (CoRs) are first assessed in
vaccine recipients. This analysis does not assess causation, as a CoR may fail
to be a CoP. We propose a causal CoP analysis that estimates the controlled
vaccine efficacy curve across biomarker levels $s$, $CVE(s)$, equal to one
minus the ratio of the controlled-risk curve $r_C(s)$ at $s$ and placebo risk,
where $r_C(s)$ is causal risk if all participants are assigned vaccine and the
biomarker is set to $s$. The criterion for a useful CoP is wide variability of
$CVE(s)$ in $s$. Moreover, estimation of $r_C(s)$ is of interest in itself,
especially in studies without a placebo arm. For estimation of $r_C(s)$,
measured confounders can be adjusted for by any regression method that
accommodates missing biomarkers, to which we add sensitivity analysis to
quantify robustness of CoP evidence to unmeasured confounding. Application to
two harmonized phase 3 trials supports that 50% neutralizing antibody titer has
value as a controlled vaccine efficacy CoP for virologically confirmed dengue
(VCD): in CYD14 the point estimate (95% confidence interval) for $CVE(s)$
accounting for measured confounders and building in conservative margin for
unmeasured confounding increases from 29.6% (95% CI 3.5 to 45.9) at titer 1:36
to 78.5% (95% CI 67.9 to 86.8) at titer 1:1200; these estimates are 17.4% (95%
CI -14.4 to 36.5) and 84.5% (95% CI 79.6 to 89.1) for CYD15.",http://arxiv.org/abs/2107.05734v1,"Peter B Gilbert, Youyi Fong, Marco Carone"
12,Rank-Based Identification of High-dimensional Surrogate Markers: Application to Vaccinology,"In vaccine trials with long-term participant follow-up, it is of great
importance to identify surrogate markers that accurately infer long-term immune
responses. These markers offer practical advantages such as providing early,
indirect evidence of vaccine efficacy, and can accelerate vaccine development
while identifying potential biomarkers. High-throughput technologies like
RNA-sequencing have emerged as promising tools for understanding complex
biological systems and informing new treatment strategies. However, these data
are high-dimensional, presenting unique statistical challenges for existing
surrogate marker identification methods. We introduce Rank-based Identification
of high-dimensional SurrogatE Markers (RISE), a novel approach designed for
small sample, high-dimensional settings typical in modern vaccine experiments.
RISE employs a non-parametric univariate test to screen variables for promising
candidates, followed by surrogate evaluation on independent data. Our
simulation studies demonstrate RISE's desirable properties, including type one
error rate control and empirical power under various conditions. Applying RISE
to a clinical trial for inactivated influenza vaccination, we sought to
identify genes whose post-vaccination expression could serve as a surrogate for
the induced immune response. This analysis revealed a signature of genes whose
combined expression at 1 day post-injection appears to be a reasonable
surrogate for the neutralising antibody titres at 28 days after vaccination.
Pathways related to innate antiviral signalling and interferon stimulation were
strongly represented in this derived surrogate, providing a clear immunological
interpretation.",http://arxiv.org/abs/2502.03030v1,"Arthur Hughes, Layla Parast, Rodolphe Thibaut, Boris P Hejblum"
13,Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision,"Clinical trials are essential for drug development but are extremely
expensive and time-consuming to conduct. It is beneficial to study similar
historical trials when designing a clinical trial. However, lengthy trial
documents and lack of labeled data make trial similarity search difficult. We
propose a zero-shot clinical trial retrieval method, Trial2Vec, which learns
through self-supervision without annotating similar clinical trials.
Specifically, the meta-structure of trial documents (e.g., title, eligibility
criteria, target disease) along with clinical knowledge (e.g., UMLS knowledge
base https://www.nlm.nih.gov/research/umls/index.html) are leveraged to
automatically generate contrastive samples. Besides, Trial2Vec encodes trial
documents considering meta-structure thus producing compact embeddings
aggregating multi-aspect information from the whole document. We show that our
method yields medically interpretable embeddings by visualization and it gets a
15% average improvement over the best baselines on precision/recall for trial
retrieval, which is evaluated on our labeled 1600 trial pairs. In addition, we
prove the pre-trained embeddings benefit the downstream trial outcome
prediction task over 240k trials. Software ias available at
https://github.com/RyanWangZf/Trial2Vec.",http://arxiv.org/abs/2206.14719v2,"Zifeng Wang, Jimeng Sun"
14,SPOT: Sequential Predictive Modeling of Clinical Trial Outcome with Meta-Learning,"Clinical trials are essential to drug development but time-consuming, costly,
and prone to failure. Accurate trial outcome prediction based on historical
trial data promises better trial investment decisions and more trial success.
Existing trial outcome prediction models were not designed to model the
relations among similar trials, capture the progression of features and designs
of similar trials, or address the skewness of trial data which causes inferior
performance for less common trials.
  To fill the gap and provide accurate trial outcome prediction, we propose
Sequential Predictive mOdeling of clinical Trial outcome (SPOT) that first
identifies trial topics to cluster the multi-sourced trial data into relevant
trial topics. It then generates trial embeddings and organizes them by topic
and time to create clinical trial sequences. With the consideration of each
trial sequence as a task, it uses a meta-learning strategy to achieve a point
where the model can rapidly adapt to new tasks with minimal updates. In
particular, the topic discovery module enables a deeper understanding of the
underlying structure of the data, while sequential learning captures the
evolution of trial designs and outcomes. This results in predictions that are
not only more accurate but also more interpretable, taking into account the
temporal patterns and unique characteristics of each trial topic. We
demonstrate that SPOT wins over the prior methods by a significant margin on
trial outcome benchmark data: with a 21.5\% lift on phase I, an 8.9\% lift on
phase II, and a 5.5\% lift on phase III trials in the metric of the area under
precision-recall curve (PR-AUC).",http://arxiv.org/abs/2304.05352v1,"Zifeng Wang, Cao Xiao, Jimeng Sun"
15,Smooth and probabilistic PARAFAC model with auxiliary covariates,"In immunological and clinical studies, matrix-valued time-series data
clustering is increasingly popular. Researchers are interested in finding
low-dimensional embedding of subjects based on potentially high-dimensional
longitudinal features and investigating relationships between static clinical
covariates and the embedding. These studies are often challenging due to high
dimensionality, as well as the sparse and irregular nature of sample collection
along the time dimension. We propose a smoothed probabilistic PARAFAC model
with covariates (SPACO) to tackle these two problems while utilizing auxiliary
covariates of interest. We provide intensive simulations to test different
aspects of SPACO and demonstrate its use on an immunological data set from
patients with SARs-CoV-2 infection.",http://arxiv.org/abs/2104.05184v3,Leying Guan
16,Optimal Allocation of Gold Standard Testing under Constrained Availability: Application to Assessment of HIV Treatment Failure,"The World Health Organization (WHO) guidelines for monitoring the
effectiveness of HIV treatment in resource-limited settings (RLS) are mostly
based on clinical and immunological markers (e.g., CD4 cell counts). Recent
research indicates that the guidelines are inadequate and can result in high
error rates. Viral load (VL) is considered the ""gold standard"", yet its
widespread use is limited by cost and infrastructure. In this paper, we propose
a diagnostic algorithm that uses information from routinely-collected clinical
and immunological markers to guide a selective use of VL testing for diagnosing
HIV treatment failure, under the assumption that VL testing is available only
at a certain portion of patient visits. Our algorithm identifies the patient
subpopulation, such that the use of limited VL testing on them minimizes a
pre-defined risk (e.g., misdiagnosis error rate). Diagnostic properties of our
proposal algorithm are assessed by simulations. For illustration, data from the
Miriam Hospital Immunology Clinic (RI, USA) are analyzed.",http://arxiv.org/abs/2010.00692v1,"Tao Liu, Joseph W Hogan, Lisa Wang, Shangxuan Zhang, Rami Kantor"
17,Exploring the Generalization of Cancer Clinical Trial Eligibility Classifiers Across Diseases,"Clinical trials are pivotal in medical research, and NLP can enhance their
success, with application in recruitment. This study aims to evaluate the
generalizability of eligibility classification across a broad spectrum of
clinical trials. Starting with phase 3 cancer trials, annotated with seven
eligibility exclusions, then to determine how well models can generalize to
non-cancer and non-phase 3 trials. To assess this, we have compiled eligibility
criteria data for five types of trials: (1) additional phase 3 cancer trials,
(2) phase 1 and 2 cancer trials, (3) heart disease trials, (4) type 2 diabetes
trials, and (5) observational trials for any disease, comprising 2,490
annotated eligibility criteria across seven exclusion types. Our results show
that models trained on the extensive cancer dataset can effectively handle
criteria commonly found in non-cancer trials, such as autoimmune diseases.
However, they struggle with criteria disproportionately prevalent in cancer
trials, like prior malignancy. We also experiment with few-shot learning,
demonstrating that a limited number of disease-specific examples can partially
overcome this performance gap. We are releasing this new dataset of annotated
eligibility statements to promote the development of cross-disease
generalization in clinical trial classification.",http://arxiv.org/abs/2403.17135v1,"Yumeng Yang, Ashley Gilliam, Ethan B Ludmir, Kirk Roberts"
18,TrialSynth: Generation of Synthetic Sequential Clinical Trial Data,"Analyzing data from past clinical trials is part of the ongoing effort to
optimize the design, implementation, and execution of new clinical trials and
more efficiently bring life-saving interventions to market. While there have
been recent advances in the generation of static context synthetic clinical
trial data, due to both limited patient availability and constraints imposed by
patient privacy needs, the generation of fine-grained synthetic time-sequential
clinical trial data has been challenging. Given that patient trajectories over
an entire clinical trial are of high importance for optimizing trial design and
efforts to prevent harmful adverse events, there is a significant need for the
generation of high-fidelity time-sequence clinical trial data. Here we
introduce TrialSynth, a Variational Autoencoder (VAE) designed to address the
specific challenges of generating synthetic time-sequence clinical trial data.
Distinct from related clinical data VAE methods, the core of our method
leverages Hawkes Processes (HP), which are particularly well-suited for
modeling event-type and time gap prediction needed to capture the structure of
sequential clinical trial data. Our experiments demonstrate that TrialSynth
surpasses the performance of other comparable methods that can generate
sequential clinical trial data at varying levels of fidelity / privacy
tradeoff, enabling the generation of highly accurate event sequences across
multiple real-world sequential event datasets with small patient source
populations. Notably, our empirical findings highlight that TrialSynth not only
outperforms existing clinical sequence-generating methods but also produces
data with superior utility while empirically preserving patient privacy.",http://arxiv.org/abs/2409.07089v2,"Chufan Gao, Mandis Beigi, Afrah Shafquat, Jacob Aptekar, Jimeng Sun"
19,Clinical Trial Information Extraction with BERT,"Natural language processing (NLP) of clinical trial documents can be useful
in new trial design. Here we identify entity types relevant to clinical trial
design and propose a framework called CT-BERT for information extraction from
clinical trial text. We trained named entity recognition (NER) models to
extract eligibility criteria entities by fine-tuning a set of pre-trained BERT
models. We then compared the performance of CT-BERT with recent baseline
methods including attention-based BiLSTM and Criteria2Query. The results
demonstrate the superiority of CT-BERT in clinical trial NLP.",http://arxiv.org/abs/2110.10027v1,"Xiong Liu, Greg L Hersch, Iya Khalil, Murthy Devarakonda"
20,CTP-LLM: Clinical Trial Phase Transition Prediction Using Large Language Models,"New medical treatment development requires multiple phases of clinical
trials. Despite the significant human and financial costs of bringing a drug to
market, less than 20% of drugs in testing will make it from the first phase to
final approval. Recent literature indicates that the design of the trial
protocols significantly contributes to trial performance. We investigated
Clinical Trial Outcome Prediction (CTOP) using trial design documents to
predict phase transitions automatically. We propose CTP-LLM, the first Large
Language Model (LLM) based model for CTOP. We also introduce the
PhaseTransition (PT) Dataset; which labels trials based on their progression
through the regulatory process and serves as a benchmark for CTOP evaluation.
Our fine-tuned GPT-3.5-based model (CTP-LLM) predicts clinical trial phase
transition by analyzing the trial's original protocol texts without requiring
human-selected features. CTP-LLM achieves a 67% accuracy rate in predicting
trial phase transitions across all phases and a 75% accuracy rate specifically
in predicting the transition from Phase~III to final approval. Our experimental
performance highlights the potential of LLM-powered applications in forecasting
clinical trial outcomes and assessing trial design.",http://arxiv.org/abs/2408.10995v1,"Michael Reinisch, Jianfeng He, Chenxi Liao, Sauleh Ahmad Siddiqui, Bei Xiao"
21,Automatically Labeling Clinical Trial Outcomes: A Large-Scale Benchmark for Drug Development,"Background The cost of drug discovery and development is substantial, with
clinical trial outcomes playing a critical role in regulatory approval and
patient care. However, access to large-scale, high-quality clinical trial
outcome data remains limited, hindering advancements in predictive modeling and
evidence-based decision-making.
  Methods We present the Clinical Trial Outcome (CTO) benchmark, a fully
reproducible, large-scale repository encompassing approximately 125,000 drug
and biologics trials. CTO integrates large language model (LLM) interpretations
of publications, trial phase progression tracking, sentiment analysis from news
sources, stock price movements of trial sponsors, and additional trial-related
metrics. Furthermore, we manually annotated a dataset of clinical trials
conducted between 2020 and 2024 to enhance the quality and reliability of
outcome labels.
  Results The trial outcome labels in the CTO benchmark agree strongly with
expert annotations, achieving an F1 score of 94 for Phase 3 trials and 91
across all phases. Additionally, benchmarking standard machine learning models
on our manually annotated dataset revealed distribution shifts in recent
trials, underscoring the necessity of continuously updated labeling approaches.
  Conclusions By analyzing CTO's performance on recent clinical trials, we
demonstrate the ongoing need for high-quality, up-to-date trial outcome labels.
We publicly release the CTO knowledge base and annotated labels at
https://chufangao.github.io/CTOD, with regular updates to support research on
clinical trial outcomes and inform data-driven improvements in drug
development.",http://arxiv.org/abs/2406.10292v3,"Chufan Gao, Jathurshan Pradeepkumar, Trisha Das, Shivashankar Thati, Jimeng Sun"
22,Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial Generation,"Machine learning (ML) exhibits promise in the clinical domain. However, it is
constrained by data scarcity and ethical considerations, as the generation of
clinical trials presents significant challenges due to stringent privacy
regulations, high costs, and the extended duration required for conducting
studies with human participants. Despite the advancements of large language
models (LLMs) in general generation tasks, their potential in facilitating the
generation of synthetic clinical trials is under-explored. To address this gap,
we introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs
to generate artificial yet realistic and diverse clinical trials with binary
success/failure labels. Experiments conducted on real clinical trials from the
\url{ClinicalTrials.gov} database demonstrate that our synthetic data can
effectively augment real datasets. Furthermore, by fine-tuning a pre-trained
model as a binary classifier on synthetic clinical trial datasets, we
demonstrate that this augmentation enhances model training for downstream tasks
such as trial outcome prediction. Our findings suggest that LLMs for synthetic
clinical trial generation hold promise for accelerating clinical research and
upholding ethical standards for patient privacy. The code is publicly available
at
https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.",http://arxiv.org/abs/2410.12476v2,"Zerui Xu, Fang Wu, Yuanyuan Zhang, Yue Zhao"
23,From Bench to Bedside: A Review of Clinical Trials in Drug Discovery and Development,"Clinical trials are an indispensable part of the drug development process,
bridging the gap between basic research and clinical application. During the
development of new drugs, clinical trials are used not only to evaluate the
safety and efficacy of the drug but also to explore its dosage, treatment
regimens, and potential side effects. This review discusses the various stages
of clinical trials, including Phase I (safety assessment), Phase II
(preliminary efficacy evaluation), Phase III (large-scale validation), and
Phase IV (post-marketing surveillance), highlighting the characteristics of
each phase and their interrelationships. Additionally, the paper addresses the
major challenges encountered in clinical trials, such as ethical issues,
subject recruitment difficulties, diversity and representativeness concerns,
and proposes strategies for overcoming these challenges. With the advancement
of technology, innovative technologies such as artificial intelligence, big
data, and digitalization are gradually transforming clinical trial design and
implementation, improving trial efficiency and data quality. The article also
looks forward to the future of clinical trials, particularly the impact of
emerging therapies such as gene therapy and immunotherapy on trial design, as
well as the importance of regulatory reforms and global collaboration. In
conclusion, the core role of clinical trials in drug development will continue
to drive the progress of innovative drug development and clinical treatment.",http://arxiv.org/abs/2412.09378v2,"Tianyang Wang, Ming Liu, Benji Peng, Xinyuan Song, Charles Zhang, Xintian Sun, Qian Niu, Junyu Liu, Silin Chen, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Yunze Wang, Yichao Zhang, Cheng Fei, Lawrence KQ Yan"
24,Estimating Design Operating Characteristics in Bayesian Adaptive Clinical Trials,"Bayesian adaptive designs have gained popularity in all phases of clinical
trials with numerous new developments in the past few decades. During the
COVID-19 pandemic, the need to establish evidence for the effectiveness of
vaccines, therapeutic treatments and policies that could resolve or control the
crisis emphasized the advantages offered by efficient and flexible clinical
trial designs. In many COVID-19 clinical trials, due to the high level of
uncertainty, Bayesian adaptive designs were considered advantageous. Designing
Bayesian adaptive trials, however, requires extensive simulation studies that
are generally considered challenging, particularly in time-sensitive settings
such as a pandemic. In this article, we propose a set of methods for efficient
estimation and uncertainty quantification for design operating characteristics
of Bayesian adaptive trials. Specifically, we model the sampling distribution
of Bayesian probability statements that are commonly used as the basis of
decision making. To showcase the implementation and performance of the proposed
approach, we use a clinical trial design with an ordinal disease-progression
scale endpoint that was popular among COVID-19 trial. However, the proposed
methodology may be applied generally in clinical trial context where design
operating characteristics cannot be obtained analytically.",http://arxiv.org/abs/2105.03022v2,Shirin Golchi
25,Artificial Intelligence for In Silico Clinical Trials: A Review,"A clinical trial is an essential step in drug development, which is often
costly and time-consuming. In silico trials are clinical trials conducted
digitally through simulation and modeling as an alternative to traditional
clinical trials. AI-enabled in silico trials can increase the case group size
by creating virtual cohorts as controls. In addition, it also enables
automation and optimization of trial design and predicts the trial success
rate. This article systematically reviews papers under three main topics:
clinical simulation, individualized predictive modeling, and computer-aided
trial design. We focus on how machine learning (ML) may be applied in these
applications. In particular, we present the machine learning problem
formulation and available data sources for each task. We end with discussing
the challenges and opportunities of AI for in silico trials in real-world
applications.",http://arxiv.org/abs/2209.09023v1,"Zifeng Wang, Chufan Gao, Lucas M Glass, Jimeng Sun"
26,Language Interaction Network for Clinical Trial Approval Estimation,"Clinical trial outcome prediction seeks to estimate the likelihood that a
clinical trial will successfully reach its intended endpoint. This process
predominantly involves the development of machine learning models that utilize
a variety of data sources such as descriptions of the clinical trials,
characteristics of the drug molecules, and specific disease conditions being
targeted. Accurate predictions of trial outcomes are crucial for optimizing
trial planning and prioritizing investments in a drug portfolio. While previous
research has largely concentrated on small-molecule drugs, there is a growing
need to focus on biologics-a rapidly expanding category of therapeutic agents
that often lack the well-defined molecular properties associated with
traditional drugs. Additionally, applying conventional methods like graph
neural networks to biologics data proves challenging due to their complex
nature. To address these challenges, we introduce the Language Interaction
Network (LINT), a novel approach that predicts trial outcomes using only the
free-text descriptions of the trials. We have rigorously tested the
effectiveness of LINT across three phases of clinical trials, where it achieved
ROC-AUC scores of 0.770, 0.740, and 0.748 for phases I, II, and III,
respectively, specifically concerning trials involving biologic interventions.",http://arxiv.org/abs/2405.06662v1,"Chufan Gao, Tianfan Fu, Jimeng Sun"
27,Speed and accuracy in a visual motion discrimination task as performed by rats,"We find that rats, like primates and humans, perform better on the random dot
motion task when they take more time to respond. We provide evidence that this
improvement is due to stimulus integration. Rats increase their response
latency modestly as a function of trial difficulty. Rats can modulate response
latency more strongly on a trial by trial basis, apparently on the basis of
reward-related parameters.",http://arxiv.org/abs/1206.0311v1,"Pamela Reinagel, Emily Mankin, Adam Calhoun"
28,A Latent Gaussian Process Model with Application to Monitoring Clinical Trials,"In many clinical trials treatments need to be repeatedly applied as diseases
relapse frequently after remission over a long period of time (e.g., 35 weeks).
Most research in statistics focuses on the overall trial design, such as sample
size and power calculation, or on the data analysis after trials are completed.
Little is done to improve the efficiency of trial monitoring, such as early
termination of trials due to futility. The challenge faced in such trial
monitoring is mostly caused by the need to properly model repeated outcomes
from patients. We propose a Bayesian trial monitoring scheme for clinical
trials with repeated and potentially cyclic binary outcomes. We construct a
latent Gaussian process (LGP) to model discrete longitudinal data in those
trials. LGP describes the underlying latent process that gives rise to the
observed longitudinal binary outcomes. The posterior consistency property of
the proposed model is studied. Posterior inference is conducted with a hybrid
Monte Carlo algorithm. Simulation studies are conducted under various clinical
scenarios, and a case study is reported based on a real-life trial.",http://arxiv.org/abs/1403.7853v1,"Yanxun Xu, Yuan Ji"
29,Clinical trial site matching with improved diversity using fair policy learning,"The ongoing pandemic has highlighted the importance of reliable and efficient
clinical trials in healthcare. Trial sites, where the trials are conducted, are
chosen mainly based on feasibility in terms of medical expertise and access to
a large group of patients. More recently, the issue of diversity and inclusion
in clinical trials is gaining importance. Different patient groups may
experience the effects of a medical drug/ treatment differently and hence need
to be included in the clinical trials. These groups could be based on
ethnicity, co-morbidities, age, or economic factors. Thus, designing a method
for trial site selection that accounts for both feasibility and diversity is a
crucial and urgent goal. In this paper, we formulate this problem as a ranking
problem with fairness constraints. Using principles of fairness in machine
learning, we learn a model that maps a clinical trial description to a ranked
list of potential trial sites. Unlike existing fairness frameworks, the group
membership of each trial site is non-binary: each trial site may have access to
patients from multiple groups. We propose fairness criteria based on
demographic parity to address such a multi-group membership scenario. We test
our method on 480 real-world clinical trials and show that our model results in
a list of potential trial sites that provides access to a diverse set of
patients while also ensuing a high number of enrolled patients.",http://arxiv.org/abs/2204.06501v1,"Rakshith S Srinivasa, Cheng Qian, Brandon Theodorou, Jeffrey Spaeder, Cao Xiao, Lucas Glass, Jimeng Sun"
30,Ongoing Vaccine and Monoclonal Antibody HIV Prevention Efficacy Trials and Considerations for Sequel Efficacy Trial Designs,"Four randomized placebo-controlled efficacy trials of a candidate vaccine or
passively infused monoclonal antibody for prevention of HIV-1 infection are
underway (HVTN 702 in South African men and women; HVTN 705 in sub-Saharan
African women; HVTN 703/HPTN 081 in sub-Saharan African women; HVTN 704/HPTN
085 in U.S., Peruvian, Brazilian, and Swiss men or transgender persons who have
sex with men). Several challenges are posed to the optimal design of the sequel
efficacy trials, including: (1) how to account for the evolving mosaic of
effective prevention interventions that may be part of the trial design or
standard of prevention; (2) how to define viable and optimal sequel trial
designs depending on the primary efficacy results and secondary 'correlates of
protection' results of each of the ongoing trials; and (3) how to define the
primary objective of sequel efficacy trials if HIV-1 incidence is expected to
be very low in all study arms such that a standard trial design has a steep
opportunity cost. After summarizing the ongoing trials, I discuss statistical
science considerations for sequel efficacy trial designs, both generally and
specifically to each trial listed above. One conclusion is that the results of
'correlates of protection' analyses, which ascertain how different host
immunological markers and HIV-1 viral features impact HIV-1 risk and prevention
efficacy, have an important influence on sequel trial design. This influence is
especially relevant for the monoclonal antibody trials because of the focused
pre-trial hypothesis that potency and coverage of serum neutralization
constitutes a surrogate endpoint for HIV-1 infection... (see manuscript for the
full abstract)",http://arxiv.org/abs/1906.08409v1,Peter B Gilbert
31,Augmenting adaptive immunity: progress and challenges in the quantitative engineering and analysis of adaptive immune receptor repertoires,"The adaptive immune system is a natural diagnostic and therapeutic. It
recognizes threats earlier than clinical symptoms manifest and neutralizes
antigen with exquisite specificity. Recognition specificity and broad
reactivity is enabled via adaptive B- and T-cell receptors: the immune receptor
repertoire. The human immune system, however, is not omnipotent. Our natural
defense system sometimes loses the battle to parasites and microbes and even
turns against us in the case of cancer, autoimmune and inflammatory disease. A
long-standing dream of immunoengineers has been, therefore, to mechanistically
understand how the immune system sees, reacts and remembers antigens. Only very
recently, experimental and computational methods have achieved sufficient
quantitative resolution to start querying and engineering adaptive immunity
with great precision. In specific, these innovations have been applied with the
greatest fervency and success in immunotherapy, autoimmunity and vaccine
design. The work here highlights advances, challenges and future directions of
quantitative approaches which seek to advance the fundamental understanding of
immunological phenomena, and reverse engineer the immune system to produce
auspicious biopharmaceutical drugs and immunodiagnostics. Our review indicates
that the merger of fundamental immunology, computational immunology and
digital-biotechnology minimizes black box engineering, thereby advancing both
immunological knowledge and as well immunoengineering methodologies.",http://arxiv.org/abs/1904.04105v2,"Alex J Brown, Igor Snapkov, Rahmad Akbar, Milena Pavlovi, Enkelejda Miho, Geir K Sandve, Victor Greiff"
32,Sample size for a non-inferiority clinical trial with time-to-event data in the presence of competing risks,"The analysis and planning methods for competing risks model have been
described in the literatures in recent decades, and non-inferiority clinical
trials are helpful in current pharmaceutical practice. Analytical methods for
non-inferiority clinical trials in the presence of competing risks were
investigated by Parpia et al., who indicated that the proportional
sub-distribution hazard model is appropriate in the context of biological
studies. However, the analytical methods of competing risks model differ from
those appropriate for analyzing non-inferiority clinical trials with a single
outcome; thus, a corresponding method for planning such trials is necessary. A
sample size formula for non-inferiority clinical trials in the presence of
competing risks based on the proportional sub-distribution hazard model is
presented in this paper. The primary endpoint relies on the sub-distribution
hazard ratio. A total of 120 simulations and an example based on a randomized
controlled trial verified the empirical performance of the presented formula.
The results demonstrate that the empirical power of sample size formulas based
on the Weibull distribution for non-inferiority clinical trials with competing
risks can reach the targeted power.",http://arxiv.org/abs/1802.10245v1,"Dong Han, Zheng Chen, Yawen Hou"
33,Clinical Trial Recommendations Using Semantics-Based Inductive Inference and Knowledge Graph Embeddings,"Designing a new clinical trial entails many decisions, such as defining a
cohort and setting the study objectives to name a few, and therefore can
benefit from recommendations based on exhaustive mining of past clinical trial
records. Here, we propose a novel recommendation methodology, based on neural
embeddings trained on a first-of-a-kind knowledge graph of clinical trials. We
addressed several important research questions in this context, including
designing a knowledge graph (KG) for clinical trial data, effectiveness of
various KG embedding (KGE) methods for it, a novel inductive inference using
KGE, and its use in generating recommendations for clinical trial design. We
used publicly available data from clinicaltrials.gov for the study. Results
show that our recommendations approach achieves relevance scores of 70%-83%,
measured as the text similarity to actual clinical trial elements, and the most
relevant recommendation can be found near the top of list. Our study also
suggests potential improvement in training KGE using node semantics.",http://arxiv.org/abs/2309.15979v1,"Murthy V Devarakonda, Smita Mohanty, Raja Rao Sunkishala, Nag Mallampalli, Xiong Liu"
34,MatchMiner-AI: An Open-Source Solution for Cancer Clinical Trial Matching,"Clinical trials drive improvements in cancer treatments and outcomes.
However, most adults with cancer do not participate in trials, and trials often
fail to enroll enough patients to answer their scientific questions. Artificial
intelligence could accelerate matching of patients to appropriate clinical
trials. Here, we describe the development and evaluation of the MatchMiner-AI
pipeline for clinical trial searching and ranking. MatchMiner-AI focuses on
matching patients to potential trials based on core criteria describing
clinical ""spaces,"" or disease contexts, targeted by a trial. It aims to
accelerate the human work of identifying potential matches, not to fully
automate trial screening. The pipeline includes modules for extraction of key
information from a patient's longitudinal electronic health record; rapid
ranking of candidate trial-patient matches based on embeddings in vector space;
and classification of whether a candidate match represents a reasonable
clinical consideration. Code and synthetic data are available at
https://huggingface.co/ksg-dfci/MatchMiner-AI . Model weights based on
synthetic data are available at https://huggingface.co/ksg-dfci/TrialSpace and
https://huggingface.co/ksg-dfci/TrialChecker . A simple cancer clinical trial
search engine to demonstrate pipeline components is available at
https://huggingface.co/spaces/ksg-dfci/trial_search_alpha .",http://arxiv.org/abs/2412.17228v1,"Ethan Cerami, Pavel Trukhanov, Morgan A Paul, Michael J Hassett, Irbaz B Riaz, James Lindsay, Emily Mallaber, Harry Klein, Gufran Gungor, Matthew Galvin, Stephen C Van Nostrand, Joyce Yu, Tali Mazor, Kenneth L Kehl"
35,TrialDura: Hierarchical Attention Transformer for Interpretable Clinical Trial Duration Prediction,"The clinical trial process, a critical phase in drug development, is
essential for developing new treatments. The primary goal of interventional
clinical trials is to evaluate the safety and efficacy of drug-based treatments
for specific diseases. However, these trials are often lengthy,
labor-intensive, and expensive. The duration of a clinical trial significantly
impacts overall costs, making efficient timeline management crucial for
controlling budgets and ensuring the economic feasibility of research. To
address this issue, We propose TrialDura, a machine learning-based method that
estimates the duration of clinical trials using multimodal data, including
disease names, drug molecules, trial phases, and eligibility criteria. Then, we
encode them into Bio-BERT embeddings specifically tuned for biomedical contexts
to provide a deeper and more relevant semantic understanding of clinical trial
data. Finally, the model's hierarchical attention mechanism connects all of the
embeddings to capture their interactions and predict clinical trial duration.
Our proposed model demonstrated superior performance with a mean absolute error
(MAE) of 1.04 years and a root mean square error (RMSE) of 1.39 years compared
to the other models, indicating more accurate clinical trial duration
prediction. Publicly available code can be found at:
https://anonymous.4open.science/r/TrialDura-F196.",http://arxiv.org/abs/2404.13235v2,"Ling Yue, Jonathan Li, Sixue Xing, Md Zabirul Islam, Bolun Xia, Tianfan Fu, Jintai Chen"
36,A general Bayesian approach to design adaptive clinical trials with time-to-event outcomes,"Clinical trials are an integral component of medical research. Trials require
careful design to, for example, maintain the safety of participants, use
resources efficiently and allow clinically meaningful conclusions to be drawn.
Adaptive clinical trials (i.e. trials that can be altered based on evidence
that has accrued) are often more efficient, informative and ethical than
standard or non-adaptive trials because they require fewer participants, target
more promising treatments, and can stop early with sufficient evidence of
effectiveness or harm. The design of adaptive trials requires the
pre-specification of adaptions that are permissible throughout the conduct of
the trial. Proposed adaptive designs are then usually evaluated through
simulation which provides indicative metrics of performance (e.g. statistical
power and type-1 error) under different scenarios. Trial simulation requires
assumptions about the data generating process to be specified but correctly
specifying these in practice can be difficult, particularly for new and
emerging diseases. To address this, we propose an approach to design adaptive
clinical trials without needing to specify the complete data generating
process. To facilitate this, we consider a general Bayesian framework where
inference about the treatment effect on a time-to-event outcome can be
performed via the partial likelihood. As a consequence, the proposed approach
to evaluate trial designs is robust to the specific form of the baseline hazard
function. The benefits of this approach are demonstrated through the redesign
of a recent clinical trial to evaluate whether a third dose of a vaccine
provides improved protection against gastroenteritis in Australian Indigenous
infants.",http://arxiv.org/abs/2303.00901v1,"James M McGree, Antony M Overstall, Mark Jones, Robert K Mahar"
37,Comparing Biomarkers as Trial Level General Surrogates,"An intermediate response measure that accurately predicts efficacy in a new
setting can reduce trial cost and time to product licensure. In this paper, we
define a trial level general surrogate as a trial level intermediate response
that accurately predicts trial level clinical responses. Methods for evaluating
trial level general surrogates have been developed previously. Many methods in
the literature use trial level intermediate responses for prediction. However,
all existing methods focus on surrogate evaluation and prediction in new
settings, rather than comparison of candidate trial level surrogates, and few
formalize the use of cross validation to quantify the expected prediction
error. Our proposed method uses Bayesian non-parametric modeling and
cross-validation to estimate the absolute prediction error for use in
evaluating and comparing candidate trial level general surrogates. Simulations
show that our method performs well across a variety of scenarios. We use our
method to evaluate and to compare candidate trial level general surrogates in
several multi-national trials of a pentavalent rotavirus vaccine. We identify
two immune measures that have potential value as trial level general surrogates
and use the measures to predict efficacy in a trial with no clinical outcomes
measured.",http://arxiv.org/abs/1507.01825v1,"Erin E Gabriel, Michael J Daniels, M Elizabeth Halloran"
38,Dynamic borrowing from historical controls via the synthetic prior with covariates in randomized clinical trials,"Motivated by a rheumatoid arthritis clinical trial, we propose a new Bayesian
method called SPx, standing for synthetic prior with covariates, to borrow
information from historical trials to reduce the control group size in a new
trial. The method involves a novel use of Bayesian model averaging to balance
between multiple possible relationships between the historical and new trial
data, allowing the historical data to be dynamically trusted or discounted as
appropriate. We require only trial-level summary statistics, which are
available more often than patient-level data. Through simulations and an
application to the rheumatoid arthritis trial we show that SPx can
substantially reduce the control group size while maintaining Frequentist
properties.",http://arxiv.org/abs/2410.07242v1,"Daniel E Schwartz, Yuan Ji, Li Wang"
39,Adversity Index for Clinical Trials: An Inclusive Approach for Analysis of Safety Data,"This paper proposes a new method for analysis of adverse event data in
clinical trials. The method is illustrated by application to data on 4 phase
III clinical trials, two on breast cancer and two on diabetes mellitus.",http://arxiv.org/abs/1806.00204v1,"Sharayu Paranjpe, Anil Gore"
40,From RAGs to riches: Utilizing large language models to write documents for clinical trials,"This manuscript has now been published: - Link to article on journal website:
https://journals.sagepub.com/doi/10.1177/17407745251320806 - Pubmed link:
https://pubmed.ncbi.nlm.nih.gov/40013826/",http://arxiv.org/abs/2402.16406v2,"Nigel Markey, Ilyass ElMansouri, Gaetan Rensonnet, Casper van Langen, Christoph Meier"
41,MiStImm: a simulation tool to compare classical nonsef-centered immune models with a novel self-centered model,"Our main purpose is to compare classical nonself-centered, two-signal
theoretical models of the adaptive immune system with a novel, self-centered,
one-signal model developed by our research group. Our model hypothesizes that
the immune system of a fetus is capable learning the limited set of self
antigens but unable to prepare itself for the unlimited variety of nonself
antigens. We have built a computational model that simulates the development of
the adaptive immune system. For simplicity, we concentrated on humoral immunity
and its major components: T cells, B cells, antibodies, interleukins,
non-immune self cells, and foreign antigens. Our model is a microscopic one,
similar to the interacting particle models of statistical physics and
agent-based models in immunology. Furthermore, our model is stochastic: events
are considered random and modeled by a continuous time, finite state Markov
process, that is, they are controlled by finitely many independent exponential
clocks.
  We investigate under what conditions can an immune memory be created that
results in a more effective immune response to a repeated infection. The
simulations show that our self-centered model is realistic. Moreover, in case
of a primary adaptive immune reaction, it can destroy infections more
efficiently than a classical nonself-centered model.
  Predictions of our theoretical model were clinically supported by
autoimmune-related adverse events in high-dose immune checkpoint inhibitor
immunotherapy trials and also by safe and successful low-dose immune checkpoint
inhibitor combination treatment of heavily pretreated stage IV cancer patients
who had exhausted all conventional treatments.
  The MiStImm simulation tool and source codes are available at the address
https://github.com/kerepesi/MiStImm.",http://arxiv.org/abs/1507.00950v2,"Tams Szabados, Csaba Kerepesi, Tibor Bakcs"
42,Relational Dynamics in Perception: Impacts on trial-to-trial variation,"We show that trial-to-trial variability in sensory detection of a weak visual
stimulus is dramatically diminished when rather than presenting a fixed
stimulus contrast, fluctuations in a subject's judgment are matched by
fluctuations in stimulus contrast. This attenuation of fluctuations does not
involve a change in the subject's psychometric function. The result is
consistent with the interpretation of trial-to-trial variability in this
sensory detection task being a high-level meta-cognitive control process that
explores for something that our brains are so used to: subject-object
relational dynamics.",http://arxiv.org/abs/1102.1384v1,"Shimon Marom, Avner Wallach"
43,PyTrial: Machine Learning Software and Benchmark for Clinical Trial Applications,"Clinical trials are conducted to test the effectiveness and safety of
potential drugs in humans for regulatory approval. Machine learning (ML) has
recently emerged as a new tool to assist in clinical trials. Despite this
progress, there have been few efforts to document and benchmark ML4Trial
algorithms available to the ML research community. Additionally, the
accessibility to clinical trial-related datasets is limited, and there is a
lack of well-defined clinical tasks to facilitate the development of new
algorithms.
  To fill this gap, we have developed PyTrial that provides benchmarks and
open-source implementations of a series of ML algorithms for clinical trial
design and operations. In this paper, we thoroughly investigate 34 ML
algorithms for clinical trials across 6 different tasks, including patient
outcome prediction, trial site selection, trial outcome prediction,
patient-trial matching, trial similarity search, and synthetic data generation.
We have also collected and prepared 23 ML-ready datasets as well as their
working examples in Jupyter Notebooks for quick implementation and testing.
  PyTrial defines each task through a simple four-step process: data loading,
model specification, model training, and model evaluation, all achievable with
just a few lines of code. Furthermore, our modular API architecture empowers
practitioners to expand the framework to incorporate new algorithms and tasks
effortlessly. The code is available at https://github.com/RyanWangZf/PyTrial.",http://arxiv.org/abs/2306.04018v2,"Zifeng Wang, Brandon Theodorou, Tianfan Fu, Cao Xiao, Jimeng Sun"
44,Advancing clinical trial outcomes using deep learning and predictive modelling: bridging precision medicine and patient-centered care,"The integration of artificial intelligence [AI] into clinical trials has
revolutionized the process of drug development and personalized medicine. Among
these advancements, deep learning and predictive modelling have emerged as
transformative tools for optimizing clinical trial design, patient recruitment,
and real-time monitoring. This study explores the application of deep learning
techniques, such as convolutional neural networks [CNNs] and transformerbased
models, to stratify patients, forecast adverse events, and personalize
treatment plans. Furthermore, predictive modelling approaches, including
survival analysis and time-series forecasting, are employed to predict trial
outcomes, enhancing efficiency and reducing trial failure rates. To address
challenges in analysing unstructured clinical data, such as patient notes and
trial protocols, natural language processing [NLP] techniques are utilized for
extracting actionable insights. A custom dataset comprising structured patient
demographics, genomic data, and unstructured text is curated for training and
validating these models. Key metrics, including precision, recall, and F1
scores, are used to evaluate model performance, while trade-offs between
accuracy and computational efficiency are examined to identify the optimal
model for clinical deployment. This research underscores the potential of
AI-driven methods to streamline clinical trial workflows, improve
patient-centric outcomes, and reduce costs associated with trial
inefficiencies. The findings provide a robust framework for integrating
predictive analytics into precision medicine, paving the way for more adaptive
and efficient clinical trials. By bridging the gap between technological
innovation and real-world applications, this study contributes to advancing the
role of AI in healthcare, particularly in fostering personalized care and
improving overall trial success rates.",http://arxiv.org/abs/2412.07050v1,"Sydney Anuyah, Mallika K Singh, Hope Nyavor"
45,TrialBench: Multi-Modal Artificial Intelligence-Ready Clinical Trial Datasets,"Clinical trials are pivotal for developing new medical treatments, yet they
typically pose some risks such as patient mortality, adverse events, and
enrollment failure that waste immense efforts spanning over a decade. Applying
artificial intelligence (AI) to forecast or simulate key events in clinical
trials holds great potential for providing insights to guide trial designs.
However, complex data collection and question definition requiring medical
expertise and a deep understanding of trial designs have hindered the
involvement of AI thus far. This paper tackles these challenges by presenting a
comprehensive suite of meticulously curated AIready datasets covering
multi-modal data (e.g., drug molecule, disease code, text,
categorical/numerical features) and 8 crucial prediction challenges in clinical
trial design, encompassing prediction of trial duration, patient dropout rate,
serious adverse event, mortality rate, trial approval outcome, trial failure
reason, drug dose finding, design of eligibility criteria. Furthermore, we
provide basic validation methods for each task to ensure the datasets'
usability and reliability. We anticipate that the availability of such
open-access datasets will catalyze the development of advanced AI approaches
for clinical trial design, ultimately advancing clinical trial research and
accelerating medical solution development. The curated dataset, metrics, and
basic models are publicly available at
https://github.com/ML2Health/ML2ClinicalTrials/tree/main/AI4Trial.",http://arxiv.org/abs/2407.00631v2,"Jintai Chen, Yaojun Hu, Yue Wang, Yingzhou Lu, Xu Cao, Miao Lin, Hongxia Xu, Jian Wu, Cao Xiao, Jimeng Sun, Lucas Glass, Kexin Huang, Marinka Zitnik, Tianfan Fu"
46,AutoTrial: Prompting Language Models for Clinical Trial Design,"Clinical trials are critical for drug development. Constructing the
appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for
patient recruitment) is essential for the trial's success. Proper design of
clinical trial protocols should consider similar precedent trials and their
eligibility criteria to ensure sufficient patient coverage. In this paper, we
present a method named AutoTrial to aid the design of clinical eligibility
criteria using language models. It allows (1) controllable generation under
instructions via a hybrid of discrete and neural prompting, (2) scalable
knowledge incorporation via in-context learning, and (3) explicit reasoning
chains to provide rationales for understanding the outputs. Experiments on over
70K clinical trials verify that AutoTrial generates high-quality criteria texts
that are fluent and coherent and with high accuracy in capturing the relevant
clinical concepts to the target trial. It is noteworthy that our method, with a
much smaller parameter size, gains around 60% winning rate against the GPT-3.5
baselines via human evaluations.",http://arxiv.org/abs/2305.11366v2,"Zifeng Wang, Cao Xiao, Jimeng Sun"
47,Can artificial intelligence predict clinical trial outcomes?,"The increasing complexity and cost of clinical trials, particularly in the
context of oncology and advanced therapies, pose significant challenges for
drug development. This study evaluates the predictive capabilities of large
language models (LLMs) such as GPT-3.5, GPT-4, and HINT in determining clinical
trial outcomes. By leveraging a curated dataset of trials from
ClinicalTrials.gov, we compare the models' performance using metrics including
balanced accuracy, specificity, recall, and Matthews Correlation Coefficient
(MCC). Results indicate that GPT-4o demonstrates robust performance in early
trial phases, achieving high recall but facing limitations in specificity.
Conversely, the HINT model excels in recognizing negative outcomes,
particularly in later trial phases, offering a balanced approach across diverse
endpoints. Oncology trials, characterized by high complexity, remain
challenging for all models. Additionally, trial duration and disease categories
influence predictive performance, with longer durations and complex diseases
such as neoplasms reducing accuracy. This study highlights the complementary
strengths of LLMs and HINT, providing insights into optimizing predictive tools
for clinical trial design and risk management. Future advancements in LLMs are
essential to address current gaps in handling negative outcomes and complex
domains.",http://arxiv.org/abs/2411.17595v1,"Shuyi Jin, Lu Chen, Hongru Ding, Meijie Wang, Lun Yu"
48,Three mechanistically different variability and noise sources in the trial-to-trial fluctuations of responses to brain stimulation,"Motor-evoked potentials (MEPs) are among the few directly observable
responses to external brain stimulation and serve a variety of applications,
often in the form of input-output (IO) curves. Previous statistical models with
two variability sources inherently consider the small MEPs at the low-side
plateau as part of the neural recruitment properties. However, recent studies
demonstrated that small MEP responses under resting conditions are contaminated
and over-shadowed by background noise of mostly technical quality, e.g., caused
by the amplifier, and suggested that the neural recruitment curve should
continue below this noise level. This work intends to separate physiological
variability from background noise and improve the description of recruitment
behaviour. We developed a triple-variability-source model around a logarithmic
logistic function without a lower plateau and incorporated an additional source
for background noise. Compared to models with two or fewer variability sources,
our approach better described IO characteristics, evidenced by lower Bayesian
Information Criterion scores across all subjects and pulse shapes. The model
independently extracted hidden variability information across the stimulated
neural system and isolated it from background noise, which led to an accurate
estimation of the IO curve parameters. This new model offers a robust tool to
analyse brain stimulation IO curves in clinical and experimental neuroscience
and reduces the risk of spurious results from inappropriate statistical
methods. The presented model together with the corresponding calibration method
provides a more accurate representation of MEP responses and variability
sources, advances our understanding of cortical excitability, and may improve
the assessment of neuromodulation effects.",http://arxiv.org/abs/2412.16997v1,"Ke Ma, Siwei Liu, Mengjie Qin, Stefan Goetz"
49,Predicting Clinical Trial Results by Implicit Evidence Integration,"Clinical trials provide essential guidance for practicing Evidence-Based
Medicine, though often accompanying with unendurable costs and risks. To
optimize the design of clinical trials, we introduce a novel Clinical Trial
Result Prediction (CTRP) task. In the CTRP framework, a model takes a
PICO-formatted clinical trial proposal with its background as input and
predicts the result, i.e. how the Intervention group compares with the
Comparison group in terms of the measured Outcome in the studied Population.
While structured clinical evidence is prohibitively expensive for manual
collection, we exploit large-scale unstructured sentences from medical
literature that implicitly contain PICOs and results as evidence. Specifically,
we pre-train a model to predict the disentangled results from such implicit
evidence and fine-tune the model with limited data on the downstream datasets.
Experiments on the benchmark Evidence Integration dataset show that the
proposed model outperforms the baselines by large margins, e.g., with a 10.7%
relative gain over BioBERT in macro-F1. Moreover, the performance improvement
is also validated on another dataset composed of clinical trials related to
COVID-19.",http://arxiv.org/abs/2010.05639v1,"Qiao Jin, Chuanqi Tan, Mosha Chen, Xiaozhong Liu, Songfang Huang"
50,The Leaf Clinical Trials Corpus: a new resource for query generation from clinical trial eligibility criteria,"Identifying cohorts of patients based on eligibility criteria such as medical
conditions, procedures, and medication use is critical to recruitment for
clinical trials. Such criteria are often most naturally described in free-text,
using language familiar to clinicians and researchers. In order to identify
potential participants at scale, these criteria must first be translated into
queries on clinical databases, which can be labor-intensive and error-prone.
Natural language processing (NLP) methods offer a potential means of such
conversion into database queries automatically. However they must first be
trained and evaluated using corpora which capture clinical trials criteria in
sufficient detail. In this paper, we introduce the Leaf Clinical Trials (LCT)
corpus, a human-annotated corpus of over 1,000 clinical trial eligibility
criteria descriptions using highly granular structured labels capturing a range
of biomedical phenomena. We provide details of our schema, annotation process,
corpus quality, and statistics. Additionally, we present baseline information
extraction results on this corpus as benchmarks for future work.",http://arxiv.org/abs/2207.13757v1,"Nicholas J Dobbins, Tony Mullen, Ozlem Uzuner, Meliha Yetisgen"
51,Towards quantum computing for clinical trial design and optimization: A perspective on new opportunities and challenges,"Clinical trials are pivotal in the drug discovery process to determine the
safety and efficacy of a drug candidate. The high failure rates of these trials
are attributed to deficiencies in clinical model development and protocol
design. Improvements in the clinical drug design process could therefore yield
significant benefits for all stakeholders involved. This paper examines the
current challenges faced in clinical trial design and optimization, reviews
established classical computational approaches, and introduces quantum
algorithms aimed at enhancing these processes. Specifically, the focus is on
three critical aspects: clinical trial simulations, site selection, and cohort
identification. This study aims to provide a comprehensive framework that
leverages quantum computing to innovate and refine the efficiency and
effectiveness of clinical trials.",http://arxiv.org/abs/2404.13113v1,"Hakan Doga, M Emre Sahin, Joao BettencourtSilva, Anh Pham, Eunyoung Kim, Alan Andress, Sudhir Saxena, Aritra Bose, Laxmi Parida, Jan Lukas Robertus, Hideaki Kawaguchi, Radwa Soliman, Daniel Blankenberg"
52,Adaptive Cohort Size Determination Method for Bayesian Optimal Interval Phase I/II Design to Shorten Clinical Trial Duration,"Recently, the strategy for dose optimization in oncology has shifted to
conduct Phase 2 randomized controlled trials with multiple doses. Optimal
biologic dose selection from Phase 1 trial data to determine candidate doses
for Phase 2 trials has been gaining attention. This study proposes a novel
adaptive cohort size determination method for optimal biologic dose-finding to
accelerate trials. The cohort size expansion is determined adaptively depending
on the toxicity and efficacy data of the ongoing trial. In a simulation, the
proposed method shortened the trial duration and maintained accuracy. The trial
duration was reduced by an average of approximately 20% compared with the
non-adaptive cohort size determination design. The cohort size expansion is
demonstrated using a simple example.",http://arxiv.org/abs/2302.06088v1,Masahiro Kojima
53,ClinicalAgent: Clinical Trial Multi-Agent System with Large Language Model-based Reasoning,"Large Language Models (LLMs) and multi-agent systems have shown impressive
capabilities in natural language tasks but face challenges in clinical trial
applications, primarily due to limited access to external knowledge.
Recognizing the potential of advanced clinical trial tools that aggregate and
predict based on the latest medical data, we propose an integrated solution to
enhance their accessibility and utility. We introduce Clinical Agent System
(ClinicalAgent), a clinical multi-agent system designed for clinical trial
tasks, leveraging GPT-4, multi-agent architectures, LEAST-TO-MOST, and ReAct
reasoning technology. This integration not only boosts LLM performance in
clinical contexts but also introduces novel functionalities. The proposed
method achieves competitive predictive performance in clinical trial outcome
prediction (0.7908 PR-AUC), obtaining a 0.3326 improvement over the standard
prompt Method. Publicly available code can be found at
https://anonymous.4open.science/r/ClinicalAgent-6671.",http://arxiv.org/abs/2404.14777v2,"Ling Yue, Sixue Xing, Jintai Chen, Tianfan Fu"
54,Predictive Directions for Individualized Treatment Selection in Clinical Trials,"In many clinical trials, individuals in different subgroups have experience
differential treatment effects. This leads to individualized differences in
treatment benefit. In this article, we introduce the general concept of
predictive directions, which are risk scores motivated by potential outcomes
considerations. These techniques borrow heavily from sufficient dimension
reduction (SDR) and causal inference methodology. Under some conditions, one
can use existing methods from the SDR literature to estimate the directions
assuming an idealized complete data structure, which subsequently yields an
obvious extension to clinical trial datasets. In addition, we generalize the
direction idea to a nonlinear setting that exploits support vector machines.
The methodology is illustrated with application to a series of colorectal
cancer clinical trials.",http://arxiv.org/abs/1807.03375v1,"Debashis Ghosh, Youngjoo Cho"
55,Clinical Trials Ontology Engineering with Large Language Models,"Managing clinical trial information is currently a significant challenge for
the medical industry, as traditional methods are both time-consuming and
costly. This paper proposes a simple yet effective methodology to extract and
integrate clinical trial data in a cost-effective and time-efficient manner.
Allowing the medical industry to stay up-to-date with medical developments.
Comparing time, cost, and quality of the ontologies created by humans, GPT3.5,
GPT4, and Llama3 (8b & 70b). Findings suggest that large language models (LLM)
are a viable option to automate this process both from a cost and time
perspective. This study underscores significant implications for medical
research where real-time data integration from clinical trials could become the
norm.",http://arxiv.org/abs/2412.14387v1,Berkan akr
56,Forum on immune digital twins: a meeting report,"Medical digital twins are computational models of human biology relevant to a
given medical condition, which can be tailored to an individual patient,
thereby predicting the course of disease and individualized treatments, an
important goal of personalized medicine. The immune system, which has a central
role in many diseases, is highly heterogeneous between individuals, and thus
poses a major challenge for this technology. If medical digital twins are to
faithfully capture the characteristics of a patient's immune system, we need to
answer many questions, such as: What do we need to know about the immune system
to build mathematical models that reflect features of an individual? What data
do we need to collect across the different scales of immune system action? What
are the right modeling paradigms to properly capture immune system complexity?
In February 2023, an international group of experts convened in Lake Nona, FL
for two days to discuss these and other questions related to digital twins of
the immune system. The group consisted of clinicians, immunologists,
biologists, and mathematical modelers, representative of the interdisciplinary
nature of medical digital twin development. A video recording of the entire
event is available. This paper presents a synopsis of the discussions, brief
descriptions of ongoing digital twin projects at different stages of progress.
It also proposes a 5-year action plan for further developing this technology.
The main recommendations are to identify and pursue a small number of promising
use cases, to develop stimulation-specific assays of immune function in a
clinical setting, and to develop a database of existing computational immune
models, as well as advanced modeling technology and infrastructure.",http://arxiv.org/abs/2310.18374v1,"Reinhard Laubenbacher, Fred Adler, Gary An, Filippo Castiglione, Stephen Eubank, Luis L Fonseca, James Glazier, Tomas Helikar, Marti JettTilton, Denise Kirschner, Paul Macklin, Borna Mehrad, Beth Moore, Virginia Pasour, Ilya Shmulevich, Amber Smith, Isabel Voigt, Thomas E Yankeelov, Tjalf Ziemssen"
57,Combining Evidence from Clinical Trials in Conditional or Accelerated Approval,"Conditional (European Medicines Agency) or accelerated (U.S. Food and Drug
Administration) approval of drugs allow earlier access to promising new
treatments that address unmet medical needs. Certain post-marketing
requirements must typically be met in order to obtain full approval, such as
conducting a new post-market clinical trial. We study the applicability of the
recently developed harmonic mean Chi-squared test to this conditional or
accelerated approval framework. The proposed approach can be used both to
support the design of the post-market trial and the analysis of the combined
evidence provided by both trials. Other methods considered are the two-trials
rule, Fisher's criterion and Stouffer's method. In contrast to some of the
traditional methods, the harmonic mean Chi-squared test always requires a
post-market clinical trial. If the p-value from the pre-market clinical trial
is << 0.025, a smaller sample size for the post-market clinical trial is needed
than with the two-trials rule. For illustration, we apply the harmonic mean
Chi-squared test to a drug which received conditional (and later full) market
licensing by the EMA. A simulation study is conducted to study the operating
characteristics of the harmonic mean Chi-squared test and two-trials rule in
more detail. We finally investigate the applicability of these two methods to
compute the power at interim of an ongoing post-market trial. These results are
expected to aid in the design and assessment of the required post-market
studies in terms of the level of evidence required for full approval.",http://arxiv.org/abs/2112.11898v2,"Manja Deforth, Charlotte Micheloud, Kit C Roes, Leonhard Held"
58,Multi-disciplinary fairness considerations in machine learning for clinical trials,"While interest in the application of machine learning to improve healthcare
has grown tremendously in recent years, a number of barriers prevent deployment
in medical practice. A notable concern is the potential to exacerbate
entrenched biases and existing health disparities in society. The area of
fairness in machine learning seeks to address these issues of equity; however,
appropriate approaches are context-dependent, necessitating domain-specific
consideration. We focus on clinical trials, i.e., research studies conducted on
humans to evaluate medical treatments. Clinical trials are a relatively
under-explored application in machine learning for healthcare, in part due to
complex ethical, legal, and regulatory requirements and high costs. Our aim is
to provide a multi-disciplinary assessment of how fairness for machine learning
fits into the context of clinical trials research and practice. We start by
reviewing the current ethical considerations and guidelines for clinical trials
and examine their relationship with common definitions of fairness in machine
learning. We examine potential sources of unfairness in clinical trials,
providing concrete examples, and discuss the role machine learning might play
in either mitigating potential biases or exacerbating them when applied without
care. Particular focus is given to adaptive clinical trials, which may employ
machine learning. Finally, we highlight concepts that require further
investigation and development, and emphasize new approaches to fairness that
may be relevant to the design of clinical trials.",http://arxiv.org/abs/2205.08875v1,"Isabel Chien, Nina Deliu, Richard E Turner, Adrian Weller, Sofia S Villar, Niki Kilbertus"
59,Model-based Pre-clinical Trials for Medical Devices Using Statistical Model Checking,"Clinical trials are considered as the golden standard for medical device
validation. However, many sacrifices have to be made during the design and
conduction of the trials due to cost considerations and partial information,
which may compromise the significance of the trial results. In this paper, we
proposed a model-based pre-clinical trial framework using statistical model
checking. Physiological models represent disease mechanism, which enable
automated adjudication of simulation results. Sampling of the patient
parameters and hypothesis testing are performed by statistical model checker.
The framework enables a broader range of hypothesis to be tested with
guaranteed statistical significance, which are useful complements to the
clinical trials. We demonstrated our framework with a pre-clinical trial on
implantable cardioverter defibrillators.",http://arxiv.org/abs/2106.11917v1,"Haochen Yang, Jicheng Gu, Zhihao Jiang"
60,Next generation clinical trials: Seamless designs and master protocols,"Background: Drug development is often inefficient, costly and lengthy, yet it
is essential for evaluating the safety and efficacy of new interventions.
Compared with other disease areas, this is particularly true for Phase II / III
cancer clinical trials where high attrition rates and reduced regulatory
approvals are being seen. In response to these challenges, seamless clinical
trials and master protocols have emerged to streamline the drug development
process. Methods: Seamless clinical trials, characterized by their ability to
transition seamlessly from one phase to another, can lead to accelerating the
development of promising therapies while Master protocols provide a framework
for investigating multiple treatment options and patient subgroups within a
single trial. Results: We discuss the advantages of these methods through real
trial examples and the principals that lead to their success while also
acknowledging the associated regulatory considerations and challenges.
Conclusion: Seamless designs and Master protocols have the potential to improve
confirmatory clinical trials. In the disease area of cancer, this ultimately
means that patients can receive life-saving treatments sooner.",http://arxiv.org/abs/2405.06353v1,"Abigail Burdon, Thomas Jaki, Xijin Chen, Pavel Mozgunov, Haiyan Zheng, Richard Baird"
61,HINT: Hierarchical Interaction Network for Trial Outcome Prediction Leveraging Web Data,"Clinical trials are crucial for drug development but are time consuming,
expensive, and often burdensome on patients. More importantly, clinical trials
face uncertain outcomes due to issues with efficacy, safety, or problems with
patient recruitment. If we were better at predicting the results of clinical
trials, we could avoid having to run trials that will inevitably fail more
resources could be devoted to trials that are likely to succeed. In this paper,
we propose Hierarchical INteraction Network (HINT) for more general, clinical
trial outcome predictions for all diseases based on a comprehensive and diverse
set of web data including molecule information of the drugs, target disease
information, trial protocol and biomedical knowledge. HINT first encode these
multi-modal data into latent embeddings, where an imputation module is designed
to handle missing data. Next, these embeddings will be fed into the knowledge
embedding module to generate knowledge embeddings that are pretrained using
external knowledge on pharmaco-kinetic properties and trial risk from the web.
Then the interaction graph module will connect all the embedding via domain
knowledge to fully capture various trial components and their complex relations
as well as their influences on trial outcomes. Finally, HINT learns a dynamic
attentive graph neural network to predict trial outcome. Comprehensive
experimental results show that HINT achieves strong predictive performance,
obtaining 0.772, 0.607, 0.623, 0.703 on PR-AUC for Phase I, II, III, and
indication outcome prediction, respectively. It also consistently outperforms
the best baseline method by up to 12.4\% on PR-AUC.",http://arxiv.org/abs/2102.04252v3,"Tianfan Fu, Kexin Huang, Cao Xiao, Lucas M Glass, Jimeng Sun"
62,Text Classification of Cancer Clinical Trial Eligibility Criteria,"Automatic identification of clinical trials for which a patient is eligible
is complicated by the fact that trial eligibility is stated in natural
language. A potential solution to this problem is to employ text classification
methods for common types of eligibility criteria. In this study, we focus on
seven common exclusion criteria in cancer trials: prior malignancy, human
immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness,
drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase
III cancer trials with these exclusions annotated at the trial level. We
experiment with common transformer models as well as a new pre-trained clinical
trial BERT model. Our results demonstrate the feasibility of automatically
classifying common exclusion criteria. Additionally, we demonstrate the value
of a pre-trained language model specifically for clinical trials, which yields
the highest average performance across all criteria.",http://arxiv.org/abs/2309.07812v2,"Yumeng Yang, Soumya Jayaraj, Ethan B Ludmir, Kirk Roberts"
63,Clinical Trial Active Learning,"This paper presents a novel approach to active learning that takes into
account the non-independent and identically distributed (non-i.i.d.) structure
of a clinical trial setting. There exists two types of clinical trials:
retrospective and prospective. Retrospective clinical trials analyze data after
treatment has been performed; prospective clinical trials collect data as
treatment is ongoing. Typically, active learning approaches assume the dataset
is i.i.d. when selecting training samples; however, in the case of clinical
trials, treatment results in a dependency between the data collected at the
current and past visits. Thus, we propose prospective active learning to
overcome the limitations present in traditional active learning methods and
apply it to disease detection in optical coherence tomography (OCT) images,
where we condition on the time an image was collected to enforce the i.i.d.
assumption. We compare our proposed method to the traditional active learning
paradigm, which we refer to as retrospective in nature. We demonstrate that
prospective active learning outperforms retrospective active learning in two
different types of test settings.",http://arxiv.org/abs/2307.11209v1,"Zoe Fowler, Kiran Kokilepersaud, Mohit Prabhushankar, Ghassan AlRegib"
64,Improving clinical trial interpretation with ACCEPT analyses,"Effective decision making from randomised controlled clinical trials relies
on robust interpretation of the numerical results. However, the language we use
to describe clinical trials can cause confusion both in trial design and in
comparing results across trials. ACceptability Curve Estimation using
Probability Above Threshold (ACCEPT) aids comparison between trials (even where
of different designs) by harmonising reporting of results, acknowledging
different interpretations of the results may be valid in different situations,
and moving the focus from comparison to a pre-specified value to interpretation
of the trial data. ACCEPT can be applied to historical trials or incorporated
into statistical analysis plans for future analyses. An online tool enables
ACCEPT on up to three trials simultaneously.",http://arxiv.org/abs/2203.11164v2,"Michelle N Clements, Ian R White, Andrew J Copas, Victoria Cornelius, Suzie Cro, David T Dunn, Matteo Quartagno, Rebecca M Turner, Conor D Tweed, A Sarah Walker"
65,Combining Real-World and Randomized Control Trial Data Using Data-Adaptive Weighting via the On-Trial Score,"Clinical trials with a hybrid control arm (a control arm constructed from a
combination of randomized patients and real-world data on patients receiving
usual care in standard clinical practice) have the potential to decrease the
cost of randomized trials while increasing the proportion of trial patients
given access to novel therapeutics. However, due to stringent trial inclusion
criteria and differences in care and data quality between trials and community
practice, trial patients may have systematically different outcomes compared to
their real-world counterparts. We propose a new method for analyses of trials
with a hybrid control arm that efficiently controls bias and type I error.
Under our proposed approach, selected real-world patients are weighted by a
function of the ""on-trial score,"" which reflects their similarity to trial
patients. In contrast to previously developed hybrid control designs that
assign the same weight to all real-world patients, our approach upweights of
real-world patients who more closely resemble randomized control patients while
dissimilar patients are discounted. Estimates of the treatment effect are
obtained via Cox proportional hazards models. We compare our approach to
existing approaches via simulations and apply these methods to a study using
electronic health record data. Our proposed method is able to control type I
error, minimize bias, and decrease variance when compared to using only trial
data in nearly all scenarios examined. Therefore, our new approach can be used
when conducting clinical trials by augmenting the standard-of-care arm with
weighted patients from the EHR to increase power without inducing bias.",http://arxiv.org/abs/2108.08756v1,"Joanna Harton, Brian Segal, Ronac Mamtani, Nandita Mitra, Rebecca Hubbard"
66,Equipoise calibration of clinical trial design,"Statistical methods for clinical trial design are currently unable to rely on
a sufficiently precise and general definition of what is an adequately powered
study. Operationally, this definition is needed to ensure an alignment by
design between statistical significance and clinical interpretability. To
address this gap, this paper shows how to calibrate randomised trial designs to
establishing strong clinical equipoise imbalance. Among several equipoise
models, the least informed population distribution of the pre-trial odds of the
design hypotheses is recommended here as the most practical calibrator. Under
this model, primary analysis outcomes of common phase 3 superiority designs are
shown to provide at least 90% evidence of equipoise imbalance. Designs carrying
95% power at 5% false positive rate are shown to demonstrate even stronger
equipoise imbalance, providing an operational definition of a robustly powered
study. Equipoise calibration is then applied to design of clinical development
plans comprising randomised phase 2 and phase 3 studies. Development plans
using oncology clinical endpoints are shown to provide strong equipoise
imbalance when positive outcomes are observed in phase 2 and in phase 3.
Establishing equipoise imbalance on a statistical basis when a positive phase 2
is not confirmed in phase 3 is shown to require large sample sizes unlikely to
be associated with clinically meaningful effect sizes. Equipoise calibration is
proposed as an effective clinical trial methodology ensuring that the
statistical properties of clinical trial outcomes are clinically interpretable.
Strong equipoise imbalance is achieved for designs carrying 95% power at 5%
false positive rate, regardless of whether the primary outcome is positive or
negative. Sponsors should consider raising power of their designs beyond
current practice to achieve more conclusive results.",http://arxiv.org/abs/2501.03009v1,Fabio Rigat
67,CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions,"A clinical trial is a study that evaluates new biomedical interventions. To
design new trials, researchers draw inspiration from those current and
completed. In 2022, there were on average more than 100 clinical trials
submitted to ClinicalTrials.gov every day, with each trial having a mean of
approximately 1500 words [1]. This makes it nearly impossible to keep up to
date. To mitigate this issue, we have created a batch clinical trial summarizer
called CliniDigest using GPT-3.5. CliniDigest is, to our knowledge, the first
tool able to provide real-time, truthful, and comprehensive summaries of
clinical trials. CliniDigest can reduce up to 85 clinical trial descriptions
(approximately 10,500 words) into a concise 200-word summary with references
and limited hallucinations. We have tested CliniDigest on its ability to
summarize 457 trials divided across 27 medical subdomains. For each field,
CliniDigest generates summaries of $\mu=153,\ \sigma=69 $ words, each of which
utilizes $\mu=54\%,\ \sigma=30\% $ of the sources. A more comprehensive
evaluation is planned and outlined in this paper.",http://arxiv.org/abs/2307.14522v2,"Renee D White, Tristan Peng, Pann Sripitak, Alexander Rosenberg Johansen, Michael Snyder"
68,PRISM: Patient Records Interpretation for Semantic Clinical Trial Matching using Large Language Models,"Clinical trial matching is the task of identifying trials for which patients
may be potentially eligible. Typically, this task is labor-intensive and
requires detailed verification of patient electronic health records (EHRs)
against the stringent inclusion and exclusion criteria of clinical trials. This
process is manual, time-intensive, and challenging to scale up, resulting in
many patients missing out on potential therapeutic options. Recent advancements
in Large Language Models (LLMs) have made automating patient-trial matching
possible, as shown in multiple concurrent research studies. However, the
current approaches are confined to constrained, often synthetic datasets that
do not adequately mirror the complexities encountered in real-world medical
data. In this study, we present the first, end-to-end large-scale empirical
evaluation of clinical trial matching using real-world EHRs. Our study
showcases the capability of LLMs to accurately match patients with appropriate
clinical trials. We perform experiments with proprietary LLMs, including GPT-4
and GPT-3.5, as well as our custom fine-tuned model called OncoLLM and show
that OncoLLM, despite its significantly smaller size, not only outperforms
GPT-3.5 but also matches the performance of qualified medical doctors. All
experiments were carried out on real-world EHRs that include clinical notes and
available clinical trials from a single cancer center in the United States.",http://arxiv.org/abs/2404.15549v2,"Shashi Kant Gupta, Aditya Basu, Mauro Nievas, Jerrin Thomas, Nathan Wolfrath, Adhitya Ramamurthi, Bradley Taylor, Anai N Kothari, Regina Schwind, Therica M Miller, Sorena NadafRahrov, Yanshan Wang, Hrituraj Singh"
69,TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model,"Clinical trials are indispensable for medical research and the development of
new treatments. However, clinical trials often involve thousands of
participants and can span several years to complete, with a high probability of
failure during the process. Recently, there has been a burgeoning interest in
virtual clinical trials, which simulate real-world scenarios and hold the
potential to significantly enhance patient safety, expedite development, reduce
costs, and contribute to the broader scientific knowledge in healthcare.
Existing research often focuses on leveraging electronic health records (EHRs)
to support clinical trial outcome prediction. Yet, trained with limited
clinical trial outcome data, existing approaches frequently struggle to perform
accurate predictions. Some research has attempted to generate EHRs to augment
model development but has fallen short in personalizing the generation for
individual patient profiles. Recently, the emergence of large language models
has illuminated new possibilities, as their embedded comprehensive clinical
knowledge has proven beneficial in addressing medical issues. In this paper, we
propose a large language model-based digital twin creation approach, called
TWIN-GPT. TWIN-GPT can establish cross-dataset associations of medical
information given limited data, generating unique personalized digital twins
for different patients, thereby preserving individual patient characteristics.
Comprehensive experiments show that using digital twins created by TWIN-GPT can
boost the clinical trial outcome prediction, exceeding various previous
prediction approaches.",http://arxiv.org/abs/2404.01273v2,"Yue Wang, Tianfan Fu, Yinlong Xu, Zihan Ma, Hongxia Xu, Yingzhou Lu, Bang Du, Honghao Gao, Jian Wu"
70,Dynamic causal modelling of immune heterogeneity,"An interesting inference drawn by some Covid-19 epidemiological models is
that there exists a proportion of the population who are not susceptible to
infection -- even at the start of the current pandemic. This paper introduces a
model of the immune response to a virus. This is based upon the same sort of
mean-field dynamics as used in epidemiology. However, in place of the location,
clinical status, and other attributes of people in an epidemiological model, we
consider the state of a virus, B and T-lymphocytes, and the antibodies they
generate. Our aim is to formalise some key hypotheses as to the mechanism of
resistance. We present a series of simple simulations illustrating changes to
the dynamics of the immune response under these hypotheses. These include
attenuated viral cell entry, pre-existing cross-reactive humoral
(antibody-mediated) immunity, and enhanced T-cell dependent immunity. Finally,
we illustrate the potential application of this sort of model by illustrating
variational inversion (using simulated data) of this model to illustrate its
use in testing hypotheses. In principle, this furnishes a fast and efficient
immunological assay--based on sequential serology--that provides a (i)
quantitative measure of latent immunological responses and (ii) a Bayes optimal
classification of the different kinds of immunological response (c.f., glucose
tolerance tests used to test for insulin resistance). This may be especially
useful in assessing SARS-CoV-2 vaccines.",http://arxiv.org/abs/2009.08411v1,"Thomas Parr, Anjali Bhat, Peter Zeidman, Aimee Goel, Alexander J Billig, Rosalyn Moran, Karl J Friston"
71,Towards More Flexible False Positive Control in Phase III Randomized Clinical Trials,"Phase III randomized clinical trials play a monumentally critical role in the
evaluation of new medical products. Because of the intrinsic nature of
uncertainty embedded in our capability in assessing the efficacy of a medical
product, interpretation of trial results relies on statistical principles to
control the error of false positives below desirable level. The
well-established statistical hypothesis testing procedure suffers from two
major limitations, namely, the lack of flexibility in the thresholds to claim
success and the lack of capability of controlling the total number of false
positives that could be yielded by the large volume of trials. We propose two
general theoretical frameworks based on the conventional frequentist paradigm
and Bayesian perspectives, which offer realistic, flexible and effective
solutions to these limitations. Our methods are based on the distribution of
the effect sizes of the population of trials of interest. The estimation of
this distribution is practically feasible as clinicaltrials.gov provides a
centralized data repository with unbiased coverage of clinical trials. We
provide a detailed development of the two frameworks with numerical results
obtained for industry sponsored Phase III randomized clinical trials.",http://arxiv.org/abs/1902.08229v1,"Changyu Shen, Xiaochun Li"
72,A Machine Learning Approach for Recruitment Prediction in Clinical Trial Design,"Significant advancements have been made in recent years to optimize patient
recruitment for clinical trials, however, improved methods for patient
recruitment prediction are needed to support trial site selection and to
estimate appropriate enrollment timelines in the trial design stage. In this
paper, using data from thousands of historical clinical trials, we explore
machine learning methods to predict the number of patients enrolled per month
at a clinical trial site over the course of a trial's enrollment duration. We
show that these methods can reduce the error that is observed with current
industry standards and propose opportunities for further improvement.",http://arxiv.org/abs/2111.07407v1,"Jingshu Liu, Patricia J Allen, Luke Benz, Daniel Blickstein, Evon Okidi, Xiao Shi"
73,A web application for the design of multi-arm clinical trials,"Multi-arm designs provide an effective means of evaluating several treatments
within the same clinical trial. Given the large number of treatments now
available for testing in many disease areas, it has been argued that their
utilisation should increase. However, for any given clinical trial there are
numerous possible multi-arm designs that could be used, and choosing between
them can be a difficult task. This task is complicated further by a lack of
available easy-to-use software for designing multi-arm trials. To aid the wider
implementation of multi-arm clinical trial designs, we have developed a web
application for sample size calculation when using a variety of popular
multiple comparison corrections. Furthermore, the application supports sample
size calculation to control several varieties of power, as well as the
determination of optimised arm-wise allocation ratios. It is built using the
Shiny package in the R programming language, is free to access on any device
with an internet browser, and requires no programming knowledge to use. The
application provides the core information required by statisticians and
clinicians to review the operating characteristics of a chosen multi-arm
clinical trial design. We hope that it will assist with the future utilisation
of such designs in practice.",http://arxiv.org/abs/1906.09178v1,"Michael J Grayling, James MS Wason"
74,Assessing the Validity of a a priori Patient-Trial Generalizability Score using Real-world Data from a Large Clinical Data Research Network: A Colorectal Cancer Clinical Trial Case Study,"Existing trials had not taken enough consideration of their population
representativeness, which can lower the effectiveness when the treatment is
applied in real-world clinical practice. We analyzed the eligibility criteria
of Bevacizumab colorectal cancer treatment trials, assessed their a priori
generalizability, and examined how it affects patient outcomes when applied in
real-world clinical settings. To do so, we extracted patient-level data from a
large collection of electronic health records (EHRs) from the OneFlorida
consortium. We built a zero-inflated negative binomial model using a composite
patient-trial generalizability (cPTG) score to predict patients clinical
outcomes (i.e., number of serious adverse events, (SAEs)). Our study results
provide a body of evidence that 1) the cPTG scores can predict patient
outcomes; and 2) patients who are more similar to the study population in the
trials that were used to develop the treatment will have a significantly lower
possibility to experience serious adverse events.",http://arxiv.org/abs/1906.10163v1,"Qian Li, Zhe He, Yi Guo, Hansi Zhang, Thomas J George Jr, William Hogan, Neil Charness, Jiang Bian"
75,On Bayesian Sequential Clinical Trial Designs,"Clinical trials usually involve sequential patient entry. When designing a
clinical trial, it is often desirable to include a provision for interim
analyses of accumulating data with the potential for stopping the trial early.
We review Bayesian sequential clinical trial designs based on posterior
probabilities, posterior predictive probabilities, and decision-theoretic
frameworks. A pertinent question is whether Bayesian sequential designs need to
be adjusted for the planning of interim analyses. We answer this question from
three perspectives: a frequentist-oriented perspective, a calibrated Bayesian
perspective, and a subjective Bayesian perspective. We also provide new
insights into the likelihood principle, which is commonly tied to statistical
inference and decision making in sequential clinical trials. Some theoretical
results are derived, and numerical studies are conducted to illustrate and
assess these designs.",http://arxiv.org/abs/2112.09644v3,"Tianjian Zhou, Yuan Ji"
76,Design Considerations for Factorial Adaptive Multi-Arm Multi-Stage (FAST) Clinical Trials,"Multi-Arm, Multi-Stage (MAMS) clinical trial designs allow for multiple
therapies to be compared across a spectrum of clinical trial phases. MAMS
designs can be categorized into several overarching design groups, including
adaptive designs (AD) and multi-arm (MA) designs. Factorial clinical trials
designs represent an additional group of designs which can provide increased
efficiency relative to fixed, traditional designs. In this work, we explore
design choices associated with Factorial Adaptive Multi-Arm Multi-Stage (FAST)
designs, which represent the combination of factorial and MAMS designs. This
category of trial can potentially offer benefits similar to both MAMS and
factorial designs. This work is motivated by a proposed clinical trial under
development.",http://arxiv.org/abs/2310.12830v1,"Jonathan Beall, Jordan Elm, Mathew W Semler, Li Wang, Todd Rice, Hooman Kamel, William Mack, Akshitkumar M Mistry"
77,Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials,"Online reinforcement learning (RL) algorithms offer great potential for
personalizing treatment for participants in clinical trials. However, deploying
an online, autonomous algorithm in the high-stakes healthcare setting makes
quality control and data quality especially difficult to achieve. This paper
proposes algorithm fidelity as a critical requirement for deploying online RL
algorithms in clinical trials. It emphasizes the responsibility of the
algorithm to (1) safeguard participants and (2) preserve the scientific utility
of the data for post-trial analyses. We also present a framework for
pre-deployment planning and real-time monitoring to help algorithm developers
and clinical researchers ensure algorithm fidelity. To illustrate our
framework's practical application, we present real-world examples from the
Oralytics clinical trial. Since Spring 2023, this trial successfully deployed
an autonomous, online RL algorithm to personalize behavioral interventions for
participants at risk for dental disease.",http://arxiv.org/abs/2402.17003v2,"Anna L Trella, Kelly W Zhang, Inbal NahumShani, Vivek Shetty, Iris Yan, Finale DoshiVelez, Susan A Murphy"
78,Augmented Binary Method for Basket Trials (ABBA),"In several clinical areas, traditional clinical trials often use a responder
outcome, a composite endpoint that involves dichotomising a continuous measure.
An augmented binary method that improves power whilst retaining the original
responder endpoint has previously been proposed. The method leverages
information from the the undichotomised component to improve power. We extend
this method for basket trials, which are gaining popularity in many clinical
areas. For clinical areas where response outcomes are used, we propose the new
Augmented Binary method for BAsket trials (ABBA) enhances efficiency by
borrowing information on the treatment effect between subtrials. The method is
developed within a latent variable framework using a Bayesian hierarchical
modelling approach. We investigate the properties of the proposed methodology
by analysing point estimates and credible intervals in various simulation
scenarios, comparing them to the standard analysis for basket trials that
assumes binary outcome. Our method results in a reduction of 95% high density
interval of the posterior distribution of the log odds ratio and an increase in
power when the treatment effect is consistent across subtrials. We illustrate
our approach using real data from two clinical trials in rheumatology.",http://arxiv.org/abs/2408.08636v1,"Svetlana Cherlin, James M S Wason"
79,On an Approach to Bayesian Sample Sizing in Clinical Trials,"This paper explores an approach to Bayesian sample size determination in
clinical trials. The approach falls into the category of what is often called
""proper Bayesian"", in that it does not mix frequentist concepts with Bayesian
ones. A criterion for a ""successful trial"" is defined in terms of a posterior
probability, its probability is assessed using the marginal distribution of the
data, and this probability forms the basis for choosing sample sizes. We
illustrate with a standard problem in clinical trials, that of establishing
superiority of a new drug over a control.",http://arxiv.org/abs/1204.4460v1,"Robb J Muirhead, Adina I Soaita"
80,Assurance for clinical trial design with normally distributed outcomes: eliciting uncertainty about variances,"The assurance method is growing in popularity in clinical trial planning. The
method involves eliciting a prior distribution for the treatment effect, and
then calculating the probability that a proposed trial will produce a
`successful' outcome. For normally distributed observations, uncertainty about
the variance of the normal distribution also needs to be accounted for, but
there is little guidance in the literature on how to elicit a distribution for
a variance parameter. We present a simple elicitation method, and illustrate
how the elicited distribution is incorporated within an assurance calculation.
We also consider multi-stage trials, where a decision to proceed with a larger
trial will follow from the outcome of a smaller trial; we illustrate the role
of the elicted distribution in assessing the information provided by a proposed
smaller trial. Free software is available for implementing our methods.",http://arxiv.org/abs/1702.00978v2,"Ziyad A Alhussain, Jeremy E Oakley"
81,Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint,"Clinical trials are indispensable in developing new treatments, but they face
obstacles in patient recruitment and retention, hindering the enrollment of
necessary participants. To tackle these challenges, deep learning frameworks
have been created to match patients to trials. These frameworks calculate the
similarity between patients and clinical trial eligibility criteria,
considering the discrepancy between inclusion and exclusion criteria. Recent
studies have shown that these frameworks outperform earlier approaches.
However, deep learning models may raise fairness issues in patient-trial
matching when certain sensitive groups of individuals are underrepresented in
clinical trials, leading to incomplete or inaccurate data and potential harm.
To tackle the issue of fairness, this work proposes a fair patient-trial
matching framework by generating a patient-criterion level fairness constraint.
The proposed framework considers the inconsistency between the embedding of
inclusion and exclusion criteria among patients of different sensitive groups.
The experimental results on real-world patient-trial and patient-criterion
matching tasks demonstrate that the proposed framework can successfully
alleviate the predictions that tend to be biased.",http://arxiv.org/abs/2303.13790v1,"ChiaYuan Chang, Jiayi Yuan, Sirui Ding, Qiaoyu Tan, Kai Zhang, Xiaoqian Jiang, Xia Hu, Na Zou"
82,Scrybe: A Secure Audit Trail for Clinical Trial Data Fusion,"Clinical trials are a multi-billion dollar industry. One of the biggest
challenges facing the clinical trial research community is satisfying Part 11
of Title 21 of the Code of Federal Regulations and ISO 27789. These controls
provide audit requirements that guarantee the reliability of the data contained
in the electronic records. Context-aware smart devices and wearable IoT devices
have become increasingly common in clinical trials. Electronic Data Capture
(EDC) and Clinical Data Management Systems (CDMS) do not currently address the
new challenges introduced using these devices. The healthcare digital threat
landscape is continually evolving, and the prevalence of sensor fusion and
wearable devices compounds the growing attack surface. We propose Scrybe, a
permissioned blockchain, to store proof of clinical trial data provenance. We
illustrate how Scrybe addresses each control and the limitations of the
Ethereum-based blockchains. Finally, we provide a proof-of-concept integration
with REDCap to show tamper resistance.",http://arxiv.org/abs/2109.05649v1,"Jon Oakley, Carl Worley, Lu Yu, Richard Brooks, Ilker Ozcelik, Anthony Skjellum, Jihad Obeid"
83,Local Explanations for Clinical Search Engine results,"Health care professionals rely on treatment search engines to efficiently
find adequate clinical trials and early access programs for their patients.
However, doctors lose trust in the system if its underlying processes are
unclear and unexplained. In this paper, a model-agnostic explainable method is
developed to provide users with further information regarding the reasons why a
clinical trial is retrieved in response to a query. To accomplish this, the
engine generates features from clinical trials using by using a knowledge
graph, clinical trial data and additional medical resources. and a
crowd-sourcing methodology is used to determine their importance. Grounded on
the proposed methodology, the rationale behind retrieving the clinical trials
is explained in layman's terms so that healthcare processionals can
effortlessly perceive them. In addition, we compute an explainability score for
each of the retrieved items, according to which the items can be ranked. The
experiments validated by medical professionals suggest that the proposed
methodology induces trust in targeted as well as in non-targeted users, and
provide them with reliable explanations and ranking of retrieved items.",http://arxiv.org/abs/2110.12891v1,"Edeline Contempr, Zoltn Szlvik, Majid Mohammadi, Erick Velazquez, Annette ten Teije, Ilaria Tiddi"
84,A data science approach to drug safety: Semantic and visual mining of adverse drug events from clinical trials of pain treatments,"Clinical trials are the basis of Evidence-Based Medicine. Trial results are
reviewed by experts and consensus panels for producing meta-analyses and
clinical practice guidelines. However, reviewing these results is a long and
tedious task, hence the meta-analyses and guidelines are not updated each time
a new trial is published. Moreover, the independence of experts may be
difficult to appraise. On the contrary, in many other domains, including
medical risk analysis, the advent of data science, big data and visual
analytics allowed moving from expert-based to fact-based knowledge. Since 12
years, many trial results are publicly available online in trial registries.
Nevertheless, data science methods have not yet been applied widely to trial
data. In this paper, we present a platform for analyzing the safety events
reported during clinical trials and published in trial registries. This
platform is based on an ontological model including 582 trials on pain
treatments, and uses semantic web technologies for querying this dataset at
various levels of granularity. It also relies on a 26-dimensional flower glyph
for the visualization of the Adverse Drug Events (ADE) rates in 13 categories
and 2 levels of seriousness. We illustrate the interest of this platform
through several use cases and we were able to find back conclusions that were
initially found during meta-analyses. The platform was presented to four
experts in drug safety, and is publicly available online, with the ontology of
pain treatment ADE.",http://arxiv.org/abs/2006.16910v2,JeanBaptiste Lamy
85,TAD-SIE: Sample Size Estimation for Clinical Randomized Controlled Trials using a Trend-Adaptive Design with a Synthetic-Intervention-Based Estimator,"Phase-3 clinical trials provide the highest level of evidence on drug safety
and effectiveness needed for market approval by implementing large randomized
controlled trials (RCTs). However, 30-40% of these trials fail mainly because
such studies have inadequate sample sizes, stemming from the inability to
obtain accurate initial estimates of average treatment effect parameters. To
remove this obstacle from the drug development cycle, we present a new
algorithm called Trend-Adaptive Design with a Synthetic-Intervention-Based
Estimator (TAD-SIE) that appropriately powers a parallel-group trial, a
standard RCT design, by leveraging a state-of-the-art hypothesis testing
strategy and a novel trend-adaptive design (TAD). Specifically, TAD-SIE uses
SECRETS (Subject-Efficient Clinical Randomized Controlled Trials using
Synthetic Intervention) for hypothesis testing, which simulates a cross-over
trial in order to boost power; doing so, makes it easier for a trial to reach
target power within trial constraints (e.g., sample size limits). To estimate
sample sizes, TAD-SIE implements a new TAD tailored to SECRETS given that
SECRETS violates assumptions under standard TADs. In addition, our TAD
overcomes the ineffectiveness of standard TADs by allowing sample sizes to be
increased across iterations without any condition while controlling
significance level with futility stopping. On a real-world Phase-3 clinical RCT
(i.e., a two-arm parallel-group superiority trial with an equal number of
subjects per arm), TAD-SIE reaches typical target operating points of 80% or
90% power and 5% significance level in contrast to baseline algorithms that
only get at best 59% power and 4% significance level.",http://arxiv.org/abs/2401.03693v1,"Sayeri Lala, Niraj K Jha"
86,Biomarker Clustering of Colorectal Cancer Data to Complement Clinical Classification,"In this paper, we describe a dataset relating to cellular and physical
conditions of patients who are operated upon to remove colorectal tumours. This
data provides a unique insight into immunological status at the point of tumour
removal, tumour classification and post-operative survival. Attempts are made
to cluster this dataset and important subsets of it in an effort to
characterize the data and validate existing standards for tumour
classification. It is apparent from optimal clustering that existing tumour
classification is largely unrelated to immunological factors within a patient
and that there may be scope for re-evaluating treatment options and survival
estimates based on a combination of tumour physiology and patient
histochemistry.",http://arxiv.org/abs/1307.1601v1,"Chris Roadknight, Uwe Aickelin, Alex Ladas, Daniele Soria, John Scholefield, Lindy Durrant"
87,baskexact: An R package for analytical calculation of basket trial operating characteristics,"Basket trials are a new type of clinical trial in which a treatment is
investigated in several subgroups. For the analysis of these trials,
information is shared between the subgroups based on the observed data to
increase the power. Many approaches for the analysis of basket trials have been
suggested, but only a few have been implemented in open source software
packages. The R package baskexact facilitates the evaluation of two basket
trial designs which use empirical Bayes techniques for sharing information.
With baskexact, operating characteristics for single-stage and two-stage
designs can be calculated analytically and optimal tuning parameters can be
selected.",http://arxiv.org/abs/2403.17510v1,Lukas Baumann
88,Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models,"Physicians considering clinical trials for their patients are met with the
laborious process of checking many text based eligibility criteria. Large
Language Models (LLMs) have shown to perform well for clinical information
extraction and clinical reasoning, including medical tests, but not yet in
real-world scenarios. This paper investigates the use of InstructGPT to assist
physicians in determining eligibility for clinical trials based on a patient's
summarised medical profile. Using a prompting strategy combining one-shot,
selection-inference and chain-of-thought techniques, we investigate the
performance of LLMs on 10 synthetically created patient profiles. Performance
is evaluated at four levels: ability to identify screenable eligibility
criteria from a trial given a medical profile; ability to classify for each
individual criterion whether the patient qualifies; the overall classification
whether a patient is eligible for a clinical trial and the percentage of
criteria to be screened by physician. We evaluated against 146 clinical trials
and a total of 4,135 eligibility criteria. The LLM was able to correctly
identify the screenability of 72% (2,994/4,135) of the criteria. Additionally,
72% (341/471) of the screenable criteria were evaluated correctly. The
resulting trial level classification as eligible or ineligible resulted in a
recall of 0.5. By leveraging LLMs with a physician-in-the-loop, a recall of 1.0
and precision of 0.71 on clinical trial level can be achieved while reducing
the amount of criteria to be checked by an estimated 90%. LLMs can be used to
assist physicians with pre-screening of patients for clinical trials. By
forcing instruction-tuned LLMs to produce chain-of-thought responses, the
reasoning can be made transparent to and the decision process becomes amenable
by physicians, thereby making such a system feasible for use in real-world
scenarios.",http://arxiv.org/abs/2304.07396v2,"Danny M den Hamer, Perry Schoor, Tobias B Polak, Daniel Kapitan"
89,Large-scale Virtual Clinical Trials of Closed-loop Treatments for People with Type 1 Diabetes,"We propose a virtual clinical trial for assessing the safety and efficacy of
closed-loop diabetes treatments prior to an actual clinical trial. Such virtual
trials enable rapid and risk-free pretrial testing of algorithms, and they can
be used to compare different treatment variations for large and diverse
populations. The participants are represented by multiple mathematical models,
consisting of stochastic differential equations, and we use Monte Carlo
closed-loop simulations to compute detailed statistics of the closed-loop
treatments. We implement the virtual clinical trial using high-performance
software and hardware, and we present an example trial with two mathematical
models of one~million participants over 52~weeks (i.e., two~million
simulations), which can be completed in 2~h 9~min.",http://arxiv.org/abs/2205.01332v1,"Tobias K S Ritschel, Asbjrn Thode Reenberg, John Bagterp Jrgensen"
90,Making all pairwise comparisons in multi-arm clinical trials without control treatment,"The standard paradigm for confirmatory clinical trials is to compare
experimental treatments with a control, for example the standard of care or a
placebo. However, it is not always the case that a suitable control exists.
Efficient statistical methodology is well studied in the setting of randomised
controlled trials. This is not the case if one wishes to compare several
experimental with no control arm. We propose hypothesis testing methods
suitable for use in such a setting. These methods are efficient, ensuring the
error rate is controlled at exactly the desired rate with no conservatism. This
in turn yields an improvement in power when compared with standard methods one
might otherwise consider using, such as a Bonferroni adjustment. The proposed
testing procedure is also highly flexible. We show how it may be extended for
use in multi-stage adaptive trials, covering the majority of scenarios in which
one might consider the use of such procedures in the clinical trials setting.
With such a highly flexible nature, these methods may also be applied more
broadly outside of a clinical trials setting.",http://arxiv.org/abs/2410.20908v1,"Thomas Burnett, Thomas Jaki"
91,A computational hierarchy in human cortex,"Hierarchies feature prominently in anatomical accounts of cortical
organisation. An open question is which computational (algorithmic) processes
are implemented by these hierarchies. One renowned hypothesis is that cortical
hierarchies implement a model of the world's causal structure and serve to
infer environmental states from sensory inputs. This view, which casts
perception as hierarchical Bayesian inference, has become a highly influential
concept in both basic and clinical neuroscience. So far, however, a direct
correspondence between the predicted order of hierarchical Bayesian
computations and the sequence of evoked neuronal activity has not been
demonstrated. Here, we present evidence for this correspondence from
neuroimaging and electrophysiological data in healthy volunteers. Trial-wise
sequences of hierarchical computations were inferred from participants'
behaviour during a social learning task that required multi-level inference
about intentions. We found that the temporal sequence of neuronal activity
matched the order of computations as predicted by the theory. These findings
provide strong evidence for the operation of hierarchical Bayesian inference in
human cortex. Furthermore, our approach offers a novel strategy for the
combined computational-physiological phenotyping of patients with disorders of
perception, such as schizophrenia or autism.",http://arxiv.org/abs/1709.02323v1,"Andreea O Diaconescu, Vladimir Litvak, Christoph Mathys, Lars Kasper, Karl J Friston, Klaas E Stephan"
92,Bayesian deep neural networks for low-cost neurophysiological markers of Alzheimer's disease severity,"As societies around the world are ageing, the number of Alzheimer's disease
(AD) patients is rapidly increasing. To date, no low-cost, non-invasive
biomarkers have been established to advance the objectivization of AD diagnosis
and progression assessment. Here, we utilize Bayesian neural networks to
develop a multivariate predictor for AD severity using a wide range of
quantitative EEG (QEEG) markers. The Bayesian treatment of neural networks both
automatically controls model complexity and provides a predictive distribution
over the target function, giving uncertainty bounds for our regression task. It
is therefore well suited to clinical neuroscience, where data sets are
typically sparse and practitioners require a precise assessment of the
predictive uncertainty. We use data of one of the largest prospective AD EEG
trials ever conducted to demonstrate the potential of Bayesian deep learning in
this domain, while comparing two distinct Bayesian neural network approaches,
i.e., Monte Carlo dropout and Hamiltonian Monte Carlo.",http://arxiv.org/abs/1812.04994v2,"Wolfgang Fruehwirt, Adam D Cobb, Martin Mairhofer, Leonard Weydemann, Heinrich Garn, Reinhold Schmidt, Thomas Benke, Peter DalBianco, Gerhard Ransmayr, Markus Waser, Dieter Grossegger, Pengfei Zhang, Georg Dorffner, Stephen Roberts"
93,Selection Induced Contrast Estimate (SICE) Effect: An Attempt to Quantify the Impact of Some Patient Selection Criteria in Randomized Clinical Trials,"Defining the Inclusion/Exclusion (I/E) criteria of a trial is one of the most
important steps during a trial design. Increasingly complex I/E criteria
potentially create information imbalance and transparency issues between the
people who design and run the trials and those who consume the information
produced by the trials. In order to better understand and quantify the impact
of a category of I/E criteria on observed treatment effects, a concept, named
the Selection Induced Contrast Estimate (SICE) effect, is introduced and
formulated in this paper. The SICE effect can exist in controlled clinical
trials when treatment affects the correlation between a marker used for
selection and the response of interest. This effect is demonstrated with both
simulations and real clinical trial data. Although the statistical elements
behind the SICE effect have been well studied, explicitly formulating and
studying this effect can benefit several areas, including better transparency
in I/E criteria, meta-analysis of multiple clinical trials, treatment effect
interpretation in real-world medical practice, etc.",http://arxiv.org/abs/2001.02036v1,"Junshui Ma, Daniel J Holder"
94,Data monitoring committees for clinical trials evaluating treatments of COVID-19,"The first cases of coronavirus disease 2019 (COVID-19) were reported in
December 2019 and the outbreak of SARS-CoV-2 was declared a pandemic in March
2020 by the World Health Organization. This sparked a plethora of
investigations into diagnostics and vaccination for SARS-CoV-2, as well as
treatments for COVID-19. Since COVID-19 is a severe disease associated with a
high mortality, clinical trials in this disease should be monitored by a data
monitoring committee (DMC), also known as data safety monitoring board (DSMB).
DMCs in this indication face a number of challenges including fast recruitment
requiring an unusually high frequency of safety reviews, more frequent use of
complex designs and virtually no prior experience with the disease. In this
paper, we provide a perspective on the work of DMCs for clinical trials of
treatments for COVID-19. More specifically, we discuss organizational aspects
of setting up and running DMCs for COVID-19 trials, in particular for trials
with more complex designs such as platform trials or adaptive designs.
Furthermore, statistical aspects of monitoring clinical trials of treatments
for COVID-19 are considered. Some recommendations are made regarding the
presentation of the data, stopping rules for safety monitoring and the use of
external data. The proposed stopping boundaries are assessed in a simulation
study motivated by clinical trials in COVID-19.",http://arxiv.org/abs/2008.10992v1,"Tobias Mtze, Tim Friede"
95,Coping with Information Loss and the Use of Auxiliary Sources of Data: A Report from the NISS Ingram Olkin Forum Series on Unplanned Clinical Trial Disruptions,"Clinical trials disruption has always represented a non negligible part of
the ending of interventional studies. While the SARS-CoV-2 (COVID-19) pandemic
has led to an impressive and unprecedented initiation of clinical research, it
has also led to considerable disruption of clinical trials in other disease
areas, with around 80% of non-COVID-19 trials stopped or interrupted during the
pandemic. In many cases the disrupted trials will not have the planned
statistical power necessary to yield interpretable results. This paper
describes methods to compensate for the information loss arising from trial
disruptions by incorporating additional information available from auxiliary
data sources. The methods described include the use of auxiliary data on
baseline and early outcome data available from the trial itself and frequentist
and Bayesian approaches for the incorporation of information from external data
sources. The methods are illustrated by application to the analysis of
artificial data based on the Primary care pediatrics Learning Activity
Nutrition (PLAN) study, a clinical trial assessing a diet and exercise
intervention for overweight children, that was affected by the COVID-19
pandemic. We show how all of the methods proposed lead to an increase in
precision relative to use of complete case data only.",http://arxiv.org/abs/2206.11238v1,"Silvia Calderazzo, Sergey Tarima, Carissa Reid, Nancy Flournoy, Tim Friede, Nancy Geller, James L Rosenberger, Nigel Stallard, Moreno Ursino, Marc Vandemeulebroecke, Kelly Van Lancker, Sarah Zohar"
96,Recent advances in methodology for clinical trials in small populations: the InSPiRe project,"Where there are a limited number of patients, such as in a rare disease,
clinical trials in these small populations present several challenges,
including statistical issues. This led to an EU FP7 call for proposals in 2013.
One of the three projects funded was the Innovative Methodology for Small
Populations Research (InSPiRe) project. This paper summarizes the main results
of the project, which was completed in 2017. The InSPiRe project has led to
development of novel statistical methodology for clinical trials in small
populations in four areas. We have explored new decision-making methods for
small population clinical trials using a Bayesian decision-theoretic framework
to compare costs with potential benefits, developed approaches for targeted
treatment trials, enabling simultaneous identification of subgroups and
confirmation of treatment effect for these patients, worked on early phase
clinical trial design and on extrapolation from adult to pediatric studies,
developing methods to enable use of pharmacokinetics and pharmacodynamics data,
and also developed improved robust meta-analysis methods for a small number of
trials to support the planning, analysis and interpretation of a trial as well
as enabling extrapolation between patient groups. In addition to scientific
publications, we have contributed to regulatory guidance and produced free
software in order to facilitate implementation of the novel methods.",http://arxiv.org/abs/1811.02504v1,"T Friede, M Posch, S Zohar, C Alberti, N Benda, E Comets, S Day, A Dmitrenko, A Graf, B K Gnhan, S W Hee, F Lentz, J Madan, F Miller, T Ondra, M Pearce, C Rver, A Tournazi, S Unkel, M Ursino, G Wassmer, N Stallard"
97,Oncology clinical trial design planning based on a multistate model that jointly models progression-free and overall survival endpoints,"When planning an oncology clinical trial, the usual approach is to assume
proportional hazards and even an exponential distribution for time-to-event
endpoints. Often, besides the gold-standard endpoint overall survival (OS),
progression-free survival (PFS) is considered as a second confirmatory
endpoint. We use a survival multistate model to jointly model these two
endpoints and find that neither exponential distribution nor proportional
hazards will typically hold for both endpoints simultaneously. The multistate
model provides a stochastic process approach to model the dependency of such
endpoints neither requiring latent failure times nor explicit dependency
modelling such as copulae. We use the multistate model framework to simulate
clinical trials with endpoints OS and PFS and show how design planning
questions can be answered using this approach. In particular, non-proportional
hazards for at least one of the endpoints are naturally modelled as well as
their dependency to improve planning. We consider an oncology trial on
non-small-cell lung cancer as a motivating example from which we derive
relevant trial design questions. We then illustrate how clinical trial design
can be based on simulations from a multistate model. Key applications are
co-primary endpoints and group-sequential designs. Simulations for these
applications show that the standard simplifying approach may very well lead to
underpowered or overpowered clinical trials. Our approach is quite general and
can be extended to more complex trial designs, further endpoints, and other
therapeutic areas. An R package is available on CRAN.",http://arxiv.org/abs/2301.10059v2,"Alexandra Erdmann, Jan Beyersmann, Kaspar Rufibach"
98,A Contextual-bandit-based Approach for Informed Decision-making in Clinical Trials,"Clinical trials involving multiple treatments utilize randomization of the
treatment assignments to enable the evaluation of treatment efficacies in an
unbiased manner. Such evaluation is performed in post hoc studies that usually
use supervised-learning methods that rely on large amounts of data collected in
a randomized fashion. That approach often proves to be suboptimal in that some
participants may suffer and even die as a result of having not received the
most appropriate treatments during the trial. Reinforcement-learning methods
improve the situation by making it possible to learn the treatment efficacies
dynamically during the course of the trial, and to adapt treatment assignments
accordingly. Recent efforts using \textit{multi-arm bandits}, a type of
reinforcement-learning methods, have focused on maximizing clinical outcomes
for a population that was assumed to be homogeneous. However, those approaches
have failed to account for the variability among participants that is becoming
increasingly evident as a result of recent clinical-trial-based studies. We
present a contextual-bandit-based online treatment optimization algorithm that,
in choosing treatments for new participants in the study, takes into account
not only the maximization of the clinical outcomes but also the patient
characteristics. We evaluated our algorithm using a real clinical trial dataset
from the International Stroke Trial. The results of our retrospective analysis
indicate that the proposed approach performs significantly better than either a
random assignment of treatments (the current gold standard) or a
multi-arm-bandit-based approach, providing substantial gains in the percentage
of participants who are assigned the most suitable treatments. The
contextual-bandit and multi-arm bandit approaches provide 72.63% and 64.34%
gains, respectively, compared to a random assignment.",http://arxiv.org/abs/1809.00258v1,"Yogatheesan Varatharajah, Brent Berry, Sanmi Koyejo, Ravishankar Iyer"
99,Contextual Constrained Learning for Dose-Finding Clinical Trials,"Clinical trials in the medical domain are constrained by budgets. The number
of patients that can be recruited is therefore limited. When a patient
population is heterogeneous, this creates difficulties in learning subgroup
specific responses to a particular drug and especially for a variety of
dosages. In addition, patient recruitment can be difficult by the fact that
clinical trials do not aim to provide a benefit to any given patient in the
trial. In this paper, we propose C3T-Budget, a contextual constrained clinical
trial algorithm for dose-finding under both budget and safety constraints. The
algorithm aims to maximize drug efficacy within the clinical trial while also
learning about the drug being tested. C3T-Budget recruits patients with
consideration of the remaining budget, the remaining time, and the
characteristics of each group, such as the population distribution, estimated
expected efficacy, and estimation credibility. In addition, the algorithm aims
to avoid unsafe dosages. These characteristics are further illustrated in a
simulated clinical trial study, which corroborates the theoretical analysis and
demonstrates an efficient budget usage as well as a balanced learning-treatment
trade-off.",http://arxiv.org/abs/2001.02463v2,"HyunSuk Lee, Cong Shen, James Jordon, Mihaela van der Schaar"
100,Improving efficiency of inference in clinical trials with external control data,"Suppose we are interested in the effect of a treatment in a clinical trial.
The efficiency of inference may be limited due to small sample size. However,
external control data are often available from historical studies. Motivated by
an application to Helicobacter pylori infection, we show how to borrow strength
from such data to improve efficiency of inference in the clinical trial. Under
an exchangeability assumption about the potential outcome mean, we show that
the semiparametric efficiency bound for estimating the average treatment effect
can be reduced by incorporating both the clinical trial data and external
controls. We then derive a doubly robust and locally efficient estimator. The
improvement in efficiency is prominent especially when the external control
dataset has a large sample size and small variability. Our method allows for a
relaxed overlap assumption, and we illustrate with the case where the clinical
trial only contains a treated group. We also develop doubly robust and locally
efficient approaches that extrapolate the causal effect in the clinical trial
to the external population and the overall population. Our results also offer a
meaningful implication for trial design and data collection. We evaluate the
finite-sample performance of the proposed estimators via simulation. In the
Helicobacter pylori infection application, our approach shows that the
combination treatment has potential efficacy advantages over the triple
therapy.",http://arxiv.org/abs/2011.07234v2,"Xinyu Li, Wang Miao, Fang Lu, XiaoHua Zhou"
101,Applying the Estimand and Target Trial frameworks to external control analyses using observational data: a case study in the solid tumor setting,"In causal inference, the correct formulation of the scientific question of
interest is a crucial step. Here we apply the estimand framework to a
comparison of the outcomes of patient-level clinical trials and observational
data to help structure the clinical question. In addition, we complement the
estimand framework with the target trial framework to address specific issues
in defining the estimand attributes using observational data and discuss
synergies and differences of the two frameworks. Whereas the estimand framework
proves useful to address the challenge that in clinical trials and routine
clinical practice patients may switch to subsequent systemic therapies after
the initially assigned systematic treatment, the target trial framework
supports addressing challenges around baseline confounding and the index date.
We apply the combined framework to compare long-term outcomes of a pooled set
of three previously reported randomized phase 3 trials studying patients with
metastatic non-small cell lung cancer receiving front-line chemotherapy
(randomized clinical trial cohort) and similar patients treated with front-line
chemotherapy as part of routine clinical care (observational comparative
cohort). We illustrate the process to define the estimand attributes and select
the estimator to estimate the estimand of interest while accounting for key
baseline confounders, index date, and receipt of subsequent therapies. The
proposed combined framework provides more clarity on the causal contrast of
interest and the estimator to adopt and thus facilitates design and
interpretation of the analyses.",http://arxiv.org/abs/2208.06707v1,"Letizia Polito, Qixing Liang, Navdeep Pal, Philani Mpofu, Ahmed Sawas, Olivier Humblet, Kaspar Rufibach, Dominik Heinzmann"
102,Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology,"Clinical trial matching is a key process in health delivery and discovery. In
practice, it is plagued by overwhelming unstructured data and unscalable manual
processing. In this paper, we conduct a systematic study on scaling clinical
trial matching using large language models (LLMs), with oncology as the focus
area. Our study is grounded in a clinical trial matching system currently in
test deployment at a large U.S. health network. Initial findings are promising:
out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate
eligibility criteria of clinical trials and extract complex matching logic
(e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially
outperform prior strong baselines and may serve as a preliminary solution to
help triage patient-trial candidates with humans in the loop. Our study also
reveals a few significant growth areas for applying LLMs to end-to-end clinical
trial matching, such as context limitation and accuracy, especially in
structuring patient information from longitudinal medical records.",http://arxiv.org/abs/2308.02180v3,"Cliff Wong, Sheng Zhang, Yu Gu, Christine Moung, Jacob Abel, Naoto Usuyama, Roshanthi Weerasinghe, Brian Piening, Tristan Naumann, Carlo Bifulco, Hoifung Poon"
103,Negative Spillover: A Potential Source of Bias in Pragmatic Clinical Trials,"Pragmatic clinical trials evaluate the effectiveness of health interventions
in real-world settings. Negative spillover can arise in a pragmatic trial if
the study intervention affects how scarce resources are allocated between
patients in the intervention and comparison groups. This can harm patients
assigned to the control group and lead to overestimation of treatment effect.
While this type of negative spillover is often addressed in trials of social
welfare and public health interventions, there is little recognition of this
source of bias in the medical literature. In this article, I examine what
causes negative spillover and how it may have led clinical trial investigators
to overestimate the effect of patient navigation, AI-based physiological
alarms, and elective induction of labor. I also suggest ways to detect negative
spillover and design trials that avoid this potential source of bias.",http://arxiv.org/abs/2309.10978v4,Sean Mann
104,Incorporating external data for analyzing randomized clinical trials: A transfer learning approach,"Randomized clinical trials are the gold standard for analyzing treatment
effects, but high costs and ethical concerns can limit recruitment, potentially
leading to invalid inferences. Incorporating external trial data with similar
characteristics into the analysis using transfer learning appears promising for
addressing these issues. In this paper, we present a formal framework for
applying transfer learning to the analysis of clinical trials, considering
three key perspectives: transfer algorithm, theoretical foundation, and
inference method. For the algorithm, we adopt a parameter-based transfer
learning approach to enhance the lasso-adjusted stratum-specific estimator
developed for estimating treatment effects. A key component in constructing the
transfer learning estimator is deriving the regression coefficient estimates
within each stratum, accounting for the bias between source and target data. To
provide a theoretical foundation, we derive the $l_1$ convergence rate for the
estimated regression coefficients and establish the asymptotic normality of the
transfer learning estimator. Our results show that when external trial data
resembles current trial data, the sample size requirements can be reduced
compared to using only the current trial data. Finally, we propose a consistent
nonparametric variance estimator to facilitate inference. Numerical studies
demonstrate the effectiveness and robustness of our proposed estimator across
various scenarios.",http://arxiv.org/abs/2409.04126v1,"Yujia Gu, Hanzhong Liu, Wei Ma"
105,Enhancing Clinical Trial Patient Matching through Knowledge Augmentation and Reasoning with Multi-Agents,"Matching patients effectively and efficiently for clinical trials is a
significant challenge due to the complexity and variability of patient profiles
and trial criteria. This paper introduces Multi-Agents for Knowledge
Augmentation and Reasoning (MAKAR), a novel framework that enhances
patient-trial matching by integrating domain-specific knowledge with structured
reasoning. MAKAR consists of two key modules: the Augmentation Module and the
Reasoning Module. The Augmentation Module enriches trial criteria by
incorporating detailed explanations of relevant concepts to ensure clarity and
completeness. The Reasoning Module then evaluates each health condition,
following a structured, step-wise approach to determine eligibility and make
the final matching decision. This paper enhances patient-trial matching by
leveraging the agency and reasoning capabilities of large language models
(LLMs) through automated agent interactions, including collaboration, critique,
and navigation. Experimental results on a public dataset demonstrate that our
framework surpasses existing benchmarks, achieving up to an 8% improvement in
accuracy for specific criteria. Furthermore, in a real-world offline test,
MAKAR achieved a 100% accuracy. These findings show MAKAR's potential as a
scalable and robust solution for clinical trial patient matching.",http://arxiv.org/abs/2411.14637v2,"Hanwen Shi, Jin Zhang, Kunpeng Zhang"
106,A Conservative Approach to Leveraging External Evidence for Effective Clinical Trial Design,"Prior probabilities of clinical hypotheses are not systematically used for
clinical trial design yet, due to a concern that poor priors may lead to poor
decisions. To address this concern, a conservative approach to Bayesian trial
design is illustrated here, requiring that the operational characteristics of
the primary trial outcome are stronger than the prior. This approach is
complementary to current Bayesian design methods, in that it insures against
prior-data conflict by defining a sample size commensurate to a discrete design
prior. This approach is ethical, in that it requires designs appropriate to
achieving pre-specified levels of clinical equipoise imbalance. Practical
examples are discussed, illustrating design of trials with binary or time to
event endpoints. Moderate increases in phase II study sample size are shown to
deliver strong levels of overall evidence for go/no-go clinical development
decisions. Levels of negative evidence provided by group sequential
confirmatory designs are found negligible, highlighting the importance of
complementing efficacy boundaries with non-binding futility criteria.",http://arxiv.org/abs/2211.02381v4,Fabio Rigat
107,"Leveraging Historical Data for High-Dimensional Regression Adjustment, a Composite Covariate Approach","The amount of data collected from patients involved in clinical trials is
continuously growing. All patient characteristics are potential covariates that
could be used to improve clinical trial analysis and power. However, the
restricted number of patients in phases I and II studies limits the possible
number of covariates included in the analyses. In this paper, we investigate
the cost/benefit ratio of including covariates in the analysis of clinical
trials. Within this context, we address the long-running question ""What is the
optimum number of covariates to include in a clinical trial?"" To further
improve the cost/benefit ratio of covariates, historical data can be leveraged
to pre-specify the covariate weights, which can be viewed as the definition of
a new composite covariate. We analyze the use of a composite covariate while
estimating the treatment effect in small clinical trials. A composite covariate
limits the loss of degrees of freedom and the risk of overfitting.",http://arxiv.org/abs/2103.14421v1,"Samuel Branders, Alvaro Pereira, Guillaume Bernard, Marie Ernst, Adelin Albert"
108,Utilizing ChatGPT to Enhance Clinical Trial Enrollment,"Clinical trials are a critical component of evaluating the effectiveness of
new medical interventions and driving advancements in medical research.
Therefore, timely enrollment of patients is crucial to prevent delays or
premature termination of trials. In this context, Electronic Health Records
(EHRs) have emerged as a valuable tool for identifying and enrolling eligible
participants. In this study, we propose an automated approach that leverages
ChatGPT, a large language model, to extract patient-related information from
unstructured clinical notes and generate search queries for retrieving
potentially eligible clinical trials. Our empirical evaluation, conducted on
two benchmark retrieval collections, shows improved retrieval performance
compared to existing approaches when several general-purposed and task-specific
prompts are used. Notably, ChatGPT-generated queries also outperform
human-generated queries in terms of retrieval performance. These findings
highlight the potential use of ChatGPT to enhance clinical trial enrollment
while ensuring the quality of medical service and minimizing direct risks to
patients.",http://arxiv.org/abs/2306.02077v1,"Georgios Peikos, Symeon Symeonidis, Pranav Kasela, Gabriella Pasi"
109,Multivariate Rank-Based Analysis of Multiple Endpoints in Clinical Trials: A Global Test Approach,"Clinical trials often involve the assessment of multiple endpoints to
comprehensively evaluate the efficacy and safety of interventions. In the work,
we consider a global nonparametric testing procedure based on multivariate rank
for the analysis of multiple endpoints in clinical trials. Unlike other
existing approaches that rely on pairwise comparisons for each individual
endpoint, the proposed method directly incorporates the multivariate ranks of
the observations. By considering the joint ranking of all endpoints, the
proposed approach provides robustness against diverse data distributions and
censoring mechanisms commonly encountered in clinical trials. Through extensive
simulations, we demonstrate the superior performance of the multivariate
rank-based approach in controlling type I error and achieving higher power
compared to existing rank-based methods. The simulations illustrate the
advantages of leveraging multivariate ranks and highlight the robustness of the
approach in various settings. The proposed method offers an effective tool for
the analysis of multiple endpoints in clinical trials, enhancing the
reliability and efficiency of outcome evaluations.",http://arxiv.org/abs/2306.15380v2,"Kexuan Li, Lingli Yang, Shaofei Zhao, Susie Sinks, Luan Lin, Peng Sun"
110,Team IELAB at TREC Clinical Trial Track 2023: Enhancing Clinical Trial Retrieval with Neural Rankers and Large Language Models,"We describe team ielab from CSIRO and The University of Queensland's approach
to the 2023 TREC Clinical Trials Track. Our approach was to use neural rankers
but to utilise Large Language Models to overcome the issue of lack of training
data for such rankers. Specifically, we employ ChatGPT to generate relevant
patient descriptions for randomly selected clinical trials from the corpus.
This synthetic dataset, combined with human-annotated training data from
previous years, is used to train both dense and sparse retrievers based on
PubmedBERT. Additionally, a cross-encoder re-ranker is integrated into the
system. To further enhance the effectiveness of our approach, we prompting
GPT-4 as a TREC annotator to provide judgments on our run files. These
judgments are subsequently employed to re-rank the results. This architecture
tightly integrates strong PubmedBERT-based rankers with the aid of SOTA Large
Language Models, demonstrating a new approach to clinical trial retrieval.",http://arxiv.org/abs/2401.01566v1,"Shengyao Zhuang, Bevan Koopman, Guido Zuccon"
111,Using Large Language Models to Generate Clinical Trial Tables and Figures,"Tables, figures, and listings (TFLs) are essential tools for summarizing
clinical trial data. Creation of TFLs for reporting activities is often a
time-consuming task encountered routinely during the execution of clinical
trials. This study explored the use of large language models (LLMs) to automate
the generation of TFLs through prompt engineering and few-shot transfer
learning. Using public clinical trial data in ADaM format, our results
demonstrated that LLMs can efficiently generate TFLs with prompt instructions,
showcasing their potential in this domain. Furthermore, we developed a
conservational agent named Clinical Trial TFL Generation Agent: An app that
matches user queries to predefined prompts that produce customized programs to
generate specific predefined TFLs.",http://arxiv.org/abs/2409.12046v2,"Yumeng Yang, Peter Krusche, Kristyn Pantoja, Cheng Shi, Ethan Ludmir, Kirk Roberts, Gen Zhu"
112,Centrality in Collaboration: A Novel Algorithm for Social Partitioning Gradients in Community Detection for Multiple Oncology Clinical Trial Enrollments,"Patients at a comprehensive cancer center who do not achieve cure or
remission following standard treatments often become candidates for clinical
trials. Patients who participate in a clinical trial may be suitable for other
studies. A key factor influencing patient enrollment in subsequent clinical
trials is the structured collaboration between oncologists and most responsible
physicians. Possible identification of these collaboration networks can be
achieved through the analysis of patient movements between clinical trial
intervention types with social network analysis and community detection
algorithms. In the detection of oncologist working groups, the present study
evaluates three community detection algorithms: Girvan-Newman, Louvain and an
algorithm developed by the author. Girvan-Newman identifies each intervention
as their own community, while Louvain groups interventions in a manner that is
difficult to interpret. In contrast, the author's algorithm groups
interventions in a way that is both intuitive and informative, with a gradient
evident in social partitioning that is particularly useful for epidemiological
research. This lays the groundwork for future subgroup analysis of clustered
interventions.",http://arxiv.org/abs/2411.01394v2,"Benjamin Smith, Tyler Pittman, Wei Xu"
113,Incorporating external information in analyses of clinical trials with binary outcomes,"External information, such as prior information or expert opinions, can play
an important role in the design, analysis and interpretation of clinical
trials. However, little attention has been devoted thus far to incorporating
external information in clinical trials with binary outcomes, perhaps due to
the perception that binary outcomes can be treated as normally-distributed
outcomes by using normal approximations. In this paper we show that these two
types of clinical trials could behave differently, and that special care is
needed for the analysis of clinical trials with binary outcomes. In particular,
we first examine a simple but commonly used univariate Bayesian approach and
observe a technical flaw. We then study the full Bayesian approach using
different beta priors and a new frequentist approach based on the notion of
confidence distribution (CD). These approaches are illustrated and compared
using data from clinical studies and simulations. The full Bayesian approach is
theoretically sound, but surprisingly, under skewed prior distributions, the
estimate derived from the marginal posterior distribution may not fall between
those from the marginal prior and the likelihood of clinical trial data. This
counterintuitive phenomenon, which we call the ""discrepant posterior
phenomenon,"" does not occur in the CD approach. The CD approach is also
computationally simpler and can be applied directly to any prior distribution,
symmetric or skewed.",http://arxiv.org/abs/1304.6208v1,"Minge Xie, Regina Y Liu, C V Damaraju, William H Olson"
114,Sample size calculation based on the difference in restricted mean time lost for clinical trials with competing risks,"Computation of sample size is important when designing clinical trials. The
presence of competing risks makes the design of clinical trials with
time-to-event endpoints cumbersome. A model based on the subdistribution hazard
ratio (SHR) is commonly used for trials under competing risks. However, this
approach has some limitations related to model assumptions and clinical
interpretation. Considering such limitations, the difference in restricted mean
time lost (RMTLd) is recommended as an alternative indicator. In this paper, we
propose a sample size calculation method based on the RMTLd for the Weibull
distribution (RMTLdWeibull) for clinical trials, which considers experimental
conditions such as equal allocation, uniform accrual, uniform loss to
follow-up, and administrative censoring. Simulation results show that sample
size calculation based on the RMTLdWeibull can generally achieve a predefined
power level and maintain relative robustness. Moreover, the performance of the
sample size calculation based on the RMTLdWeibull is similar or superior to
that based on the SHR. Even if the event time does not follow the Weibull
distribution, the sample size calculation based on the RMTLdWeibull still
performs well. The results also verify the performance of the sample size
calculation method based on the RMTLdWeibull. From the perspective of the
results of this study, clinical interpretation, application conditions and
statistical performance, we recommend that when designing clinical trials in
the presence of competing risks, the RMTLd indicator be applied for sample size
calculation and subsequent effect size measurement.",http://arxiv.org/abs/2311.12293v1,"Xiang Geng, Zhaojin Li, Chengfeng Zhang, Yanjie Wang, Haoning Shen, Zhiheng Huang, Yawen Hou, Zheng Chen"
115,Simulation-based Bayesian predictive probability of success for interim monitoring of clinical trials with competing event data: two case studies,"Bayesian predictive probabilities of success (PPoS) use interim trial data to
calculate the probability of trial success. These quantities can be used to
optimize trial size or to stop for futility. In this paper, we describe a
simulation-based approach to compute the PPoS for clinical trials with
competing event data, for which no specific methodology is currently available.
The proposed procedure hinges on modelling the joint distribution of time to
event and event type by specifying Bayesian models for the cause-specific
hazards of all event types. This allows the prediction of outcome data at the
conclusion of the trial. The PPoS is obtained by numerically averaging the
probability of success evaluated at fixed parameter values over the posterior
distribution of the parameters. Our work is motivated by two randomised
clinical trials: the I-SPY COVID phase II trial for the treatment of severe
COVID-19 (NCT04488081) and the STHLM3 prostate cancer diagnostic trial
(ISRCTN84445406), both of which are characterised by competing event data. We
present different modelling alternatives for the joint distribution of time to
event and event type and show how the choice of the prior distributions can be
used to assess the PPoS under different scenarios. The role of the PPoS
analyses in the decision making process for these two trials is also discussed.",http://arxiv.org/abs/2412.15899v1,"Chiara Micoli, Alessio Crippa, Jason T Connor, ISPY COVID Consortium, Martin Eklund, Andrea Discacciati"
116,Summary of Information Theoretic Quantities,"Information theory is a practical and theoretical framework developed for the
study of communication over noisy channels. Its probabilistic basis and
capacity to relate statistical structure to function make it ideally suited for
studying information flow in the nervous system. As a framework it has a number
of useful properties: it provides a general measure sensitive to any
relationship, not only linear effects; its quantities have meaningful units
which in many cases allow direct comparison between different experiments; and
it can be used to study how much information can be gained by observing neural
responses in single experimental trials, rather than in averages over multiple
trials. A variety of information theoretic quantities are in common use in
neuroscience - including the Shannon entropy, Kullback-Leibler divergence, and
mutual information. In this entry, we introduce and define these quantities.
Further details on how these quantities can be estimated in practice are
provided in the entry ""Estimation of Information-Theoretic Quantities"" and
examples of application of these techniques in neuroscience can be found in the
entry ""Applications of Information-Theoretic Quantities in Neuroscience"".",http://arxiv.org/abs/1501.01854v1,"Robin A A Ince, Stefano Panzeri, Simon R Schultz"
117,How essential are unstructured clinical narratives and information fusion to clinical trial recruitment?,"Electronic health records capture patient information using structured
controlled vocabularies and unstructured narrative text. While structured data
typically encodes lab values, encounters and medication lists, unstructured
data captures the physician's interpretation of the patient's condition,
prognosis, and response to therapeutic intervention. In this paper, we
demonstrate that information extraction from unstructured clinical narratives
is essential to most clinical applications. We perform an empirical study to
validate the argument and show that structured data alone is insufficient in
resolving eligibility criteria for recruiting patients onto clinical trials for
chronic lymphocytic leukemia (CLL) and prostate cancer. Unstructured data is
essential to solving 59% of the CLL trial criteria and 77% of the prostate
cancer trial criteria. More specifically, for resolving eligibility criteria
with temporal constraints, we show the need for temporal reasoning and
information integration with medical events within and across unstructured
clinical narratives and structured data.",http://arxiv.org/abs/1502.04049v1,"Preethi Raghavan, James L Chen, Eric FoslerLussier, Albert M Lai"
118,Deep Learning Derived Histopathology Image Score for Increasing Phase 3 Clinical Trial Probability of Success,"Failures in Phase 3 clinical trials contribute to expensive cost of drug
development in oncology. To drastically reduce such cost, responders to an
oncology treatment need to be identified early on in the drug development
process with limited amount of patient data before the planning of Phase 3
clinical trials. Despite the challenge of small sample size, we pioneered the
use of deep-learning derived digital pathology scores to identify responders
based on the immunohistochemistry images of the target antigen expressed in
tumor biopsy samples from a Phase 1 Non-small Cell Lung Cancer clinical trial.
Based on repeated 10-fold cross validations, the deep-learning derived score on
average achieved 4% higher AUC of ROC curve and 6% higher AUC of
Precision-Recall curve comparing to the tumor proportion score (TPS) based
clinical benchmark. In a small independent testing set of patients, we also
demonstrated that the deep-learning derived score achieved numerically at least
25% higher responder rate in the enriched population than the TPS clinical
benchmark.",http://arxiv.org/abs/2011.05406v1,"Qi Tang, Vardaan Kishore Kumar"
119,Systematic Literature Review on Clinical Trial Eligibility Matching,"Clinical trial eligibility matching is a critical yet often labor-intensive
and error-prone step in medical research, as it ensures that participants meet
precise criteria for safe and reliable study outcomes. Recent advances in
Natural Language Processing (NLP) have shown promise in automating and
improving this process by rapidly analyzing large volumes of unstructured
clinical text and structured electronic health record (EHR) data. In this
paper, we present a systematic overview of current NLP methodologies applied to
clinical trial eligibility screening, focusing on data sources, annotation
practices, machine learning approaches, and real-world implementation
challenges. A comprehensive literature search (spanning Google Scholar,
Mendeley, and PubMed from 2015 to 2024) yielded high-quality studies, each
demonstrating the potential of techniques such as rule-based systems, named
entity recognition, contextual embeddings, and ontology-based normalization to
enhance patient matching accuracy. While results indicate substantial
improvements in screening efficiency and precision, limitations persist
regarding data completeness, annotation consistency, and model scalability
across diverse clinical domains. The review highlights how explainable AI and
standardized ontologies can bolster clinician trust and broaden adoption.
Looking ahead, further research into advanced semantic and temporal
representations, expanded data integration, and rigorous prospective
evaluations is necessary to fully realize the transformative potential of NLP
in clinical trial recruitment.",http://arxiv.org/abs/2503.00863v1,"Muhammad Talha Sharif, Abdul Rehman"
120,Quality assurance and reporting for FLASH clinical trials:the experience of the FEATHER trial,"Research on ultra-high dose rate (UHDR) radiation therapy has indicated its
potential to spare normal tissue while maintaining equivalent tumor control
compared to conventional treatments. First clinical trials are underway. The
randomized phase II/III FEATHER clinical trial at the Paul Scherrer Institute
in collaboration with the University of Zurich Animal Hospital is one of the
first curative domestic animal trials to be attempted, and it is designed to
provide a good example for human trials. However, the lack of standardized
quality assurance (QA) guidelines for FLASH clinical trials presents a
significant challenge in trial design. This work aims to demonstrate the
development and testing of QA and reporting procedures implemented in the
FEATHER clinical trial. We have expanded the clinical QA program to include
UHDR-specific QA and additional patient-specific QA. Furthermore, we have
modified the monitor readout to enable time-resolved measurements, allowing
delivery log files to be used for dose and dose rate recalculations. Finally,
we developed a reporting strategy encompassing relevant parameters for
retrospective studies. We evaluated our QA and reporting procedures with
simulated treatments. This testing confirmed that our QA procedures effectively
ensure the correct and safe delivery of the planned dose. Additionally, we
demonstrated that we could reconstruct the delivered dose and dose rate using
the delivery log files. We developed and used in practice a comprehensive QA
and reporting protocol for a FLASH clinical trial at the Paul Scherrer
Institute. This work aims to establish guidelines and standardize reporting
practices for future advancements in the FLASH-RT field.",http://arxiv.org/abs/2502.02677v1,"Isabella Colizzi, Robert Schaefer, Jonas Brueckner, Gaia Dellepiane, Martin Grossmann, Maximilan Koerner, Antony John Lomax, David Meer, Benno Rohrer, Carla Rohrer Bley, Michele Togno, Serena Psoroulas"
121,Application of Multiple Imputation When Using Propensity Score Methods to Generalize Clinical Trials to Target Populations of Interest,"When the distribution of treatment effect modifiers differs between the trial
sample and target population, inverse probability weighting (IPSW) can be
applied to achieve an unbiased estimate of the population average treatment
effect in the target population. The statistical validity of IPSW is threatened
when there are missing data in the target population, including potential
missingness in trial sample. However, missing data methods have not been
adequately discussed in the current literature. We conducted a set of
simulation studies to determine how to apply multiple imputation (MI) in the
context of IPSW. We specifically addressed questions such as which variables to
include in the imputation model and whether they should come from trial or
non-trial portion of the target population. Based on our findings, we recommend
including all potential effect modifiers and trial indicator from both trial
and non-trial populations, as well as treatment and outcome variables from
trial sample in the imputation model as main effects. Additionally, we have
illustrated ideas by transporting findings from the Frequent Hemodialysis
Network (FHN) Daily Trial to the United States Renal Stage System (USRDS)
population.",http://arxiv.org/abs/2202.00827v2,"Albee Y Ling, Maria E MontezRath, Kris Kapphahn, Manisha Desai"
122,Adaptive Randomization Methods for Sequential Multiple Assignment Randomized Trials (SMARTs) via Thompson Sampling,"Response-adaptive randomization (RAR) has been studied extensively in
conventional, single-stage clinical trials, where it has been shown to yield
ethical and statistical benefits, especially in trials with many treatment
arms. However, RAR and its potential benefits are understudied in sequential
multiple assignment randomized trials (SMARTs), which are the gold-standard
trial design for evaluation of multi-stage treatment regimes. We propose a
suite of RAR algorithms for SMARTs based on Thompson Sampling (TS), a widely
used RAR method in single-stage trials in which treatment randomization
probabilities are aligned with the estimated probability that the treatment is
optimal. We focus on two common objectives in SMARTs: (i) comparison of the
regimes embedded in the trial, and (ii) estimation of an optimal embedded
regime. We develop valid post-study inferential procedures for treatment
regimes under the proposed algorithms. This is nontrivial, as (even in
single-stage settings) RAR can lead to nonnormal limiting distributions of
estimators. Our algorithms are the first for RAR in multi-stage trials that
account for nonregularity in the estimand. Empirical studies based on
real-world SMARTs show that TS can improve in-trial subject outcomes without
sacrificing efficiency for post-trial comparisons.",http://arxiv.org/abs/2401.03268v1,"Peter Norwood, Marie Davidian, Eric Laber"
123,Conditional Similarity Triplets Enable Covariate-Informed Representations of Single-Cell Data,"Single-cell technologies enable comprehensive profiling of diverse immune
cell-types through the measurement of multiple genes or proteins per individual
cell. In order to translate immune signatures assayed from blood or tissue into
powerful diagnostics, machine learning approaches are often employed to compute
immunological summaries or per-sample featurizations, which can be used as
inputs to models for outcomes of interest. Current supervised learning
approaches for computing per-sample representations are trained only to
accurately predict a single outcome and do not take into account relevant
additional clinical features or covariates that are likely to also be measured
for each sample. Here, we introduce a novel approach for incorporating measured
covariates in optimizing model parameters to ultimately specify per-sample
encodings that accurately affect both immune signatures and additional clinical
information. Our introduced method CytoCoSet is a set-based encoding method for
learning per-sample featurizations, which formulates a loss function with an
additional triplet term penalizing samples with similar covariates from having
disparate embedding results in per-sample representations. Overall,
incorporating clinical covariates enables the learning of encodings for each
individual sample that ultimately improve prediction of clinical outcome.",http://arxiv.org/abs/2406.08638v2,"ChiJane Chen, Haidong Yi, Natalie Stanley"
124,The IBEX Knowledge-Base: Achieving more together with open science,"Iterative Bleaching Extends multipleXity (IBEX) is a versatile method for
highly multiplexed imaging of diverse tissues. Based on open science
principles, we created the IBEX Knowledge-Base, a resource for reagents,
protocols and more, to empower innovation.",http://arxiv.org/abs/2407.19059v1,"Andrea J Radtke, Ifeanyichukwu Anidi, Leanne Arakkal, Armando ArroyoMejias, Rebecca T Beuschel, Katy Borner, Colin J Chu, Beatrice Clark, Menna R Clatworthy, Jake Colautti, Joshua Croteau, Saven Denha, Rose Dever, Walderez O Dutra, Sonja Fritzsche, Spencer Fullam, Michael Y Gerner, Anita Gola, Kenneth J Gollob, Jonathan M Hernandez, Jyh Liang Hor, Hiroshi Ichise, Zhixin Jing, Danny Jonigk, Evelyn Kandov, Wolfgang Kastenmueller, Joshua F E Koenig, Aanandita Kothurkar, Alexandra Y Kreins, Ian Lamborn, Yuri Lin, Katia Luciano Pereira Morais, Aleksandra Lunich, Jean C S Luz, Ryan B MacDonald, Chen Makranz, Vivien I Maltez, Ryan V Moriaty, Juan M OcampoGodinez, Vitoria M Olyntho, Kartika Padhan, Kirsten Remmert, Nathan Richoz, Edward C Schrom, Wanjing Shang, Lihong Shi, Rochelle M Shih, Emily Speranza, Salome Stierli, Sarah A Teichmann, Tibor Z Veres, Megan Vierhout, Brianna T Wachter, Adam K WadeVallance, Margaret Williams, Nathan Zangger, Ronald N Germain, Ziv Yaniv"
125,Utilising high-dimensional data in randomised clinical trials: a review of methods and practice,"Introduction: Even in effectively conducted randomised trials, the
probability of a successful study remains relatively low. With recent advances
in the next-generation sequencing technologies, there is a rapidly growing
number of high-dimensional data, including genetic, molecular and phenotypic
information, that have improved our understanding of driver genes, drug
targets, and drug mechanisms of action. The leveraging of high-dimensional data
holds promise for increased success of clinical trials. Methods: We provide an
overview of methods for utilising high-dimensional data in clinical trials. We
also investigate the use of these methods in practice through a review of
recently published randomised clinical trials that utilise high-dimensional
genetic data. The review includes articles that were published between 2019 and
2021, identified through the PubMed database. Results: Out of 174 screened
articles, 100 (57.5%) were randomised clinical trials that collected
high-dimensional data. The most common clinical area was oncology (30%),
followed by chronic diseases (28%), nutrition and ageing (18%) and
cardiovascular diseases (7%). The most common types of data analysed were gene
expression data (70%), followed by DNA data (21%). The most common method of
analysis (36.3%) was univariable analysis. Articles that described
multivariable analyses used standard statistical methods. Most of the clinical
trials had two arms. Discussion: New methodological approaches are required for
more efficient analysis of the increasing amount of high-dimensional data
collected in randomised clinical trials. We highlight the limitations and
barriers to the current use of high-dimensional data in trials, and suggest
potential avenues for improvement and future work.",http://arxiv.org/abs/2305.10174v2,"Svetlana Cherlin, Theophile Bigirumurame, Michael J Grayling, Jrmie Nsengimana, Luke Ouma, Aida Santaolalla, Fang Wan, S Faye Williamson, James M S Wason"
126,How biomedical papers accumulated their clinical citations: A large-scale retrospective analysis based on PubMed,"This paper explored the temporal characteristics of clinical citations of
biomedical papers, including how long it takes to receive its first clinical
citation (the initial stage) and how long it takes to receive two or more
clinical citations after its first clinical citation (the build-up stage). Over
23 million biomedical papers in PubMed between 1940 and 2013 and their clinical
citations are used as the research data. We divide these biomedical papers into
three groups and four categories from clinical citation level and translational
science perspectives. We compare the temporal characteristics of biomedical
papers of different groups or categories. From the perspective of clinical
citation level, the results show that highly clinically cited papers had
obvious advantages of receiving clinical citations over medium and lowly
clinically cited papers in both the initial and build-up stages. Meanwhile, as
the number of clinical citations increased in the build-up stage, the
difference in the length of time to receive the corresponding number of
clinical citations among the three groups of biomedical papers significantly
increased. From the perspective of translational science, the results reveal
that biomedical papers closer to clinical science more easily receive clinical
citations than papers closer to basic science in both the initial and build-up
stages. Moreover, we found that highly clinically cited papers had the
desperate advantage of receiving clinical citations over even the clinical
guidelines or clinical trials. The robustness analysis of the two aspects
demonstrates the reliability of our results.",http://arxiv.org/abs/2404.01072v1,"Xin Li, Xuli Tang, Wei Lu"
127,"Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data","Understanding the neural implementation of complex human behaviors is one of
the major goals in neuroscience. To this end, it is crucial to find a true
representation of the neural data, which is challenging due to the high
complexity of behaviors and the low signal-to-ratio (SNR) of the signals. Here,
we propose a novel unsupervised learning framework, Neural Latent Aligner
(NLA), to find well-constrained, behaviorally relevant neural representations
of complex behaviors. The key idea is to align representations across repeated
trials to learn cross-trial consistent information. Furthermore, we propose a
novel, fully differentiable time warping model (TWM) to resolve the temporal
misalignment of trials. When applied to intracranial electrocorticography
(ECoG) of natural speaking, our model learns better representations for
decoding behaviors than the baseline models, especially in lower dimensional
space. The TWM is empirically validated by measuring behavioral coherence
between aligned trials. The proposed framework learns more cross-trial
consistent representations than the baselines, and when visualized, the
manifold reveals shared neural trajectories across trials.",http://arxiv.org/abs/2308.06443v1,"Cheol Jun Cho, Edward F Chang, Gopala K Anumanchipalli"
128,Exploring the Human Connectome Topology in Group Studies,"Visually comparing brain networks, or connectomes, is an essential task in
the field of neuroscience. Especially relevant to the field of clinical
neuroscience, group studies that examine differences between populations or
changes over time within a population enable neuroscientists to reason about
effective diagnoses and treatments for a range of neuropsychiatric disorders.
In this paper, we specifically explore how visual analytics tools can be used
to facilitate various clinical neuroscience tasks, in which observation and
analysis of meaningful patterns in the connectome can support patient diagnosis
and treatment. We conduct a survey of visualization tasks that enable clinical
neuroscience activities, and further explore how existing connectome
visualization tools support or fail to support these tasks. Based on our
investigation of these tasks, we introduce a novel visualization tool,
NeuroCave, to support group studies analyses. We discuss how our design
decisions (the use of immersive visualization, the use of hierarchical
clustering and dimensionality reduction techniques, and the choice of visual
encodings) are motivated by these tasks. We evaluate NeuroCave through two use
cases that illustrate the utility of interactive connectome visualization in
clinical neuroscience contexts. In the first use case, we study sex differences
using functional connectomes and discover hidden connectome patterns associated
with well-known cognitive differences in spatial and verbal abilities. In the
second use case, we show how the utility of visualizing the brain in different
topological space coupled with clustering information can reveal the brain's
intrinsic structure.",http://arxiv.org/abs/1706.10297v1,"Johnson J G Keiriz, Liang Zhan, Morris Chukhman, Olu Ajilore, Alex D Leow, Angus G Forbes"
129,"Sequential nonparametrics and semiparametrics: Theory, implementation and applications to clinical trials","One of Pranab K. Sen's major research areas is sequential nonparametrics and
semiparametrics and their applications to clinical trials, to which he has made
many important contributions. Herein we review a number of these contributions
and related developments. We also describe some recent work on nonparametric
and semiparametric inference and the associated computational methods in
time-sequential clinical trials with survival endpoints.",http://arxiv.org/abs/0805.2492v1,"Tze Leung Lai, Zheng Su"
130,Generalized Likelihood Ratio Statistics and Uncertainty Adjustments in Efficient Adaptive Design of Clinical Trials,"A new approach to adaptive design of clinical trials is proposed in a general
multiparameter exponential family setting, based on generalized likelihood
ratio statistics and optimal sequential testing theory. These designs are easy
to implement, maintain the prescribed Type I error probability, and are
asymptotically efficient. Practical issues involved in clinical trials allowing
mid-course adaptation and the large literature on this subject are discussed,
and comparisons between the proposed and existing designs are presented in
extensive simulation studies of their finite-sample performance, measured in
terms of the expected sample size and power functions.",http://arxiv.org/abs/1105.4667v1,"Jay Bartroff, Tze Leung Lai"
131,TMU at TREC Clinical Trials Track 2023,"This paper describes Toronto Metropolitan University's participation in the
TREC Clinical Trials Track for 2023. As part of the tasks, we utilize advanced
natural language processing techniques and neural language models in our
experiments to retrieve the most relevant clinical trials. We illustrate the
overall methodology, experimental settings, and results of our implementation
for the run submission as part of Team - V-TorontoMU.",http://arxiv.org/abs/2403.12088v1,"Aritra Kumar Lahiri, Emrul Hasan, Qinmin Vivian Hu, Cherie Ding"
132,"Considerations for the planning, conduct and reporting of clinical trials with interim analyses","Interim analyses are prevalent in clinical trials. Although methodology is
well established, there are aspects of how to operationalize and interpret
interim analyses which remain unclear to many stakeholders. In this paper, a
team of statisticians from the pharmaceutical industry, academia, and
regulatory agencies provide a multi-stakeholder perspective on the key concepts
behind interim analyses and considerations on terminology. We illustrate our
proposals using a hypothetical clinical trial.",http://arxiv.org/abs/2410.01478v2,"Elina Asikanius, Benjamin Hofner, Lisa V Hampson, Gernot Wassmer, Christopher Jennison, Tobias Mielke, Cornelia Ursula Kunz, Kaspar Rufibach"
133,A Bayesian seamless phase I-II trial design with two stages for cancer clinical trials with drug combinations,"The use of drug combinations in clinical trials is increasingly common during
the last years since a more favorable therapeutic response may be obtained by
combining drugs. In phase I clinical trials, most of the existing methodology
recommends a one unique dose combination as ""optimal"", which may result in a
subsequent failed phase II clinical trial since other dose combinations may
present higher treatment efficacy for the same level of toxicity. We are
particularly interested in the setting where it is necessary to wait a few
cycles of therapy to observe an efficacy outcome and the phase I and II
population of patients are different with respect to treatment efficacy. Under
these circumstances, it is common practice to implement two-stage designs where
a set of maximum tolerated dose combinations is selected in a first stage, and
then studied in a second stage for treatment efficacy. In this article we
present a new two-stage design for early phase clinical trials with drug
combinations. In the first stage, binary toxicity data is used to guide the
dose escalation and set the maximum tolerated dose combinations. In the second
stage, we take the set of maximum tolerated dose combinations recommended from
the first stage, which remains fixed along the entire second stage, and through
adaptive randomization, we allocate subsequent cohorts of patients in dose
combinations that are likely to have high posterior median time to progression.
The methodology is assessed with extensive simulations and exemplified with a
real trial.",http://arxiv.org/abs/1809.04348v3,"Jos L Jimnez, Sungjin Kim, Mourad Tighiouart"
134,On the relevance of prognostic information for clinical trials: A theoretical quantification,"The question of how individual patient data from cohort studies or historical
clinical trials can be leveraged for designing more powerful, or smaller yet
equally powerful, clinical trials becomes increasingly important in the era of
digitalisation. Today, the traditional statistical analyses approaches may seem
questionable to practitioners in light of ubiquitous historical covariate
information.
  Several methodological developments aim at incorporating historical
information in the design and analysis of future clinical trials, most
importantly Bayesian information borrowing, propensity score methods,
stratification, and covariate adjustment. Recently, adjusting the analysis with
respect to a prognostic score, which was obtained from some machine learning
procedure applied to historical data, has been suggested and we study the
potential of this approach for randomised clinical trials.
  In an idealised situation of a normal outcome in a two-arm trial with 1:1
allocation, we derive a simple sample size reduction formula as a function of
two criteria characterising the prognostic score: (1) The coefficient of
determination $R^2$ on historical data and (2) the correlation $\rho$ between
the estimated and the true unknown prognostic scores. While maintaining the
same power, the original total sample size $n$ planned for the unadjusted
analysis reduces to $(1 - R^2 \rho^2) \times n$ in an adjusted analysis.
Robustness in less ideal situations was assessed empirically. We conclude that
there is potential for substantially more powerful or smaller trials, but only
when prognostic scores can be accurately estimated.",http://arxiv.org/abs/2111.03391v1,"Sandra Siegfried, Stephen Senn, Torsten Hothorn"
135,The use of large language models to enhance cancer clinical trial educational materials,"Cancer clinical trials often face challenges in recruitment and engagement
due to a lack of participant-facing informational and educational resources.
This study investigated the potential of Large Language Models (LLMs),
specifically GPT4, in generating patient-friendly educational content from
clinical trial informed consent forms. Using data from ClinicalTrials.gov, we
employed zero-shot learning for creating trial summaries and one-shot learning
for developing multiple-choice questions, evaluating their effectiveness
through patient surveys and crowdsourced annotation. Results showed that
GPT4-generated summaries were both readable and comprehensive, and may improve
patients' understanding and interest in clinical trials. The multiple-choice
questions demonstrated high accuracy and agreement with crowdsourced
annotators. For both resource types, hallucinations were identified that
require ongoing human oversight. The findings demonstrate the potential of LLMs
""out-of-the-box"" to support the generation of clinical trial education
materials with minimal trial-specific engineering, but implementation with a
human-in-the-loop is still needed to avoid misinformation risks.",http://arxiv.org/abs/2412.01955v2,"Mingye Gao, Aman Varshney, Shan Chen, Vikram Goddla, Jack Gallifant, Patrick Doyle, Claire Novack, Maeve DillonMartin, Teresia Perkins, Xinrong Correia, Erik Duhaime, Howard Isenstein, Elad Sharon, Lisa Soleymani Lehmann, David Kozono, Brian Anthony, Dmitriy Dligach, Danielle S Bitterman"
136,TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network,"Clinical trials are critical for drug development but often suffer from
expensive and inefficient patient recruitment. In recent years, machine
learning models have been proposed for speeding up patient recruitment via
automatically matching patients with clinical trials based on longitudinal
patient electronic health records (EHR) data and eligibility criteria of
clinical trials. However, they either depend on trial-specific expert rules
that cannot expand to other trials or perform matching at a very general level
with a black-box model where the lack of interpretability makes the model
results difficult to be adopted.
  To provide accurate and interpretable patient trial matching, we introduce a
personalized dynamic tree-based memory network model named TREEMENT. It
utilizes hierarchical clinical ontologies to expand the personalized patient
representation learned from sequential EHR data, and then uses an attentional
beam-search query learned from eligibility criteria embedding to offer a
granular level of alignment for improved performance and interpretability. We
evaluated TREEMENT against existing models on real-world datasets and
demonstrated that TREEMENT outperforms the best baseline by 7% in terms of
error reduction in criteria-level matching and achieves state-of-the-art
results in its trial-level matching ability. Furthermore, we also show TREEMENT
can offer good interpretability to make the model results easier for adoption.",http://arxiv.org/abs/2307.09942v1,"Brandon Theodorou, Cao Xiao, Jimeng Sun"
137,An overview of open source Deep Learning-based libraries for Neuroscience,"In recent years, deep learning revolutionized machine learning and its
applications, producing results comparable to human experts in several domains,
including neuroscience. Each year, hundreds of scientific publications present
applications of deep neural networks for biomedical data analysis. Due to the
fast growth of the domain, it could be a complicated and extremely
time-consuming task for worldwide researchers to have a clear perspective of
the most recent and advanced software libraries. This work contributes to
clarify the current situation in the domain, outlining the most useful
libraries that implement and facilitate deep learning application to
neuroscience, allowing scientists to identify the most suitable options for
their research or clinical projects. This paper summarizes the main
developments in Deep Learning and their relevance to Neuroscience; it then
reviews neuroinformatic toolboxes and libraries, collected from the literature
and from specific hubs of software projects oriented to neuroscience research.
The selected tools are presented in tables detailing key features grouped by
domain of application (e.g. data type, neuroscience area, task), model
engineering (e.g. programming language, model customization) and technological
aspect (e.g. interface, code source). The results show that, among a high
number of available software tools, several libraries are standing out in terms
of functionalities for neuroscience applications. The aggregation and
discussion of this information can help the neuroscience community to devolop
their research projects more efficiently and quickly, both by means of readily
available tools, and by knowing which modules may be improved, connected or
added.",http://arxiv.org/abs/2301.05057v1,"Louis Fabrice Tshimanga, Manfredo Atzori, Federico Del Pup, Maurizio Corbetta"
138,Multimodal Clinical Trial Outcome Prediction with Large Language Models,"The clinical trial is a pivotal and costly process, often spanning multiple
years and requiring substantial financial resources. Therefore, the development
of clinical trial outcome prediction models aims to exclude drugs likely to
fail and holds the potential for significant cost savings. Recent data-driven
attempts leverage deep learning methods to integrate multimodal data for
predicting clinical trial outcomes. However, these approaches rely on manually
designed modal-specific encoders, which limits both the extensibility to adapt
new modalities and the ability to discern similar information patterns across
different modalities. To address these issues, we propose a multimodal
mixture-of-experts (LIFTED) approach for clinical trial outcome prediction.
Specifically, LIFTED unifies different modality data by transforming them into
natural language descriptions. Then, LIFTED constructs unified noise-resilient
encoders to extract information from modal-specific language descriptions.
Subsequently, a sparse Mixture-of-Experts framework is employed to further
refine the representations, enabling LIFTED to identify similar information
patterns across different modalities and extract more consistent
representations from those patterns using the same expert model. Finally, a
mixture-of-experts module is further employed to dynamically integrate
different modality representations for prediction, which gives LIFTED the
ability to automatically weigh different modalities and pay more attention to
critical information. The experiments demonstrate that LIFTED significantly
enhances performance in predicting clinical trial outcomes across all three
phases compared to the best baseline, showcasing the effectiveness of our
proposed key components.",http://arxiv.org/abs/2402.06512v4,"Wenhao Zheng, Liaoyaqi Wang, Dongshen Peng, Hongxia Xu, Yun Li, Hongtu Zhu, Tianfan Fu, Huaxiu Yao"
139,Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials,"Due to the substantial number of clinicians, patients, and data collection
environments involved in clinical trials, gathering data of superior quality
poses a significant challenge. In clinical trials, patients are assessed based
on their speech data to detect and monitor cognitive and mental health
disorders. We propose using these speech recordings to verify the identities of
enrolled patients and identify and exclude the individuals who try to enroll
multiple times in the same trial. Since clinical studies are often conducted
across different countries, creating a system that can perform speaker
verification in diverse languages without additional development effort is
imperative. We evaluate pre-trained TitaNet, ECAPA-TDNN, and SpeakerNet models
by enrolling and testing with speech-impaired patients speaking English,
German, Danish, Spanish, and Arabic languages. Our results demonstrate that
tested models can effectively generalize to clinical speakers, with less than
2.7% EER for European Languages and 8.26% EER for Arabic. This represents a
significant step in developing more versatile and efficient speaker
verification systems for cognitive and mental health clinical trials that can
be used across a wide range of languages and dialects, substantially reducing
the effort required to develop speaker verification systems for multiple
languages. We also evaluate how speech tasks and number of speakers involved in
the trial influence the performance and show that the type of speech tasks
impacts the model performance.",http://arxiv.org/abs/2404.01981v2,"Ali Akram, Marija Stanojevic, Malikeh Ehghaghi, Jekaterina Novikova"
140,Choriocapillaris Flow Signal Impairment in Sorsby Fundus Dystrophy,"Purpose: To quantify choriocapillaris flow alterations in early Sorsby Fundus
Dystrophy (SFD) and to investigate the relationship of choriocapillaris flow
with the choroidal and outer retinal microstructure. Methods: In this
prospective case-control study, 18 eyes of 11 patients with early SFD and 32
eyes of 32 controls without ocular pathology underwent multimodal imaging
including spectral-domain optical coherence tomography (OCT)followed by
deep-learning-based layer segmentation. OCT-angiography (OCT-A) was performed
to quantify choriocapillaris flow signal deficits (FDs). Differences in
choriocapillaris flow area percentage between SFD patients and controls were
determined and a structure-function correlation with outer retinal layer
thicknesses were analyzed based on mixed model analysis. Results: SFD patients
exhibited a significantly greater choriocapillaris FDs area percentage than
controls (estimate [95% CI] 32.05% [24.31-39.80] vs. 23.36% [20.64-26.09],
P<0.001), even when adjusting for age. Choroidal thickness was a structural OCT
surrogate of the choriocapillaris FD area percentage (-0.82% per 100
micrometer, P=0.017), whereas retinal-pigment-epithelium-drusen-complex
thickness was not informative regarding choriocapillaris FDs (P=0.932). The
choriocapillaris FD area percentage was associated with an altered
microstructure of the overlying photoreceptors (outer-segments, inner-segments
and outer-nuclear-layer thinning of -0.31, -0.12 and -0.47 $\mu$m per %FD,
respectively, P<0.001). Conclusions: Patients with early SFD exhibit pronounced
abnormalities of choriocapillaris flow signal on OCT-A, which are not limited
to areas of sub-RPE deposits seen in OCT imaging. Thus, analysis of the
choriocapillaris flow may enable clinical trials at earlier disease stages in
SFD and possibly in mimicking diseases with an impaired Bruchs membrane
including age-related macular degeneration.",http://arxiv.org/abs/2107.11361v1,"Kristina Hess, Kristin Raming, Martin Gliem, Peter Charbel Issa, Philipp Herrmann, Frank G Holz, Maximilian Pfau"
141,LinkedCT: A Linked Data Space for Clinical Trials,"The Linked Clinical Trials (LinkedCT) project aims at publishing the first
open semantic web data source for clinical trials data. The database exposed by
LinkedCT is generated by (1) transforming existing data sources of clinical
trials into RDF, and (2) discovering semantic links between the records in the
trials data and several other data sources. In this paper, we discuss several
challenges involved in these two steps and present the methodology used in
LinkedCT to overcome these challenges. Our approach for semantic link discovery
involves using state-of-the-art approximate string matching techniques combined
with ontology-based semantic matching of the records, all performed in a
declarative and easy-to-use framework. We present an evaluation of the
performance of our proposed techniques in several link discovery scenarios in
LinkedCT.",http://arxiv.org/abs/0908.0567v1,"Oktie Hassanzadeh, Anastasios Kementsietsidis, Lipyeow Lim, Renee J Miller, Min Wang"
142,Active Clinical Trials for Personalized Medicine,"Individualized treatment rules (ITRs) tailor treatments according to
individual patient characteristics. They can significantly improve patient care
and are thus becoming increasingly popular. The data collected during
randomized clinical trials are often used to estimate the optimal ITRs.
However, these trials are generally expensive to run, and, moreover, they are
not designed to efficiently estimate ITRs. In this paper, we propose a
cost-effective estimation method from an active learning perspective. In
particular, our method recruits only the ""most informative"" patients (in terms
of learning the optimal ITRs) from an ongoing clinical trial. Simulation
studies and real-data examples show that our active clinical trial method
significantly improves on competing methods. We derive risk bounds and show
that they support these observed empirical advantages.",http://arxiv.org/abs/1404.2971v2,"Stanislav Minsker, YingQi Zhao, Guang Cheng"
143,Robust Detection of Covariate-Treatment Interactions in Clinical Trials,"Detection of interactions between treatment effects and patient descriptors
in clinical trials is critical for optimizing the drug development process. The
increasing volume of data accumulated in clinical trials provides a unique
opportunity to discover new biomarkers and further the goal of personalized
medicine, but it also requires innovative robust biomarker detection methods
capable of detecting non-linear, and sometimes weak, signals. We propose a set
of novel univariate statistical tests, based on the theory of random walks,
which are able to capture non-linear and non-monotonic covariate-treatment
interactions. We also propose a novel combined test, which leverages the power
of all of our proposed univariate tests into a single general-case tool. We
present results for both synthetic trials as well as real-world clinical
trials, where we compare our method with state-of-the-art techniques and
demonstrate the utility and robustness of our approach.",http://arxiv.org/abs/1712.08211v1,"Baptiste Goujaud, Eric W Tramel, Pierre Courtiol, Mikhail Zaslavskiy, Gilles Wainrib"
144,A Blockchain Framework for Managing and Monitoring Data in Multi-Site Clinical Trials,"The cost of conducting multi-site clinical trials has significantly increased
over time, with site monitoring, data management, and amendments being key
drivers. Clinical trial data management approaches typically rely on a central
database, and require manual efforts to encode and maintain data capture and
reporting requirements. To reduce the administrative burden, time, and effort
of ensuring data integrity and privacy in multi-site trials, we propose a novel
data management framework based on permissioned blockchain technology. We
demonstrate how our framework, which uses smart contracts and private channels,
enables confidential data communication, protocol enforcement, and and an
automated audit trail. We compare this framework with the traditional data
management approach and evaluate its effectiveness in satisfying the major
requirements of multi-site clinical trials. We show that our framework ensures
enforcement of IRB-related regulatory requirements across multiple sites and
stakeholders.",http://arxiv.org/abs/1902.03975v1,"Olivia Choudhury, Noor Fairoza, Issa Sylla, Amar Das"
145,Learning for Dose Allocation in Adaptive Clinical Trials with Safety Constraints,"Phase I dose-finding trials are increasingly challenging as the relationship
between efficacy and toxicity of new compounds (or combination of them) becomes
more complex. Despite this, most commonly used methods in practice focus on
identifying a Maximum Tolerated Dose (MTD) by learning only from toxicity
events. We present a novel adaptive clinical trial methodology, called Safe
Efficacy Exploration Dose Allocation (SEEDA), that aims at maximizing the
cumulative efficacies while satisfying the toxicity safety constraint with high
probability. We evaluate performance objectives that have operational meanings
in practical clinical trials, including cumulative efficacy,
recommendation/allocation success probabilities, toxicity violation
probability, and sample efficiency. An extended SEEDA-Plateau algorithm that is
tailored for the increase-then-plateau efficacy behavior of molecularly
targeted agents (MTA) is also presented. Through numerical experiments using
both synthetic and real-world datasets, we show that SEEDA outperforms
state-of-the-art clinical trial designs by finding the optimal dose with higher
success rate and fewer patients.",http://arxiv.org/abs/2006.05026v2,"Cong Shen, Zhiyang Wang, Sofia S Villar, Mihaela van der Schaar"
146,High-performance Uncertainty Quantification in Large-scale Virtual Clinical Trials of Closed-loop Diabetes Treatment,"In this paper, we propose a virtual clinical trial for assessing the
performance and identifying risks in closed-loop diabetes treatments. Virtual
clinical trials enable fast and risk-free tests of many treatment variations
for large populations of fictive patients (represented by mathematical models).
We use closed-loop Monte Carlo simulation, implemented in high-performance
software and hardware, to quantify the uncertainty in treatment performance as
well as to compare the performance in different scenarios or of different
closed-loop treatments. Our software can be used for testing a wide variety of
control strategies ranging from heuristical approaches to nonlinear model
predictive control. We present an example of a virtual clinical trial with one
million patients over 52 weeks, and we use high-performance software and
hardware to conduct the virtual trial in 1 h and 22 min.",http://arxiv.org/abs/2202.13927v1,"Asbjrn Thode Reenberg, Tobias K S Ritschel, Bernd Dammann, John Bagterp Jrgensen"
147,Automatic prediction of cognitive and functional decline can significantly decrease the number of subjects required for clinical trials in early Alzheimer's disease,"INTRODUCTION: Heterogeneity in the progression of Alzheimer's disease makes
it challenging to predict the rate of cognitive and functional decline for
individual patients. Tools for short-term prediction could help enrich clinical
trial designs and focus prevention strategies on the most at-risk patients.
METHOD: We built a prognostic model using baseline cognitive scores and
MRI-based features to determine which subjects with mild cognitive impairment
remained stable and which functionally declined (measured by a two-point
increase in CDR-SB) over 2 and 3-year follow-up periods, periods typical of the
length of clinical trials. RESULTS: Combining both sets of features yields 77%
accuracy (81% sensitivity and 75% specificity) to predict cognitive decline at
2 years (74% accuracy at 3 years with 75% sensitivity and 73% specificity).
Using this tool to select trial participants yields a 3.8-fold decrease in the
required sample size for a 2-year study (2.8-fold decrease for a 3-year study)
for a hypothesized 25% treatment effect to reduce cognitive decline.
DISCUSSION: This cohort enrichment tool could accelerate treatment development
by increasing power in clinical trials.",http://arxiv.org/abs/2101.08346v1,"Neda Shafiee, Mahsa Dadar, Simon Ducharme, D Louis Collins"
148,Incorporating External Data into the Analysis of Clinical Trials via Bayesian Additive Regression Trees,"Most clinical trials involve the comparison of a new treatment to a control
arm (e.g., the standard of care) and the estimation of a treatment effect.
External data, including historical clinical trial data and real-world
observational data, are commonly available for the control arm. Borrowing
information from external data holds the promise of improving the estimation of
relevant parameters and increasing the power of detecting a treatment effect if
it exists. In this paper, we propose to use Bayesian additive regression trees
(BART) for incorporating external data into the analysis of clinical trials,
with a specific goal of estimating the conditional or population average
treatment effect. BART naturally adjusts for patient-level covariates and
captures potentially heterogeneous treatment effects across different data
sources, achieving flexible borrowing. Simulation studies demonstrate that BART
compares favorably to a hierarchical linear model and a normal-normal
hierarchical model. We illustrate the proposed method with an acupuncture
trial.",http://arxiv.org/abs/2103.08754v1,"Tianjian Zhou, Yuan Ji"
149,Predicting Intervention Approval in Clinical Trials through Multi-Document Summarization,"Clinical trials offer a fundamental opportunity to discover new treatments
and advance the medical knowledge. However, the uncertainty of the outcome of a
trial can lead to unforeseen costs and setbacks. In this study, we propose a
new method to predict the effectiveness of an intervention in a clinical trial.
Our method relies on generating an informative summary from multiple documents
available in the literature about the intervention under study. Specifically,
our method first gathers all the abstracts of PubMed articles related to the
intervention. Then, an evidence sentence, which conveys information about the
effectiveness of the intervention, is extracted automatically from each
abstract. Based on the set of evidence sentences extracted from the abstracts,
a short summary about the intervention is constructed. Finally, the produced
summaries are used to train a BERT-based classifier, in order to infer the
effectiveness of an intervention. To evaluate our proposed method, we introduce
a new dataset which is a collection of clinical trials together with their
associated PubMed articles. Our experiments, demonstrate the effectiveness of
producing short informative summaries and using them to predict the
effectiveness of an intervention.",http://arxiv.org/abs/2204.00290v1,"Georgios Katsimpras, Georgios Paliouras"
150,Pragmatic Clinical Trials in the Rubric of Structural Causal Models,"Explanatory studies, such as randomized controlled trials, are targeted to
extract the true causal effect of interventions on outcomes and are by design
adjusted for covariates through randomization. On the contrary, observational
studies are a representation of events that occurred without intervention. Both
can be illustrated using the Structural Causal Model (SCM), and do-calculus can
be employed to estimate the causal effects. Pragmatic clinical trials (PCT)
fall between these two ends of the trial design spectra and are thus hard to
define. Due to its pragmatic nature, no standardized representation of PCT
through SCM has been yet established. In this paper, we approach this problem
by proposing a generalized representation of PCT under the rubric of structural
causal models (SCM). We discuss different analysis techniques commonly employed
in PCT using the proposed graphical model, such as intention-to-treat,
as-treated, and per-protocol analysis. To show the application of our proposed
approach, we leverage an experimental dataset from a pragmatic clinical trial.
Our proposition of SCM through PCT creates a pathway to leveraging do-calculus
and related mathematical operations on clinical datasets.",http://arxiv.org/abs/2204.13782v1,"Riddhiman Adib, Sheikh Iqbal Ahamed, Mohammad Adibuzzaman"
151,Contextual aggregation and rapid updating of trial outcomes within a user-friendly open-source environment,"The delayed and incomplete availability of historical findings and the lack
of integrative and user-friendly software hampers the reliable interpretation
of new clinical data. We developed a free, open, and user-friendly clinical
trial aggregation program combining a large and representative sample of
existing trial data with the latest classical and Bayesian meta-analytical
models, including clear output visualizations. Our software is of particular
interest for (post-graduate) educational programs (e.g., medicine,
epidemiology) and global health initiatives. We demonstrate the database,
interface, and plot functionality with a recent randomized controlled trial on
effective epileptic seizure reduction in children treated for a parasitic brain
infection. The single trial data is placed into context and we show how to
interpret new results against existing knowledge instantaneously. Our program
is of particular interest to those working on the contextualizing of medical
findings. It may facilitate the advancement of global clinical progress as
efficiently and openly as possible and simulate further bridging clinical data
with the latest biostatistical models.",http://arxiv.org/abs/2306.14061v1,"Frantiek Barto, EricJan Wagenmakers, Christiaan H Vinkers, Kees P J Braun, Willem M Otte"
152,Mathematical programming tools for randomization purposes in small two-arm clinical trials: A case study with real data,"Modern randomization methods in clinical trials are invariably adaptive,
meaning that the assignment of the next subject to a treatment group uses the
accumulated information in the trial. Some of the recent adaptive randomization
methods use mathematical programming to construct attractive clinical trials
that balance the group features, such as their sizes and covariate
distributions of their subjects. We review some of these methods and compare
their performance with common covariate-adaptive randomization methods for
small clinical trials. We introduce an energy distance measure that compares
the discrepancy between the two groups using the joint distribution of the
subjects' covariates. This metric is more appealing than evaluating the
discrepancy between the groups using their marginal covariate distributions.
Using numerical experiments, we demonstrate the advantages of the mathematical
programming methods under the new measure. In the supplementary material, we
provide R codes to reproduce our study results and facilitate comparisons of
different randomization procedures.",http://arxiv.org/abs/2402.06058v1,"Alan R Vazquez, Weng Kee Wong"
153,Clinical Trials Protocol Authoring using LLMs,"This report embarks on a mission to revolutionize clinical trial protocol
development through the integration of advanced AI technologies. With a focus
on leveraging the capabilities of generative AI, specifically GPT-4, this
initiative aimed to streamline and enhance the efficiency and accuracy of
clinical trial protocols. The methodology encompassed a detailed analysis and
preparation of comprehensive drug and study level metadata, followed by the
deployment of GPT-4 for automated protocol section generation. Results
demonstrated a significant improvement in protocol authoring, highlighted by
increases in efficiency, accuracy, and the customization of protocols to
specific trial requirements. Challenges encountered during model selection and
prompt engineering were systematically addressed, leading to refined
methodologies that capitalized on the advanced text generation capabilities of
GPT-4. This project not only showcases the practical applications and benefits
of generative AI in clinical trial design but also sets a foundation for future
innovations in the field.",http://arxiv.org/abs/2404.05044v2,"Morteza Maleki, SeyedAli Ghahari"
154,Design of Bayesian Clinical Trials with Clustered Data and Multiple Endpoints,"In the design of clinical trials, it is essential to assess the design
operating characteristics (i.e., the probabilities of making correct
decisions). Common practice for the evaluation of operating characteristics in
Bayesian clinical trials relies on estimating the sampling distribution of
posterior summaries via Monte Carlo simulation. It is computationally intensive
to repeat this estimation process for each design configuration considered,
particularly for clustered data that are analyzed using complex,
high-dimensional models. In this paper, we propose an efficient method to
assess operating characteristics and determine sample sizes for Bayesian trials
with clustered data and multiple endpoints. We prove theoretical results that
enable posterior probabilities to be modelled as a function of the sample size.
Using these functions, we assess operating characteristics at a range of sample
sizes given simulations conducted at only two sample sizes. These theoretical
results are also leveraged to quantify the impact of simulation variability on
our sample size recommendations. The applicability of our methodology is
illustrated using a current clinical trial with clustered data.",http://arxiv.org/abs/2501.13218v1,"Luke Hagar, Shirin Golchi"
155,Estimands in Hematologic Oncology Trials,"The estimand framework included in the addendum to the ICH E9 guideline
facilitates discussions to ensure alignment between the key question of
interest, the analysis, and interpretation. Therapeutic knowledge and drug
mechanism play a crucial role in determining the strategy and defining the
estimand for clinical trial designs. Clinical trials in patients with
hematological malignancies often present unique challenges for trial design due
to complexity of treatment options and existence of potential curative but
highly risky procedures, e.g. stem cell transplant or treatment sequence across
different phases (induction, consolidation, maintenance). Here, we illustrate
how to apply the estimand framework in hematological clinical trials and how
the estimand framework can address potential difficulties in trial result
interpretation.
  This paper is a result of a cross-industry collaboration to connect the
International Conference on Harmonisation (ICH) E9 addendum concepts to
applications. Three randomized phase 3 trials will be used to consider common
challenges including intercurrent events in hematologic oncology trials to
illustrate different scientific questions and the consequences of the estimand
choice for trial design, data collection, analysis, and interpretation.
Template language for describing estimand in both study protocols and
statistical analysis plans is suggested for statisticians' reference.",http://arxiv.org/abs/2010.00957v1,"Steven Sun, HansJochen Weber, Emily Butler, Kaspar Rufibach, Satrajit Roychoudhury"
156,How mature are survival data at the time of an interim analysis in a clinical trial with a survival outcome?,"In a clinical trial with a survival outcome, an interim analysis is often
performed to allow for early stopping for efficacy. If the interim analysis is
early in the trial, one might conclude that a new treatment is more effective
(compared to e.g.\ a placebo) and stop the trial, whereas the survival curves
in the trial arms are not mature for the research question under investigation,
for example because the curves are still close to 1 at that time. This means
that the decision is based on a small percentage of the events in the long run
only; possibly the events of the more frail patients in the trial who may not
be representative for the whole group of patients. It may not be sensible to
conclude effectiveness based on so little information. Criteria to determine
the moment the interim analysis will be performed, should be chosen with care,
and include the maturity of the data at the time of the interim analysis. Here,
the expected survival rates at the interim analysis play a role. In this paper
we will derive the asymptotic distribution of the Kaplan-Meier curves at the
(random) moment the interim analysis will be performed for a one and two arm
clinical trial. Based on this distribution, an interval in which the Kaplan
Meier curves will fall into (with probability 95\%) is derived and could be
used to plan the moment of the interim analysis in the design stage of the
trial, so before the trial starts.",http://arxiv.org/abs/2305.04103v1,"Marianne A Jonker, Steven Teerenstra"
157,Active Learning for Developing Personalized Treatment,"The personalization of treatment via bio-markers and other risk categories
has drawn increasing interest among clinical scientists. Personalized treatment
strategies can be learned using data from clinical trials, but such trials are
very costly to run. This paper explores the use of active learning techniques
to design more efficient trials, addressing issues such as whom to recruit, at
what point in the trial, and which treatment to assign, throughout the duration
of the trial. We propose a minimax bandit model with two different optimization
criteria, and discuss the computational challenges and issues pertaining to
this approach. We evaluate our active learning policies using both simulated
data, and data modeled after a clinical trial for treating depressed
individuals, and contrast our methods with other plausible active learning
policies.",http://arxiv.org/abs/1202.3714v1,"Kun Deng, Joelle Pineau, Susan A Murphy"
158,Clinical trial design enabling -optimal treatment rules,"Medical research has evolved conventions for choosing sample size in
randomized clinical trials that rest on the theory of hypothesis testing.
Bayesians have argued that trials should be designed to maximize subjective
expected utility in settings of clinical interest. This perspective is
compelling given a credible prior distribution on treatment response, but
Bayesians have struggled to provide guidance on specification of priors. We use
the frequentist statistical decision theory of Wald (1950) to study design of
trials under ambiguity. We show that {\epsilon}-optimal rules exist when trials
have large enough sample size. An {\epsilon}-optimal rule has expected welfare
within {\epsilon} of the welfare of the best treatment in every state of
nature. Equivalently, it has maximum regret no larger than {\epsilon}. We
consider trials that draw predetermined numbers of subjects at random within
groups stratified by covariates and treatments. The principal analytical
findings are simple sufficient conditions on sample sizes that ensure existence
of {\epsilon}-optimal treatment rules when outcomes are bounded. These
conditions are obtained by application of Hoeffding (1963) large deviations
inequalities to evaluate the performance of empirical success rules.",http://arxiv.org/abs/1509.07913v1,"Charles F Manski, Aleksey Tetenov"
159,Attention-Based LSTM Network for COVID-19 Clinical Trial Parsing,"COVID-19 clinical trial design is a critical task in developing therapeutics
for the prevention and treatment of COVID-19. In this study, we apply a deep
learning approach to extract eligibility criteria variables from COVID-19
trials to enable quantitative analysis of trial design and optimization.
Specifically, we train attention-based bidirectional Long Short-Term Memory
(Att-BiLSTM) models and use the optimal model to extract entities (i.e.,
variables) from the eligibility criteria of COVID-19 trials. We compare the
performance of Att-BiLSTM with traditional ontology-based method. The result on
a benchmark dataset shows that Att-BiLSTM outperforms the ontology model.
Att-BiLSTM achieves a precision of 0.942, recall of 0.810, and F1 of 0.871,
while the ontology model only achieves a precision of 0.715, recall of 0.659,
and F1 of 0.686. Our analyses demonstrate that Att-BiLSTM is an effective
approach for characterizing patient populations in COVID-19 clinical trials.",http://arxiv.org/abs/2012.10063v1,"Xiong Liu, Luca A Finelli, Greg L Hersch, Iya Khalil"
160,TrialEnroll: Predicting Clinical Trial Enrollment Success with Deep & Cross Network and Large Language Models,"Clinical trials need to recruit a sufficient number of volunteer patients to
demonstrate the statistical power of the treatment (e.g., a new drug) in curing
a certain disease. Clinical trial recruitment has a significant impact on trial
success. Forecasting whether the recruitment process would be successful before
we run the trial would save many resources and time. This paper develops a
novel deep & cross network with large language model (LLM)-augmented text
feature that learns semantic information from trial eligibility criteria and
predicts enrollment success. The proposed method enables interpretability by
understanding which sentence/word in eligibility criteria contributes heavily
to prediction. We also demonstrate the empirical superiority of the proposed
method (0.7002 PR-AUC) over a bunch of well-established machine learning
methods. The code and curated dataset are publicly available at
https://anonymous.4open.science/r/TrialEnroll-7E12.",http://arxiv.org/abs/2407.13115v1,"Ling Yue, Sixue Xing, Jintai Chen, Tianfan Fu"
161,Unveiling Trail Making Test: Visual and manual trajectories indexing multiple executive processes,"The Trail Making Test (TMT) is one of the most popular neuropsychological
tests in the clinical assessment of executive functions (EF) and research in a
wide range of clinical conditions. In addition to its sensitivity to executive
dysfunction, the TMT presents several strengths: it is simple and intuitive, it
is easy to understand for patients, and has a short administration. However, it
has important limitations. First, the underlying EFs articulated during the
task are not well discriminated, which makes it a test with low specificity.
Second, the traditional pen-and-paper version presents one trial per condition
which introduces high variability. Third, only the total time is quantified,
which does not allow for a detailed analysis. Fourth, it has a fixed spatial
configuration per condition. In the present study we designed a computerized
version of the TMT (cTMT) to overcome its main limitations. Eye and hand
positions are simultaneously measured with high resolution, several trials are
acquired, and spatial configuration of the targets is controlled. Our results
showed a very similar performance profile compared to the traditional TMT.
Moreover, it revealed similarities and differences in eye movements between the
two parts of the task. Most importantly, we found an internal working memory
measure of the cTMT based on hand and eye movements that showed an association
to a validated working memory task. Additionally, we found another internal
measure of the TMT, also based on hand and eye movements, that we propose as a
potential marker of inhibitory control. Our results showed that executive
functions can be studied in more detail using traditional tests combined with
powerful digital setups. Finally, our study paved the way for a detailed
analysis of other complex tasks used for clinical evaluation, providing a
deeper understanding of the processes underlying its resolution.",http://arxiv.org/abs/2109.15255v1,"Ignacio Linari, Gustavo Juantorena, Agustin Ibaez, Agustin Petroni, Juan E Kamienkowski"
162,"Boundary crossing Random Walks, clinical trials and multinomial sequential estimation","A sufficient condition for the uniqueness of multinomial sequential unbiased
estimators is provided generalizing a classical result for binomial samples.
Unbiased estimators are applied to infer the parameters of multidimensional or
multinomial Random Walks which are observed until they reach a boundary. An
application to clinical trials is presented.",http://arxiv.org/abs/1101.4038v2,"Enrico Bibbona, Alessandro Rubba"
163,"Discussion of ""Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues"" by W. DuMouchel","Discussion of ""Multivariate Bayesian Logistic Regression for Analysis of
Clinical Trial Safety Issues"" by W. DuMouchel [arXiv:1210.0385].",http://arxiv.org/abs/1210.0655v1,"Bradley W McEvoy, Ram C Tiwari"
164,"Discussion of ""Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues"" by W. DuMouchel","Discussion of ""Multivariate Bayesian Logistic Regression for Analysis of
Clinical Trial Safety Issues"" by W. DuMouchel [arXiv:1210.0385].",http://arxiv.org/abs/1210.0658v1,Don Berry
165,"Rejoinder to ""Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues""","Rejoinder to ""Multivariate Bayesian Logistic Regression for Analysis of
Clinical Trial Safety Issues"" by W. DuMouchel [arXiv:1210.0385].",http://arxiv.org/abs/1210.0669v1,William DuMouchel
166,A generic rule-based system for clinical trial patient selection,"The n2c2 2018 Challenge task 1 aimed to identify patients who meet lists of
heterogeneous inclusion/exclusion criteria for a hypothetical clinical trial.
We demonstrate a generic rule-based natural language pipeline can support this
task with decent performance (the average F1 score on the test set is 0.89,
ranked the 8th out of 45 teams ).",http://arxiv.org/abs/1907.06860v1,"Jianlin Shi, Kevin Graves, John F Hurdle"
167,[Invited Discussion] Randomization Tests to Address Disruptions in Clinical Trials: A Report from the NISS Ingram Olkin Forum Series on Unplanned Clinical Trial Disruptions,"Disruptions in clinical trials may be due to external events like pandemics,
warfare, and natural disasters. Resulting complications may lead to unforeseen
intercurrent events (events that occur after treatment initiation and affect
the interpretation of the clinical question of interest or the existence of the
measurements associated with it). In Uschner et al. (2023), several example
clinical trial disruptions are described: treatment effect drift, population
shift, change of care, change of data collection, and change of availability of
study medication. A complex randomized controlled trial (RCT) setting with
(planned or unplanned) intercurrent events is then described, and randomization
tests are presented as a means for non-parametric inference that is robust to
violations of assumption typically made in clinical trials. While estimation
methods like Targeted Learning (TL) are valid in such settings, we do not see
where the authors make the case that one should be going for a randomization
test in such disrupted RCTs. In this discussion, we comment on the
appropriateness of TL and the accompanying TL Roadmap in the context of
disrupted clinical trials. We highlight a few key articles related to the broad
applicability of TL for RCTs and real-world data (RWD) analyses with
intercurrent events. We begin by introducing TL and motivating its utility in
Section 2, and then in Section 3 we provide a brief overview of the TL Roadmap.
In Section 4 we recite the example clinical trial disruptions presented in
Uschner et al. (2023), discussing considerations and solutions based on the
principles of TL. We request in an authors' rejoinder a clear theoretical
demonstration with specific examples in this setting that a randomization test
is the only valid inferential method relative to one based on following the TL
Roadmap.",http://arxiv.org/abs/2408.09060v1,"Rachael V Phillips, Mark J van der Laan"
168,Substantial Doubt Remains about the Efficacy of Anti-Amyloid Antibodies,"Alzheimer's disease (AD) is a prevalent, progressive, and ultimately fatal
neurodegenerative disorder that is defined pathologically by the accumulation
of amyloid plaques and tau neurofibrillary tangles in the brain. There remains
an unmet need for therapies that can halt or slow the course of AD. To address
this need, the FDA has provided a mechanism, under its Accelerated Approval
pathway, for potential therapeutics to be approved based in part on their
ability to reduce brain amyloid. Through this pathway, two monoclonal
anti-amyloid antibodies, aducanumab and lecanemab, have been approved for
clinical use. More recently, another amyloid-lowering antibody, donanemab,
generated a statistically significant outcome in a phase 3 clinical trial and
will shortly come under FDA review. While these monoclonal antibodies are not
yet routinely used in clinical practice, the series of recent positive clinical
trials has fostered enthusiasm amongst some AD experts. Here, we discuss three
key limitations regarding recent anti-amyloid clinical trials: (1) there is
little to no evidence that amyloid reduction correlates with clinical outcome,
(2) the reported efficacy of anti-amyloid therapies may be partly, or wholly,
explained by functional unblinding, and (3) donanemab in its phase 3 trial had
no effect on tau burden, the pathological hallmark more closely related to
cognition. Taken together, these observations call into question the efficacy
of anti-amyloid therapies.",http://arxiv.org/abs/2310.15456v2,"Leonardino A Digma MD, Joseph R Winer PhD, Michael D Greicius MD"
169,Anytime-valid inference in N-of-1 trials,"App-based N-of-1 trials offer a scalable experimental design for assessing
the effects of health interventions at an individual level. Their practical
success depends on the strong motivation of participants, which, in turn,
translates into high adherence and reduced loss to follow-up. One way to
maintain participant engagement is by sharing their interim results.
Continuously testing hypotheses during a trial, known as ""peeking"", can also
lead to shorter, lower-risk trials by detecting strong effects early.
Nevertheless, traditionally, results are only presented upon the trial's
conclusion. In this work, we introduce a potential outcomes framework that
permits interim peeking of the results and enables statistically valid
inferences to be drawn at any point during N-of-1 trials. Our work builds on
the growing literature on valid confidence sequences, which enables
anytime-valid inference with uniform type-1 error guarantees over time. We
propose several causal estimands for treatment effects applicable in an N-of-1
trial and demonstrate, through empirical evaluation, that the proposed approach
results in valid confidence sequences over time. We anticipate that
incorporating anytime-valid inference into clinical trials can significantly
enhance trial participation and empower participants.",http://arxiv.org/abs/2309.07353v1,"Ivana Malenica, Yongyi Guo, Kyra Gan, Stefan Konigorski"
170,Bayesian leveraging of historical control data for a clinical trial with time-to-event endpoint,"The recent 21st Century Cures Act propagates innovations to accelerate the
discovery, development, and delivery of 21st century cures. It includes the
broader application of Bayesian statistics and the use of evidence from
clinical expertise. An example of the latter is the use of trial-external (or
historical) data, which promises more efficient or ethical trial designs. We
propose a Bayesian meta-analytic approach to leveraging historical data for
time-to-event endpoints, which are common in oncology and cardiovascular
diseases. The approach is based on a robust hierarchical model for piecewise
exponential data. It allows for various degrees of between trial-heterogeneity
and for leveraging individual as well as aggregate data. An ovarian carcinoma
trial and a non-small-cell cancer trial illustrate methodological and practical
aspects of leveraging historical data for the analysis and design of
time-to-event trials.",http://arxiv.org/abs/1908.07265v2,"Satrajit Roychoudhury, Beat Neuenschwander"
171,Trialstreamer: Mapping and Browsing Medical Evidence in Real-Time,"We introduce Trialstreamer, a living database of clinical trial reports. Here
we mainly describe the evidence extraction component; this extracts from
biomedical abstracts key pieces of information that clinicians need when
appraising the literature, and also the relations between these. Specifically,
the system extracts descriptions of trial participants, the treatments compared
in each arm (the interventions), and which outcomes were measured. The system
then attempts to infer which interventions were reported to work best by
determining their relationship with identified trial outcome measures. In
addition to summarizing individual trials, these extracted data elements allow
automatic synthesis of results across many trials on the same topic. We apply
the system at scale to all reports of randomized controlled trials indexed in
MEDLINE, powering the automatic generation of evidence maps, which provide a
global view of the efficacy of different interventions combining data from all
relevant clinical trials on a topic. We make all code and models freely
available alongside a demonstration of the web interface.",http://arxiv.org/abs/2005.10865v1,"Benjamin E Nye, Ani Nenkova, Iain J Marshall, Byron C Wallace"
172,Inference for natural mediation effects under case-cohort sampling with applications in identifying COVID-19 vaccine correlates of protection,"Combating the SARS-CoV2 pandemic will require the fast development of
effective preventive vaccines. Regulatory agencies may open accelerated
approval pathways for vaccines if an immunological marker can be established as
a mediator of a vaccine's protection. A rich source of information for
identifying such correlates are large-scale efficacy trials of COVID-19
vaccines, where immune responses are measured subject to a case-cohort sampling
design. We propose two approaches to estimation of mediation parameters in the
context of case-cohort sampling designs. We establish the theoretical
large-sample efficiency of our proposed estimators and evaluate them in a
realistic simulation to understand whether they can be employed in the analysis
of COVID-19 vaccine efficacy trials.",http://arxiv.org/abs/2103.02643v1,"David Benkeser, Ivn Daz, Jialu Ran"
173,Clinical Evidence Engine: Proof-of-Concept For A Clinical-Domain-Agnostic Decision Support Infrastructure,"Abstruse learning algorithms and complex datasets increasingly characterize
modern clinical decision support systems (CDSS). As a result, clinicians cannot
easily or rapidly scrutinize the CDSS recommendation when facing a difficult
diagnosis or treatment decision in practice. Over-trust or under-trust are
frequent. Prior research has explored supporting such assessments by explaining
DST data inputs and algorithmic mechanisms. This paper explores a different
approach: Providing precisely relevant, scientific evidence from biomedical
literature. We present a proof-of-concept system, Clinical Evidence Engine, to
demonstrate the technical and design feasibility of this approach across three
domains (cardiovascular diseases, autism, cancer). Leveraging Clinical BioBERT,
the system can effectively identify clinical trial reports based on lengthy
clinical questions (e.g., ""risks of catheter infection among adult patients in
intensive care unit who require arterial catheters, if treated with povidone
iodine-alcohol""). This capability enables the system to identify clinical
trials relevant to diagnostic/treatment hypotheses -- a clinician's or a
CDSS's. Further, Clinical Evidence Engine can identify key parts of a clinical
trial abstract, including patient population (e.g., adult patients in intensive
care unit who require arterial catheters), intervention (povidone
iodine-alcohol), and outcome (risks of catheter infection). This capability
opens up the possibility of enabling clinicians to 1) rapidly determine the
match between a clinical trial and a clinical question, and 2) understand the
result and contexts of the trial without extensive reading. We demonstrate this
potential by illustrating two example use scenarios of the system. We discuss
the idea of designing DST explanations not as specific to a DST or an
algorithm, but as a domain-agnostic decision support infrastructure.",http://arxiv.org/abs/2111.00621v1,"Bojian Hou, Hao Zhang, Gur Ladizhinsky, Gur Ladizhinsky, Stephen Yang, Volodymyr Kuleshov, Fei Wang, Qian Yang"
174,Doctor2Vec: Dynamic Doctor Representation Learning for Clinical Trial Recruitment,"Massive electronic health records (EHRs) enable the success of learning
accurate patient representations to support various predictive health
applications. In contrast, doctor representation was not well studied despite
that doctors play pivotal roles in healthcare. How to construct the right
doctor representations? How to use doctor representation to solve important
health analytic problems? In this work, we study the problem on {\it clinical
trial recruitment}, which is about identifying the right doctors to help
conduct the trials based on the trial description and patient EHR data of those
doctors. We propose doctor2vec which simultaneously learns 1) doctor
representations from EHR data and 2) trial representations from the description
and categorical information about the trials. In particular, doctor2vec
utilizes a dynamic memory network where the doctor's experience with patients
are stored in the memory bank and the network will dynamically assign weights
based on the trial representation via an attention mechanism. Validated on
large real-world trials and EHR data including 2,609 trials, 25K doctors and
430K patients, doctor2vec demonstrated improved performance over the best
baseline by up to $8.7\%$ in PR-AUC. We also demonstrated that the doctor2vec
embedding can be transferred to benefit data insufficiency settings including
trial recruitment in less populated/newly explored country with $13.7\%$
improvement or for rare diseases with $8.1\%$ improvement in PR-AUC.",http://arxiv.org/abs/1911.10395v1,"Siddharth Biswal, Cao Xiao, Lucas M Glass, Elizabeth Milkovits, Jimeng Sun"
175,COMPOSE: Cross-Modal Pseudo-Siamese Network for Patient Trial Matching,"Clinical trials play important roles in drug development but often suffer
from expensive, inaccurate and insufficient patient recruitment. The
availability of massive electronic health records (EHR) data and trial
eligibility criteria (EC) bring a new opportunity to data driven patient
recruitment. One key task named patient-trial matching is to find qualified
patients for clinical trials given structured EHR and unstructured EC text
(both inclusion and exclusion criteria). How to match complex EC text with
longitudinal patient EHRs? How to embed many-to-many relationships between
patients and trials? How to explicitly handle the difference between inclusion
and exclusion criteria? In this paper, we proposed CrOss-Modal PseudO-SiamEse
network (COMPOSE) to address these challenges for patient-trial matching. One
path of the network encodes EC using convolutional highway network. The other
path processes EHR with multi-granularity memory network that encodes
structured patient records into multiple levels based on medical ontology.
Using the EC embedding as query, COMPOSE performs attentional record alignment
and thus enables dynamic patient-trial matching. COMPOSE also introduces a
composite loss term to maximize the similarity between patient records and
inclusion criteria while minimize the similarity to the exclusion criteria.
Experiment results show COMPOSE can reach 98.0% AUC on patient-criteria
matching and 83.7% accuracy on patient-trial matching, which leads 24.3%
improvement over the best baseline on real-world patient-trial matching tasks.",http://arxiv.org/abs/2006.08765v1,"Junyi Gao, Cao Xiao, Lucas M Glass, Jimeng Sun"
176,Mondrian Processes for Flow Cytometry Analysis,"Analysis of flow cytometry data is an essential tool for clinical diagnosis
of hematological and immunological conditions. Current clinical workflows rely
on a manual process called gating to classify cells into their canonical types.
This dependence on human annotation limits the rate, reproducibility, and
complexity of flow cytometry analysis. In this paper, we propose using Mondrian
processes to perform automated gating by incorporating prior information of the
kind used by gating technicians. The method segments cells into types via
Bayesian nonparametric trees. Examining the posterior over trees allows for
interpretable visualizations and uncertainty quantification - two vital
qualities for implementation in clinical practice.",http://arxiv.org/abs/1711.07673v2,"Disi Ji, Eric Nalisnick, Padhraic Smyth"
177,Statistical Neuroscience in the Single Trial Limit,"Individual neurons often produce highly variable responses over nominally
identical trials, reflecting a mixture of intrinsic ""noise"" and systematic
changes in the animal's cognitive and behavioral state. Disentangling these
sources of variability is of great scientific interest in its own right, but it
is also increasingly inescapable as neuroscientists aspire to study more
complex and naturalistic animal behaviors. In these settings, behavioral
actions never repeat themselves exactly and may rarely do so even
approximately. Thus, new statistical methods that extract reliable features of
neural activity using few, if any, repeated trials are needed. Accurate
statistical modeling in this severely trial-limited regime is challenging, but
still possible if simplifying structure in neural data can be exploited. We
review recent works that have identified different forms of simplifying
structure -- including shared gain modulations across neural subpopulations,
temporal smoothness in neural firing rates, and correlations in responses
across behavioral conditions -- and exploited them to reveal novel insights
into the trial-by-trial operation of neural circuits.",http://arxiv.org/abs/2103.05075v2,"Alex H Williams, Scott W Linderman"
178,Bayesian Information Criterion for Event-based Multi-trial Ensemble data,"Transient recurring phenomena are ubiquitous in many scientific fields like
neuroscience and meteorology. Time inhomogenous Vector Autoregressive Models
(VAR) may be used to characterize peri-event system dynamics associated with
such phenomena, and can be learned by exploiting multi-dimensional data
gathering samples of the evolution of the system in multiple time windows
comprising, each associated with one occurrence of the transient phenomenon,
that we will call ""trial"". However, optimal VAR model order selection methods,
commonly relying on the Akaike or Bayesian Information Criteria (AIC/BIC), are
typically not designed for multi-trial data. Here we derive the BIC methods for
multi-trial ensemble data which are gathered after the detection of the events.
We show using simulated bivariate AR models that the multi-trial BIC is able to
recover the real model order. We also demonstrate with simulated transient
events and real data that the multi-trial BIC is able to estimate a
sufficiently small model order for dynamic system modeling.",http://arxiv.org/abs/2204.14096v1,"Kaidi Shao, Nikos K Logothetis, Michel Besserve"
179,Adding flexibility to clinical trial designs: an example-based guide to the practical use of adaptive designs,"Adaptive designs for clinical trials permit alterations to a study in
response to accumulating data in order to make trials more flexible, ethical
and efficient. These benefits are achieved while preserving the integrity and
validity of the trial, through the pre-specification and proper adjustment for
the possible alterations during the course of the trial. Despite much research
in the statistical literature highlighting the potential advantages of adaptive
designs over traditional fixed designs, the uptake of such methods in clinical
research has been slow. One major reason for this is that different adaptations
to trial designs, as well as their advantages and limitations, remain
unfamiliar to large parts of the clinical community. The aim of this paper is
to clarify where adaptive designs can be used to address specific questions of
scientific interest; we introduce the main features of adaptive designs and
commonly used terminology, highlighting their utility and pitfalls, and
illustrate their use through case studies of adaptive trials ranging from
early-phase dose escalation to confirmatory Phase III studies.",http://arxiv.org/abs/2006.12811v1,"Thomas Burnett, Pavel Mozgunov, Philip Pallmann, Sofia S Villar, Graham M Wheeler, Thomas Jaki"
180,Estimands and their Estimators for Clinical Trials Impacted by the COVID-19 Pandemic: A Report from the NISS Ingram Olkin Forum Series on Unplanned Clinical Trial Disruptions,"The COVID-19 pandemic continues to affect the conduct of clinical trials
globally. Complications may arise from pandemic-related operational challenges
such as site closures, travel limitations and interruptions to the supply chain
for the investigational product, or from health-related challenges such as
COVID-19 infections. Some of these complications lead to unforeseen
intercurrent events in the sense that they affect either the interpretation or
the existence of the measurements associated with the clinical question of
interest. In this article, we demonstrate how the ICH E9(R1) Addendum on
estimands and sensitivity analyses provides a rigorous basis to discuss
potential pandemic-related trial disruptions and to embed these disruptions in
the context of study objectives and design elements. We introduce several
hypothetical estimand strategies and review various causal inference and
missing data methods, as well as a statistical method that combines unbiased
and possibly biased estimators for estimation. To illustrate, we describe the
features of a stylized trial, and how it may have been impacted by the
pandemic. This stylized trial will then be re-visited by discussing the changes
to the estimand and the estimator to account for pandemic disruptions. Finally,
we outline considerations for designing future trials in the context of
unforeseen disruptions.",http://arxiv.org/abs/2202.03531v1,"Kelly Van Lancker, Sergey Tarima, Jonathan Bartlett, Madeline Bauer, Bharani BharaniDharan, Frank Bretz, Nancy Flournoy, Hege Michiels, Camila Olarte Parra, James L Rosenberger, Suzie Cro"
181,Decentralized Clinical Trials in the Era of Real-World Evidence: A Statistical Perspective,"There has been a growing trend that activities relating to clinical trials
take place at locations other than traditional trial sites (hence decentralized
clinical trials or DCTs), some of which are at settings of real-world clinical
practice. Although there are numerous benefits of DCTs, this also brings some
implications on a number of issues relating to the design, conduct, and
analysis of DCTs. The Real-World Evidence Scientific Working Group of the
American Statistical Association Biopharmaceutical Section has been reviewing
the field of DCTs and provides in this paper considerations for decentralized
trials from a statistical perspective. This paper first discusses selected
critical decentralized elements that may have statistical implications on the
trial and then summarizes regulatory guidance, framework, and initiatives on
DCTs. More discussions are presented by focusing on the design (including
construction of estimand), implementation, statistical analysis plan (including
missing data handling), and reporting of safety events. Some additional
considerations (e.g., ethical considerations, technology infrastructure, study
oversight, data security and privacy, and regulatory compliance) are also
briefly discussed. This paper is intended to provide statistical considerations
for decentralized trials of medical products to support regulatory
decision-making.",http://arxiv.org/abs/2410.06591v1,"Jie Chen, Junrui Di, Nadia Daizadeh, Ying Lu, Hongwei Wang, YuanLi Shen, Jennifer Kirk, Frank W Rockhold, Herbert Pang, Jing Zhao, Weili He, Andrew Potter, Hana Lee"
182,An Information Extraction Approach to Prescreen Heart Failure Patients for Clinical Trials,"To reduce the large amount of time spent screening, identifying, and
recruiting patients into clinical trials, we need prescreening systems that are
able to automate the data extraction and decision-making tasks that are
typically relegated to clinical research study coordinators. However, a major
obstacle is the vast amount of patient data available as unstructured free-form
text in electronic health records. Here we propose an information
extraction-based approach that first automatically converts unstructured text
into a structured form. The structured data are then compared against a list of
eligibility criteria using a rule-based system to determine which patients
qualify for enrollment in a heart failure clinical trial. We show that we can
achieve highly accurate results, with recall and precision values of 0.95 and
0.86, respectively. Our system allowed us to significantly reduce the time
needed for prescreening patients from a few weeks to a few minutes. Our
open-source information extraction modules are available for researchers and
could be tested and validated in other cardiovascular trials. An approach such
as the one we demonstrate here may decrease costs and expedite clinical trials,
and could enhance the reproducibility of trials across institutions and
populations.",http://arxiv.org/abs/1609.01594v1,"Abhishek Kalyan Adupa, Ravi Prakash Garg, Jessica CoronaCox, Sanjiv J Shah, Siddhartha R Jonnalagadda"
183,Automating the Compilation of Potential Core-Outcomes for Clinical Trials,"Due to increased access to clinical trial outcomes and analysis, researchers
and scientists are able to iterate or improve upon relevant approaches more
effectively. However, the metrics and related results of clinical trials
typically do not follow any standardization in their reports, making it more
difficult for researchers to parse the results of different trials. The
objective of this paper is to describe an automated method utilizing natural
language processing in order to describe the probable core outcomes of clinical
trials, in order to alleviate the issues around disparate clinical trial
outcomes. As the nature of this process is domain specific, BioBERT was
employed in order to conduct a multi-class entity normalization task. In
addition to BioBERT, an unsupervised feature-based approach making use of only
the encoder output embedding representations for the outcomes and labels was
utilized. Finally, cosine similarity was calculated across the vectors to
obtain the semantic similarity. This method was able to both harness the
domain-specific context of each of the tokens from the learned embeddings of
the BioBERT model as well as a more stable metric of sentence similarity. Some
common outcomes identified using the Jaccard similarity in each of the
classifications were compiled, and while some are untenable, a pipeline for
which this automation process could be conducted was established.",http://arxiv.org/abs/2101.04076v1,"Shwetha Bharadwaj, Melanie Laffin"
184,HeCiX: Integrating Knowledge Graphs and Large Language Models for Biomedical Research,"Despite advancements in drug development strategies, 90% of clinical trials
fail. This suggests overlooked aspects in target validation and drug
optimization. In order to address this, we introduce HeCiX-KG,
Hetionet-Clinicaltrials neXus Knowledge Graph, a novel fusion of data from
ClinicalTrials.gov and Hetionet in a single knowledge graph. HeCiX-KG combines
data on previously conducted clinical trials from ClinicalTrials.gov, and
domain expertise on diseases and genes from Hetionet. This offers a thorough
resource for clinical researchers. Further, we introduce HeCiX, a system that
uses LangChain to integrate HeCiX-KG with GPT-4, and increase its usability.
HeCiX shows high performance during evaluation against a range of clinically
relevant issues, proving this model to be promising for enhancing the
effectiveness of clinical research. Thus, this approach provides a more
holistic view of clinical trials and existing biological data.",http://arxiv.org/abs/2407.14030v1,"Prerana Sanjay Kulkarni, Muskaan Jain, Disha Sheshanarayana, Srinivasan Parthiban"
185,Distribution-Based Sub-Population Selection (DSPS): A Method for in-Silico Reproduction of Clinical Trials Outcomes,"Background and Objective: Diabetes presents a significant challenge to
healthcare due to the negative impact of poor blood sugar control on health and
associated complications. Computer simulation platforms, notably exemplified by
the UVA/Padova Type 1 Diabetes simulator, has emerged as a promising tool for
advancing diabetes treatments by simulating patient responses in a virtual
environment. The UVA Virtual Lab (UVLab) is a new simulation platform to mimic
the metabolic behavior of people with Type 2 diabetes (T2D) with a large
population of 6062 virtual subjects. Methods: The work introduces the
Distribution-Based Population Selection (DSPS) method, a systematic approach to
identifying virtual subsets that mimic the clinical behavior observed in real
trials. The method transforms the sub-population selection task into a Linear
Programing problem, enabling the identification of the largest representative
virtual cohort. This selection process centers on key clinical outcomes in
diabetes research, such as HbA1c and Fasting plasma Glucose (FPG), ensuring
that the statistical properties (moments) of the selected virtual
sub-population closely resemble those observed in real-word clinical trial.
Results: DSPS method was applied to the insulin degludec (IDeg) arm of a phase
3 clinical trial. This method was used to select a sub-population of virtual
subjects that closely mirrored the clinical trial data across multiple key
metrics, including glycemic efficacy, insulin dosages, and cumulative
hypoglycemia events over a 26-week period. Conclusion: The DSPS algorithm is
able to select virtual sub-population within UVLab to reproduce and predict the
outcomes of a clinical trial. This statistical method can bridge the gap
between large population simulation platforms and previously conducted clinical
trials.",http://arxiv.org/abs/2409.00232v2,"Mohammadreza Ganji, Anas El Fathi, Chiara Fabris, Dayu Lv, Boris Kovatchev, Marc Breton"
186,SynRL: Aligning Synthetic Clinical Trial Data with Human-preferred Clinical Endpoints Using Reinforcement Learning,"Each year, hundreds of clinical trials are conducted to evaluate new medical
interventions, but sharing patient records from these trials with other
institutions can be challenging due to privacy concerns and federal
regulations. To help mitigate privacy concerns, researchers have proposed
methods for generating synthetic patient data. However, existing approaches for
generating synthetic clinical trial data disregard the usage requirements of
these data, including maintaining specific properties of clinical outcomes, and
only use post hoc assessments that are not coupled with the data generation
process. In this paper, we propose SynRL which leverages reinforcement learning
to improve the performance of patient data generators by customizing the
generated data to meet the user-specified requirements for synthetic data
outcomes and endpoints. Our method includes a data value critic function to
evaluate the quality of the generated data and uses reinforcement learning to
align the data generator with the users' needs based on the critic's feedback.
We performed experiments on four clinical trial datasets and demonstrated the
advantages of SynRL in improving the quality of the generated synthetic data
while keeping the privacy risks low. We also show that SynRL can be utilized as
a general framework that can customize data generation of multiple types of
synthetic data generators. Our code is available at
https://anonymous.4open.science/r/SynRL-DB0F/.",http://arxiv.org/abs/2411.07317v2,"Trisha Das, Zifeng Wang, Afrah Shafquat, Mandis Beigi, Jason Mezey, Jacob Aptekar, Jimeng Sun"
187,Validating GAN-BioBERT: A Methodology For Assessing Reporting Trends In Clinical Trials,"In the past decade, there has been much discussion about the issue of biased
reporting in clinical research. Despite this attention, there have been limited
tools developed for the systematic assessment of qualitative statements made in
clinical research, with most studies assessing qualitative statements relying
on the use of manual expert raters, which limits their size. Also, previous
attempts to develop larger scale tools, such as those using natural language
processing, were limited by both their accuracy and the number of categories
used for the classification of their findings. With these limitations in mind,
this study's goal was to develop a classification algorithm that was both
suitably accurate and finely grained to be applied on a large scale for
assessing the qualitative sentiment expressed in clinical trial abstracts.
Additionally, this study seeks to compare the performance of the proposed
algorithm, GAN-BioBERT, to previous studies as well as to expert manual rating
of clinical trial abstracts. This study develops a three-class sentiment
classification algorithm for clinical trial abstracts using a semi-supervised
natural language process model based on the Bidirectional Encoder
Representation from Transformers (BERT) model, from a series of clinical trial
abstracts annotated by a group of experts in academic medicine. Results: The
use of this algorithm was found to have a classification accuracy of 91.3%,
with a macro F1-Score of 0.92, which is a significant improvement in accuracy
when compared to previous methods and expert ratings, while also making the
sentiment classification finer grained than previous studies. The proposed
algorithm, GAN-BioBERT, is a suitable classification model for the large-scale
assessment of qualitative statements in clinical trial literature, providing an
accurate, reproducible tool for the large-scale study of clinical publication
trends.",http://arxiv.org/abs/2106.00665v1,"Joshua J Myszewski, Emily Klossowski, Patrick Meyer, Kristin Bevil, Lisa Klesius, Kristopher M Schroeder"
188,A shared latent space matrix factorisation method for recommending new trial evidence for systematic review updates,"Clinical trial registries can be used to monitor the production of trial
evidence and signal when systematic reviews become out of date. However, this
use has been limited to date due to the extensive manual review required to
search for and screen relevant trial registrations. Our aim was to evaluate a
new method that could partially automate the identification of trial
registrations that may be relevant for systematic review updates. We identified
179 systematic reviews of drug interventions for type 2 diabetes, which
included 537 clinical trials that had registrations in ClinicalTrials.gov. We
tested a matrix factorisation approach that uses a shared latent space to learn
how to rank relevant trial registrations for each systematic review, comparing
the performance to document similarity to rank relevant trial registrations.
The two approaches were tested on a holdout set of the newest trials from the
set of type 2 diabetes systematic reviews and an unseen set of 141 clinical
trial registrations from 17 updated systematic reviews published in the
Cochrane Database of Systematic Reviews. The matrix factorisation approach
outperformed the document similarity approach with a median rank of 59 and
recall@100 of 60.9%, compared to a median rank of 138 and recall@100 of 42.8%
in the document similarity baseline. In the second set of systematic reviews
and their updates, the highest performing approach used document similarity and
gave a median rank of 67 (recall@100 of 62.9%). The proposed method was useful
for ranking trial registrations to reduce the manual workload associated with
finding relevant trials for systematic review updates. The results suggest that
the approach could be used as part of a semi-automated pipeline for monitoring
potentially new evidence for inclusion in a review update.",http://arxiv.org/abs/1709.06758v4,"Didi Surian, Adam G Dunn, Liat Orenstein, Rabia Bashir, Enrico Coiera, Florence T Bourgeois"
189,Quantifying the Diaspora of Knowledge in the Last Century,"Academic research is driven by several factors causing different disciplines
to act as ""sources"" or ""sinks"" of knowledge. However, how the flow of authors'
research interests -- a proxy of human knowledge -- evolved across time is
still poorly understood. Here, we build a comprehensive map of such flows
across one century, revealing fundamental periods in the raise of interest in
areas of human knowledge. We identify and quantify the most attractive topics
over time, when a relatively significant number of researchers moved from their
original area to another one, causing what we call a ""diaspora of the
knowledge"" towards sinks of scientific interest, and we relate these points to
crucial historical and political events. Noticeably, only a few areas -- like
Medicine, Physics or Chemistry -- mainly act as sources of the diaspora,
whereas areas like Material Science, Chemical Engineering, Neuroscience,
Immunology and Microbiology or Environmental Science behave like sinks.",http://arxiv.org/abs/1604.00696v1,"Manlio De Domenico, Elisa Omodei, Alex Arenas"
190,"Hegemonic structure of basic, clinical and patented knowledge on Ebola research: a US army reductionist initiative","Background: In this paper, we present an approach to understand how the
basic, clinical and patent knowledge on Ebola is organized and
intercommunicated and what leading factor could be shaping the evolution of the
knowledge translation process for this disease. Methodology: A combination of
citation network analysis; analysis of Medical heading Subject (MeSH) and Gene
Ontology (GO) terms, and quantitative content analysis for patents and
scientific literature, aimed to map the organization of Ebola research was
carried out. Results: We found six putative research fronts (i.e. clusters of
high interconnected papers). Three research fronts are basic research on Ebola
virus structural proteins: glycoprotein, VP40 and VP35, respectively. There is
a fourth research front of basic research papers on pathogenesis, which is the
organizing hub of Ebola research. A fifth research front is pre-clinical
research focused on vaccines and glycoproteins. Finally, a
clinical-epidemiology research front related to the disease outbreaks was
identified. The network structure of patent families shows that the dominant
design is the use of Ebola virus proteins as targets of vaccines and other
immunological treatments. Therefore, patents network organization resembles the
organization of the scientific literature. Specifically, the knowledge on Ebola
would flow from higher (clinical-epidemiology) to intermediated
(cellular-tissular pathogenesis) to lower (molecular interactions) levels of
organization. Conclusion: Our results suggest a strong reductionist approach
for Ebola research probably influenced by the lethality of the disease. On the
other hand, the ownership profile of the patent families network and the main
researches relationship with the United State Army suggest a strong involvement
of this military institution in Ebola research.",http://arxiv.org/abs/1607.08479v1,"David FajardoOrtiz, Josw OrtegaSanchezdeTagle, VictorM Castano"
191,Primary analysis method for incomplete CD4 count data from IMPI trial and other trials with similar setting,"The National Research Council panel on prevention and treatment of missing
data in clinical trials recommends that primary analysis methods are carefully
selected before appropriate sensitivity analysis methods can be chosen. In this
paper, we recommend an appropriate primary analysis method for handling CD4
count data from the IMPI trial and trials with similar settings. The estimand
of interest in the IMPI trial is the effectiveness estimand hypothesis. We
discussed, compared, and contrasted results from complete case analysis and
simple imputation methods, with the direct-likelihood and multiple imputation
methods. The simple imputation methods produced biased estimates of treatment
effect. However, the maximum likelihood and the multiple imputation methods
produced consistent estimates of treatment effect. The maximum likelihood or
the multiple imputation approaches produced unbiased and consistent estimates.
Therefore, either the maximum likelihood or the multiple imputation methods,
under the assumption that the data are missing at random can be considered as
primary analysis method when one is considering sensitivity analysis to dropout
using the CD4 count data from the IMPI trial and other trials with similar
settings.",http://arxiv.org/abs/2105.03197v1,"AbdulKarim Iddrisu, Abukari Alhassan"
192,Applications of Information Theory to Analysis of Neural Data,"Information theory is a practical and theoretical framework developed for the
study of communication over noisy channels. Its probabilistic basis and
capacity to relate statistical structure to function make it ideally suited for
studying information flow in the nervous system. It has a number of useful
properties: it is a general measure sensitive to any relationship, not only
linear effects; it has meaningful units which in many cases allow direct
comparison between different experiments; and it can be used to study how much
information can be gained by observing neural responses in single trials,
rather than in averages over multiple trials. A variety of information
theoretic quantities are commonly used in neuroscience - (see entry
""Definitions of Information-Theoretic Quantities""). In this entry we review
some applications of information theory in neuroscience to study encoding of
information in both single neurons and neuronal populations.",http://arxiv.org/abs/1501.01860v1,"Simon R Schultz, Robin A A Ince, Stefano Panzeri"
193,Enhancing spatial auditory attention decoding with neuroscience-inspired prototype training,"The spatial auditory attention decoding (Sp-AAD) technology aims to determine
the direction of auditory attention in multi-talker scenarios via neural
recordings. Despite the success of recent Sp-AAD algorithms, their performance
is hindered by trial-specific features in EEG data. This study aims to improve
decoding performance against these features. Studies in neuroscience indicate
that spatial auditory attention can be reflected in the topological
distribution of EEG energy across different frequency bands. This insight
motivates us to propose Prototype Training, a neuroscience-inspired method for
Sp-AAD. This method constructs prototypes with enhanced energy distribution
representations and reduced trial-specific characteristics, enabling the model
to better capture auditory attention features. To implement prototype training,
an EEGWaveNet that employs the wavelet transform of EEG is further proposed.
Detailed experiments indicate that the EEGWaveNet with prototype training
outperforms other competitive models on various datasets, and the effectiveness
of the proposed method is also validated. As a training method independent of
model architecture, prototype training offers new insights into the field of
Sp-AAD.",http://arxiv.org/abs/2407.06498v1,"Zelin Qiu, Jianjun Gu, Dingding Yao, Junfeng Li"
194,Using instrumental variables to disentangle treatment and placebo effects in blinded and unblinded randomized clinical trials influenced by unmeasured confounders,"Clinical trials traditionally employ blinding as a design mechanism to reduce
the influence of placebo effects. In practice, however, it can be difficult or
impossible to blind study participants and unblinded trials are common in
medical research. Here we show how instrumental variables can be used to
quantify and disentangle treatment and placebo effects in randomized clinical
trials comparing control and active treatments in the presence of confounders.
The key idea is to use randomization to separately manipulate treatment
assignment and psychological encouragement messages that increase the
participants' desire for improved symptoms. The proposed approach is able to
improve the estimation of treatment effects in blinded studies and, most
importantly, opens the doors to account for placebo effects in unblinded
trials.",http://arxiv.org/abs/1606.04896v2,Elias Chaibub Neto
195,trialr: Bayesian Clinical Trial Designs in R and Stan,"This manuscript introduces an \proglang{R} package called \pkg{trialr} that
implements a collection of clinical trial methods in \proglang{Stan} and
\proglang{R}. In this article, we explore three methods in detail. The first is
the continual reassessment method for conducting phase I dose-finding trials
that seek a maximum tolerable dose. The second is EffTox, a dose-finding design
that scrutinises doses by joint efficacy and toxicity outcomes. The third is
the augmented binary method for modelling the probability of treatment success
in phase II oncology trials with reference to repeated measures of continuous
tumour size and binary indicators of treatment failure. We emphasise in this
article the benefits that stem from having access to posterior samples,
including flexible inference and powerful visualisation. We hope that this
package encourages the use of Bayesian methods in clinical trials.",http://arxiv.org/abs/1907.00161v1,Kristian Brock
196,Clinical trials impacted by the COVID-19 pandemic: Adaptive designs to the rescue?,"Very recently the new pathogen severe acute respiratory syndrome coronavirus
2 (SARS-CoV-2) was identified and the coronavirus disease 2019 (COVID-19)
declared a pandemic by the World Health Organization. The pandemic has a number
of consequences for the ongoing clinical trials in non-COVID-19 conditions.
Motivated by four currently ongoing clinical trials in a variety of disease
areas we illustrate the challenges faced by the pandemic and sketch out
possible solutions including adaptive designs. Guidance is provided on (i)
where blinded adaptations can help; (ii) how to achieve type I error rate
control, if required; (iii) how to deal with potential treatment effect
heterogeneity; (iv) how to utilize early readouts; and (v) how to utilize
Bayesian techniques. In more detail approaches to resizing a trial affected by
the pandemic are developed including considerations to stop a trial early, the
use of group-sequential designs or sample size adjustment. All methods
considered are implemented in a freely available R shiny app. Furthermore,
regulatory and operational issues including the role of data monitoring
committees are discussed.",http://arxiv.org/abs/2005.13979v1,"Cornelia Ursula Kunz, Silke Jrgens, Frank Bretz, Nigel Stallard, Kelly Van Lancker, Dong Xi, Sarah Zohar, Christoph Gerlinger, Tim Friede"
197,ITTC @ TREC 2021 Clinical Trials Track,"This paper describes the submissions of the Natural Language Processing (NLP)
team from the Australian Research Council Industrial Transformation Training
Centre (ITTC) for Cognitive Computing in Medical Technologies to the TREC 2021
Clinical Trials Track. The task focuses on the problem of matching eligible
clinical trials to topics constituting a summary of a patient's admission
notes. We explore different ways of representing trials and topics using NLP
techniques, and then use a common retrieval model to generate the ranked list
of relevant trials for each topic. The results from all our submitted runs are
well above the median scores for all topics, but there is still plenty of scope
for improvement.",http://arxiv.org/abs/2202.07858v1,"Thinh Hung Truong, Yulia Otmakhova, Rahmad Mahendra, Timothy Baldwin, Jey Han Lau, Trevor Cohn, Lawrence Cavedon, Damiano Spina, Karin Verspoor"
198,A matching design for augmenting a randomized clinical trial with external control,"The use of information from real world to assess the effectiveness of medical
products is becoming increasingly popular and more acceptable by regulatory
agencies. According to a strategic real-world evidence framework published by
U.S. Food and Drug Administration, a hybrid randomized controlled trial that
augments internal control arm with real-world data is a pragmatic approach
worth more attention. In this paper, we aim to improve on existing matching
designs for such a hybrid randomized controlled trial. In particular, we
propose to match the entire concurrent randomized clinical trial (RCT) such
that (1) the matched external control subjects used to augment the internal
control arm are as comparable as possible to the RCT population, (2) every
active treatment arm in an RCT with multiple treatments is compared with the
same control group, and (3) matching can be conducted and the matched set
locked before treatment unblinding to better maintain the data integrity.
Besides a weighted estimator, we also introduce a bootstrap method to obtain
its variance estimation. The finite sample performance of the proposed method
is evaluated by simulations based on data from a real clinical trial.",http://arxiv.org/abs/2203.10128v1,"Jianghao Li, Yu Du, Huayu Liu, Yanyao Yi"
199,Adaptive treatment allocation and selection in multi-arm clinical trials: a Bayesian perspective,"Clinical trials are an instrument for making informed decisions based on
evidence from well-designed experiments. Here we consider adaptive designs
mainly from the perspective of multi-arm Phase II clinical trials, in which one
or more experimental treatments are compared to a control. Treatment allocation
of individual trial participants is assumed to take place according to a fixed
block randomization, albeit with an important twist: The performance of each
treatment arm is assessed after every measured outcome, in terms of the
posterior distribution of a corresponding model parameter. Different treatments
arms are then compared to each other, according to pre-defined criteria and
using the joint posterior as the basis for such assessment. If a treatment is
found to be sufficiently clearly inferior to the currently best candidate, it
can be closed off either temporarily or permanently from further participant
accrual. The latter possibility provides a method for adaptive treatment
selection, including early stopping of the trial. The main development in the
paper is in terms of binary outcomes, but some extensions, notably for handling
time-to-event data, are discussed as well. The presentation is to a large
extent comparative and expository.",http://arxiv.org/abs/2104.03398v2,"Elja Arjas, Dario Gasbarra"
200,The Future will be Different than Today: Model Evaluation Considerations when Developing Translational Clinical Biomarker,"Finding translational biomarkers stands center stage of the future of
personalized medicine in healthcare. We observed notable challenges in
identifying robust biomarkers as some with great performance in one scenario
often fail to perform well in new trials (e.g. different population,
indications). With rapid development in the clinical trial world (e.g. assay,
disease definition), new trials very likely differ from legacy ones in many
perspectives and in development of biomarkers this heterogeneity should be
considered. In response, we recommend considering building in the heterogeneity
when evaluating biomarkers. In this paper, we present one evaluation strategy
by using leave-one-study-out (LOSO) in place of conventional cross-validation
(cv) methods to account for the potential heterogeneity across trials used for
building and testing the biomarkers. To demonstrate the performance of K-fold
vs LOSO cv in estimating the effect size of biomarkers, we leveraged data from
clinical trials and simulation studies. In our assessment, LOSO cv provided a
more objective estimate of the future performance. This conclusion remained
true across different evaluation metrics and different statistical methods.",http://arxiv.org/abs/2107.08787v1,"Yichen Lu, Jane Fridlyand, Tiffany Tang, Ting Qi, Noah Simon, Ning Leng"
201,Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching,"The process of matching patients with suitable clinical trials is essential
for advancing medical research and providing optimal care. However, current
approaches face challenges such as data standardization, ethical
considerations, and a lack of interoperability between Electronic Health
Records (EHRs) and clinical trial criteria. In this paper, we explore the
potential of large language models (LLMs) to address these challenges by
leveraging their advanced natural language generation capabilities to improve
compatibility between EHRs and clinical trial descriptions. We propose an
innovative privacy-aware data augmentation approach for LLM-based patient-trial
matching (LLM-PTM), which balances the benefits of LLMs while ensuring the
security and confidentiality of sensitive patient data. Our experiments
demonstrate a 7.32% average improvement in performance using the proposed
LLM-PTM method, and the generalizability to new data is improved by 12.12%.
Additionally, we present case studies to further illustrate the effectiveness
of our approach and provide a deeper understanding of its underlying
principles.",http://arxiv.org/abs/2303.16756v2,"Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, Xia Hu"
202,Using Bayesian Statistics in Confirmatory Clinical Trials in the Regulatory Setting,"Bayesian statistics plays a pivotal role in advancing medical science by
enabling healthcare companies, regulators, and stakeholders to assess the
safety and efficacy of new treatments, interventions, and medical procedures.
The Bayesian framework offers a unique advantage over the classical framework,
especially when incorporating prior information into a new trial with quality
external data, such as historical data or another source of co-data. In recent
years, there has been a significant increase in regulatory submissions using
Bayesian statistics due to its flexibility and ability to provide valuable
insights for decision-making, addressing the modern complexity of clinical
trials where frequentist trials are inadequate. For regulatory submissions,
companies often need to consider the frequentist operating characteristics of
the Bayesian analysis strategy, regardless of the design complexity. In
particular, the focus is on the frequentist type I error rate and power for all
realistic alternatives. This tutorial review aims to provide a comprehensive
overview of the use of Bayesian statistics in sample size determination in the
regulatory environment of clinical trials. Fundamental concepts of Bayesian
sample size determination and illustrative examples are provided to serve as a
valuable resource for researchers, clinicians, and statisticians seeking to
develop more complex and innovative designs.",http://arxiv.org/abs/2311.16506v2,Se Yoon Lee
203,A non-parametric U-statistic testing approach for multi-arm clinical trials with multivariate longitudinal data,"Randomized clinical trials (RCTs) often involve multiple longitudinal primary
outcomes to comprehensively assess treatment efficacy. The Longitudinal
Rank-Sum Test (LRST), a robust U-statistics-based, non-parametric, rank-based
method, effectively controls Type I error and enhances statistical power by
leveraging the temporal structure of the data without relying on distributional
assumptions. However, the LRST is limited to two-arm comparisons. To address
the need for comparing multiple doses against a control group in many RCTs, we
extend the LRST to a multi-arm setting. This novel multi-arm LRST provides a
flexible and powerful approach for evaluating treatment efficacy across
multiple arms and outcomes, with a strong capability for detecting the most
effective dose in multi-arm trials. Extensive simulations demonstrate that this
method maintains excellent Type I error control while providing greater power
compared to the two-arm LRST with multiplicity adjustments. Application to the
Bapineuzumab (Bapi) 301 trial further validates the multi-arm LRST's practical
utility and robustness, confirming its efficacy in complex clinical trial
analyses.",http://arxiv.org/abs/2408.10149v2,"Dhrubajyoti Ghosh, Sheng Luo"
204,Beyond p-values: a phase II dual-criterion design with statistical significance and clinical relevance,"Background: Well-designed phase II trials must have acceptable error rates
relative to a pre-specified success criterion, usually a statistically
significant p-value. Such standard designs may not always suffice from a
clinical perspective because clinical relevance may call for more. For example,
proof-of-concept in phase II often requires not only statistical significance
but also a sufficiently large effect estimate.
  Purpose: We propose dual-criterion designs to complement statistical
significance with clinical relevance, discuss their methodology, and illustrate
their implementation in phase II.
  Methods: Clinical relevance requires the effect estimate to pass a clinically
motivated threshold (the decision value). In contrast to standard designs, the
required effect estimate is an explicit design input whereas study power is
implicit. The sample size for a dual-criterion design needs careful
considerations of the study's operating characteristics (type-I error, power).
  Results: Dual-criterion designs are discussed for a randomized controlled and
a single-arm phase II trial, including decision criteria, sample size
calculations, decisions under various data scenarios, and operating
characteristics. The designs facilitate GO/NO-GO decisions due to their
complementary statistical-clinical criterion.
  Conclusion: To improve evidence-based decision-making, a formal yet
transparent quantitative framework is important. Dual-criterion designs offer
an appealing statistical-clinical compromise, which may be preferable to
standard designs if evidence against the null hypothesis alone does not suffice
for an efficacy claim.",http://arxiv.org/abs/1908.07751v1,"Satrajit Roychoudhury, Nicolas Scheuer, Beat Neuenschwander"
205,Generalizing trial evidence to target populations in non-nested designs: Applications to AIDS clinical trials,"Comparative effectiveness evidence from randomized trials may not be directly
generalizable to a target population of substantive interest when, as in most
cases, trial participants are not randomly sampled from the target population.
Motivated by the need to generalize evidence from two trials conducted in the
AIDS Clinical Trials Group (ACTG), we consider weighting, regression and doubly
robust estimators to estimate the causal effects of HIV interventions in a
specified population of people living with HIV in the USA. We focus on a
non-nested trial design and discuss strategies for both point and variance
estimation of the target population average treatment effect. Specifically in
the generalizability context, we demonstrate both analytically and empirically
that estimating the known propensity score in trials does not increase the
variance for each of the weighting, regression and doubly robust estimators. We
apply these methods to generalize the average treatment effects from two ACTG
trials to specified target populations and operationalize key practical
considerations. Finally, we report on a simulation study that investigates the
finite-sample operating characteristics of the generalizability estimators and
their sandwich variance estimators.",http://arxiv.org/abs/2103.04907v2,"Fan Li, Ashley L Buchanan, Stephen R Cole"
206,Matching Patients to Clinical Trials with Large Language Models,"Patient recruitment is challenging for clinical trials. We introduce
TrialGPT, an end-to-end framework for zero-shot patient-to-trial matching with
large language models. TrialGPT comprises three modules: it first performs
large-scale filtering to retrieve candidate trials (TrialGPT-Retrieval); then
predicts criterion-level patient eligibility (TrialGPT-Matching); and finally
generates trial-level scores (TrialGPT-Ranking). We evaluate TrialGPT on three
cohorts of 183 synthetic patients with over 75,000 trial annotations.
TrialGPT-Retrieval can recall over 90% of relevant trials using less than 6% of
the initial collection. Manual evaluations on 1,015 patient-criterion pairs
show that TrialGPT-Matching achieves an accuracy of 87.3% with faithful
explanations, close to the expert performance. The TrialGPT-Ranking scores are
highly correlated with human judgments and outperform the best-competing models
by 43.8% in ranking and excluding trials. Furthermore, our user study reveals
that TrialGPT can reduce the screening time by 42.6% in patient recruitment.
Overall, these results have demonstrated promising opportunities for
patient-to-trial matching with TrialGPT.",http://arxiv.org/abs/2307.15051v5,"Qiao Jin, Zifeng Wang, Charalampos S Floudas, Fangyuan Chen, Changlin Gong, Dara BrackenClarke, Elisabetta Xue, Yifan Yang, Jimeng Sun, Zhiyong Lu"
207,Maximum Matchings in Graphs for Allocating Kidney Paired Donation,"Relatives and friends of an end-stage renal disease patient who offer to
donate a kidney are often found to be incompatible with their intended
recipients. Kidney paired donation matches one patient and his incompatible
donor with another patient and donor in the same situation for an organ
exchange. Let patient- donor pairs be the vertices of an undirected graph G,
with an edge connecting any two reciprocally compatible vertices. A matching in
G is a feasible set of paired donations. We describe various optimization
problems on kidney paired donation graphs G and the merits of each in clinical
transplantation. Because some matches are geographically undesirable, and the
expected lifespan of a transplanted kidney depends on the immunologic
concordance of donor and recipient, we weight the edges of G and seek a maximum
edge-weight matching. Unfortunately, such matchings might not have the maximum
cardinality; there is a risk of an unpredictable trade-off between the quality
and quantity of paired donations. We propose an edge-weighting of G which
guarantees that every matching with maximum weight also has maximum
cardinality, and also maximizes the number of transplants for an exceptional
subset of recipients, while reducing travel and favoring immunologic
concordance.",http://arxiv.org/abs/1710.00953v3,"Sommer Gentry, Michal Mankowski, T S Michael, Dorry Segev"
208,Comment on ``Quantum noise influencing human behaviour could fake effectiveness of drugs in clinical trials'',Here are discussed some problems concerning quant-ph/0208006.,http://arxiv.org/abs/quant-ph/0209024v1,Alexander Yu Vlasov
209,Two-Stage Penalized Regression Screening to Detect Biomarker-Treatment Interactions in Randomized Clinical Trials,"High-dimensional biomarkers such as genomics are increasingly being measured
in randomized clinical trials. Consequently, there is a growing interest in
developing methods that improve the power to detect biomarker-treatment
interactions. We adapt recently proposed two-stage interaction detecting
procedures in the setting of randomized clinical trials. We also propose a new
stage 1 multivariate screening strategy using ridge regression to account for
correlations among biomarkers. For this multivariate screening, we prove the
asymptotic between-stage independence, required for family-wise error rate
control, under biomarker-treatment independence. Simulation results show that
in various scenarios, the ridge regression screening procedure can provide
substantially greater power than the traditional one-biomarker-at-a-time
screening procedure in highly correlated data. We also exemplify our approach
in two real clinical trial data applications.",http://arxiv.org/abs/2004.12028v2,"Jixiong Wang, Ashish Patel, James M S Wason, Paul J Newcombe"
210,Learning Modular Safe Policies in the Bandit Setting with Application to Adaptive Clinical Trials,"The stochastic multi-armed bandit problem is a well-known model for studying
the exploration-exploitation trade-off. It has significant possible
applications in adaptive clinical trials, which allow for dynamic changes in
the treatment allocation probabilities of patients. However, most bandit
learning algorithms are designed with the goal of minimizing the expected
regret. While this approach is useful in many areas, in clinical trials, it can
be sensitive to outlier data, especially when the sample size is small. In this
paper, we define and study a new robustness criterion for bandit problems.
Specifically, we consider optimizing a function of the distribution of returns
as a regret measure. This provides practitioners more flexibility to define an
appropriate regret measure. The learning algorithm we propose to solve this
type of problem is a modification of the BESA algorithm [Baransi et al., 2014],
which considers a more general version of regret. We present a regret bound for
our approach and evaluate it empirically both on synthetic problems as well as
on a dataset from the clinical trial literature. Our approach compares
favorably to a suite of standard bandit algorithms.",http://arxiv.org/abs/1903.01026v3,"Hossein Aboutalebi, Doina Precup, Tibor Schuster"
211,BOP2-DC: Bayesian optimal phase II designs with dual-criterion decision making,"The conventional phase II trial design paradigm is to make the go/no-go
decision based on the hypothesis testing framework. Statistical significance
itself alone, however, may not be sufficient to establish that the drug is
clinically effective enough to warrant confirmatory phase III trials. We
propose the Bayesian optimal phase II trial design with dual-criterion decision
making (BOP2-DC), which incorporates both statistical significance and clinical
relevance into decision making. Based on the posterior probability that the
treatment effect reaches the lower reference value (statistical significance)
and the clinically meaningful value (clinical significance), BOP2-DC allows for
go/consider/no-go decisions, rather than a binary go/no-go decision, and it is
optimized to maximize the probability of a go decision when the treatment is
effective or minimize the sample size when the treatment is futile. BOP2-DC is
highly flexible and accommodates various types of endpoints, including binary,
continuous, time-to-event, multiple, and co-primary endpoints, in single-arm
and randomized trials. Simulation studies show that the BOP2-DC design yields
desirable operating characteristics. The software to implement BOP2-DC is
freely available at \url{www.trialdesign.org}.",http://arxiv.org/abs/2112.10880v1,"Yujie Zhao, Daniel Li, Rong Liu, Ying Yuan"
212,SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models on Natural Language Inference for Clinical Trials,"This paper describes our submission to Task 2 of SemEval-2024: Safe
Biomedical Natural Language Inference for Clinical Trials. The Multi-evidence
Natural Language Inference for Clinical Trial Data (NLI4CT) consists of a
Textual Entailment (TE) task focused on the evaluation of the consistency and
faithfulness of Natural Language Inference (NLI) models applied to Clinical
Trial Reports (CTR). We test 2 distinct approaches, one based on finetuning and
ensembling Masked Language Models and the other based on prompting Large
Language Models using templates, in particular, using Chain-Of-Thought and
Contrastive Chain-Of-Thought. Prompting Flan-T5-large in a 2-shot setting leads
to our best system that achieves 0.57 F1 score, 0.64 Faithfulness, and 0.56
Consistency.",http://arxiv.org/abs/2404.03977v1,"Mathilde Aguiar, Pierre Zweigenbaum, Nona Naderi"
213,Counting Clinical Trials: New Evidence on Pharmaceutical Sector Productivity,"We develop a method for assigning high-quality labels to unstructured text.
This method is based on fine-tuning an efficient, open-source language model
with data extracted from a large, proprietary language model. We apply this
method to construct a census of published clinical trials. With these data, we
revisit a literature that contends that pharmaceutical sector productivity is
declining. Central to this conclusion are measurements of substantial increases
in the quantity of clinical trials over time, unmatched by trends in measures
of output. In our data, the quantity, quality, and composition of clinical
trials are stable since 2010. We show that previous measurements are an
artifact of biases introduced by shifts in the composition of other forms of
research.",http://arxiv.org/abs/2405.08030v5,"Maya M Durvasula, Sabri Eyuboglu, David M Ritzwoller"
214,Clinical trial cohort selection using Large Language Models on n2c2 Challenges,"Clinical trials are a critical process in the medical field for introducing
new treatments and innovations. However, cohort selection for clinical trials
is a time-consuming process that often requires manual review of patient text
records for specific keywords. Though there have been studies on standardizing
the information across the various platforms, Natural Language Processing (NLP)
tools remain crucial for spotting eligibility criteria in textual reports.
Recently, pre-trained large language models (LLMs) have gained popularity for
various NLP tasks due to their ability to acquire a nuanced understanding of
text. In this paper, we study the performance of large language models on
clinical trial cohort selection and leverage the n2c2 challenges to benchmark
their performance. Our results are promising with regard to the incorporation
of LLMs for simple cohort selection tasks, but also highlight the difficulties
encountered by these models as soon as fine-grained knowledge and reasoning are
required.",http://arxiv.org/abs/2501.11114v1,"Chien Amy Tai, Xavier Tannier"
215,On Preparing a List of Random treatment Assigns,"This paper presents the foundations of a computer oriented approach for
preparing a list of random treatment assignments to be adopted in randomised
controlled trials. Software is presented which can be applied in the earliest
stage of clinical trials and bioequivalence assays. This allocation of patients
to treatment in clinical trials ensures exactly equal treatment numbers. The
investigation of the randomness properties of an assignment leads to the
concept of a 'strong randomised list'. The new approach introduced in this note
is based on thresholds and produces a strong randomised list of treatment
assignments.",http://arxiv.org/abs/1502.03301v1,"N S SantosMagalhaes, H M de Oliveira, A J Alves"
216,Incertus.jl -- The Julia Lego Blocks for Randomized Clinical Trial Designs,"In this paper, we present Insertus.jl, the Julia package that can help the
user generate a randomization sequence of a given length for a multi-arm trial
with a pre-specified target allocation ratio and assess the operating
characteristics of the chosen randomization method through Monte Carlo
simulations. The developed package is computationally efficient, and it can be
invoked in R. Furthermore, the package is open-ended -- it can flexibly
accommodate new randomization procedures and evaluate their statistical
properties via simulation. It may be also helpful for validating other
randomization methods for which software is not readily available. In summary,
Insertus.jl can be used as ``Lego Blocks'' to construct a fit-for-purpose
randomization procedure for a given clinical trial design.",http://arxiv.org/abs/2407.14248v1,"Yevgen Ryeznik, Oleksandr Sverdlov"
217,A Clinical Trial Design Approach to Auditing Language Models in Healthcare Setting,"We present an audit mechanism for language models, with a focus on models
deployed in the healthcare setting. Our proposed mechanism takes inspiration
from clinical trial design where we posit the language model audit as a single
blind equivalence trial, with the comparison of interest being the subject
matter experts. We show that using our proposed method, we can follow
principled sample size and power calculations, leading to the requirement of
sampling minimum number of records while maintaining the audit integrity and
statistical soundness. Finally, we provide a real-world example of the audit
used in a production environment in a large-scale public health network.",http://arxiv.org/abs/2411.16702v2,"Lovedeep Gondara, Jonathan Simkin"
218,Sample-targeted clinical trial adaptation,"Clinical trial adaptation refers to any adjustment of the trial protocol
after the onset of the trial. The main goal is to make the process of
introducing new medical interventions to patients more efficient by reducing
the cost and the time associated with evaluating their safety and efficacy. The
principal question is how should adaptation be performed so as to minimize the
chance of distorting the outcome of the trial. We propose a novel method for
achieving this. Unlike previous work our approach focuses on trial adaptation
by sample size adjustment. We adopt a recently proposed stratification
framework based on collected auxiliary data and show that this information
together with the primary measured variables can be used to make a
probabilistically informed choice of the particular sub-group a sample should
be removed from. Experiments on simulated data are used to illustrate the
effectiveness of our method and its application in practice.",http://arxiv.org/abs/1411.3919v1,Ognjen Arandjelovic
219,Principal causal effect identification and surrogate endpoint evaluation by multiple trials,"Principal stratification is a causal framework to analyze randomized
experiments with a post-treatment variable between the treatment and endpoint
variables. Because the principal strata defined by the potential outcomes of
the post-treatment variable are not observable, we generally cannot identify
the causal effects within principal strata. Motivated by a real data set of
phase III adjuvant colon clinical trials, we propose approaches to identifying
and estimating the principal causal effects via multiple trials. For the
identifiability, we remove the commonly-used exclusion restriction assumption
by stipulating that the principal causal effects are homogeneous across these
trials. To remove another commonly-used monotonicity assumption, we give a
necessary condition for the local identifiability, which requires at least
three trials. Applying our approaches to the data from adjuvant colon clinical
trials, we find that the commonly-used monotonicity assumption is untenable,
and disease-free survival with three-year follow-up is a valid surrogate
endpoint for overall survival with five-year follow-up, which satisfies both
the causal necessity and the causal sufficiency. We also propose a sensitivity
analysis approach based on Bayesian hierarchical models to investigate the
impact of the deviation from the homogeneity assumption.",http://arxiv.org/abs/1507.05935v1,"Zhichao Jiang, Peng Ding, Zhi Geng"
220,Optimizing Trial Designs for Targeted Therapies,"An important objective in the development of targeted therapies is to
identify the populations where the treatment under consideration has positive
benefit risk balance. We consider pivotal clinical trials, where the efficacy
of a treatment is tested in an overall population and/or in a pre-specified
subpopulation. Based on a decision theoretic framework we derive optimized
trial designs by maximizing utility functions. Features to be optimized include
the sample size and the population in which the trial is performed (the full
population or the targeted subgroup only) as well as the underlying multiple
test procedure. The approach accounts for prior knowledge of the efficacy of
the drug in the considered populations using a two dimensional prior
distribution. The considered utility functions account for the costs of the
clinical trial as well as the expected benefit when demonstrating efficacy in
the different subpopulations. We model utility functions from a sponsor's as
well as from a public health perspective, reflecting actual civil interests.
Examples of optimized trial designs obtained by numerical optimization are
presented for both perspectives.",http://arxiv.org/abs/1606.03987v1,"Thomas Ondra, Sebastian Jobjrnsson, Robert A Beckman, CarlFredrik Burman, Franz Knig, Nigel Stallard, Martin Posch"
221,Imbalanced Randomization in Clinical Trials,"Randomization is a common technique used in clinical trials to eliminate
potential bias and confounders in a patient population. Equal allocation to
treatment groups is the standard due to its optimal efficiency in many cases.
However, in certain scenarios, unequal allocation can improve efficiency. In
superiority trials with more than two groups, the optimal randomization is not
always a balanced randomization. In non-inferiority trials, additive margin
with equal variance is the only instance with balanced randomization. Optimal
randomization for non-inferiority trials can be far from 1:1 and can greatly
improve efficiency, a fact which is commonly overlooked. A tool for sample size
calculation for non-inferiority trials with additive or multiplicative margin
with normal, binomial or Poisson distribution is available at
http://www.statlab.wisc.edu/shiny/SSNI/.",http://arxiv.org/abs/1806.06020v3,"Thevaa Chandereng, Xiaodan Wei, Rick Chappell"
222,Bayesian adaptive N-of-1 trials for estimating population and individual treatment effects,"This article proposes a novel adaptive design algorithm that can be used to
find optimal treatment allocations in N-of-1 clinical trials. This new
methodology uses two Laplace approximations to provide a computationally
efficient estimate of population and individual random effects within a
repeated measures, adaptive design framework. Given the efficiency of this
approach, it is also adopted for treatment selection to target the collection
of data for the precise estimation of treatment effects. To evaluate this
approach, we consider both a simulated and motivating N-of-1 clinical trial
from the literature. For each trial, our methods were compared to the
multi-armed bandit approach and a randomised N-of-1 trial design in terms of
identifying the best treatment for each patient and the information gained
about the model parameters. The results show that our new approach selects
designs that are highly efficient in achieving each of these objectives. As
such, we propose our Laplace-based algorithm as an efficient approach for
designing adaptive N-of-1 trials.",http://arxiv.org/abs/1911.00878v3,"S G Jagath Senarathne, Antony M Overstall, James M McGree"
223,A Scalable AI Approach for Clinical Trial Cohort Optimization,"FDA has been promoting enrollment practices that could enhance the diversity
of clinical trial populations, through broadening eligibility criteria.
However, how to broaden eligibility remains a significant challenge. We propose
an AI approach to Cohort Optimization (AICO) through transformer-based natural
language processing of the eligibility criteria and evaluation of the criteria
using real-world data. The method can extract common eligibility criteria
variables from a large set of relevant trials and measure the generalizability
of trial designs to real-world patients. It overcomes the scalability limits of
existing manual methods and enables rapid simulation of eligibility criteria
design for a disease of interest. A case study on breast cancer trial design
demonstrates the utility of the method in improving trial generalizability.",http://arxiv.org/abs/2109.02808v1,"Xiong Liu, Cheng Shi, Uday Deore, Yingbo Wang, Myah Tran, Iya Khalil, Murthy Devarakonda"
224,TrialView: An AI-powered Visual Analytics System for Temporal Event Data in Clinical Trials,"Randomized controlled trials (RCT) are the gold standards for evaluating the
efficacy and safety of therapeutic interventions in human subjects. In addition
to the pre-specified endpoints, trial participants' experience reveals the time
course of the intervention. Few analytical tools exist to summarize and
visualize the individual experience of trial participants. Visual analytics
allows integrative examination of temporal event patterns of patient
experience, thus generating insights for better care decisions. Towards this
end, we introduce TrialView, an information system that combines graph
artificial intelligence (AI) and visual analytics to enhance the dissemination
of trial data. TrialView offers four distinct yet interconnected views:
Individual, Cohort, Progression, and Statistics, enabling an interactive
exploration of individual and group-level data. The TrialView system is a
general-purpose analytical tool for a broad class of clinical trials. The
system is powered by graph AI, knowledge-guided clustering, explanatory
modeling, and graph-based agglomeration algorithms. We demonstrate the system's
effectiveness in analyzing temporal event data through a case study.",http://arxiv.org/abs/2310.04586v1,"Zuotian Li, Xiang Liu, Zelei Cheng, Yingjie Chen, Wanzhu Tu, Jing Su"
225,Biomarker-Guided Adaptive Enrichment Design with Threshold Detection for Clinical Trials with Time-to-Event Outcome,"Biomarker-guided designs are increasingly used to evaluate personalized
treatments based on patients' biomarker status in Phase II and III clinical
trials. With adaptive enrichment, these designs can improve the efficiency of
evaluating the treatment effect in biomarker-positive patients by increasing
their proportion in the randomized trial. While time-to-event outcomes are
often used as the primary endpoint to measure treatment effects for a new
therapy in severe diseases like cancer and cardiovascular diseases, there is
limited research on biomarker-guided adaptive enrichment trials in this
context. Such trials almost always adopt hazard ratio methods for statistical
measurement of treatment effects. In contrast, restricted mean survival time
(RMST) has gained popularity for analyzing time-to-event outcomes because it
offers more straightforward interpretations of treatment effects and does not
require the proportional hazard assumption. This paper proposes a two-stage
biomarker-guided adaptive RMST design with threshold detection and patient
enrichment. We develop sophisticated methods for identifying the optimal
biomarker threshold, treatment effect estimators in the biomarker-positive
subgroup, and approaches for type I error rate, power analysis, and sample size
calculation. We present a numerical example of re-designing an oncology trial.
An extensive simulation study is conducted to evaluate the performance of the
proposed design.",http://arxiv.org/abs/2406.06426v1,"Kaiyuan Hua, Hwanhee Hong, Xiaofei Wang"
226,Predictive Probabilities Made Simple: A Fast and Accurate Method for Clinical Trial Decision Making,"Bayesian predictive probabilities are commonly used for interim monitoring of
clinical trials through efficacy and futility stopping rules. Despite their
usefulness, calculation of predictive probabilities, particularly in
pre-experiment trial simulation, can be a significant challenge. We introduce
an approximation for computing predictive probabilities using either a p-value
or a posterior probability that significantly reduces this burden. We show the
approximation has a high degree of concordance with standard Monte Carlo
imputation methods for computing predictive probabilities, and present five
simulation studies comparing the approximation to the full predictive
probability for a range of primary analysis strategies: dichotomous,
time-to-event, and ordinal endpoints, as well as historical borrowing and
longitudinal modeling. We find that this faster method of predictive
probability approximation works well in all five applications, thus
significantly reducing the computational burden of trial simulation, allowing
more virtual trials to be simulated to achieve greater precision in estimating
trial operating characteristics.",http://arxiv.org/abs/2406.11406v1,"Joe Marion, Liz Lorenzi, Cora AllenSavietta, Scott Berry, Kert Viele"
227,Confidence intervals for adaptive trial designs I: A methodological review,"Regulatory guidance notes the need for caution in the interpretation of
confidence intervals (CIs) constructed during and after an adaptive clinical
trial. Conventional CIs of the treatment effects are prone to undercoverage (as
well as other undesirable properties) in many adaptive designs, because they do
not take into account the potential and realised trial adaptations. This paper
is the first in a two-part series that explores CIs for adaptive trials. It
provides a comprehensive review of the methods to construct CIs for adaptive
designs, while the second paper illustrates how to implement these in practice
and proposes a set of guidelines for trial statisticians. We describe several
classes of techniques for constructing CIs for adaptive clinical trials, before
providing a systematic literature review of available methods, classified by
the type of adaptive design. As part of this, we assess, through a proposed
traffic light system, which of several desirable features of CIs (such as
achieving nominal coverage and consistency with the hypothesis test decision)
each of these methods holds.",http://arxiv.org/abs/2411.08495v1,"David S Robertson, Thomas Burnett, Babak ChoodariOskooei, Munya Dimairo, Michael Grayling, Philip Pallmann, Thomas Jaki"
228,Confidence intervals for adaptive trial designs II: Case study and practical guidance,"In adaptive clinical trials, the conventional confidence interval (CI) for a
treatment effect is prone to undesirable properties such as undercoverage and
potential inconsistency with the final hypothesis testing decision.
Accordingly, as is stated in recent regulatory guidance on adaptive designs,
there is the need for caution in the interpretation of CIs constructed during
and after an adaptive clinical trial. However, it may be unclear which of the
available CIs in the literature are preferable. This paper is the second in a
two-part series that explores CIs for adaptive trials. Part I provided a
methodological review of approaches to construct CIs for adaptive designs. In
this paper (part II), we present an extended case study based around a
two-stage group sequential trial, including a comprehensive simulation study of
the proposed CIs for this setting. This facilitates an expanded description of
considerations around what makes for an effective CI procedure following an
adaptive trial. We show that the CIs can have notably different properties.
Finally, we propose a set of guidelines for researchers around the choice of
CIs and the reporting of CIs following an adaptive design.",http://arxiv.org/abs/2411.08771v1,"David S Robertson, Thomas Burnett, Babak ChoodariOskooei, Munya Dimairo, Michael Grayling, Philip Pallmann, Thomas Jaki"
229,End-To-End Clinical Trial Matching with Large Language Models,"Matching cancer patients to clinical trials is essential for advancing
treatment and patient care. However, the inconsistent format of medical free
text documents and complex trial eligibility criteria make this process
extremely challenging and time-consuming for physicians. We investigated
whether the entire trial matching process - from identifying relevant trials
among 105,600 oncology-related clinical trials on clinicaltrials.gov to
generating criterion-level eligibility matches - could be automated using Large
Language Models (LLMs). Using GPT-4o and a set of 51 synthetic Electronic
Health Records (EHRs), we demonstrate that our approach identifies relevant
candidate trials in 93.3% of cases and achieves a preliminary accuracy of 88.0%
when matching patient-level information at the criterion level against a
baseline defined by human experts. Utilizing LLM feedback reveals that 39.3%
criteria that were initially considered incorrect are either ambiguous or
inaccurately annotated, leading to a total model accuracy of 92.7% after
refining our human baseline. In summary, we present an end-to-end pipeline for
clinical trial matching using LLMs, demonstrating high precision in screening
and matching trials to individual patients, even outperforming the performance
of qualified medical doctors. Our fully end-to-end pipeline can operate
autonomously or with human supervision and is not restricted to oncology,
offering a scalable solution for enhancing patient-trial matching in real-world
settings.",http://arxiv.org/abs/2407.13463v1,"Dyke Ferber, Lars Hilgers, Isabella C Wiest, MarieElisabeth Lemann, Jan Clusmann, Peter Neidlinger, Jiefu Zhu, Georg Wlflein, Jacqueline Lammert, Maximilian Tschochohei, Heiko Bhme, Dirk Jger, Mihaela Aldea, Daniel Truhn, Christiane Hper, Jakob Nikolas Kather"
230,"Treatment Effect Quantification for Time-to-event Endpoints -- Estimands, Analysis Strategies, and beyond","A draft addendum to ICH E9 has been released for public consultation in
August 2017. The addendum focuses on two topics particularly relevant for
randomized confirmatory clinical trials: estimands and sensitivity analyses.
The need to amend ICH E9 grew out of the realization of a lack of alignment
between the objectives of a clinical trial stated in the protocol and the
accompanying quantification of the ""treatment effect"" reported in a regulatory
submission. We embed time-to-event endpoints in the estimand framework, and
discuss how the four estimand attributes described in the addendum apply to
time-to-event endpoints. We point out that if the proportional hazards
assumption is not met, the estimand targeted by the most prevalent methods used
to analyze time-to-event endpoints, logrank test and Cox regression, depends on
the censoring distribution. We discuss for a large randomized clinical trial
how the analyses for the primary and secondary endpoints as well as the
sensitivity analyses actually performed in the trial can be seen in the context
of the addendum. To the best of our knowledge, this is the first attempt to do
so for a trial with a time-to-event endpoint. Questions that remain open with
the addendum for time-to-event endpoints and beyond are formulated, and
recommendations for planning of future trials are given. We hope that this will
provide a contribution to developing a common framework based on the final
version of the addendum that can be applied to design, protocols, statistical
analysis plans, and clinical study reports in the future.",http://arxiv.org/abs/1711.07518v4,Kaspar Rufibach
231,Efficient adaptive designs for clinical trials of interventions for COVID-19,"The COVID-19 pandemic has led to an unprecedented response in terms of
clinical research activity. An important part of this research has been focused
on randomized controlled clinical trials to evaluate potential therapies for
COVID-19. The results from this research need to be obtained as rapidly as
possible. This presents a number of challenges associated with considerable
uncertainty over the natural history of the disease and the number and
characteristics of patients affected, and the emergence of new potential
therapies. These challenges make adaptive designs for clinical trials a
particularly attractive option. Such designs allow a trial to be modified on
the basis of interim analysis data or stopped as soon as sufficiently strong
evidence has been observed to answer the research question, without
compromising the trial's scientific validity or integrity. In this paper we
describe some of the adaptive design approaches that are available and discuss
particular issues and challenges associated with their use in the pandemic
setting. Our discussion is illustrated by details of four ongoing COVID-19
trials that have used adaptive designs.",http://arxiv.org/abs/2005.13309v1,"Nigel Stallard, Lisa Hampson, Norbert Benda, Werner Brannath, Tom Burnett, Tim Friede, Peter K Kimani, Franz Koenig, Johannes Krisam, Pavel Mozgunov, Martin Posch, James Wason, Gernot Wassmer, John Whitehead, S Faye Williamson, Sarah Zohar, Thomas Jaki"
232,Leveraging external data in the analysis of randomized controlled trials: a comparative analysis,"The use of patient-level information from previous studies, registries, and
other external datasets can support the analysis of single-arm and randomized
clinical trials to evaluate and test experimental treatments. However, the
integration of external data in the analysis of clinical trials can also
compromise the scientific validity of the results due to selection bias,
study-to-study differences, unmeasured confounding, and other distortion
mechanisms. Therefore, leveraging external data in the analysis of a clinical
trial requires the use of appropriate methods that can detect, prevent or
mitigate the risks of bias and potential distortion mechanisms. We review
several methods that have been previously proposed to leverage external
datasets, such as matching procedures or random effects modeling. Different
methods present distinct trade-offs between risks and efficiencies. We conduct
a comparative analysis of statistical methods to leverage external data and
analyze randomized clinical trials. Multiple operating characteristics are
discussed, such as the control of false positive results, power, and the bias
of the treatment effect estimates, across candidate statistical methods. We
compare the statistical methods through a broad set of simulation scenarios. We
then compare the methods using a collection of datasets with individual
patient-level information from several glioblastoma studies in order to provide
recommendations for future glioblastoma trials.",http://arxiv.org/abs/2408.13409v1,"Gopal Kotecha, Daniel E Schwartz, Steffen Ventz, Lorenzo Trippa"
233,Designing and evaluating advanced adaptive randomised clinical trials: a practical guide,"Background
  Advanced adaptive randomised clinical trials are increasingly used. Compared
to their conventional counterparts, their flexibility may make them more
efficient, increase the probability of obtaining conclusive results without
larger samples than necessary, and increase the probability that individual
participants are allocated to more promising interventions. However, limited
guidance is available on designing and evaluating the performance of advanced
adaptive trials.
  Methods
  We summarise the methodological considerations and provide practical guidance
on the entire workflow of planning and evaluating advanced adaptive trials
using adaptive stopping, adaptive arm dropping, and response-adaptive
randomisation within a Bayesian statistical framework.
  Results
  This comprehensive practical guide covers the key methodological decisions
for advanced adaptive trials and their specification and evaluation using
statistical simulation. These considerations include interventions and common
control use; outcome type and generation; analysis timing and outcome-data lag;
allocation rules; analysis model; adaptation rules for stopping and arm
dropping; clinical scenarios assessed; performance metrics; calibration;
sensitivity analyses; and reporting. The considerations are covered in the
context of realistic examples, along with simulation code using the adaptr R
package.
  Conclusions
  This practical guide will help clinical trialists, methodologists, and
biostatisticians design and evaluate advanced adaptive trials.",http://arxiv.org/abs/2501.08765v1,"Anders Granholm, Aksel Karl Georg Jensen, Theis Lange, Anders Perner, Morten Hylander Mller, Benjamin Skov KaasHansen"
234,Multi-center clinical trials: Randomization and ancillary statistics,"The purpose of this paper is to investigate and develop methods for analysis
of multi-center randomized clinical trials which only rely on the randomization
process as a basis of inference. Our motivation is prompted by the fact that
most current statistical procedures used in the analysis of randomized
multi-center studies are model based. The randomization feature of the trials
is usually ignored. An important characteristic of model based analysis is that
it is straightforward to model covariates. Nevertheless, in nearly all model
based analyses, the effects due to different centers and, in general, the
design of the clinical trials are ignored. An alternative to a model based
analysis is to have analyses guided by the design of the trial. Our development
of design based methods allows the incorporation of centers as well as other
features of the trial design. The methods make use of conditioning on the
ancillary statistics in the sample space generated by the randomization
process. We have investigated the power of the methods and have found that, in
the presence of center variation, there is a significant increase in power. The
methods have been extended to group sequential trials with similar increases in
power.",http://arxiv.org/abs/0807.4002v1,"Lu Zheng, Marvin Zelen"
235,FRAMM: Fair Ranking with Missing Modalities for Clinical Trial Site Selection,"Despite many efforts to address the disparities, the underrepresentation of
gender, racial, and ethnic minorities in clinical trials remains a problem and
undermines the efficacy of treatments on minorities. This paper focuses on the
trial site selection task and proposes FRAMM, a deep reinforcement learning
framework for fair trial site selection. We focus on addressing two real-world
challenges that affect fair trial sites selection: the data modalities are
often not complete for many potential trial sites, and the site selection needs
to simultaneously optimize for both enrollment and diversity since the problem
is necessarily a trade-off between the two with the only possible way to
increase diversity post-selection being through limiting enrollment via caps.
To address the missing data challenge, FRAMM has a modality encoder with a
masked cross-attention mechanism for handling missing data, bypassing data
imputation and the need for complete data in training. To handle the need for
making efficient trade-offs, FRAMM uses deep reinforcement learning with a
specifically designed reward function that simultaneously optimizes for both
enrollment and fairness.
  We evaluate FRAMM using 4,392 real-world clinical trials ranging from 2016 to
2021 and show that FRAMM outperforms the leading baseline in enrollment-only
settings while also achieving large gains in diversity. Specifically, it is
able to produce a 9% improvement in diversity with similar enrollment levels
over the leading baselines. That improved diversity is further manifested in
achieving up to a 14% increase in Hispanic enrollment, 27% increase in Black
enrollment, and 60% increase in Asian enrollment compared to selecting sites
with an enrollment-only model.",http://arxiv.org/abs/2305.19407v1,"Brandon Theodorou, Lucas Glass, Cao Xiao, Jimeng Sun"
236,Monitoring overall survival in pivotal trials in indolent cancers,"Indolent cancers are characterized by long overall survival (OS) times.
Therefore, powering a clinical trial to provide definitive assessment of the
effects of an experimental intervention on OS in a reasonable timeframe is
generally infeasible. Instead, the primary outcome in many pivotal trials is an
intermediate clinical response such as progression-free survival (PFS). In
several recently reported pivotal trials of interventions for indolent cancers
that yielded promising results on an intermediate outcome, however, more mature
data or post-approval trials showed concerning OS trends. These problematic
results have prompted a keen interest in quantitative approaches for monitoring
OS that can support regulatory decision-making related to the risk of an
unacceptably large detrimental effect on OS. For example, the US Food and Drug
Administration, the American Association for Cancer Research, and the American
Statistical Association recently organized a one-day multi-stakeholder workshop
entitled 'Overall Survival in Oncology Clinical Trials'. In this paper, we
propose OS monitoring guidelines tailored for the setting of indolent cancers.
Our pragmatic approach is modeled, in part, on the monitoring guidelines the
FDA has used in cardiovascular safety trials conducted in Type 2 Diabetes
Mellitus. We illustrate proposals through application to several examples
informed by actual case studies.",http://arxiv.org/abs/2310.20658v3,"Thomas R Fleming, Lisa V Hampson, Bharani BharaniDharan, Frank Bretz, Arunava Chakravartty, Thibaud Coroller, Evanthia Koukouli, Janet Wittes, Nigel Yateman, Emmanuel Zuber"
237,Modelling Immunological Memory,"Accurate immunological models offer the possibility of performing
highthroughput experiments in silico that can predict, or at least suggest, in
vivo phenomena. In this chapter, we compare various models of immunological
memory. We first validate an experimental immunological simulator, developed by
the authors, by simulating several theories of immunological memory with known
results. We then use the same system to evaluate the predicted effects of a
theory of immunological memory. The resulting model has not been explored
before in artificial immune systems research, and we compare the simulated in
silico output with in vivo measurements. Although the theory appears valid, we
suggest that there are a common set of reasons why immunological memory models
are a useful support tool; not conclusive in themselves.",http://arxiv.org/abs/1004.3932v1,"Simon Garret, Martin Robbins, Joanne Walker, William Wilson, Uwe Aickelin"
238,Time-to-event estimands and loss to follow-up in oncology in light of the estimands framework,"Time-to-event estimands are central to many oncology clinical trials. The
estimand framework (addendum to the ICH E9 guideline) calls for precisely
defining the treatment effect of interest to align with the clinical question
of interest and requires predefining the handling of intercurrent events that
occur after treatment initiation and either preclude the observation of an
event of interest or impact the interpretation of the treatment effect. We
discuss a practical problem in clinical trial design and execution, i.e. in
some clinical contexts it is not feasible to systematically follow patients to
an event of interest. Loss to follow-up in the presence of intercurrent events
can affect the meaning and interpretation of the study results. We provide
recommendations for trial design, stressing the need for close alignment of the
clinical question of interest and study design, impact on data collection and
other practical implications. When patients cannot be systematically followed,
compromise may be necessary to select the best available estimand that can be
feasibly estimated under the circumstances. We discuss the use of sensitivity
and supplementary analyses to examine assumptions of interest.",http://arxiv.org/abs/2203.01781v3,"Jonathan Siegel, HansJochen Weber, Stefan Englert, Feng Liu"
239,"Single-World Intervention Graphs for Defining, Identifying, and Communicating Estimands in Clinical Trials","Confusion often arises when attempting to articulate target estimand(s) of a
clinical trial in plain language. We aim to rectify this confusion by using a
type of causal graph called the Single-World Intervention Graph (SWIG) to
provide a visual representation of the estimand that can be effectively
communicated to interdisciplinary stakeholders. These graphs not only display
estimands, but also illustrate the assumptions under which a causal estimand is
identifiable by presenting the graphical relationships between the treatment,
intercurrent events, and clinical outcomes. To demonstrate its usefulness in
pharmaceutical research, we present examples of SWIGs for various intercurrent
event strategies specified in the ICH E9(R1) addendum, as well as an example
from a real-world clinical trial for chronic pain. Latex code to generate all
the SWIGs shown is this paper is made available. We advocate clinical trialists
adopt the use of SWIGs in their estimand discussions during the planning stages
of their studies.",http://arxiv.org/abs/2206.01249v1,"Alex Ocampo, Jemar R Bather"
240,Sequential monitoring of response-adaptive randomized clinical trials,"Clinical trials are complex and usually involve multiple objectives such as
controlling type I error rate, increasing power to detect treatment difference,
assigning more patients to better treatment, and more. In literature, both
response-adaptive randomization (RAR) procedures (by changing randomization
procedure sequentially) and sequential monitoring (by changing analysis
procedure sequentially) have been proposed to achieve these objectives to some
degree. In this paper, we propose to sequentially monitor response-adaptive
randomized clinical trial and study it's properties. We prove that the
sequential test statistics of the new procedure converge to a Brownian motion
in distribution. Further, we show that the sequential test statistics
asymptotically satisfy the canonical joint distribution defined in Jennison and
Turnbull (\citeyearJT00). Therefore, type I error and other objectives can be
achieved theoretically by selecting appropriate boundaries. These results open
a door to sequentially monitor response-adaptive randomized clinical trials in
practice. We can also observe from the simulation studies that, the proposed
procedure brings together the advantages of both techniques, in dealing with
power, total sample size and total failure numbers, while keeps the type I
error. In addition, we illustrate the characteristics of the proposed procedure
by redesigning a well-known clinical trial of maternal-infant HIV transmission.",http://arxiv.org/abs/1010.3901v1,"Hongjian Zhu, Feifang Hu"
241,Multi-armed Bandit Models for the Optimal Design of Clinical Trials: Benefits and Challenges,"Multi-armed bandit problems (MABPs) are a special type of optimal control
problem well suited to model resource allocation under uncertainty in a wide
variety of contexts. Since the first publication of the optimal solution of the
classic MABP by a dynamic index rule, the bandit literature quickly diversified
and emerged as an active research topic. Across this literature, the use of
bandit models to optimally design clinical trials became a typical motivating
application, yet little of the resulting theory has ever been used in the
actual design and analysis of clinical trials. To this end, we review two MABP
decision-theoretic approaches to the optimal allocation of treatments in a
clinical trial: the infinite-horizon Bayesian Bernoulli MABP and the
finite-horizon variant. These models possess distinct theoretical properties
and lead to separate allocation rules in a clinical trial design context. We
evaluate their performance compared to other allocation rules, including fixed
randomization. Our results indicate that bandit approaches offer significant
advantages, in terms of assigning more patients to better treatments, and
severe limitations, in terms of their resulting statistical power. We propose a
novel bandit-based patient allocation rule that overcomes the issue of low
power, thus removing a potential barrier for their use in practice.",http://arxiv.org/abs/1507.08025v1,"Sofa S Villar, Jack Bowden, James Wason"
242,"The Optimal Design of Clinical Trials with Potential Biomarker Effects, A Novel Computational Approach","As a future trend of healthcare, personalized medicine tailors medical
treatments to individual patients. It requires to identify a subset of patients
with the best response to treatment. The subset can be defined by a biomarker
(e.g. expression of a gene) and its cutoff value. Topics on subset
identification have received massive attention. There are over 2 million hits
by keyword searches on Google Scholar. However, how to properly incorporate the
identified subsets/biomarkers to design clinical trials is not trivial and
rarely discussed in the literature, which leads to a gap between research
results and real-world drug development.
  To fill in this gap, we formulate the problem of clinical trial design into
an optimization problem involving high-dimensional integration, and propose a
novel computational solution based on Monte-Carlo and smoothing methods. Our
method utilizes the modern techniques of General-Purpose computing on Graphics
Processing Units for large-scale parallel computing. Compared to the standard
method in three-dimensional problems, our approach is more accurate and 133
times faster. This advantage increases when dimensionality increases. Our
method is scalable to higher-dimensional problems since the precision bound is
a finite number not affected by dimensionality.
  Our software will be available on GitHub and CRAN, which can be applied to
guide the design of clinical trials to incorporate the biomarker better.
Although our research is motivated by the design of clinical trials, the method
can be used widely to solve other optimization problems involving
high-dimensional integration.",http://arxiv.org/abs/2005.10494v1,"Yitao Lu, Julie Zhou, Li Xing, Xuekui Zhang"
243,Multi-classifier prediction of knee osteoarthritis progression from incomplete imbalanced longitudinal data,"Conventional inclusion criteria used in osteoarthritis clinical trials are
not very effective in selecting patients who would benefit from a therapy being
tested. Typically majority of selected patients show no or limited disease
progression during a trial period. As a consequence, the effect of the tested
treatment cannot be observed, and the efforts and resources invested in running
the trial are not rewarded. This could be avoided, if selection criteria were
more predictive of the future disease progression.
  In this article, we formulated the patient selection problem as a multi-class
classification task, with classes based on clinically relevant measures of
progression (over a time scale typical for clinical trials). Using data from
two long-term knee osteoarthritis studies OAI and CHECK, we tested multiple
algorithms and learning process configurations (including multi-classifier
approaches, cost-sensitive learning, and feature selection), to identify the
best performing machine learning models. We examined the behaviour of the best
models, with respect to prediction errors and the impact of used features, to
confirm their clinical relevance. We found that the model-based selection
outperforms the conventional inclusion criteria, reducing by 20-25% the number
of patients who show no progression. This result might lead to more efficient
clinical trials.",http://arxiv.org/abs/1909.13408v2,"Pawe Widera, Paco M J Welsing, Christoph Ladel, John Loughlin, Floris P J G Lafeber, Florence Petit Dop, Jonathan Larkin, Harrie Weinans, Ali Mobasheri, Jaume Bacardit"
244,Zero-Shot Clinical Trial Patient Matching with LLMs,"Matching patients to clinical trials is a key unsolved challenge in bringing
new drugs to market. Today, identifying patients who meet a trial's eligibility
criteria is highly manual, taking up to 1 hour per patient. Automated screening
is challenging, however, as it requires understanding unstructured clinical
text. Large language models (LLMs) offer a promising solution. In this work, we
explore their application to trial matching. First, we design an LLM-based
system which, given a patient's medical history as unstructured clinical text,
evaluates whether that patient meets a set of inclusion criteria (also
specified as free text). Our zero-shot system achieves state-of-the-art scores
on the n2c2 2018 cohort selection benchmark. Second, we improve the data and
cost efficiency of our method by identifying a prompting strategy which matches
patients an order of magnitude faster and more cheaply than the status quo, and
develop a two-stage retrieval pipeline that reduces the number of tokens
processed by up to a third while retaining high performance. Third, we evaluate
the interpretability of our system by having clinicians evaluate the natural
language justifications generated by the LLM for each eligibility decision, and
show that it can output coherent explanations for 97% of its correct decisions
and 75% of its incorrect ones. Our results establish the feasibility of using
LLMs to accelerate clinical trial operations.",http://arxiv.org/abs/2402.05125v3,"Michael Wornow, Alejandro Lozano, Dev Dash, Jenelle Jindal, Kenneth W Mahaffey, Nigam H Shah"
245,CTBench: A Comprehensive Benchmark for Evaluating Language Model Capabilities in Clinical Trial Design,"CTBench is introduced as a benchmark to assess language models (LMs) in
aiding clinical study design. Given study-specific metadata, CTBench evaluates
AI models' ability to determine the baseline features of a clinical trial (CT),
which include demographic and relevant features collected at the trial's start
from all participants. These baseline features, typically presented in CT
publications (often as Table 1), are crucial for characterizing study cohorts
and validating results. Baseline features, including confounders and
covariates, are also necessary for accurate treatment effect estimation in
studies involving observational data. CTBench consists of two datasets:
""CT-Repo,"" containing baseline features from 1,690 clinical trials sourced from
clinicaltrials.gov, and ""CT-Pub,"" a subset of 100 trials with more
comprehensive baseline features gathered from relevant publications. Two
LM-based evaluation methods are developed to compare the actual baseline
feature lists against LM-generated responses. ""ListMatch-LM"" and
""ListMatch-BERT"" use GPT-4o and BERT scores (at various thresholds),
respectively, for evaluation. To establish baseline results, advanced prompt
engineering techniques using LLaMa3-70B-Instruct and GPT-4o in zero-shot and
three-shot learning settings are applied to generate potential baseline
features. The performance of GPT-4o as an evaluator is validated through
human-in-the-loop evaluations on the CT-Pub dataset, where clinical experts
confirm matches between actual and LM-generated features. The results highlight
a promising direction with significant potential for improvement, positioning
CTBench as a useful tool for advancing research on AI in CT design and
potentially enhancing the efficacy and robustness of CTs.",http://arxiv.org/abs/2406.17888v1,"Nafis Neehal, Bowen Wang, Shayom Debopadhaya, Soham Dan, Keerthiram Murugesan, Vibha Anand, Kristin P Bennett"
246,Clarifying the Role of the Mantel-Haenszel Risk Difference Estimator in Randomized Clinical Trials,"The Mantel-Haenszel (MH) risk difference estimator, commonly used in
randomized clinical trials for binary outcomes, calculates a weighted average
of stratum-specific risk difference estimators. Traditionally, this method
requires the stringent assumption that risk differences are homogeneous across
strata, also known as the common risk difference assumption. In our article, we
relax this assumption and adopt a modern perspective, viewing the MH risk
difference estimator as an approach for covariate adjustment in randomized
clinical trials, distinguishing its use from that in meta-analysis and
observational studies. We demonstrate that the MH risk difference estimator
consistently estimates the average treatment effect within a standard
super-population framework, which is often the primary interest in randomized
clinical trials, in addition to estimating a weighted average of
stratum-specific risk difference. We rigorously study its properties under both
the large-stratum and sparse-stratum asymptotic regimes. Furthermore, for
either estimand, we propose a unified robust variance estimator that improves
over the popular variance estimators by Greenland and Robins (1985) and Sato et
al. (1989) and has provable consistency across both asymptotic regimes,
regardless of assuming common risk differences. Extensions of our theoretical
results also provide new insights into the Cochran-Mantel-Haenszel test and the
post-stratification estimator. Our findings are thoroughly validated through
simulations and a clinical trial example.",http://arxiv.org/abs/2408.12541v1,"Xiaoyu Qiu, Yuhan Qian, Jaehwan Yi, Jinqiu Wang, Yu Du, Yanyao Yi, Ting Ye"
247,A Prototype Model of Zero-Trust Architecture Blockchain with EigenTrust-Based Practical Byzantine Fault Tolerance Protocol to Manage Decentralized Clinical Trials,"The COVID-19 pandemic necessitated the emergence of decentralized Clinical
Trials (DCTs) due to patient retention, accelerate trials, improve data
accessibility, enable virtual care, and facilitate seamless communication
through integrated systems. However, integrating systems in DCTs exposes
clinical data to potential security threats, making them susceptible to theft
at any stage, a high risk of protocol deviations, and monitoring issues. To
mitigate these challenges, blockchain technology serves as a secure framework,
acting as a decentralized ledger, creating an immutable environment by
establishing a zero-trust architecture, where data are deemed untrusted until
verified. In combination with Internet of Things (IoT)-enabled wearable
devices, blockchain secures the transfer of clinical trial data on private
blockchains during DCT automation and operations. This paper proposes a
prototype model of the Zero-Trust Architecture Blockchain (z-TAB) to integrate
patient-generated clinical trial data during DCT operation management. The
EigenTrust-based Practical Byzantine Fault Tolerance (T-PBFT) algorithm has
been incorporated as a consensus protocol, leveraging Hyperledger Fabric.
Furthermore, the Internet of Things (IoT) has been integrated to streamline
data processing among stakeholders within the blockchain platforms. Rigorous
evaluation has been done to evaluate the quality of the system.",http://arxiv.org/abs/2408.16885v1,"Ashok Kumar Peepliwall, Hari Mohan Pandey, Surya Prakash, Anand A Mahajan, Sudhinder Singh Chowhan, Vinesh Kumar, Rahul Sharma"
248,Doubly robust estimation and sensitivity analysis with outcomes truncated by death in multi-arm clinical trials,"In clinical trials, the observation of participant outcomes may frequently be
hindered by death, leading to ambiguity in defining a scientifically meaningful
final outcome for those who die. Principal stratification methods are valuable
tools for addressing the average causal effect among always-survivors, i.e.,
the average treatment effect among a subpopulation in the principal strata of
those who would survive regardless of treatment assignment. Although robust
methods for the truncation-by-death problem in two-arm clinical trials have
been previously studied, its expansion to multi-arm clinical trials remains
unknown. In this article, we study the identification of a class of survivor
average causal effect estimands with multiple treatments under monotonicity and
principal ignorability, and first propose simple weighting and regression
approaches. As a further improvement, we then derive the efficient influence
function to motivate doubly robust estimators for the survivor average causal
effects in multi-arm clinical trials. We also articulate sensitivity methods
under violations of key causal assumptions. Extensive simulations are conducted
to investigate the finite-sample performance of the proposed methods, and a
real data example is used to illustrate how to operationalize the proposed
estimators and the sensitivity methods in practice.",http://arxiv.org/abs/2410.07483v1,"Jiaqi Tong, Chao Cheng, Guangyu Tong, Michael O Harhay, Fan Li"
249,naplib-python: Neural Acoustic Data Processing and Analysis Tools in Python,"Recently, the computational neuroscience community has pushed for more
transparent and reproducible methods across the field. In the interest of
unifying the domain of auditory neuroscience, naplib-python provides an
intuitive and general data structure for handling all neural recordings and
stimuli, as well as extensive preprocessing, feature extraction, and analysis
tools which operate on that data structure. The package removes many of the
complications associated with this domain, such as varying trial durations and
multi-modal stimuli, and provides a general-purpose analysis framework that
interfaces easily with existing toolboxes used in the field.",http://arxiv.org/abs/2304.01799v1,"Gavin Mischler, Vinay Raghavan, Menoua Keshishian, Nima Mesgarani"
250,An introduction to group sequential methods: planning and multi-aspect optimization,"A group sequential clinical trial design can be an attractive option when
planning a pivotal trial as this approach has the ability to stop the trial
early for success, whilst also being well accepted from a regulatory review
perspective. Compared to a single stage design there are more moving parts to
consider and optimise when planning a group sequential trial. This tutorial
briefly outlines the group sequential methodology before detailing some of the
key operating characteristics and how these can be estimated, optimised and
ultimately presented to decision makers when aligning on a final study design.",http://arxiv.org/abs/2303.01040v1,Fraser I Lewis
251,Estimating the Causal Effects of T Cell Receptors,"A central question in human immunology is how a patient's repertoire of T
cells impacts disease. Here, we introduce a method to infer the causal effects
of T cell receptor (TCR) sequences on patient outcomes using observational TCR
repertoire sequencing data and clinical outcomes data. Our approach corrects
for unobserved confounders, such as a patient's environment and life history,
by using the patient's immature, pre-selection TCR repertoire. The
pre-selection repertoire can be estimated from nonproductive TCR data, which is
widely available. It is generated by a randomized mutational process, V(D)J
recombination, which provides a natural experiment. We show formally how to use
the pre-selection repertoire to draw causal inferences, and develop a scalable
neural-network estimator for our identification formula. Our method produces an
estimate of the effect of interventions that add a specific TCR sequence to
patient repertoires. As a demonstration, we use it to analyze the effects of
TCRs on COVID-19 severity, uncovering potentially therapeutic TCRs that are (1)
observed in patients, (2) bind SARS-CoV-2 antigens in vitro and (3) have strong
positive effects on clinical outcomes.",http://arxiv.org/abs/2410.14127v1,"Eli N Weinstein, Elizabeth B Wood, David M Blei"
252,Learning Eligibility in Cancer Clinical Trials using Deep Neural Networks,"Interventional cancer clinical trials are generally too restrictive, and some
patients are often excluded on the basis of comorbidity, past or concomitant
treatments, or the fact that they are over a certain age. The efficacy and
safety of new treatments for patients with these characteristics are,
therefore, not defined. In this work, we built a model to automatically predict
whether short clinical statements were considered inclusion or exclusion
criteria. We used protocols from cancer clinical trials that were available in
public registries from the last 18 years to train word-embeddings, and we
constructed a~dataset of 6M short free-texts labeled as eligible or not
eligible. A text classifier was trained using deep neural networks, with
pre-trained word-embeddings as inputs, to predict whether or not short
free-text statements describing clinical information were considered eligible.
We additionally analyzed the semantic reasoning of the word-embedding
representations obtained and were able to identify equivalent treatments for a
type of tumor analogous with the drugs used to treat other tumors. We show that
representation learning using {deep} neural networks can be successfully
leveraged to extract the medical knowledge from clinical trial protocols for
potentially assisting practitioners when prescribing treatments.",http://arxiv.org/abs/1803.08312v3,"Aurelia Bustos, Antonio Pertusa"
253,New designs for Bayesian adaptive cluster-randomized trials,"Adaptive approaches, allowing for more flexible trial design, have been
proposed for individually randomized trials to save time or reduce sample size.
However, adaptive designs for cluster-randomized trials in which groups of
participants rather than individuals are randomized to treatment arms are less
common. Motivated by a cluster-randomized trial designed to assess the
effectiveness of a machine-learning based clinical decision support system for
physicians treating patients with depression, two Bayesian adaptive designs for
cluster-randomized trials are proposed to allow for early stopping for efficacy
at pre-planned interim analyses. The difference between the two designs lies in
the way that participants are sequentially recruited. Given a maximum number of
clusters as well as maximum cluster size allowed in the trial, one design
sequentially recruits clusters with the given maximum cluster size, while the
other recruits all clusters at the beginning of the trial but sequentially
enrolls individual participants until the trial is stopped early for efficacy
or the final analysis has been reached. The design operating characteristics
are explored via simulations for a variety of scenarios and two outcome types
for the two designs. The simulation results show that for different outcomes
the design choice may be different. We make recommendations for designs of
Bayesian adaptive cluster-randomized trial based on the simulation results.",http://arxiv.org/abs/2201.02301v1,"Junwei Shen, Shirin Golchi, Erica E M Moodie, David Benrimoh"
254,Prognostic Adjustment with Efficient Estimators to Unbiasedly Leverage Historical Data in Randomized Trials,"Although randomized controlled trials (RCTs) are a cornerstone of comparative
effectiveness, they typically have much smaller sample size than observational
studies because of financial and ethical considerations. Therefore there is
interest in using plentiful historical data (either observational data or prior
trials) to reduce trial sizes. Previous estimators developed for this purpose
rely on unrealistic assumptions, without which the added data can bias the
treatment effect estimate. Recent work proposed an alternative method
(prognostic covariate adjustment) that imposes no additional assumptions and
increases efficiency in trial analyses. The idea is to use historical data to
learn a prognostic model: a regression of the outcome onto the covariates. The
predictions from this model, generated from the RCT subjects' baseline
variables, are then used as a covariate in a linear regression analysis of the
trial data. In this work, we extend prognostic adjustment to trial analyses
with nonparametric efficient estimators, which are more powerful than linear
regression. We provide theory that explains why prognostic adjustment improves
small-sample point estimation and inference without any possibility of bias.
Simulations corroborate the theory: efficient estimators using prognostic
adjustment compared to without provides greater power (i.e., smaller standard
errors) when the trial is small. Population shifts between historical and trial
data attenuate benefits but do not introduce bias. We showcase our estimator
using clinical trial data provided by Novo Nordisk A/S that evaluates insulin
therapy for individuals with type II diabetes.",http://arxiv.org/abs/2305.19180v4,"Lauren D Liao, Emilie HjbjerreFrandsen, Alan E Hubbard, Alejandro Schuler"
255,Blinded sample size re-estimation in three-arm trials with 'gold standard' design,"The sample size of a clinical trial relies on information about nuisance
parameters such as the outcome variance. When no or only limited information is
available, it has been proposed to include an internal pilot study in the
design of the trial. Based on the results of the internal pilot study, the
initially planned sample size can be adjusted. In this paper, we study blinded
sample size re-estimation in the 'gold standard' design for normally
distributed outcomes. The 'gold standard' design is a three-arm clinical trial
design which includes an active and a placebo control in addition to an
experimental treatment. We compare several sample size re-estimation procedures
in a simulation study assessing operating characteristics including power and
type I error. We find that sample size re-estimation based on the popular
one-sample variance estimator results in overpowered trials. Moreover, sample
size re-estimation based on unbiased variance estimators such as the Xing-Ganju
variance estimator results in underpowered trials, as it is expected since an
overestimation of the variance and thus the sample size is in general required
for the re-estimation procedure to eventually meet the target power. Moreover,
we propose an inflation factor for the sample size re-estimation with the
Xing-Ganju variance estimator and show that this approach results in adequately
powered trials. Due to favorable features of Xing-Ganju variance estimator such
as unbiasedness and a distribution independent of the group means, the
inflation factor does not depend on the nuisance parameter and, therefore, can
be calculated prior to a trial.",http://arxiv.org/abs/1610.09878v2,"Tobias Mtze, Tim Friede"
256,DeepEnroll: Patient-Trial Matching with Deep Embedding and Entailment Prediction,"Clinical trials are essential for drug development but often suffer from
expensive, inaccurate and insufficient patient recruitment. The core problem of
patient-trial matching is to find qualified patients for a trial, where patient
information is stored in electronic health records (EHR) while trial
eligibility criteria (EC) are described in text documents available on the web.
How to represent longitudinal patient EHR? How to extract complex logical rules
from EC? Most existing works rely on manual rule-based extraction, which is
time consuming and inflexible for complex inference. To address these
challenges, we proposed DeepEnroll, a cross-modal inference learning model to
jointly encode enrollment criteria (text) and patients records (tabular data)
into a shared latent space for matching inference. DeepEnroll applies a
pre-trained Bidirectional Encoder Representations from Transformers(BERT) model
to encode clinical trial information into sentence embedding. And uses a
hierarchical embedding model to represent patient longitudinal EHR. In
addition, DeepEnroll is augmented by a numerical information embedding and
entailment module to reason over numerical information in both EC and EHR.
These encoders are trained jointly to optimize patient-trial matching score. We
evaluated DeepEnroll on the trial-patient matching task with demonstrated on
real world datasets. DeepEnroll outperformed the best baseline by up to 12.4%
in average F1.",http://arxiv.org/abs/2001.08179v2,"Xingyao Zhang, Cao Xiao, Lucas M Glass, Jimeng Sun"
257,From Estimands to Robust Inference of Treatment Effects in Platform Trials,"A platform trial is an innovative clinical trial design that uses a master
protocol (i.e., one overarching protocol) to evaluate multiple treatments in an
ongoing manner and can accelerate the evaluation of new treatments. However,
its flexibility introduces inferential challenges, with two fundamental ones
being the precise definition of treatment effects and robust, efficient
inference on these effects. Central to these challenges is defining an
appropriate target population for the estimand, as the populations represented
by some commonly used analysis approaches can arbitrarily depend on the
randomization ratio or trial type. For the first time, this article presents a
clear framework for constructing a clinically meaningful estimand with precise
specificity regarding the population of interest. The proposed entire
concurrently eligible (ECE) population not only preserves the integrity of
randomized comparisons but also remains invariant to both the randomization
ratio and trial type. This lays the groundwork for future design, analysis, and
research of platform trials. Then, we develop weighting and post-stratification
methods for estimation of treatment effects with minimal assumptions. To fully
leverage the efficiency potential of platform trials, we also consider a
model-assisted approach for baseline covariate adjustment to gain efficiency
while maintaining robustness against model misspecification. We derive and
compare asymptotic distributions of proposed estimators in theory and propose
robust variance estimators. The proposed estimators are empirically evaluated
in a simulation study and applied to the SIMPLIFY trial, using the R package
RobinCID.",http://arxiv.org/abs/2411.12944v3,"Yuhan Qian, Yifan Yi, Jun Shao, Yanyao Yi, Gregory Levin, Nicole MayerHamblett, Patrick J Heagerty, Ting Ye"
258,Increased adaptive immune responses and proper feedback regulation protect against clinical dengue,"Dengue is the most prevalent arthropod-borne viral disease. Clinical symptoms
of dengue virus (DENV) infection range from classical mild dengue fever to
severe, life-threatening dengue shock syndrome. However, most DENV infections
cause few or no symptoms. Asymptomatic DENV-infected patients provide a unique
opportunity to decipher the host immune responses leading to virus elimination
without negative impact on t v 'health. We used an integrated approach of
transcriptional profiling and immunological analysis comparing a Cambodian
population of strictly asymptomatic viremic individuals with clinical dengue
patients. Whereas inflammatory pathways and innate immune responses were
similar between asymptomatic individuals and clinical dengue patients,
expression of proteins related to antigen presentation and subsequent T and B
cell activation pathways were differentially regulated, independent of viral
load or previous DENV infection. Feedback mechanisms controlled the immune
response in asymptomatic viremic individuals as demonstrated by increased
activation of T cell apoptosis-related pathways and Fc$\gamma$RIIB signaling
associated with decreased anti-DENV specific antibody concentrations. Taken
together, our data illustrate that symptom-free DENV infection in children is
determined by increased activation of the adaptive immune compartment and
proper control mechanisms leading to elimination of viral infection without
excessive immune activation, having implications for novel vaccine development
strategies.",http://arxiv.org/abs/1712.05692v1,"Etienne SimonLoriere, Veasna Duong, Ahmed Tawfik, Sivlin Ung, Sowath Ly, Isabelle Casademont, Matthieu Prot, Nomie Courtejoie, Kevin Bleakley, Philippe Buchy, Arnaud Tarantola, Philippe Dussart, Tineke Cantaert, Anavaj Sakuntabhai"
259,Potential of proteasome inhibitors to inhibit cytokine storm in critical stage COVID-19 patients,"Patients infected with SARS-CoV-2 show a wide spectrum of clinical
manifestations ranging from mild febrile illness and cough up to acute
respiratory distress syndrome, multiple organ failure and death. Data from
patients with severe clinical manifestations compared to patients with mild
symptoms indicate that highly dysregulated exuberant inflammatory responses
correlate with severity of disease and lethality. Significantly elevated
cytokine levels, i.e. cytokine storm, seem to play a central role in severity
and lethality in COVID-19. We have previously shown that excessive cytokine
release induced by highly pathogenic avian H5N1 influenza A virus was reduced
by application of proteasome inhibitors. In the present study we present
experimental data of a central cellular pro-inflammatory signal pathways,
NF-kappaB, in the context of published clinical data from COVID-19 patients and
develop a hypothesis for a therapeutic approach aiming at the simultaneous
inhibition of whole cascades of pro-inflammatory cytokines and chemokines via
blocking the nuclear translocation of NF-kappaB by proteasome inhibitors. The
simultaneous inhibition of multiple cytokines/chemokines using clinically
approved proteasome inhibitors is expected to have a higher therapeutic
potential compared to single target approaches to prevent cascade (i.e.
triggering, synergistic, and redundant) effects of multiple induced cytokines
and may provide an additional therapeutic option to be explored for treatment
of critical stage COVID-19 patients.",http://arxiv.org/abs/2008.10404v1,"Ralf Kircheis, Emanuel Haasbach, Daniel Lueftenegger, Willm T Heyken, Matthias Ocker, Oliver Planz"
260,TrialGraph: Machine Intelligence Enabled Insight from Graph Modelling of Clinical Trials,"A major impediment to successful drug development is the complexity, cost,
and scale of clinical trials. The detailed internal structure of clinical trial
data can make conventional optimization difficult to achieve. Recent advances
in machine learning, specifically graph-structured data analysis, have the
potential to enable significant progress in improving the clinical trial
design. TrialGraph seeks to apply these methodologies to produce a
proof-of-concept framework for developing models which can aid drug development
and benefit patients. In this work, we first introduce a curated clinical trial
data set compiled from the CT.gov, AACT and TrialTrove databases (n=1191
trials; representing one million patients) and describe the conversion of this
data to graph-structured formats. We then detail the mathematical basis and
implementation of a selection of graph machine learning algorithms, which
typically use standard machine classifiers on graph data embedded in a
low-dimensional feature space. We trained these models to predict side effect
information for a clinical trial given information on the disease, existing
medical conditions, and treatment. The MetaPath2Vec algorithm performed
exceptionally well, with standard Logistic Regression, Decision Tree, Random
Forest, Support Vector, and Neural Network classifiers exhibiting typical
ROC-AUC scores of 0.85, 0.68, 0.86, 0.80, and 0.77, respectively. Remarkably,
the best performing classifiers could only produce typical ROC-AUC scores of
0.70 when trained on equivalent array-structured data. Our work demonstrates
that graph modelling can significantly improve prediction accuracy on
appropriate datasets. Successive versions of the project that refine modelling
assumptions and incorporate more data types can produce excellent predictors
with real-world applications in drug development.",http://arxiv.org/abs/2112.08211v1,"Christopher Yacoumatos, Stefano Bragaglia, Anshul Kanakia, Nils Svangrd, Jonathan Mangion, Claire Donoghue, Jim Weatherall, Faisal M Khan, Khader Shameer"
261,Dose Finding with Escalation with Overdose Control (EWOC) in Cancer Clinical Trials,"Traditionally, the major objective in phase I trials is to identify a
working-dose for subsequent studies, whereas the major endpoint in phase II and
III trials is treatment efficacy. The dose sought is typically referred to as
the maximum tolerated dose (MTD). Several statistical methodologies have been
proposed to select the MTD in cancer phase I trials. In this manuscript, we
focus on a Bayesian adaptive design, known as escalation with overdose control
(EWOC). Several aspects of this design are discussed, including large sample
properties of the sequence of doses selected in the trial, choice of prior
distributions, and use of covariates. The methodology is exemplified with
real-life examples of cancer phase I trials. In particular, we show in the
recently completed ABR-217620 (naptumomab estafenatox) trial that omitting an
important predictor of toxicity when dose assignments to cancer patients are
determined results in a high percent of patients experiencing severe side
effects and a significant proportion treated at sub-optimal doses.",http://arxiv.org/abs/1011.6479v1,"Mourad Tighiouart, Andr Rogatko"
262,Likelihood reweighting methods to reduce potential bias in noninferiority trials which rely on historical data to make inference,"It is generally believed that bias is minimized in well-controlled randomized
clinical trials. However, bias can arise in active controlled noninferiority
trials because the inference relies on a previously estimated effect size
obtained from a historical trial that may have been conducted for a different
population. By implementing a likelihood reweighting method through propensity
scoring, a study designed to estimate a treatment effect in one trial
population can be used to estimate the treatment effect size in a different
target population. We illustrate this method in active controlled
noninferiority trials, although it can also be used in other types of studies,
such as historically controlled trials, meta-analyses, and comparative
effectiveness analyses.",http://arxiv.org/abs/1311.7485v1,"Lei Nie, Zhiwei Zhang, Daniel Rubin, Jianxiong Chu"
263,"Designing an exploratory phase 2b platform trial in NASH with correlated, co-primary binary endpoints","Non-alcoholic steatohepatitis (NASH) is the progressive form of nonalcoholic
fatty liver disease (NAFLD) and a disease with high unmet medical need.
Platform trials provide great benefits for sponsors and trial participants in
terms of accelerating drug development programs. In this article, we describe
some of the activities of the EU-PEARL consortium (EU Patient-cEntric clinicAl
tRial pLatforms) regarding the use of platform trials in NASH, in particular
the proposed trial design, decision rules and simulation results. For a set of
assumptions, we present the results of a simulation study recently discussed
with two health authorities and the learnings from these meetings from a trial
design perspective. Since the proposed design uses co-primary binary endpoints,
we furthermore discuss the different options and practical considerations for
simulating correlated binary endpoints.",http://arxiv.org/abs/2210.06228v2,"Elias Laurin Meyer, Peter Mesenbrink, Nicholas A Di Prospero, Juan M Perics, Ekkehard Glimm, Vlad Ratziu, Elena Sena, Franz Knig"
264,A multi-arm multi-stage design for trials with all pairwise testing,"Multi-arm multi-stage (MAMS) trials have gained popularity to enhance the
efficiency of clinical trials, potentially reducing both duration and costs.
This paper focuses on designing MAMS trials where no control treatment exists.
This can arise when multiple standard treatments are already established or no
treatment is available for a severe disease, making it unethical to withhold a
potentially helpful option. The proposed design incorporates interim analyses
to allow early termination of notably worst treatments and stops the trial
entirely if all remaining treatments are performing similarly. The proposed
design controls the familywise error rate (FWER) for all pairwise comparisons
and provides the conditions guaranteeing FWER control in the strong sense. The
FWER and power are used to calculate both the stopping boundaries and the
sample size required. Analytic solutions to compute the expected sample size
are also derived. A trial motivated by a study conducted in sepsis, where there
was no control treatment, is shown. The multi-arm multi-stage all pairwise
(MAMSAP) design proposed here is compared to multiple different approaches. For
the trial studied, the proposed method yields the lowest required maximum and
expected sample size when controlling the FWER and power at the desired levels.",http://arxiv.org/abs/2502.07013v1,"Peter Greenstreet, Thomas Jaki, Alun Bedding, Pavel Mozgunov"
265,An Empirical Likelihood Approach to Nonparametric Covariate Adjustment in Randomized Clinical Trials,"Covariate adjustment is an important tool in the analysis of randomized
clinical trials and observational studies. It can be used to increase
efficiency and thus power, and to reduce possible bias. While most statistical
tests in randomized clinical trials are nonparametric in nature, approaches for
covariate adjustment typically rely on specific regression models, such as the
linear model for a continuous outcome, the logistic regression model for a
dichotomous outcome and the Cox model for survival time. Several recent efforts
have focused on model-free covariate adjustment. This paper makes use of the
empirical likelihood method and proposes a nonparametric approach to covariate
adjustment. A major advantage of the new approach is that it automatically
utilizes covariate information in an optimal way without fitting nonparametric
regression. The usual asymptotic properties, including the Wilks-type result of
convergence to a chi-square distribution for the empirical likelihood ratio
based test, and asymptotic normality for the corresponding maximum empirical
likelihood estimator, are established. It is also shown that the resulting test
is asymptotically most powerful and that the estimator for the treatment effect
achieves the semiparametric efficiency bound. The new method is applied to the
Global Use of Strategies to Open Occluded Coronary Arteries (GUSTO)-I trial.
Extensive simulations are conducted, validating the theoretical findings.",http://arxiv.org/abs/1108.0484v1,"Xiaoru Wu, Zhiliang Ying"
266,C-PASS-PC: A Cloud-driven Prototype of Multi-Center Proactive Surveillance System for Prostate Cancer,"Currently there are many clinical trials using paper case report forms as the
primary data collection tool. Cloud Computing platforms provide big potential
for increasing efficiency through a web-based data collection interface,
especially for large-scale multi-center trials. Traditionally, clinical and
biological data for multi-center trials are stored in one dedicated,
centralized database system running at a data coordinating center (DCC). This
paper presents C-PASS-PC, a cloud-driven prototype of multi-center proactive
surveillance system for prostate cancer. The prototype is developed in PHP,
JQuery and CSS with an Oracle backend in a local Web server and database server
and deployed on Google App Engine (GAE) and Google Cloud SQL-MySQL. The
deploying process is fast and easy to follow. The C-PASS-PC prototype can be
accessed through an SSL-enabled web browser. Our approach proves the concept
that cloud computing platforms such as GAE is a suitable and flexible solution
in the near future for multi-center clinical trials.",http://arxiv.org/abs/1209.2641v1,Haibin Wang
267,Use of Historical Individual Patient Data in Analysis of Clinical Trials,"Historical data from previous clinical trials, observational studies and
health records may be utilized in analysis of clinical trials data to
strengthen inference. Under the Bayesian framework incorporation of information
obtained from any source other than the current data is facilitated through
construction of an informative prior. The existing methodology for defining an
informative prior based on historical data relies on measuring similarity to
the current data at the study level and does not take advantage of individual
patient data (IPD). This paper proposes a family of priors that utilize IPD to
strengthen statistical inference. It is demonstrated that the proposed prior
construction approach outperforms the existing methods where the historical
data are partially exchangeable with the present data. The proposed method is
applied to IPD from a set of trials in non-small cell lung cancer.",http://arxiv.org/abs/2002.09910v2,Shirin Golchi
268,Towards reliable and transparent vaccine phase III trials with smart contracts,"Transforming a vaccine concept into a real vaccine product is a complicated
process and includes finding suitable antigens and regulatory, technical, and
manufacturing obstacles. A relevant issue within this scope is the clinical
trial process. Monitoring and ensuring the integrity of trial data using the
traditional system is not always feasible. The search for a vaccine against the
coronavirus SARS-CoV-2 illustrates this situation. The scientific credibility
of findings from several vaccines' clinical trials contributed to distorted
perceptions concerning the benefits and risks of the drug. This scenario is
ideal for applying technologies such as Blockchain and Smart Contracts in
healthcare issues. This paper proposes a protocol based on Smart Contracts,
named VaccSC, to enable transparency, accounting, and confidentiality to Phase
III of vaccine experiments. The protocol was implemented in Solidity language,
and results show that the VaccSC enables double-blindness, randomization, and
the auditability of clinical data, even in the presence of dishonest
participants.",http://arxiv.org/abs/2102.07022v1,"Ivan da Silva Sendin, Rodrigo Sanches Miani"
269,Understanding Clinical Trial Reports: Extracting Medical Entities and Their Relations,"The best evidence concerning comparative treatment effectiveness comes from
clinical trials, the results of which are reported in unstructured articles.
Medical experts must manually extract information from articles to inform
decision-making, which is time-consuming and expensive. Here we consider the
end-to-end task of both (a) extracting treatments and outcomes from full-text
articles describing clinical trials (entity identification) and, (b) inferring
the reported results for the former with respect to the latter (relation
extraction). We introduce new data for this task, and evaluate models that have
recently achieved state-of-the-art results on similar tasks in Natural Language
Processing. We then propose a new method motivated by how trial results are
typically presented that outperforms these purely data-driven baselines.
Finally, we run a fielded evaluation of the model with a non-profit seeking to
identify existing drugs that might be re-purposed for cancer, showing the
potential utility of end-to-end evidence extraction systems.",http://arxiv.org/abs/2010.03550v3,"Benjamin E Nye, Jay DeYoung, Eric Lehman, Ani Nenkova, Iain J Marshall, Byron C Wallace"
270,Customizing Knowledge Graph Embedding to Improve Clinical Study Recommendation,"Inferring knowledge from clinical trials using knowledge graph embedding is
an emerging area. However, customizing graph embeddings for different use cases
remains a significant challenge. We propose custom2vec, an algorithmic
framework to customize graph embeddings by incorporating user preferences in
training the embeddings. It captures user preferences by adding custom nodes
and links derived from manually vetted results of a separate information
retrieval method. We propose a joint learning objective to preserve the
original network structure while incorporating the user's custom annotations.
We hypothesize that the custom training improves user-expected predictions, for
example, in link prediction tasks. We demonstrate the effectiveness of
custom2vec for clinical trials related to non-small cell lung cancer (NSCLC)
with two customization scenarios: recommending immuno-oncology trials
evaluating PD-1 inhibitors and exploring similar trials that compare new
therapies with a standard of care. The results show that custom2vec training
achieves better performance than the conventional training methods. Our
approach is a novel way to customize knowledge graph embeddings and enable more
accurate recommendations and predictions.",http://arxiv.org/abs/2212.14102v1,"Xiong Liu, Iya Khalil, Murthy Devarakonda"
271,A Connection Between Covariate Adjustment and Stratified Randomization in Randomized Clinical Trials,"The statistical efficiency of randomized clinical trials can be improved by
incorporating information from baseline covariates (i.e., pre-treatment patient
characteristics). This can be done in the design stage using stratified
(permutated block) randomization or in the analysis stage through covariate
adjustment. This article makes a connection between covariate adjustment and
stratified randomization in a general framework where all regular,
asymptotically linear estimators are identified as augmented estimators. From a
geometric perspective, covariate adjustment can be viewed as an attempt to
approximate the optimal augmentation function, and stratified randomization
improves a given approximation by moving it closer to the optimal augmentation
function. The efficiency benefit of stratified randomization is asymptotically
equivalent to attaching an optimal augmentation term based on the
stratification factor. In designing a trial with stratified randomization, it
is not essential to include all important covariates in the stratification,
because their prognostic information can be incorporated through covariate
adjustment. Under stratified randomization, adjusting for the stratification
factor only in data analysis is not expected to improve efficiency, and the key
to efficient estimation is incorporating prognostic information from all
important covariates. These observations are confirmed in a simulation study
and illustrated using real clinical trial data.",http://arxiv.org/abs/2401.11352v3,Zhiwei Zhang
272,A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial,"Dental disease is a prevalent chronic condition associated with substantial
financial burden, personal suffering, and increased risk of systemic diseases.
Despite widespread recommendations for twice-daily tooth brushing, adherence to
recommended oral self-care behaviors remains sub-optimal due to factors such as
forgetfulness and disengagement. To address this, we developed Oralytics, a
mHealth intervention system designed to complement clinician-delivered
preventative care for marginalized individuals at risk for dental disease.
Oralytics incorporates an online reinforcement learning algorithm to determine
optimal times to deliver intervention prompts that encourage oral self-care
behaviors. We have deployed Oralytics in a registered clinical trial. The
deployment required careful design to manage challenges specific to the
clinical trials setting in the U.S. In this paper, we (1) highlight key design
decisions of the RL algorithm that address these challenges and (2) conduct a
re-sampling analysis to evaluate algorithm design decisions. A second phase
(randomized control trial) of Oralytics is planned to start in spring 2025.",http://arxiv.org/abs/2409.02069v2,"Anna L Trella, Kelly W Zhang, Hinal Jajal, Inbal NahumShani, Vivek Shetty, Finale DoshiVelez, Susan A Murphy"
273,Controlled LLM-based Reasoning for Clinical Trial Retrieval,"Matching patients to clinical trials demands a systematic and reasoned
interpretation of documents which require significant expert-level background
knowledge, over a complex set of well-defined eligibility criteria. Moreover,
this interpretation process needs to operate at scale, over vast knowledge
bases of trials. In this paper, we propose a scalable method that extends the
capabilities of LLMs in the direction of systematizing the reasoning over sets
of medical eligibility criteria, evaluating it in the context of real-world
cases. The proposed method overlays a Set-guided reasoning method for LLMs. The
proposed framework is evaluated on TREC 2022 Clinical Trials, achieving results
superior to the state-of-the-art: NDCG@10 of 0.693 and Precision@10 of 0.73.",http://arxiv.org/abs/2409.18998v1,"Mael Jullien, Alex Bogatu, Harriet Unsworth, Andre Freitas"
274,Digital Twinning of the Human Ventricular Activation Sequence to Clinical 12-lead ECGs and Magnetic Resonance Imaging Using Realistic Purkinje Networks for in Silico Clinical Trials,"Cardiac in silico clinical trials can virtually assess the safety and
efficacy of therapies using human-based modelling and simulation. These
technologies can provide mechanistic explanations for clinically observed
pathological behaviour. Designing virtual cohorts for in silico trials requires
exploiting clinical data to capture the physiological variability in the human
population. The clinical characterisation of ventricular activation and the
Purkinje network is challenging, especially non-invasively. Our study aims to
present a novel digital twinning pipeline that can efficiently generate and
integrate Purkinje networks into human multiscale biventricular models based on
subject-specific clinical 12-lead electrocardiogram and magnetic resonance
recordings. Essential novel features of the pipeline are the human-based
Purkinje network generation method, personalisation considering ECG R wave
progression as well as QRS morphology, and translation from reduced-order
Eikonal models to equivalent biophysically-detailed monodomain ones. We
demonstrate ECG simulations in line with clinical data with clinical
image-based multiscale models with Purkinje in four control subjects and two
hypertrophic cardiomyopathy patients (simulated and clinical QRS complexes with
Pearson's correlation coefficients > 0.7). Our methods also considered possible
differences in the density of Purkinje myocardial junctions in the
Eikonal-based inference as regional conduction velocities. These differences
translated into regional coupling effects between Purkinje and myocardial
models in the monodomain formulation. In summary, we demonstrate a digital twin
pipeline enabling simulations yielding clinically-consistent ECGs with clinical
CMR image-based biventricular multiscale models, including personalised
Purkinje in healthy and cardiac disease conditions.",http://arxiv.org/abs/2306.13740v1,"Julia Camps, Lucas Arantes Berg, Zhinuo Jenny Wang, Rafael Sebastian, Leto Luana Riebel, Ruben Doste, Xin Zhou, Rafael Sachetto, James Coleman, Brodie Lawson, Vicente Grau, Kevin Burrage, Alfonso BuenoOrovio, Rodrigo Weber, Blanca Rodriguez"
275,On Minimum Clinically Important Difference,"In clinical trials, minimum clinically important difference (MCID) has
attracted increasing interest as an important supportive clinical and
statistical inference tool. Many estimation methods have been developed based
on various intuitions, while little theoretical justification has been
established. This paper proposes a new estimation framework of MCID using both
diagnostic measurements and patient-reported outcomes (PROs). It first provides
a precise definition of population-based MCID so that estimating such a MCID
can be formulated as a large margin classification problem. The framework is
then extended to personalized MCID to allow individualized thresholding value
for patients whose clinical profiles may affect their PRO responses. More
importantly, we show that the proposed estimation framework is asymptotically
consistent, and a finite-sample upper bound is established for its prediction
accuracy compared against the ideal MCID. The advantage of our proposed method
is also demonstrated in a variety of simulated experiments as well as
applications to two benchmark datasets and two phase-3 clinical trials.",http://arxiv.org/abs/1307.3646v2,"A S Hedayat, Junhui Wang, Tu Xu"
276,A Statistical Inference Framework for the Minimal Clinically Important Difference,"In clinical research, the effect of a treatment or intervention is widely
assessed through clinical importance, instead of statistical significance. In
this paper, we propose a principled statistical inference framework to learning
the minimal clinically important difference (MCID), a vital concept in
assessing clinical importance. We formulate the scientific question into a
novel statistical learning problem, develop an efficient algorithm for
parameter estimation, and establish the asymptotic theory for the proposed
estimator. We conduct comprehensive simulation studies to examine the finite
sample performance of the proposed method. We also re-analyze the ChAMP
(Chondral Lesions And Meniscus Procedures) trial, where the primary outcome is
the patient-reported pain score and the ultimate goal is to determine whether
there exists a significant difference in post-operative knee pain between
patients undergoing debridement versus observation of chondral lesions during
the surgery. Some previous analysis of this trial exhibited that the effect of
debriding the chondral lesions does not reach a statistical significance. Our
analysis reinforces this conclusion that the effect of debriding the chondral
lesions is not only statistically non-significant, but also clinically
un-important.",http://arxiv.org/abs/2108.11589v3,"Zehua Zhou, Leslie J Bisson, Jiwei Zhao"
277,The use of external controls: To what extent can it currently be recommended?,"With more and better clinical data being captured outside of clinical studies
and greater data sharing of clinical studies, external controls may become a
more attractive alternative to randomized clinical trials. Both industry and
regulators recognize that in situations where a randomized study cannot be
performed, external controls can provide the needed contextualization to allow
a better interpretation of studies without a randomized control. It is also
agreed that external controls will not fully replace randomized clinical trials
as the gold standard for formal proof of efficacy in drug development and the
yardstick of clinical research. However, it remains unclear in which situations
conclusions about efficacy and a positive benefit/risk can reliably be based on
the use of an external control. This paper will provide an overview on types of
external control, their applications and the different sources of bias their
use may incur, and discuss potential mitigation steps. It will also give
recommendations on how the use of external controls can be justified.",http://arxiv.org/abs/2209.07776v1,"Hans Ulrich Burger, Christoph Gerlinger, Chris Harbron, Armin Koch, Martin Posch, Justine Rochon, Anja Schiel"
278,eSource for clinical trials: Implementation and evaluation of a standards-based approach in a real world trial,"Objective: The Learning Health System (LHS) requires integration of research
into routine practice. eSource or embedding clinical trial functionalities into
routine electronic health record (EHR) systems has long been put forward as a
solution to the rising costs of research. We aimed to create and validate an
eSource solution that would be readily extensible as part of a LHS.
  Materials and Methods: The EU FP7 TRANSFoRm project's approach is based on
dual modelling, using the Clinical Research Information Model (CRIM) and the
Clinical Data Integration Model of meaning (CDIM) to bridge the gap between
clinical and research data structures, using the CDISC Operational Data Model
(ODM) standard. Validation against GCP requirements was conducted in a clinical
site, and a cluster randomised evaluation by site nested into a live clinical
trial.
  Results: Using the form definition element of ODM, we linked precisely
modelled data queries to data elements, constrained against CDIM concepts, to
enable automated patient identification for specific protocols and
prepopulation of electronic case report forms (e-CRF). Both control and eSource
sites recruited better than expected with no significant difference.
Completeness of clinical forms was significantly improved by eSource, but
Patient Related Outcome Measures (PROMs) were less well completed on
smartphones than paper in this population.
  Discussion: The TRANSFoRm approach provides an ontologically-based approach
to eSource in a low-resource, heterogeneous, highly distributed environment,
that allows precise prospective mapping of data elements in the EHR.
  Conclusion: Further studies using this approach to CDISC should optimise the
delivery of PROMS, whilst building a sustainable infrastructure for eSource
with research networks, trials units and EHR vendors.",http://arxiv.org/abs/1707.07994v1,"JeanFrancois Ethier, Vasa Curcin, Mark M McGilchrist, Sarah N Lim Choi Keung, Lei Zhao, Anna Andreasson, Piotr Brdka, Radoslaw Michalski, Theodoros N Arvanitis, Nikolaos Mastellos, Anita Burgun, Brendan C Delaney"
279,Anscombe's Model for Sequential Clinical Trials Revisited,"In Anscombe's classical model, the objective is to find the optimal
sequential rule for learning about the difference between two alternative
treatments and subsequently selecting the superior one. The population for
which the procedure is optimised has size $N$ and includes both the patients in
the trial and those which are treated with the chosen alternative after the
trial. We review earlier work on this problem and give a detailed treatment of
the problem itself. In particular, while previous work has mainly focused on
the case of conjugate normal priors for the incremental effect, we demonstrate
how to handle the problem for priors of a general form. We also discuss methods
for numerical solutions and the practical implications of the results for the
regulation of clinical trials.
  Two extensions of the model are proposed and analysed. The first breaks the
symmetry of the treatments, giving one the role of the current standard being
administered in parallel with the trial. We show how certain asymptotic results
due to Chernoff can be adapted to this asymmetric case. The other extension
assumes that $N$ is a random variable instead of a known constant.",http://arxiv.org/abs/1712.05547v1,"Sebastian Jobjrnsson, Sren Christensen"
280,A Model of a Randomized Experiment with an Application to the PROWESS Clinical Trial,"I develop a model of a randomized experiment with a binary intervention and a
binary outcome. Potential outcomes in the intervention and control groups give
rise to four types of participants. Fixing ideas such that the outcome is
mortality, some participants would live regardless, others would be saved,
others would be killed, and others would die regardless. These potential
outcome types are not observable. However, I use the model to develop
estimators of the number of participants of each type. The model relies on the
randomization within the experiment and on deductive reasoning. I apply the
model to an important clinical trial, the PROWESS trial, and I perform a Monte
Carlo simulation calibrated to estimates from the trial. The reduced form from
the trial shows a reduction in mortality, which provided a rationale for FDA
approval. However, I find that the intervention killed two participants for
every three it saved.",http://arxiv.org/abs/1908.05810v2,Amanda Kowalski
281,Guidelines for estimating causal effects in pragmatic randomized trials,"Pragmatic randomized trials are designed to provide evidence for clinical
decision-making rather than regulatory approval. Common features of these
trials include the inclusion of heterogeneous or diverse patient populations in
a wide range of care settings, the use of active treatment strategies as
comparators, unblinded treatment assignment, and the study of long-term,
clinically relevant outcomes. These features can greatly increase the
usefulness of the trial results for patients, clinicians, and other
stakeholders. However, these features also introduce an increased risk of
non-adherence, which reduces the value of the intention-to-treat effect as a
patient-centered measure of causal effect. In these settings, the per-protocol
effect provides useful complementary information for decision making.
Unfortunately, there is little guidance for valid estimation of the
per-protocol effect. Here, we present our full guidelines for analyses of
pragmatic trials that will result in more informative causal inferences for
both the intention-to-treat effect and the per-protocol effect.",http://arxiv.org/abs/1911.06030v2,"Eleanor J Murray, Sonja A Swanson, Miguel A Hernn"
282,Can the potential benefit of individualizing treatment be assessed using trial summary statistics alone?,"Individualizing treatment assignment can improve outcomes for diseases with
patient-to-patient variability in comparative treatment effects. When a
clinical trial demonstrates that some patients improve on treatment while
others do not, it is tempting to assume that treatment effect heterogeneity
exists. However, if variability in response is mainly driven by factors other
than treatment, investigating the extent to which covariate data can predict
differential treatment response is a potential waste of resources. Motivated by
recent meta-analyses assessing the potential of individualizing treatment for
major depressive disorder using only summary statistics, we provide a method
that uses summary statistics widely available in published clinical trial
results to bound the benefit of optimally assigning treatment to each patient.
We also offer alternate bounds for settings in which trial results are
stratified by another covariate. We demonstrate our approach using summary
statistics from a depression treatment trial. Our methods are implemented in
the rct2otrbounds R package, which is available at
https://github.com/ngalanter/rct2otrbounds .",http://arxiv.org/abs/2211.00163v1,"Nina Galanter, Marco Carone, Ronald C Kessler, Alex Luedtke"
283,Some t-tests for N-of-1 trials with serial correlation,"N-of-1 trials allow inference between two treatments given to a single
individual. Most often, clinical investigators analyze an individual's N-of-1
trial data with usual t-tests or simple nonparametric methods. These simple
methods do not account for serial correlation in repeated observations coming
from the individual. Existing methods accounting for serial correlation require
simulation, multiple N-of-1 trials, or both. Here, we develop t-tests that
account for serial correlation in a single individual. The development includes
effect size and precision calculations, both of which are useful for study
planning. We then evaluate and compare their Type I and II errors and interval
estimators to those of usual t-tests analogues via Monte Carlo simulation. The
serial t-tests clearly outperform the usual t-tests commonly used in reporting
N-of-1 results. Examples from N-of-1 clinical trials in fibromyalgia patients
and from a behavioral health setting exhibit how accounting for serial
correlation can change inferences. These t-tests are easily implemented and
more appropriate than simple methods commonly used; however, caution is needed
when analyzing only a few observations. Keywords: Autocorrelation; Cross-over
studies; Repeated measures analysis; Single-case experimental design;
Time-series",http://arxiv.org/abs/1904.01622v2,"Jillian Tang, Reid D Landes"
284,Prediction of cognitive decline for enrichment of Alzheimer's disease clinical trials,"A key issue to Alzheimer's disease clinical trial failures is poor
participant selection. Participants have heterogeneous cognitive trajectories
and many do not decline during trials, which reduces a study's power to detect
treatment effects. Trials need enrichment strategies to enroll individuals who
will decline. We developed machine learning models to predict cognitive
trajectories in participants with early Alzheimer's disease (n=1342) and
presymptomatic individuals (n=756) over 24 and 48 months respectively. Baseline
magnetic resonance imaging, cognitive tests, demographics, and APOE genotype
were used to classify decliners, measured by an increase in CDR-Sum of Boxes,
and non-decliners with up to 79% area under the curve (cross-validated and
out-of-sample). Using these prognostic models to recruit enriched cohorts of
decliners can reduce required sample sizes by as much as 51%, while maintaining
the same detection power, and thus may improve trial quality, derisk endpoint
failures, and accelerate therapeutic development in Alzheimer's disease.",http://arxiv.org/abs/2111.04174v4,"Angela Tam, Csar Laurent, Serge Gauthier, Christian Dansereau"
285,Generalizing Clinical Trials with Convex Hulls,"Randomized clinical trials eliminate confounding but impose strict exclusion
criteria that limit recruitment to a subset of the population. Observational
datasets are more inclusive but suffer from confounding -- often providing
overly optimistic estimates of treatment response over time due to partially
optimized physician prescribing patterns. We therefore assume that the
unconfounded treatment response lies somewhere in-between the observational
estimate before and the observational estimate after treatment assignment. This
assumption allows us to extrapolate results from exclusive trials to the
broader population by analyzing observational and trial data simultaneously
using an algorithm called Optimum in Convex Hulls (OCH). OCH represents the
treatment effect either in terms of convex hulls of conditional expectations or
convex hulls (also known as mixtures) of conditional densities. The algorithm
first learns the component expectations or densities using the observational
data and then learns the linear mixing coefficients using trial data in order
to approximate the true treatment effect; theory importantly explains why this
linear combination should hold. OCH estimates the treatment effect in terms
both expectations and densities with state of the art accuracy.",http://arxiv.org/abs/2111.13229v2,"Eric V Strobl, Thomas A Lasko"
286,Doubly robust omnibus sensitivity analysis of externally controlled trials with intercurrent events,"Externally controlled trials are crucial in clinical development when
randomized controlled trials are unethical or impractical. These trials consist
of a full treatment arm with the experimental treatment and a full external
control arm. However, they present significant challenges in learning the
treatment effect due to the lack of randomization and a parallel control group.
Besides baseline incomparability, outcome mean non-exchangeability, caused by
differences in conditional outcome distributions between external controls and
counterfactual concurrent controls, is infeasible to test and may introduce
biases in evaluating the treatment effect. Sensitivity analysis of outcome mean
non-exchangeability is thus critically important to assess the robustness of
the study's conclusions against such assumption violations. Moreover,
intercurrent events, which are ubiquitous and inevitable in clinical studies,
can further confound the treatment effect and hinder the interpretation of the
estimated treatment effects. This paper establishes a semi-parametric framework
for externally controlled trials with intercurrent events, offering doubly
robust and locally optimal estimators for primary and sensitivity analyses. We
develop an omnibus sensitivity analysis that accounts for both outcome mean
non-exchangeability and the impacts of intercurrent events simultaneously,
ensuring root-n consistency and asymptotic normality under specified
conditions. The performance of the proposed sensitivity analysis is evaluated
in simulation studies and a real-data problem.",http://arxiv.org/abs/2503.06864v1,"Chenyin Gao, Xiang Zhang, Shu Yang"
287,Exploration and Incentivizing Participation in Randomized Trials,"Participation incentives is a well-known issue inhibiting randomized
controlled trials (RCTs) in medicine, as well as a potential cause of user
dissatisfaction for RCTs in online platforms. We frame this issue as a
non-standard exploration-exploitation tradeoff: an RCT would like to explore as
uniformly as possible, whereas each ""agent"" (a patient or a user) prefers
""exploitation"", i.e., treatments that seem best. We incentivize participation
by leveraging information asymmetry between the trial and the agents. We
measure statistical performance via worst-case estimation error under
adversarially generated outcomes, a standard objective for RCTs. We obtain a
near-optimal solution in terms of this objective: an incentive-compatible
mechanism with a particular guarantee, and a nearly matching impossibility
result for any incentive-compatible mechanism. We consider three model
variants: homogeneous agents (of the same ""type"" comprising beliefs and
preferences), heterogeneous agents, and an extension with estimated type
frequencies.",http://arxiv.org/abs/2202.06191v10,"Yingkai Li, Aleksandrs Slivkins"
288,Group sequential methods for interim monitoring of randomized clinical trials with time-lagged outcome,"The primary analysis in two-arm clinical trials usually involves inference on
a scalar treatment effect parameter; e.g., depending on the outcome, the
difference of treatment-specific means, risk difference, risk ratio, or odds
ratio. Most clinical trials are monitored for the possibility of early
stopping. Because ordinarily the outcome on any given subject can be
ascertained only after some time lag, at the time of an interim analysis, among
the subjects already enrolled, the outcome is known for only a subset and is
effectively censored for those who have not been enrolled sufficiently long for
it to be observed. Typically, the interim analysis is based only on the data
from subjects for whom the outcome has been ascertained. A goal of an interim
analysis is to stop the trial as soon as the evidence is strong enough to do
so, suggesting that the analysis ideally should make the most efficient use of
all available data, thus including information on censoring as well as other
baseline and time-dependent covariates in a principled way. A general group
sequential framework is proposed for clinical trials with a time-lagged
outcome. Treatment effect estimators that take account of censoring and
incorporate covariate information at an interim analysis are derived using
semiparametric theory and are demonstrated to lead to stronger evidence for
early stopping than standard approaches. The associated test statistics are
shown to have the independent increments structure, so that standard software
can be used to obtain stopping boundaries.",http://arxiv.org/abs/2204.10739v1,"Anastasios A Tsiatis, Marie Davidian"
289,"Automated, efficient and model-free inference for randomized clinical trials via data-driven covariate adjustment","In May 2023, the U.S. Food and Drug Administration (FDA) released guidance
for industry on ""Adjustment for Covariates in Randomized Clinical Trials for
Drugs and Biological Products"". Covariate adjustment is a statistical analysis
method for improving precision and power in clinical trials by adjusting for
pre-specified, prognostic baseline variables. Though recommended by the FDA and
the European Medicines Agency (EMA), many trials do not exploit the available
information in baseline variables or make use only of the baseline measurement
of the outcome. This is likely (partly) due to the regulatory mandate to
pre-specify baseline covariates for adjustment, leading to challenges in
determining appropriate covariates and their functional forms. We will explore
the potential of automated data-adaptive methods, such as machine learning
algorithms, for covariate adjustment, addressing the challenge of
pre-specification. Specifically, our approach allows the use of complex models
or machine learning algorithms without compromising the interpretation or
validity of the treatment effect estimate and its corresponding standard error,
even in the presence of misspecified outcome working models. This contrasts the
majority of competing works which assume correct model specification for the
validity of standard errors. Our proposed estimators either necessitate
ultra-sparsity in the outcome model (which can be relaxed by limiting the
number of predictors in the model) or necessitate integration with sample
splitting to enhance their performance. As such, we will arrive at simple
estimators and standard errors for the marginal treatment effect in randomized
clinical trials, which exploit data-adaptive outcome predictions based on
prognostic baseline covariates, and have low (or no) bias in finite samples
even when those predictions are themselves biased.",http://arxiv.org/abs/2404.11150v1,"Kelly Van Lancker, Ivn Daz, Stijn Vansteelandt"
290,Bayesian adaptive bandit-based designs using the Gittins index for multi-armed trials with normally distributed endpoints,"Adaptive designs for multi-armed clinical trials have become increasingly
popular recently in many areas of medical research because of their potential
to shorten development times and to increase patient response. However,
developing response-adaptive trial designs that offer patient benefit while
ensuring the resulting trial avoids bias and provides a statistically rigorous
comparison of the different treatments included is highly challenging. In this
paper, the theory of Multi-Armed Bandit Problems is used to define a family of
near optimal adaptive designs in the context of a clinical trial with a
normally distributed endpoint with known variance. Through simulation studies
based on an ongoing trial as a motivation we report the operating
characteristics (type I error, power, bias) and patient benefit of these
approaches and compare them to traditional and existing alternative designs.
These results are then compared to those recently published in the context of
Bernoulli endpoints. Many limitations and advantages are similar in both cases
but there are also important differences, specially with respect to type I
error control. This paper proposes a simulation-based testing procedure to
correct for the observed type I error inflation that bandit-based and adaptive
rules can induce. Results presented extend recent work by considering a
normally distributed endpoint, a very common case in clinical practice yet
mostly ignored in the response-adaptive theoretical literature, and illustrate
the potential advantages of using these methods in a rare disease context. We
also recommend a suitable modified implementation of the bandit-based adaptive
designs for the case of common diseases.",http://arxiv.org/abs/1703.05172v1,"Adam Smith, Sofia S Villar"
291,Decline of COPD exacerbations in clinical trials over two decades -- a systematic review and meta-regression,"BACKGROUND: An important goal of chronic obstructive pulmonary disease (COPD)
treatment is to reduce the frequency of exacerbations. Some observations
suggest a decline in exacerbation rates in clinical trials over time. A more
systematic understanding would help to improve the design and interpretation of
COPD trials.
  METHODS: We performed a systematic review and meta-regression of the placebo
groups in published randomized controlled trials reporting exacerbations as an
outcome. A Bayesian negative binomial model was developed to accommodate
results that are reported in different formats; results are reported with
credible intervals (CI) and posterior tail probabilities ($p_B$).
  RESULTS: Of 1114 studies identified by our search, 55 were ultimately
included. Exacerbation rates decreased by 6.7% (95% CI (4.4, 9.0); $p_B$ <
0.001) per year, or 50% (95% CI (36, 61)) per decade. Adjusting for available
study and baseline characteristics such as forced expiratory volume in 1 s
(FEV1) did not alter the observed trend considerably. Two subsets of studies,
one using a true placebo group and the other allowing inhaled corticosteroids
in the ""placebo"" group, also yielded consistent results.
  CONCLUSIONS: In conclusion, this meta-regression indicates that the rate of
COPD exacerbations decreased over the past two decades to a clinically relevant
extent independent of important prognostic factors. This suggests that care is
needed in the design of new trials or when comparing results from older trials
with more recent ones. Also a considerable effect of adjunct therapy on COPD
exacerbations can be assumed.",http://arxiv.org/abs/1908.06340v1,"Stefan Andreas, Christian Rver, Judith Heinz, Sebastian Straube, Henrik Watz, Tim Friede"
292,Information Extraction of Clinical Trial Eligibility Criteria,"Clinical trials predicate subject eligibility on a diversity of criteria
ranging from patient demographics to food allergies. Trials post their
requirements as semantically complex, unstructured free-text. Formalizing trial
criteria to a computer-interpretable syntax would facilitate eligibility
determination. In this paper, we investigate an information extraction (IE)
approach for grounding criteria from trials in ClinicalTrials(dot)gov to a
shared knowledge base. We frame the problem as a novel knowledge base
population task, and implement a solution combining machine learning and
context free grammar. To our knowledge, this work is the first criteria
extraction system to apply attention-based conditional random field
architecture for named entity recognition (NER), and word2vec embedding
clustering for named entity linking (NEL). We release the resources and core
components of our system on GitHub at
https://github.com/facebookresearch/Clinical-Trial-Parser. Finally, we report
our per module and end to end performances; we conclude that our system is
competitive with Criteria2Query, which we view as the current state-of-the-art
in criteria extraction.",http://arxiv.org/abs/2006.07296v6,"Yitong Tseo, M I Salkola, Ahmed Mohamed, Anuj Kumar, Freddy Abnousi"
293,Elastic Priors to Dynamically Borrow Information from Historical Data in Clinical Trials,"Use of historical data and real-world evidence holds great potential to
improve the efficiency of clinical trials. One major challenge is how to
effectively borrow information from historical data while maintaining a
reasonable type I error. We propose the elastic prior approach to address this
challenge and achieve dynamic information borrowing. Unlike existing
approaches, this method proactively controls the behavior of dynamic
information borrowing and type I errors by incorporating a well-known concept
of clinically meaningful difference through an elastic function, defined as a
monotonic function of a congruence measure between historical data and trial
data. The elastic function is constructed to satisfy a set of
information-borrowing constraints prespecified by researchers or regulatory
agencies, such that the prior will borrow information when historical and trial
data are congruent, but refrain from information borrowing when historical and
trial data are incongruent. In doing so, the elastic prior improves power and
reduces the risk of data dredging and bias. The elastic prior is information
borrowing consistent, i.e. asymptotically controls type I and II errors at the
nominal values when historical data and trial data are not congruent, a unique
characteristics of the elastic prior approach. Our simulation study that
evaluates the finite sample characteristic confirms that, compared to existing
methods, the elastic prior has better type I error control and yields
competitive or higher power.",http://arxiv.org/abs/2009.06083v2,"Liyun Jiang, Lei Nie, Ying Yuan"
294,Multi-Task Adversarial Learning for Treatment Effect Estimation in Basket Trials,"Estimating treatment effects from observational data provides insights about
causality guiding many real-world applications such as different clinical study
designs, which are the formulations of trials, experiments, and observational
studies in medical, clinical, and other types of research. In this paper, we
describe causal inference for application in a novel clinical design called
basket trial that tests how well a new drug works in patients who have
different types of cancer that all have the same mutation. We propose a
multi-task adversarial learning (MTAL) method, which incorporates feature
selection multi-task representation learning and adversarial learning to
estimate potential outcomes across different tumor types for patients sharing
the same genetic mutation but having different tumor types. In our paper, the
basket trial is employed as an intuitive example to present this new causal
inference setting. This new causal inference setting includes, but is not
limited to basket trials. This setting has the same challenges as the
traditional causal inference problem, i.e., missing counterfactual outcomes
under different subgroups and treatment selection bias due to confounders. We
present the practical advantages of our MTAL method for the analysis of
synthetic basket trial data and evaluate the proposed estimator on two
benchmarks, IHDP and News. The results demonstrate the superiority of our MTAL
method over the competing state-of-the-art methods.",http://arxiv.org/abs/2203.05123v1,"Zhixuan Chu, Stephen L Rathbun, Sheng Li"
295,A Markov Decision Process for Response-Adaptive Randomization in Clinical Trials,"In clinical trials, response-adaptive randomization (RAR) has the appealing
ability to assign more subjects to better-performing treatments based on
interim results. The traditional RAR strategy alters the randomization ratio on
a patient-by-patient basis; this has been heavily criticized for bias due to
time-trends. An alternate approach is blocked RAR, which groups patients
together in blocks and recomputes the randomization ratio in a block-wise
fashion; the final analysis is then stratified by block. However, the typical
blocked RAR design divides patients into equal-sized blocks, which is not
generally optimal.
  This paper presents TrialMDP, an algorithm that designs two-armed blocked RAR
clinical trials. Our method differs from past approaches in that it optimizes
the size and number of blocks as well as their treatment allocations. That is,
the algorithm yields a policy that adaptively chooses the size and composition
of the next block, based on results seen up to that point in the trial.
TrialMDP is related to past works that compute optimal trial designs via
dynamic programming.
  The algorithm maximizes a utility function balancing (i) statistical power,
(ii) patient outcomes, and (iii) the number of blocks. We show that it attains
significant improvements in utility over a suite of baseline designs, and gives
useful control over the tradeoff between statistical power and patient
outcomes. It is well suited for small trials that assign high cost to failures.
  We provide TrialMDP as an R package on GitHub:
https://github.com/dpmerrell/TrialMDP",http://arxiv.org/abs/2109.14642v1,"David Merrell, Thevaa Chandereng, Yeonhee Park"
296,Modeling restricted enrollment and optimal cost-efficient design in multicenter clinical trials,"Design and forecasting of patient enrollment is among the greatest challenges
that the clinical research enterprize faces today, as inefficient enrollment
can be a major cause of drug development delays. Therefore, the development of
the innovative statistical and artificial intelligence technologies for
improving the efficiency of clinical trials operation are of the imperative
need. This paper is describing further developments in the innovative
statistical methodology for modeling and forecasting patient enrollment. The
underlying technique uses a Poisson-gamma enrollment model developed by
Anisimov & Fedorov in the previous publications and is extended here to
analytic modeling of the enrollment on country/region level. A new analytic
technique based on the approximation of the enrollment process in
country/region by a Poisson-gamma process with aggregated parameters is
developed. Another innovative direction is the development of the analytic
technique for modeling the enrollment under some restrictions (enrollment caps
in countries). Some discussion on using historic trials for better prediction
of the enrollment in the new trials is provided. These results are used for
solving the problem of optimal trial cost-efficient enrollment design: find an
optimal allocation of sites/countries that minimizes the global trial cost
given that the probability to reach an enrollment target in time is no less
than some prescribed probability. Different techniques to find an optimal
solution for high dimensional optimization problem for the cases of
unrestricted and restricted enrollment and for a small and large number of
countries are discussed.",http://arxiv.org/abs/2212.12930v1,"Vladimir Anisimov, Matthew Austin"
297,Estimating the Sampling Distribution of Posterior Decision Summaries in Bayesian Clinical Trials,"Bayesian inference and the use of posterior or posterior predictive
probabilities for decision making have become increasingly popular in clinical
trials. The current practice in Bayesian clinical trials relies on a hybrid
Bayesian-frequentist approach where the design and decision criteria are
assessed with respect to frequentist operating characteristics such as power
and type I error rate conditioning on a given set of parameters. These
operating characteristics are commonly obtained via simulation studies. The
utility of Bayesian measures, such as ``assurance"", that incorporate
uncertainty about model parameters in estimating the probabilities of various
decisions in trials has been demonstrated recently. However, the computational
burden remains an obstacle toward wider use of such criteria. In this article,
we propose methodology which utilizes large sample theory of the posterior
distribution to define parametric models for the sampling distribution of the
posterior summaries used for decision making. The parameters of these models
are estimated using a small number of simulation scenarios, thereby refining
these models to capture the sampling distribution for small to moderate sample
size. The proposed approach toward the assessment of conditional and marginal
operating characteristics and sample size determination can be considered as
simulation-assisted rather than simulation-based. It enables formal
incorporation of uncertainty about the trial assumptions via a design prior and
significantly reduces the computational burden for the design of Bayesian
trials in general.",http://arxiv.org/abs/2306.09151v2,"Shirin Golchi, James Willard"
298,Assurance Methods for designing a clinical trial with a delayed treatment effect,"An assurance calculation is a Bayesian alternative to a power calculation.
One may be performed to aid the planning of a clinical trial, specifically
setting the sample size or to support decisions about whether or not to perform
a study. Immuno-oncology is a rapidly evolving area in the development of
anticancer drugs. A common phenomenon that arises in trials of such drugs is
one of delayed treatment effects, that is, there is a delay in the separation
of the survival curves. To calculate assurance for a trial in which a delayed
treatment effect is likely to be present, uncertainty about key parameters
needs to be considered. If uncertainty is not considered, the number of
patients recruited may not be enough to ensure we have adequate statistical
power to detect a clinically relevant treatment effect and the risk of an
unsuccessful trial is increased. We present a new elicitation technique for
when a delayed treatment effect is likely and show how to compute assurance
using these elicited prior distributions. We provide an example to illustrate
how this can be used in practice and develop open-source software to implement
our methods. Our methodology has the potential to improve the success rate and
efficiency of Phase III trials in immuno-oncology and for other treatments
where a delayed treatment effect is expected to occur.",http://arxiv.org/abs/2310.06673v2,"James Salsbury, Jeremy Oakley, Steven Julious, Lisa Hampson"
299,Distilling Large Language Models for Matching Patients to Clinical Trials,"The recent success of large language models (LLMs) has paved the way for
their adoption in the high-stakes domain of healthcare. Specifically, the
application of LLMs in patient-trial matching, which involves assessing patient
eligibility against clinical trial's nuanced inclusion and exclusion criteria,
has shown promise. Recent research has shown that GPT-3.5, a widely recognized
LLM developed by OpenAI, can outperform existing methods with minimal 'variable
engineering' by simply comparing clinical trial information against patient
summaries. However, there are significant challenges associated with using
closed-source proprietary LLMs like GPT-3.5 in practical healthcare
applications, such as cost, privacy and reproducibility concerns. To address
these issues, this study presents the first systematic examination of the
efficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA
7B,13B, and 70B) for the task of patient-trial matching. Employing a
multifaceted evaluation framework, we conducted extensive automated and
human-centric assessments coupled with a detailed error analysis for each
model. To enhance the adaptability of open-source LLMs, we have created a
specialized synthetic dataset utilizing GPT-4, enabling effective fine-tuning
under constrained data conditions. Our findings reveal that open-source LLMs,
when fine-tuned on this limited and synthetic dataset, demonstrate performance
parity with their proprietary counterparts. This presents a massive opportunity
for their deployment in real-world healthcare applications. To foster further
research and applications in this field, we release both the annotated
evaluation dataset along with the fine-tuned LLM -- Trial-LLAMA -- for public
use.",http://arxiv.org/abs/2312.09958v1,"Mauro Nievas, Aditya Basu, Yanshan Wang, Hrituraj Singh"
300,ExKG-LLM: Leveraging Large Language Models for Automated Expansion of Cognitive Neuroscience Knowledge Graphs,"The paper introduces ExKG-LLM, a framework designed to automate the expansion
of cognitive neuroscience knowledge graphs (CNKG) using large language models
(LLMs). It addresses limitations in existing tools by enhancing accuracy,
completeness, and usefulness in CNKG. The framework leverages a large dataset
of scientific papers and clinical reports, applying state-of-the-art LLMs to
extract, optimize, and integrate new entities and relationships. Evaluation
metrics include precision, recall, and graph density. Results show significant
improvements: precision (0.80, +6.67%), recall (0.81, +15.71%), F1 score
(0.805, +11.81%), and increased edge nodes (21.13% and 31.92%). Graph density
slightly decreased, reflecting a broader but more fragmented structure.
Engagement rates rose by 20%, while CNKG diameter increased to 15, indicating a
more distributed structure. Time complexity improved to O(n log n), but space
complexity rose to O(n2), indicating higher memory usage. ExKG-LLM demonstrates
potential for enhancing knowledge generation, semantic search, and clinical
decision-making in cognitive neuroscience, adaptable to broader scientific
fields.",http://arxiv.org/abs/2503.06479v1,"Ali Sarabadani, Kheirolah Rahsepar Fard, Hamid Dalvand"
301,Emerging classes of antioxidant to cancer therapy: a review of clinical and experimental studies,"This review mainly focuses on the relation between antioxidants with cancer
therapy. Antioxidants have been reported to play an essential role to reduce
free radical species. Free radicals commonly cause oxidative damage which is a
common factor in the aging process, and also the vital factor of formation, and
development of major disease specially cancer. Although, since last many
decades several antioxidants belong to natural and synthetic origin have been
tested in clinical trials against oxidative stress, however these clinical
trials end up with undesirable effects. This review also complied with the most
recent findings of oxidative stress, highlighting of free racial production,
and its related oxidative damage at cellular and molecular level, with the new
and existing natural and synthetic classes of free radical scavenger and their
related clinical trials.",http://arxiv.org/abs/2003.04538v1,"QuratulAin, M Iqbal Choudhary"
302,Machine Learning for Health: Personalized Models for Forecasting of Alzheimer Disease Progression,"In this thesis the aim is to work on optimizing the modern machine learning
models for personalized forecasting of Alzheimer Disease (AD) Progression from
clinical trial data. The data comes from the TADPOLE challenge, which is one of
the largest publicly available datasets for AD research (ADNI dataset). The
goal of the project is to develop machine learning models that can be used to
perform personalized forecasts of the participants cognitive changes (e.g.,
ADAS-Cog13 scores) over the time period of 6,12, 18 and 24 months in the future
and the change in Clinical Status (CS) i.e., whether a person will convert to
AD within 2 years or not. This is important for informing current clinical
trials and better design of future clinical trials for AD. We will work with
personalized Gaussian processes as machine learning models to predict
ADAS-Cog13 score and Cox model along with a classifier to predict the
conversion in a patient within 2 years.This project is done with the
collaboration with researchers from the MIT MediaLab.",http://arxiv.org/abs/2008.02667v1,Aritra Banerjee
303,Adaptive seamless design for establishing pharmacokinetics and efficacy equivalence in developing biosimilars,"Recently, numerous pharmaceutical sponsors have expressed a great deal of
interest in the development of biosimilars, which requires clinical trials to
demonstrate the equivalence of pharmacokinetics (PK) and clinical efficacy.
Pharmacodynamics (PD) may be used in evaluating efficacy if there are relevant
PD markers available. However, in their absence, it is necessary to design the
associated clinical trials to include efficacy measures as the primary
endpoint. In this study, we propose an adaptive seamless PK and efficacy design
with the frameworks to remedy the risk of misspecification of both PK and
efficacy parameters. Here, we consider the clinical development of biosimilars
including their evaluation in patients rather than healthy volunteers under a
situation where both PK and efficacy parameters are required to demonstrate the
equivalence. To avoid the risk associated with the failure to confirm
equivalence, incorporating the new PK trial for PK equivalence within the PK
portion, which is the early stage for the efficacy part, and sample size
re-calculation for the efficacy equivalence are considered in the proposed
method. This proposal provides appealing advantages such as a shorter period,
additional cost saving, and smaller number of patients required.",http://arxiv.org/abs/1607.02283v2,"Ryuji Uozumi, Chikuma Hamada"
304,A Bayesian Joint model for Longitudinal DAS28 Scores and Competing Risk Informative Drop Out in a Rheumatoid Arthritis Clinical Trial,"Rheumatoid arthritis clinical trials are strategically designed to collect
the disease activity score of each patient over multiple clinical visits,
meanwhile a patient may drop out before their intended completion due to
various reasons. The dropout terminates the longitudinal data collection on the
patients activity score. In the presence of informative dropout, that is, the
dropout depends on latent variables from the longitudinal process, simply
applying a model to analyze the longitudinal outcomes may lead to biased
results because the assumption of random dropout is violated. In this paper we
develop a data driven Bayesian joint model for modeling DAS28 scores and
competing risk informative drop out. The motivating example is a clinical trial
of Etanercept and Methotrexate with radiographic Patient Outcomes (TEMPO,
Keystone et.al).",http://arxiv.org/abs/1801.08628v1,"Violeta G Hennessey, Luis G LeonNovelo, Juan Li, Li Zhu, Eric Chi, Joseph G Ibrahim"
305,"Aim for clinical utility, not just predictive accuracy","The predictions from an accurate prognostic model can be of great interest to
patients and clinicians. When predictions are reported to individuals, they may
decide to take action to improve their health or they may simply be comforted
by the knowledge. However, if there is a clearly defined space of actions in
the clinical context, a formal decision rule based on the prediction has the
potential to have a much broader impact. Even if it is not the intended use of
a developed prediction model, informal decision rules can often be found in
practice. The use of a prediction-based decision rule should be formalized and
compared to the standard of care in a randomized trial to assess its clinical
utility, however, evidence is needed to motivate such a trial. We outline how
observational data can be used to propose a decision rule based on a prognostic
prediction model. We then propose a framework for emulating a prediction driven
trial to evaluate the utility of a prediction-based decision rule in
observational data. A split-sample structure can and should be used to develop
the prognostic model, define the decision rule, and evaluate its clinical
utility.",http://arxiv.org/abs/1909.03801v1,"Michael C Sachs, Arvid Sjlander, Erin E Gabriel"
306,WATCH: A Workflow to Assess Treatment Effect Heterogeneity in Drug Development for Clinical Trial Sponsors,"This paper proposes a Workflow for Assessing Treatment effeCt Heterogeneity
(WATCH) in clinical drug development targeted at clinical trial sponsors. WATCH
is designed to address the challenges of investigating treatment effect
heterogeneity (TEH) in randomized clinical trials, where sample size and
multiplicity limit the reliability of findings. The proposed workflow includes
four steps: Analysis Planning, Initial Data Analysis and Analysis Dataset
Creation, TEH Exploration, and Multidisciplinary Assessment. The workflow
offers a general overview of how treatment effects vary by baseline covariates
in the observed data, and guides interpretation of the observed findings based
on external evidence and best scientific understanding. The workflow is
exploratory and not inferential/confirmatory in nature, but should be
pre-planned before data-base lock and analysis start. It is focused on
providing a general overview rather than a single specific finding or subgroup
with differential effect.",http://arxiv.org/abs/2405.00859v2,"Konstantinos Sechidis, Sophie Sun, Yao Chen, Jiarui Lu, Cong Zhang, Mark Baillie, David Ohlssen, Marc Vandemeulebroecke, Rob Hemmings, Stephen Ruberg, Bjrn Bornkamp"
307,How Cox models react to a study-specific confounder in a patient-level pooled dataset: Random-effects better cope with an imbalanced covariate across trials unless baseline hazards differ,"Combining patient-level data from clinical trials can connect rare phenomena
with clinical endpoints, but statistical techniques applied to a single trial
may become problematical when trials are pooled. Estimating the hazard of a
binary variable unevenly distributed across trials showcases a common pooled
database issue.
  We studied how an unevenly distributed binary variable can compromise the
integrity of fixed and random effects Cox proportional hazards models.
  We compared fixed effect and random effects Cox proportional hazards models
on a set of simulated datasets inspired by a 17-trial pooled database of
patients presenting with ST-segment elevation myocardial infarction (STEMI) and
non-STEMI undergoing percutaneous coronary intervention.
  An unevenly distributed covariate can bias hazard ratio estimates, inflate
standard errors, raise type I error, and reduce power. While uneveness causes
problems for all Cox proportional hazards models, random effects suffer least.
Compared to fixed effect models, random effects suffer lower bias and trade
inflated type I errors for improved power. Contrasting hazard rates between
trials prevent accurate estimates from both fixed and random effects models.
  When modeling a covariate unevenly distributed across pooled trials with
similar baseline hazard rates, Cox proportional hazards models with a random
trial effect more accurately estimate hazard ratios than fixed effects.
Differing between-trial baseline hazard rates bias both random and fixed effect
models. With an unevenly-distributed covariate and similar baseline hazard
rates across trials, a random effects Cox proportional hazards model
outperforms a fixed effect model, but cannot overcome contrasting baseline
hazard rates.",http://arxiv.org/abs/1805.02821v1,"Thomas McAndrew, Bjorn Redfors, Aaron Crowley, Yiran Zhang, Shmuel Chen, Mordechai Golomb, Maria Alu, Dominic Francese, Ori BenYehuda, Akiko Maehara, Gary Mintz, Gregg Stone, Paul Jenkins"
308,Performing clinical drug trials in children with a rare disease,"Over the past 50 years, the advancements in medical and health research have
radically changed the epidemiology of health conditions in neonates, children,
and adolescents; and clinical research has on the whole, moved forward.
However, large sections of the pediatric community remain vulnerable and
underserved, by clinical research. One reason for this is the fact that most
pediatric diseases are also rare diseases (i.e., they fit the EU definition of
a rare condition, by affecting no more than 5 in 10,000 individuals), and
indeed the majority of conditions under this umbrella heading are in fact much
rarer, affecting fewer than 1 in 100,000. Rare pediatric diseases incur
particular challenges, both in terms of actually conducting clinical trials but
also planning trials (and indeed, stimulating the preclinical research and
knowledge generation necessary to embark on clinical trials in the first
place). The pediatric regulation and orphan regulation (covering rare diseases)
were introduced to address the complexities in research and development of
medicines specifically for children and for people living with a rare disease,
respectively. The regulations have been reasonably effective, particularly in
areas where adult and pediatric diseases overlap, driving the development of
more pediatric medicines; however, challenges still remain, often exacerbated
by the rarity of the diseases. These include issues around trial planning, the
need for more innovative methodologies in smaller populations, significant
delays in trial start up and recruitment, recruitment issues (due to small
populations and the nature of the conditions), lack of endpoints, and scarce
data. This chapter will discuss some of the major challenges in delivering
trials in pediatric rare diseases while also assessing current and future
solutions to address these.",http://arxiv.org/abs/2408.07142v1,"Victoria Hedley, Rebecca Leary, Anando Sen, Anna Irvin, Emma Heslop, Volker Straub"
309,Metrics to find a surrogate endpoint of OS in metastatic oncology trials: a simulation study,"Surrogate endpoint (SE) for overall survival in cancer patients is essential
to improving the efficiency of oncology drug development. In practice, we may
discover a new patient level association with survival, based on one or more
clinical or biological features, in a discovery cohort; and then measure the
trial level association across studies in a meta-analysis to validate the SE.
To understand how well various patient level metrics would indicate the
eventual trial level association, we considered causal biological trajectories
based on bi-exponential functions, modeled the strength of their impact on
survival hazards via a parameter {\alpha}, and simulated the trajectories and
survival times in randomized trials simultaneously. We set an early time point
in the trials when the trajectory measurement became the SE value. From
simulated discovery cohorts, we compared patient level metrics including C
index, integrated brier score, and log hazard ratio between SE values and
survival times. We assembled multiple simulated studies to enable meta-analyses
to estimate the trial level association. Across all the simulation scenarios
considered here, we found tight correlations among the three patient level
metrics and similar correlations between any of them and the trial level
metric. Despite the continual increase in {\alpha}, both patient and trial
level metrics often plateaued together; their association always decreased
quickly as {\alpha} increased. This suggests that incorporating additional
biological factors into a composite SE is likely to have diminishing returns on
improving both patient level and trial level association.",http://arxiv.org/abs/2109.03421v2,Wei Zou
310,Optimal allocation strategies in platform trials,"Platform trials are randomized clinical trials that allow simultaneous
comparison of multiple interventions, usually against a common control. Arms to
test experimental interventions may enter and leave the platform over time.
This implies that the number of experimental intervention arms in the trial may
change over time. Determining optimal allocation rates to allocate patients to
the treatment and control arms in platform trials is challenging because the
change in treatment arms implies that also the optimal allocation rates will
change when treatments enter or leave the platform. In addition, the optimal
allocation depends on the analysis strategy used. In this paper, we derive
optimal treatment allocation rates for platform trials with shared controls,
assuming that a stratified estimation and testing procedure based on a
regression model, is used to adjust for time trends. We consider both, analysis
using concurrent controls only as well as analysis methods based on also
non-concurrent controls and assume that the total sample size is fixed. The
objective function to be minimized is the maximum of the variances of the
effect estimators. We show that the optimal solution depends on the entry time
of the arms in the trial and, in general, does not correspond to the square
root of $k$ allocation rule used in the classical multi-arm trials. We
illustrate the optimal allocation and evaluate the power and type 1 error rate
compared to trials using one-to-one and square root of $k$ allocations by means
of a case study.",http://arxiv.org/abs/2304.03035v1,"Marta Bofill Roig, Ekkehard Glimm, Tobias Mielke, Martin Posch"
311,A novel Phase I clinical trial design with unequal cohort sizes,"This paper introduces a new Phase I design aimed at enhancing the performance
of existing methods, including algorithm-based, model-based, and model-assisted
designs. The design, developed by integrating the concept of Fisher
information, is easily operationalized. The new design addresses the issue of
the classical designs'slow dosage escalation. Simulation demonstrate that the
proposed design markedly enhances performance in terms of efficiency, accuracy,
and reliability. Moreover, the trial duration has been notably reduced with a
large sample size.",http://arxiv.org/abs/2412.07635v1,Xiaojun Zhu
312,Modeling Disease Progression in Mild Cognitive Impairment and Alzheimer's Disease with Digital Twins,"Alzheimer's Disease (AD) is a neurodegenerative disease that affects subjects
in a broad range of severity and is assessed in clinical trials with multiple
cognitive and functional instruments. As clinical trials in AD increasingly
focus on earlier stages of the disease, especially Mild Cognitive Impairment
(MCI), the ability to model subject outcomes across the disease spectrum is
extremely important. We use unsupervised machine learning models called
Conditional Restricted Boltzmann Machines (CRBMs) to create Digital Twins of AD
subjects. Digital Twins are simulated clinical records that share baseline data
with actual subjects and comprehensively model their outcomes under
standard-of-care. The CRBMs are trained on a large set of records from subjects
in observational studies and the placebo arms of clinical trials across the AD
spectrum. These data exhibit a challenging, but common, patchwork of measured
and missing observations across subjects in the dataset, and we present a novel
model architecture designed to learn effectively from it. We evaluate
performance against a held-out test dataset and show how Digital Twins
simultaneously capture the progression of a number of key endpoints in clinical
trials across a broad spectrum of disease severity, including MCI and
mild-to-moderate AD.",http://arxiv.org/abs/2012.13455v1,"Daniele Bertolini, Anton D Loukianov, Aaron M Smith, David LiBland, Yannick Pouliot, Jonathan R Walsh, Charles K Fisher"
313,Analysis of an Incomplete Binary Outcome Dichotomized From an Underlying Continuous Variable in Clinical Trials,"In many clinical trials, outcomes of interest include binary-valued
endpoints. It is not uncommon that a binary-valued outcome is dichotomized from
a continuous outcome at a threshold of clinical interest. To reach the
objective, common approaches include (a) fitting the generalized linear mixed
model (GLMM) to the dichotomized longitudinal binary outcome and (b) imputation
method (MI): imputing the missing values in the continuous outcome,
dichotomizing it into a binary outcome, and then fitting the generalized linear
model for the ""complete"" data. We conducted comprehensive simulation studies to
compare the performance of GLMM with MI for estimating risk difference and
logarithm of odds ratio between two treatment arms at the end of study. In
those simulation studies, we considered a range of multivariate distribution
options for the continuous outcome (including a multivariate normal
distribution, a multivariate t-distribution, a multivariate log-normal
distribution, and the empirical distribution from a real clinical trial data)
to evaluate the robustness of the estimators to various data-generating models.
Simulation results demonstrate that both methods work well under those
considered distribution options, but MI is more efficient with smaller mean
squared errors compared to GLMM. We further applied both the GLMM and MI to 29
phase 3 diabetes clinical trials, and found that the MI method generally led to
smaller variance estimates compared to GLMM.",http://arxiv.org/abs/2108.02064v1,"Chenchen Ma, Xin Shen, Yongming Qu, Yu Du"
314,Factors Affecting the Performance of Automated Speaker Verification in Alzheimer's Disease Clinical Trials,"Detecting duplicate patient participation in clinical trials is a major
challenge because repeated patients can undermine the credibility and accuracy
of the trial's findings and result in significant health and financial risks.
Developing accurate automated speaker verification (ASV) models is crucial to
verify the identity of enrolled individuals and remove duplicates, but the size
and quality of data influence ASV performance. However, there has been limited
investigation into the factors that can affect ASV capabilities in clinical
environments. In this paper, we bridge the gap by conducting analysis of how
participant demographic characteristics, audio quality criteria, and severity
level of Alzheimer's disease (AD) impact the performance of ASV utilizing a
dataset of speech recordings from 659 participants with varying levels of AD,
obtained through multiple speech tasks. Our results indicate that ASV
performance: 1) is slightly better on male speakers than on female speakers; 2)
degrades for individuals who are above 70 years old; 3) is comparatively better
for non-native English speakers than for native English speakers; 4) is
negatively affected by clinician interference, noisy background, and unclear
participant speech; 5) tends to decrease with an increase in the severity level
of AD. Our study finds that voice biometrics raise fairness concerns as certain
subgroups exhibit different ASV performances owing to their inherent voice
characteristics. Moreover, the performance of ASV is influenced by the quality
of speech recordings, which underscores the importance of improving the data
collection settings in clinical trials.",http://arxiv.org/abs/2306.12444v1,"Malikeh Ehghaghi, Marija Stanojevic, Ali Akram, Jekaterina Novikova"
315,TRIALSCOPE: A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models,"The rapid digitization of real-world data offers an unprecedented opportunity
for optimizing healthcare delivery and accelerating biomedical discovery. In
practice, however, such data is most abundantly available in unstructured
forms, such as clinical notes in electronic medical records (EMRs), and it is
generally plagued by confounders. In this paper, we present TRIALSCOPE, a
unifying framework for distilling real-world evidence from population-level
observational data. TRIALSCOPE leverages biomedical language models to
structure clinical text at scale, employs advanced probabilistic modeling for
denoising and imputation, and incorporates state-of-the-art causal inference
techniques to combat common confounders. Using clinical trial specification as
generic representation, TRIALSCOPE provides a turn-key solution to generate and
reason with clinical hypotheses using observational data. In extensive
experiments and analyses on a large-scale real-world dataset with over one
million cancer patients from a large US healthcare network, we show that
TRIALSCOPE can produce high-quality structuring of real-world data and
generates comparable results to marquee cancer trials. In addition to
facilitating in-silicon clinical trial design and optimization, TRIALSCOPE may
be used to empower synthetic controls, pragmatic trials, post-market
surveillance, as well as support fine-grained patient-like-me reasoning in
precision diagnosis and treatment.",http://arxiv.org/abs/2311.01301v2,"Javier Gonzlez, Cliff Wong, Zelalem Gero, Jass Bagga, Risa Ueno, Isabel Chien, Eduard Oravkin, Emre Kiciman, Aditya Nori, Roshanthi Weerasinghe, Rom S Leidner, Brian Piening, Tristan Naumann, Carlo Bifulco, Hoifung Poon"
316,GATher: Graph Attention Based Predictions of Gene-Disease Links,"Target selection is crucial in pharmaceutical drug discovery, directly
influencing clinical trial success. Despite its importance, drug development
remains resource-intensive, often taking over a decade with significant
financial costs. High failure rates highlight the need for better early-stage
target selection. We present GATher, a graph attention network designed to
predict therapeutic gene-disease links by integrating data from diverse
biomedical sources into a graph with over 4.4 million edges. GATher
incorporates GATv3, a novel graph attention convolution layer, and
GATv3HeteroConv, which aggregates transformations for each edge type, enhancing
its ability to manage complex interactions within this extensive dataset.
Utilizing hard negative sampling and multi-task pre-training, GATher addresses
topological imbalances and improves specificity. Trained on data up to 2018 and
evaluated through 2024, our results show GATher predicts clinical trial
outcomes with a ROC AUC of 0.69 for unmet efficacy failures and 0.79 for
positive efficacy. Feature attribution methods, using Captum, highlight key
nodes and relationships, enhancing model interpretability. By 2024, GATher
improved precision in prioritizing the top 200 clinical trial targets to 14.1%,
an absolute increase of over 3.5% compared to other methods. GATher outperforms
existing models like GAT, GATv2, and HGT in predicting clinical trial outcomes,
demonstrating its potential in enhancing target validation and predicting
clinical efficacy and safety.",http://arxiv.org/abs/2409.16327v1,"David NarganesCarlon, Anniek Myatt, Mani Mudaliar, Daniel J Crowther"
317,"Enhancing Hepatopathy Clinical Trial Efficiency: A Secure, Large Language Model-Powered Pre-Screening Pipeline","Background: Recruitment for cohorts involving complex liver diseases, such as
hepatocellular carcinoma and liver cirrhosis, often requires interpreting
semantically complex criteria. Traditional manual screening methods are
time-consuming and prone to errors. While AI-powered pre-screening offers
potential solutions, challenges remain regarding accuracy, efficiency, and data
privacy. Methods: We developed a novel patient pre-screening pipeline that
leverages clinical expertise to guide the precise, safe, and efficient
application of large language models. The pipeline breaks down complex criteria
into a series of composite questions and then employs two strategies to perform
semantic question-answering through electronic health records - (1) Pathway A,
Anthropomorphized Experts' Chain of Thought strategy, and (2) Pathway B, Preset
Stances within an Agent Collaboration strategy, particularly in managing
complex clinical reasoning scenarios. The pipeline is evaluated on three key
metrics-precision, time consumption, and counterfactual inference - at both the
question and criterion levels. Results: Our pipeline achieved high precision
(0.921, in criteria level) and efficiency (0.44s per task). Pathway B excelled
in complex reasoning, while Pathway A was effective in precise data extraction
with faster processing times. Both pathways achieved comparable precision. The
pipeline showed promising results in hepatocellular carcinoma (0.878) and
cirrhosis trials (0.843). Conclusions: This data-secure and time-efficient
pipeline shows high precision in hepatopathy trials, providing promising
solutions for streamlining clinical trial workflows. Its efficiency and
adaptability make it suitable for improving patient recruitment. And its
capability to function in resource-constrained environments further enhances
its utility in clinical settings.",http://arxiv.org/abs/2502.18531v1,"Xiongbin Gui, Hanlin Lv, Xiao Wang, Longting Lv, Yi Xiao, Lei Wang"
318,How to design a pre-specified statistical analysis approach to limit p-hacking in clinical trials: the Pre-SPEC framework,"Results from clinical trials can be susceptible to bias if investigators
choose their analysis approach after seeing trial data, as this can allow them
to perform multiple analyses and then choose the method that provides the most
favourable result (commonly referred to as 'p-hacking'). Pre-specification of
the planned analysis approach is essential to help reduce such bias, as it
ensures analytical methods are chosen in advance of seeing the trial data.
However, pre-specification is only effective if done in a way that does not
allow p-hacking. For example, investigators may pre-specify a certain
statistical method such as multiple imputation, but give little detail on how
it will be implemented. Because there are many different ways to perform
multiple imputation, this approach to pre-specification is ineffective, as it
still allows investigators to analyse the data in different ways before
deciding on a final approach. In this article we describe a five-point
framework (the Pre-SPEC framework) for designing a pre-specified analysis
approach that does not allow p-hacking. This framework is intended to be used
in conjunction with the SPIRIT (Standard Protocol Items: Recommendations for
Interventional Trials) statement and other similar guidelines to help
investigators design the statistical analysis strategy for the trial's primary
outcome in the trial protocol.",http://arxiv.org/abs/1907.04078v3,"Brennan C Kahan, Gordon Forbes, Suzie Cro"
319,Analyzing Basket Trials under Multisource Exchangeability Assumptions,"Basket designs are prospective clinical trials that are devised with the
hypothesis that the presence of selected molecular features determine a
patient's subsequent response to a particular ""targeted"" treatment strategy.
Basket trials are designed to enroll multiple clinical subpopulations to which
it is assumed that the therapy in question offers beneficial efficacy in the
presence of the targeted molecular profile. The treatment, however, may not
offer acceptable efficacy to all subpopulations enrolled. Moreover, for rare
disease settings, such as oncology wherein these trials have become popular,
marginal measures of statistical evidence are difficult to interpret for
sparsely enrolled subpopulations. Consequently, basket trials pose challenges
to the traditional paradigm for trial design, which assumes inter-patient
exchangeability. The R-package \pkg{basket} facilitates the analysis of basket
trials by implementing multi-source exchangeability models. By evaluating all
possible pairwise exchangeability relationships, this hierarchical modeling
framework facilitates Bayesian posterior shrinkage among a collection of
discrete and pre-specified subpopulations. Analysis functions are provided to
implement posterior inference of the response rates and all possible
exchangeability relationships between subpopulations. In addition, the package
can identify ""poolable"" subsets of and report their response characteristics.
The functionality of the package is demonstrated using data from an oncology
study with subpopulations defined by tumor histology.",http://arxiv.org/abs/1908.00618v1,"Michael J Kane, Nan Chen, Alexander M Kaizer, Xun Jiang, H Amy Xia, Brian P Hobbs"
320,Sensitivity analysis using bias functions for studies extending inferences from a randomized trial to a target population,"Extending (generalizing or transporting) causal inferences from a randomized
trial to a target population requires ``generalizability'' or
``transportability'' assumptions, which state that randomized and
non-randomized individuals are exchangeable conditional on baseline covariates.
These assumptions are made on the basis of background knowledge, which is often
uncertain or controversial, and need to be subjected to sensitivity analysis.
We present simple methods for sensitivity analyses that do not require detailed
background knowledge about specific unknown or unmeasured determinants of the
outcome or modifiers of the treatment effect. Instead, our methods directly
parameterize violations of the assumptions using bias functions. We show how
the methods can be applied to non-nested trial designs, where the trial data
are combined with a separately obtained sample of non-randomized individuals,
as well as to nested trial designs, where a clinical trial is embedded within a
cohort sampled from the target population. We illustrate the methods using data
from a clinical trial comparing treatments for chronic hepatitis C infection.",http://arxiv.org/abs/1905.10684v1,"Issa J Dahabreh, James M Robins, Sebastien JP A Haneuse, Iman Saeed, Sarah E Robertson, Elisabeth A Stuart, Miguel A Hernn"
321,Incorporating patient-reported outcomes in dose-finding clinical trials with continuous patient enrollment,"Dose-finding clinical trials in oncology aim to estimate the maximum
tolerated dose (MTD), based on safety traditionally obtained from the
clinician's perspective. While the collection of patient-reported outcomes
(PROs) has been advocated to better inform treatment tolerability, there is a
lack of guidance and methods on how to use PROs for dose assignments and
recommendations. The PRO continual reassessment method (PRO-CRM) has been
proposed to formally incorporate PROs to estimate the MTD, requiring complete
follow-up of both clinician and patient toxicity information per dose cohort to
assign the next cohort of patients. In this paper, we propose two extensions of
the PRO-CRM, allowing continuous enrollment of patients and handling longer
toxicity observation windows to capture late-onset or cumulative toxicities.
The first method, the TITE-PRO-CRM, uses a weighted likelihood to include the
partial follow-up information from PRO in estimating the MTD during and at the
end of the trial. The second method, the TITE-CRM+PRO, uses clinician's
information solely to inform dose assignments during the trial and incorporates
PRO at the end of the trial for dose recommendation. Simulation studies show
that the TITE-PRO-CRM performs similarly to the PRO-CRM in terms of dose
recommendation and assignments during the trial while reducing trial duration.
The TITE-CRM + PRO slightly underperforms compared to the TITE-PRO-CRM, but
similar performance can be attained by requiring larger sample sizes. We also
show that the proposed methods have similar performance under higher accrual
rates, different toxicity hazards, and correlated time-to-clinician toxicity
and time-to-patient toxicity data.",http://arxiv.org/abs/2303.17705v1,"Anas Andrillon, Lucie Biard, Shing M Lee"
322,Estimands for single arm dose optimization trials in oncology,"Phase I dose escalation trials in oncology generally aim to find the maximum
tolerated dose (MTD). However, with the advent of molecular targeted therapies
and antibody drug conjugates, dose limiting toxicities are less frequently
observed, giving rise to the concept of optimal biological dose (OBD), which
considers both efficacy and toxicity. The Estimand framework presented in the
addendum of the ICH E9(R1) guidelines strengthens the dialogue between
different stakeholders by bringing in greater clarity in the clinical trial
objectives and by providing alignment between the targeted estimand under
consideration and the statistical analysis methods. However, there lacks
clarity in implementing this framework in early phase dose optimization
studies. This manuscript aims at discussing the Estimand framework for dose
optimization trials in oncology considering efficacy and toxicity through
utility functions. Such trials should include Pharmacokinetics (PK) data,
toxicity data, and efficacy data. Based on these data, the analysis methods
used to identify the optimized dose/s are also described. Focusing on
optimizing the utility function to estimate the OBD, the population-level
summary measure should reflect only the properties used for the estimating this
utility function. A detailed strategy recommendation for intercurrent events
has been provided using a real-life oncology case study. Key recommendations
regarding the estimand attributes include that in a seamless Phase I/II dose
optimization trial, the treatment attribute should start when the subject
receives the first dose. We argue that such a framework brings in additional
clarity to dose optimization trial objectives and strengthens the understanding
of the drug under consideration that would enable the correct dose to move to
Phase II of clinical development.",http://arxiv.org/abs/2501.18930v1,"Ayon Mukherjee, Jonathan L Moscovici, Zheng Liu"
323,Stem Cell Transplantation As A Dynamical System: Are Clinical Outcomes Deterministic?,"Outcomes in stem cell transplantation (SCT) are modeled using probability
theory. However the clinical course following SCT appears to demonstrate many
characteristics of dynamical systems, especially when outcomes are considered
in the context of immune reconstitution. Dynamical systems tend to evolve over
time according to mathematically determined rules. Characteristically, the
future states of the system are predicated on the states preceding them, and
there is sensitivity to initial conditions. In SCT, the interaction between
donor T cells and the recipient may be considered as such a system in which,
graft source, conditioning and early immunosuppression profoundly influence
immune reconstitution over time. This eventually determines clinical outcomes,
either the emergence of tolerance or the development of graft versus host
disease. In this paper parallels between SCT and dynamical systems are explored
and a conceptual framework for developing mathematical models to understand
disparate transplant outcomes is proposed.",http://arxiv.org/abs/1403.6365v3,"Amir A Toor, Jared D Kobulnicky, Salman Salman, Catherine H Roberts, Max JamesonLee, Jeremy Meier, Allison Scalora, Nihar Sheth, Vishal Koparde, Myrna Serrano, Gregory A Buck, William Clark, John McCarty, Harold Chung, Masoud H Manjili, Roy T Sabo, Michael C Neale"
324,Establishing Rigorous and Cost-effective Clinical Trials for Artificial Intelligence Models,"A profound gap persists between artificial intelligence (AI) and clinical
practice in medicine, primarily due to the lack of rigorous and cost-effective
evaluation methodologies. State-of-the-art and state-of-the-practice AI model
evaluations are limited to laboratory studies on medical datasets or direct
clinical trials with no or solely patient-centered controls. Moreover, the
crucial role of clinicians in collaborating with AI, pivotal for determining
its impact on clinical practice, is often overlooked. For the first time, we
emphasize the critical necessity for rigorous and cost-effective evaluation
methodologies for AI models in clinical practice, featuring
patient/clinician-centered (dual-centered) AI randomized controlled trials
(DC-AI RCTs) and virtual clinician-based in-silico trials (VC-MedAI) as an
effective proxy for DC-AI RCTs. Leveraging 7500 diagnosis records from two-step
inaugural DC-AI RCTs across 14 medical centers with 125 clinicians, our results
demonstrate the necessity of DC-AI RCTs and the effectiveness of VC-MedAI.
Notably, VC-MedAI performs comparably to human clinicians, replicating insights
and conclusions from prospective DC-AI RCTs. We envision DC-AI RCTs and
VC-MedAI as pivotal advancements, presenting innovative and transformative
evaluation methodologies for AI models in clinical practice, offering a
preclinical-like setting mirroring conventional medicine, and reshaping
development paradigms in a cost-effective and fast-iterative manner. Chinese
Clinical Trial Registration: ChiCTR2400086816.",http://arxiv.org/abs/2407.08554v2,"Wanling Gao, Yunyou Huang, Dandan Cui, Zhuoming Yu, Wenjing Liu, Xiaoshuang Liang, Jiahui Zhao, Jiyue Xie, Hao Li, Li Ma, Ning Ye, Yumiao Kang, Dingfeng Luo, Peng Pan, Wei Huang, Zhongmou Liu, Jizhong Hu, Gangyuan Zhao, Chongrong Jiang, Fan Huang, Tianyi Wei, Suqin Tang, Bingjie Xia, Zhifei Zhang, Jianfeng Zhan"
325,"Beyond Low Earth Orbit: Biomonitoring, Artificial Intelligence, and Precision Space Health","Human space exploration beyond low Earth orbit will involve missions of
significant distance and duration. To effectively mitigate myriad space health
hazards, paradigm shifts in data and space health systems are necessary to
enable Earth-independence, rather than Earth-reliance. Promising developments
in the fields of artificial intelligence and machine learning for biology and
health can address these needs. We propose an appropriately autonomous and
intelligent Precision Space Health system that will monitor, aggregate, and
assess biomedical statuses; analyze and predict personalized adverse health
outcomes; adapt and respond to newly accumulated data; and provide preventive,
actionable, and timely insights to individual deep space crew members and
iterative decision support to their crew medical officer. Here we present a
summary of recommendations from a workshop organized by the National
Aeronautics and Space Administration, on future applications of artificial
intelligence in space biology and health. In the next decade, biomonitoring
technology, biomarker science, spacecraft hardware, intelligent software, and
streamlined data management must mature and be woven together into a Precision
Space Health system to enable humanity to thrive in deep space.",http://arxiv.org/abs/2112.12554v1,"Ryan T Scott, Erik L Antonsen, Lauren M Sanders, Jaden J A Hastings, Seungmin Park, Graham Mackintosh, Robert J Reynolds, Adrienne L Hoarfrost, Aenor Sawyer, Casey S Greene, Benjamin S Glicksberg, Corey A Theriot, Daniel C Berrios, Jack Miller, Joel Babdor, Richard Barker, Sergio E Baranzini, Afshin Beheshti, Stuart Chalk, Guillermo M DelgadoAparicio, Melissa Haendel, Arif A Hamid, Philip Heller, Daniel Jamieson, Katelyn J Jarvis, John Kalantari, Kia Khezeli, Svetlana V Komarova, Matthieu Komorowski, Prachi Kothiyal, Ashish Mahabal, Uri Manor, Hector Garcia Martin, Christopher E Mason, Mona Matar, George I Mias, Jerry G Myers Jr, Charlotte Nelson, Jonathan Oribello, Patricia ParsonsWingerter, R K Prabhu, Amina Ann Qutub, Jon Rask, Amanda SaraviaButler, Suchi Saria, Nitin Kumar Singh, Frank Soboczenski, Michael Snyder, Karthik Soman, David Van Valen, Kasthuri Venkateswaran, Liz Warren, Liz Worthey, Jason H Yang, Marinka Zitnik, Sylvain V Costes"
326,"Beyond Low Earth Orbit: Biological Research, Artificial Intelligence, and Self-Driving Labs","Space biology research aims to understand fundamental effects of spaceflight
on organisms, develop foundational knowledge to support deep space exploration,
and ultimately bioengineer spacecraft and habitats to stabilize the ecosystem
of plants, crops, microbes, animals, and humans for sustained multi-planetary
life. To advance these aims, the field leverages experiments, platforms, data,
and model organisms from both spaceborne and ground-analog studies. As research
is extended beyond low Earth orbit, experiments and platforms must be maximally
autonomous, light, agile, and intelligent to expedite knowledge discovery. Here
we present a summary of recommendations from a workshop organized by the
National Aeronautics and Space Administration on artificial intelligence,
machine learning, and modeling applications which offer key solutions toward
these space biology challenges. In the next decade, the synthesis of artificial
intelligence into the field of space biology will deepen the biological
understanding of spaceflight effects, facilitate predictive modeling and
analytics, support maximally autonomous and reproducible experiments, and
efficiently manage spaceborne data and metadata, all with the goal to enable
life to thrive in deep space.",http://arxiv.org/abs/2112.12582v1,"Lauren M Sanders, Jason H Yang, Ryan T Scott, Amina Ann Qutub, Hector Garcia Martin, Daniel C Berrios, Jaden J A Hastings, Jon Rask, Graham Mackintosh, Adrienne L Hoarfrost, Stuart Chalk, John Kalantari, Kia Khezeli, Erik L Antonsen, Joel Babdor, Richard Barker, Sergio E Baranzini, Afshin Beheshti, Guillermo M DelgadoAparicio, Benjamin S Glicksberg, Casey S Greene, Melissa Haendel, Arif A Hamid, Philip Heller, Daniel Jamieson, Katelyn J Jarvis, Svetlana V Komarova, Matthieu Komorowski, Prachi Kothiyal, Ashish Mahabal, Uri Manor, Christopher E Mason, Mona Matar, George I Mias, Jack Miller, Jerry G Myers Jr, Charlotte Nelson, Jonathan Oribello, Seungmin Park, Patricia ParsonsWingerter, R K Prabhu, Robert J Reynolds, Amanda SaraviaButler, Suchi Saria, Aenor Sawyer, Nitin Kumar Singh, Frank Soboczenski, Michael Snyder, Karthik Soman, Corey A Theriot, David Van Valen, Kasthuri Venkateswaran, Liz Warren, Liz Worthey, Marinka Zitnik, Sylvain V Costes"
327,A Hassle-Free Machine Learning Method for Cohort Selection of Clinical Trials,"Traditional text classification techniques in clinical domain have heavily
relied on the manually extracted textual cues. This paper proposes a generally
supervised machine learning method that is equally hassle-free and does not use
clinical knowledge. The employed methods were simple to implement, fast to run
and yet effective. This paper proposes a novel named entity recognition (NER)
based an ensemble system capable of learning the keyword features in the
document. Instead of merely considering the whole sentence/paragraph for
analysis, the NER based keyword features can stress the important clinic
relevant phases more. In addition, to capture the semantic information in the
documents, the FastText features originating from the document level FastText
classification results are exploited.",http://arxiv.org/abs/1808.04694v1,Liu Man
328,Heterogeneous Clinical Trial Outcomes via Multi-Output Gaussian Processes,"We make use of Kronecker structure for scaling Gaussian Process models to
large-scale, heterogeneous, clinical data sets. Repeated measures, commonly
performed in clinical research, facilitate computational acceleration for
nonlinear Bayesian nonparametric models and enable exact sampling for
non-conjugate inference, when combinations of continuous and discrete endpoints
are observed. Model inference is performed in Stan, and comparisons are made
with brms on simulated data and two real clinical data sets, following a
radiological image quality theme. Scalable Gaussian Process models compare
favourably with parametric models on real data sets with 17,460 observations.
Different GP model specifications are explored, with components analogous to
random effects, and their theoretical properties are described.",http://arxiv.org/abs/2407.13283v2,"Owen Thomas, Leiv Rnneberg"
329,interAdapt -- An Interactive Tool for Designing and Evaluating Randomized Trials with Adaptive Enrollment Criteria,"The interAdapt R package is designed to be used by statisticians and clinical
investigators to plan randomized trials. It can be used to determine if certain
adaptive designs offer tangible benefits compared to standard designs, in the
context of investigators' specific trial goals and constraints. Specifically,
interAdapt compares the performance of trial designs with adaptive enrollment
criteria versus standard (non-adaptive) group sequential trial designs.
Performance is compared in terms of power, expected trial duration, and
expected sample size. Users can either work directly in the R console, or with
a user-friendly shiny application that requires no programming experience.
Several added features are available when using the shiny application. For
example, the application allows users to immediately download the results of
the performance comparison as a csv-table, or as a printable, html-based
report.",http://arxiv.org/abs/1404.0734v2,"Aaron Fisher, Harris Jaffee, Michael Rosenblum"
330,Design of Trials with Composite Endpoints with the R Package CompAREdesign,"Composite endpoints are widely used as primary endpoints in clinical trials.
Designing trials with time-to-event endpoints can be particularly challenging
because the proportional hazard assumption usually does not hold when using a
composite endpoint, even when the premise remains true for their components.
Consequently, the conventional formulae for sample size calculation do not
longer apply. We present the R package CompAREdesign by means of which the key
elements of trial designs, such as the sample size and effect sizes, can be
computed based on the information on the composite endpoint components.
CompAREdesign provides the functions to assess the sensitivity and robustness
of design calculations to variations in initial values and assumptions.
Furthermore, we describe other features of the package, such as functions for
the design of trials with binary composite endpoints, and functions to simulate
trials with composite endpoints under a wide range of scenarios.",http://arxiv.org/abs/2211.02535v1,"Jordi Corts Martinez, Marta Bofill Roig, Guadalupe Gmez Melis"
331,Optimal designs for active controlled dose finding trials with efficacy-toxicity outcomes,"Nonlinear regression models addressing both efficacy and toxicity outcomes
are increasingly used in dose-finding trials, such as in pharmaceutical drug
development. However, research on related experimental design problems for
corresponding active controlled trials is still scarce. In this paper we derive
optimal designs to estimate efficacy and toxicity in an active controlled
clinical dose finding trial when the bivariate continuous outcomes are modeled
either by polynomials up to degree 2, the Michaelis- Menten model, the Emax
model, or a combination thereof. We determine upper bounds on the number of
different doses levels required for the optimal design and provide conditions
under which the boundary points of the design space are included in the optimal
design. We also provide an analytical description of the minimally supported
$D$-optimal designs and show that they do not depend on the correlation between
the bivariate outcomes. We illustrate the proposed methods with numerical
examples and demonstrate the advantages of the $D$-optimal design for a trial,
which has recently been considered in the literature.",http://arxiv.org/abs/1601.00797v1,"Holger Dette, Katrin Kettelhake, Kirsten Schorning, Weng Kee Wong, Frank Bretz"
332,Estimating Information-Theoretic Quantities,"Information theory is a practical and theoretical framework developed for the
study of communication over noisy channels. Its probabilistic basis and
capacity to relate statistical structure to function make it ideally suited for
studying information flow in the nervous system. It has a number of useful
properties: it is a general measure sensitive to any relationship, not only
linear effects; it has meaningful units which in many cases allow direct
comparison between different experiments; and it can be used to study how much
information can be gained by observing neural responses in single trials,
rather than in averages over multiple trials. A variety of information
theoretic quantities are in common use in neuroscience - (see entry ""Summary of
Information-Theoretic Quantities""). Estimating these quantities in an accurate
and unbiased way from real neurophysiological data frequently presents
challenges, which are explained in this entry.",http://arxiv.org/abs/1501.01863v1,"Robin A A Ince, Simon R Schultz, Stefano Panzeri"
333,The IBEX Imaging Knowledge-Base: A Community Resource Enabling Adoption and Development of Immunofluoresence Imaging Methods,"The iterative bleaching extends multiplexity (IBEX) Knowledge-Base is a
central portal for researchers adopting IBEX and related 2D and 3D
immunofluorescence imaging methods. The design of the Knowledge-Base is modeled
after efforts in the open-source software community and includes three facets:
a development platform (GitHub), static website, and service for data
archiving. The Knowledge-Base facilitates the practice of open science
throughout the research life cycle by providing validation data for recommended
and non-recommended reagents, e.g., primary and secondary antibodies. In
addition to reporting negative data, the Knowledge-Base empowers method
adoption and evolution by providing a venue for sharing protocols, videos,
datasets, software, and publications. A dedicated discussion forum fosters a
sense of community among researchers while addressing questions not covered in
published manuscripts. Together, scientists from around the world are advancing
scientific discovery at a faster pace, reducing wasted time and effort, and
instilling greater confidence in the resulting data.",http://arxiv.org/abs/2412.12965v1,"Ziv Yaniv, Ifeanyichukwu U Anidi, Leanne Arakkal, Armando J ArroyoMejas, Rebecca T Beuschel, Katy Brner, Colin J Chu, Beatrice Clark, Menna R Clatworthy, Jake Colautti, Fabian Coscia, Joshua Croteau, Saven Denha, Rose Dever, Walderez O Dutra, Sonja Fritzsche, Spencer Fullam, Michael Y Gerner, Anita Gola, Kenneth J Gollob, Jonathan M Hernandez, Jyh Liang Hor, Hiroshi Ichise, Zhixin Jing, Danny Jonigk, Evelyn Kandov, Wolfgang Kastenmller, Joshua F E Koenig, Aanandita Kothurkar, Rosa K Kortekaas, Alexandra Y Kreins, Ian T Lamborn, Yuri Lin, Katia Luciano Pereira Morais, Aleksandra Lunich, Jean C S Luz, Ryan B MacDonald, Chen Makranz, Vivien I Maltez, John E McDonough, Ryan V Moriarty, Juan M OcampoGodinez, Vitoria M Olyntho, Annette Oxenius, Kartika Padhan, Kirsten Remmert, Nathan Richoz, Edward C Schrom, Wanjing Shang, Lihong Shi, Rochelle M Shih, Emily Speranza, Salome Stierli, Sarah A Teichmann, Tibor Z Veres, Megan Vierhout, Brianna T Wachter, Margaret Williams, Nathan Zangger, Ronald N Germain, Andrea J Radtke"
334,Technological Competence is a Precondition for Effective Implementation of Virtual Reality Head Mounted Displays in Human Neuroscience: A Technological Review and Meta-analysis,"Immersive virtual reality (VR) emerges as a promising research and clinical
tool. However, several studies suggest that VR induced adverse symptoms and
effects (VRISE) may undermine the health and safety standards, and the
reliability of the scientific results. In the current literature review, the
technical reasons for the adverse symptomatology are investigated to provide
suggestions and technological knowledge for the implementation of VR
head-mounted display (HMD) systems in cognitive neuroscience. The technological
systematic literature indicated features pertinent to display, sound, motion
tracking, navigation, ergonomic interactions, user experience, and computer
hardware that should be considered by the researchers. Subsequently, a
meta-analysis of 44 neuroscientific or neuropsychological studies involving VR
HMD systems was performed. The meta-analysis of the VR studies demonstrated
that new generation HMDs induced significantly less VRISE and marginally fewer
dropouts.Importantly, the commercial versions of the new generation HMDs with
ergonomic interactions had zero incidents of adverse symptomatology and
dropouts. HMDs equivalent to or greater than the commercial versions of
contemporary HMDs accompanied with ergonomic interactions are suitable for
implementation in cognitive neuroscience. In conclusion, researchers
technological competency, along with meticulous methods and reports pertinent
to software, hardware, and VRISE, are paramount to ensure the health and safety
standards and the reliability of neuroscientific results.",http://arxiv.org/abs/2101.08123v1,"Panagiotis Kourtesis, Simona Collina, Leonidas A A Doumas, Sarah E MacPherson"
335,Using routinely collected patient data to support clinical trials research in accountable care organizations,"Background: More than half (57%) of pharma clinical research spend is in
support of clinical trials. One reason is that Electronic Health Record (EHR)
systems and HIPAA privacy rules often limit how broadly patient information can
be shared, resulting in laborious human efforts to manually collect,
de-identify, and summarize patient information for use in clinical studies.
  Purpose: Conduct feasibility study for a Rheumatoid Arthritis (RA) clinical
trial in an Accountable Care Organization. Measure prevalence of RA and related
conditions matching study criteria. Evaluate automation of patient
de-identification and summarization to support patient cohort development for
clinical studies.
  Methods: Collect original clinical documentation directly from the provider
EHR system and extract clinical concepts necessary for matching study criteria.
Automatically de-identify Protected Health Information (PHI) protect patient
privacy and promote sharing. Leverage existing physician expert knowledge
sources to enable analysis of patient populations.
  Results: Prevalence of RA was four percent (4%) in the study population (mean
age 53 years, 52% female, 48% male). Clinical documentation for 3500 patient
were extracted from three (3) EHR systems. Grouped diagnosis codes revealed
high prevalence of diabetes and diseases of the circulatory system, as
expected. De-identification accurately removed 99% of PHI identifiers with 99%
sensitivity and 99% specificity.
  Conclusions: Results suggest the approach can improve automation and
accelerate planning and construction of new clinical studies in the ACO
setting. De-identification accuracy was better than previously approved
requirements defined by four (4) hospital Institutional Review Boards.",http://arxiv.org/abs/1807.00668v1,"Andrew J McMurry, Richen Zhang, Alex Foxman, Lawrence Reiter, Ronny Schnel, DeLeys Brandman"
336,Adaptive Allocation Theory in Clinical Trials,"Various adaptive randomization procedures (adaptive designs) have been
proposed to clinical trials. This paper discusses several broad families of
procedures, such as the play-the-winner rule and Markov chain model, randomized
play-the-winner rule and urn models, drop-the-loser rule, doubly biased coin
adaptive design. Asymptotic theories are presented with several pivotal proofs.
The effect of delayed responses, the power and variability comparison of these
designs are also discussed.",http://arxiv.org/abs/math/0612811v1,LiXin Zhang
337,Application of the Signature Method to Pattern Recognition in the CEQUEL Clinical Trial,"The classification procedure of streaming data usually requires various ad
hoc methods or particular heuristic models. We explore a novel non-parametric
and systematic approach to analysis of heterogeneous sequential data. We
demonstrate an application of this method to classification of the delays in
responding to the prompts, from subjects with bipolar disorder collected during
a clinical trial, using both synthetic and real examples. We show how this
method can provide a natural and systematic way to extract characteristic
features from sequential data.",http://arxiv.org/abs/1606.02074v1,"A B Kormilitzin, K E A Saunders, P J Harrison, J R Geddes, T J Lyons"
338,Group Sequential Clinical Trial Designs for Normally Distributed Outcome Variables,"In a group sequential clinical trial, accumulated data are analysed at
numerous time-points in order to allow early decisions about a hypothesis of
interest. These designs have historically been recommended for their ethical,
administrative and economic benefits. In this work, we discuss a collection of
new Stata commands for computing the stopping boundaries and required group
size of various classical group sequential designs, assuming a normally
distributed outcome variable. Following this, we demonstrate how the
performance of several designs can be compared graphically.",http://arxiv.org/abs/1710.03127v2,"Michael Grayling, James Wason, Adrian Mander"
339,Adaptive Experiments and a Rigorous Framework for Type I Error Verification and Computational Experiment Design,"This PhD thesis covers breakthroughs in several areas of adaptive experiment
design: (i) (Chapter 2) Novel clinical trial designs and statistical methods in
the era of precision medicine. (ii) (Chapter 3) Multi-armed bandit theory, with
applications to learning healthcare systems and clinical trials. (iii) (Chapter
4) Bandit and covariate processes, with finite and non-denumerable set of arms.
(iv) (Chapter 5) A rigorous framework for simulation-based verification of
adaptive design properties.",http://arxiv.org/abs/2205.09369v1,Michael Sklar
340,Statistical Methods for the meta-analysis paper by Itzhaky et al,"This document describes the statistical methods used in Itzhaky et al
(""Systematic Review and Meta-analysis: Twenty-six Years of Randomized Clinical
Trials of Psychosocial Interventions to Reduce Suicide Risk in Adolescents"").
That paper is a meta-analysis of randomized controlled clinical trials testing
methods for preventing suicidal behavior and/or ideation in youth. Particularly
on the behavior side the meta-data are challenging to analyze.
  This paper has two parts. The first is an informal discussion of the
statistical methods used. The second gives detailed mathematical derivations of
some formulas and methods.",http://arxiv.org/abs/2106.13874v2,Steven P Ellis
341,A bayesian reanalysis of the phase III aducanumab (ADU) trial,"In this article we have conducted a reanalysis of the phase III aducanumab
(ADU) summary statistics announced by Biogen, in particular the result of the
Clinical Dementia Rating-Sum of Boxes (CDR-SB). The results showed that the
evidence on the efficacy of the drug is very low and a more clearer view of the
results of clinical trials are presented in the Bayesian framework that can be
useful for future development and research in the field.",http://arxiv.org/abs/2107.03686v2,"Tommaso Costa, Franco Cauda"
342,Robust Variance Estimation for Covariate-Adjusted Unconditional Treatment Effect in Randomized Clinical Trials with Binary Outcomes,"To improve precision of estimation and power of testing hypothesis for an
unconditional treatment effect in randomized clinical trials with binary
outcomes, researchers and regulatory agencies recommend using g-computation as
a reliable method of covariate adjustment. However, the practical application
of g-computation is hindered by the lack of an explicit robust variance formula
that can be used for different unconditional treatment effects of interest. To
fill this gap, we provide explicit and robust variance estimators for
g-computation estimators and demonstrate through simulations that the variance
estimators can be reliably applied in practice.",http://arxiv.org/abs/2302.10404v2,"Ting Ye, Marlena Bannick, Yanyao Yi, Jun Shao"
343,Drug Supply Chain Optimization for Adaptive Clinical Trials,"With increasing interest in adaptive clinical trial designs, challenges are
present to drug supply chain management which may offset the benefit of
adaptive designs. Thus, it is necessary to develop an optimization tool to
facilitate the decision making and analysis of drug supply chain planning. The
challenges include the uncertainty of maximum drug supply needed, the shifting
of supply requirement, and rapid availability of new supply at decision points.
In this paper, statistical simulations are designed to optimize the pre-study
medication supply strategy and monitor ongoing drug supply using real-time data
collected with the progress of study. Particle swarm algorithm is applied when
performing optimization, where feature extraction is implemented to reduce
dimensionality and save computational cost.",http://arxiv.org/abs/2310.08721v1,"Jincheng Pang, Hong Yan, Zoe Hua"
344,The nph2ph-transform: applications to the statistical analysis of completed clinical trials,"We present several illustrations from completed clinical trials on a
statistical approach that allows us to gain useful insights regarding the time
dependency of treatment effects. Our approach leans on a simple proposition:
all non-proportional hazards (NPH) models are equivalent to a proportional
hazards model. The nph2ph transform brings an NPH model into a PH form. We
often find very simple approximations for this transform, enabling us to
analyze complex NPH observations as though they had arisen under proportional
hazards. Many techniques become available to us, and we use these to understand
treatment effects better.",http://arxiv.org/abs/2407.18905v1,"Sean M Devlin, John OQuigley"
345,Uncertainty Quantification on Clinical Trial Outcome Prediction,"The importance of uncertainty quantification is increasingly recognized in
the diverse field of machine learning. Accurately assessing model prediction
uncertainty can help provide deeper understanding and confidence for
researchers and practitioners. This is especially critical in medical diagnosis
and drug discovery areas, where reliable predictions directly impact research
quality and patient health.
  In this paper, we proposed incorporating uncertainty quantification into
clinical trial outcome predictions. Our main goal is to enhance the model's
ability to discern nuanced differences, thereby significantly improving its
overall performance.
  We have adopted a selective classification approach to fulfill our objective,
integrating it seamlessly with the Hierarchical Interaction Network (HINT),
which is at the forefront of clinical trial prediction modeling. Selective
classification, encompassing a spectrum of methods for uncertainty
quantification, empowers the model to withhold decision-making in the face of
samples marked by ambiguity or low confidence, thereby amplifying the accuracy
of predictions for the instances it chooses to classify. A series of
comprehensive experiments demonstrate that incorporating selective
classification into clinical trial predictions markedly enhances the model's
performance, as evidenced by significant upticks in pivotal metrics such as
PR-AUC, F1, ROC-AUC, and overall accuracy.
  Specifically, the proposed method achieved 32.37\%, 21.43\%, and 13.27\%
relative improvement on PR-AUC over the base model (HINT) in phase I, II, and
III trial outcome prediction, respectively. When predicting phase III, our
method reaches 0.9022 PR-AUC scores.
  These findings illustrate the robustness and prospective utility of this
strategy within the area of clinical trial predictions, potentially setting a
new benchmark in the field.",http://arxiv.org/abs/2401.03482v3,"Tianyi Chen, Yingzhou Lu, Nan Hao, Yuanyuan Zhang, Capucine Van Rechem, Jintai Chen, Tianfan Fu"
346,Ultrasonic Actuation of a Fine-Needle Improves Biopsy Yield,"Despite the ubiquitous use over the past 150 years, the functions of the
current medical needle are facilitated only by mechanical shear and cutting by
the needle tip,i.e.the lancet. In this study, we demonstrate how nonlinear
ultrasonics (NLU) extends the functionality of the medical needle far beyond
its present capability. The NLU actions were found to be localized to the
proximity of the needle tip, the SonoLancet, but the effects extend several
millimeters from the physical needle boundary. The observed nonlinear
phenomena, transient cavitation, fluid streams, translation of micro- and
nanoparticles and atomization, were quantitatively characterized. In the
fine-needle biopsy application, the SonoLancet contributed to obtaining tissue
cores with increase in tissue yield by 3-6x in different tissue types compared
to conventional needle biopsy technique using the same 21G needle. In
conclusion, the SonoLancet could be of interest to several other medical
applications, including drug or gene delivery, cell modulation, and minimally
invasive surgical procedures.",http://arxiv.org/abs/2006.16604v3,"Emanuele Perra, Eetu Lampsijrvi, Gonalo Barreto, Muhammad Arif, Tuomas Puranen, Edward Hggstrm, Kenneth P H Pritzker, Heikki J Nieminen"
347,"A hypothesis test of feasibility for external pilot trials assessing recruitment, follow-up and adherence rates","The power of a large clinical trial can be adversely affected by low
recruitment, follow-up and adherence rates. External pilot trials estimate
these rates and use them, via pre-specified decision rules, to determine if the
definitive trial is feasible and should go ahead. There is little
methodological research underpinning how these decision rules, or the sample
size of the pilot, should be chosen. In this paper we propose a hypothesis test
of the feasibility of a definitive trial, to be applied to the external pilot
data and used to make progression decisions. We quantify feasibility by the
power of the planned trial, as a function of recruitment, follow-up and
adherence rates. We use this measure to define hypotheses to test in the pilot,
propose a test statistic, and show how the error rates of this test can be
calculated for the common scenario of a two-arm parallel group definitive trial
with a single normally distributed primary endpoint. We use our method to
re-design TIGA-CUB, an external pilot trial comparing a psychotherapy with
treatment as usual for children with conduct disorders. We then extend our
formulation to include using the pilot data to estimate the standard deviation
of the primary endpoint. and incorporate this into the progression decision.",http://arxiv.org/abs/1908.05562v1,"Duncan T Wilson, Rebecca E A Walwyn, Julia Brown, Amanda J Farrin"
348,A gated group sequential design for seamless Phase II/III trial with subpopulation selection,"Due to the high cost and high failure rate of Phase III trials, seamless
Phase II/III designs are more and more popular to trial efficiency. A potential
attraction of Phase II/III design is to allow a randomized proof-of-concept
stage prior to committing to the full cost of the Phase III trial. Population
selection during the trial allows a trial to adapt and focus investment where
it is most likely to provide patient benefit. Motivated by a clinical trial to
find the population that potential benefits with dual-primary endpoints
progression free survival (PFS) and overall survival (OS), we propose a gated
group sequential design for a seamless Phase II/III trial design with
population selection. The investigated design controls the familywise error
rate and allows multiple interim analyses to enable early stopping for efficacy
or futility. Simulations and an illustrative example suggest that the proposed
gated group sequential design can have more power than the commonly used
classical group sequential design, and reduces the patient's exposure to less
effective treatment if the complementary sub-group has less significant
treatment effect. The proposed design has the potential to save drug
development cost and more quickly fulfill unmet medical needs.",http://arxiv.org/abs/2206.12536v1,"Guanhong Miao, Jason J Z Liao, Jing Yang, Keaven Anderson"
349,Simultaneous confidence intervals for an extended Koch-Rhmel design in three-arm non-inferiority trials,"Three-arm `gold-standard' non-inferiority trials are recommended for
indications where only unstable reference treatments are available and the use
of a placebo group can be justified ethically. For such trials several study
designs have been suggested that use the placebo group for testing 'assay
sensitivity', i.e. the ability of the trial to replicate efficacy. Should the
reference fail in the given trial, then non-inferiority could also be shown
with an ineffective experimental treatment and hence becomes useless. In this
paper we extend the so called Koch-R\""ohmel design where a proof of efficacy
for the experimental treatment is required in order to qualify the
non-inferiority test. While efficacy of the experimental treatment is an
indication for assay sensitivity, it does not guarantee that the reference is
sufficient efficient to let the non-inferiority claim be meaningful. It has
therefore been suggested to adaptively test non-inferiority only if the
reference demonstrates superiority to placebo and otherwise to test
$\delta$-superiority of the experimental treatment over placebo, where $\delta$
is chosen in such a way that it provides proof of non-inferiority with regard
to the reference's historical effect. In this paper we extend the previous work
by complementing its adaptive test with compatible simultaneous confidence
intervals. Confidence intervals are commonly used and suggested by regulatory
guidelines for non-inferiority trials. We show how to adopt different
approaches to simultaneous confidence intervals from the literature to the
setting of three-arm non-inferiority trials and compare these methods in a
simulation study. Finally we apply these methods to a real clinical trial
example.",http://arxiv.org/abs/2210.08931v1,"Martin Scharpenberg, Werner Brannath"
350,Generalizing the intention-to-treat effect of an active control against placebo from historical placebo-controlled trials to an active-controlled trial: A case study of the efficacy of daily oral TDF/FTC in the HPTN 084 study,"In many clinical settings, an active-controlled trial design (e.g., a
non-inferiority or superiority design) is often used to compare an experimental
medicine to an active control (e.g., an FDA-approved, standard therapy). One
prominent example is a recent phase 3 efficacy trial, HIV Prevention Trials
Network Study 084 (HPTN 084), comparing long-acting cabotegravir, a new HIV
pre-exposure prophylaxis (PrEP) agent, to the FDA-approved daily oral tenofovir
disoproxil fumarate plus emtricitabine (TDF/FTC) in a population of
heterosexual women in 7 African countries. One key complication of interpreting
study results in an active-controlled trial like HPTN 084 is that the placebo
arm is not present and the efficacy of the active control (and hence the
experimental drug) compared to the placebo can only be inferred by leveraging
other data sources. \bz{In this article, we study statistical inference for the
intention-to-treat (ITT) effect of the active control using relevant historical
placebo-controlled trials data under the potential outcomes (PO) framework}. We
highlight the role of adherence and unmeasured confounding, discuss in detail
identification assumptions and two modes of inference (point versus partial
identification), propose estimators under identification assumptions permitting
point identification, and lay out sensitivity analyses needed to relax
identification assumptions. We applied our framework to estimating the
intention-to-treat effect of daily oral TDF/FTC versus placebo in HPTN 084
using data from an earlier Phase 3, placebo-controlled trial of daily oral
TDF/FTC (Partners PrEP).",http://arxiv.org/abs/2304.03476v2,"Qijia He, Fei Gao, Oliver Dukes, Sinead DelanyMoretlwe, Bo Zhang"
351,Considerations for Master Protocols Using External Controls,"There has been an increasing use of master protocols in oncology clinical
trials because of its efficiency and flexibility to accelerate cancer drug
development. Depending on the study objective and design, a master protocol
trial can be a basket trial, an umbrella trial, a platform trial, or any other
form of trials in which multiple investigational products and/or subpopulations
are studied under a single protocol. Master protocols can use external data and
evidence (e.g., external controls) for treatment effect estimation, which can
further improve efficiency of master protocol trials. This paper provides an
overview of different types of external controls and their unique features when
used in master protocols. Some key considerations in master protocols with
external controls are discussed including construction of estimands, assessment
of fit-for-use real-world data, and considerations for different types of
master protocols. Similarities and differences between regular randomized
controlled trials and master protocols when using external controls are
discussed. A targeted learning-based causal roadmap is presented which
constitutes three key steps: (1) define a target statistical estimand that
aligns with the causal estimand for the study objective, (2) use an efficient
estimator to estimate the target statistical estimand and its uncertainty, and
(3) evaluate the impact of causal assumptions on the study conclusion by
performing sensitivity analyses. Two illustrative examples for master protocols
using external controls are discussed for their merits and possible improvement
in causal effect estimation.",http://arxiv.org/abs/2307.05050v2,"Jie Chen, Xiaoyun, Li, Chengxing, Lu, Sammy Yuan, Godwin Yung, Jingjing Ye, Hong Tian, Jianchang Lin"
352,Applying the estimands framework to non-inferiority trials: guidance on choice of hypothetical estimands for non-adherence and comparison of estimation methods,"A common concern in non-inferiority (NI) trials is that non adherence due,
for example, to poor study conduct can make treatment arms artificially
similar. Because intention to treat analyses can be anti-conservative in this
situation, per protocol analyses are sometimes recommended. However, such
advice does not consider the estimands framework, nor the risk of bias from per
protocol analyses. We therefore sought to update the above guidance using the
estimands framework, and compare estimators to improve on the performance of
per protocol analyses. We argue the main threat to validity of NI trials is the
occurrence of trial specific intercurrent events (IEs), that is, IEs which
occur in a trial setting, but would not occur in practice. To guard against
erroneous conclusions of non inferiority, we suggest an estimand using a
hypothetical strategy for trial specific IEs should be employed, with handling
of other non trial specific IEs chosen based on clinical considerations. We
provide an overview of estimators that could be used to estimate a hypothetical
estimand, including inverse probability weighting (IPW), and two instrumental
variable approaches (one using an informative Bayesian prior on the effect of
standard treatment, and one using a treatment by covariate interaction as an
instrument). We compare them, using simulation in the setting of all or nothing
compliance in two active treatment arms, and conclude both IPW and the
instrumental variable method using a Bayesian prior are potentially useful
approaches, with the choice between them depending on which assumptions are
most plausible for a given trial.",http://arxiv.org/abs/2312.00494v1,"Katy E Morgan, Ian R White, Clmence Leyrat, Simon Stanworth, Brennan C Kahan"
353,Is control of type I error rate needed in Bayesian clinical trial designs?,"Practical employment of Bayesian trial designs is still rare. Even if
accepted in principle, the regulators have commonly required that such designs
be calibrated according to an upper bound for the frequentist type I error
rate. This represents an internally inconsistent hybrid methodology, where
important advantages from following the Bayesian principles are lost. In
particular, all preplanned interim looks have an inflating multiplicity effect
on type I error rate. To present an alternative approach, we consider the
prototype case of a 2-arm superiority trial with dichotomous outcomes. The
design is adaptive, using error control based on sequentially updated posterior
probabilities, to conclude efficacy of the experimental treatment or futility
of the trial. As gatekeepers for a proposed design, the regulators have the
main responsibility in determining the parameters of the control of false
positives, whereas the trial sponsors and investigators will have a natural
role in specifying the criteria for stopping the trial due to futility. It is
suggested that the traditional frequentist operating characteristics in the
design, type I and type II error rates, be replaced, respectively, by Bayesian
criteria called False Discovery Probability (FDP) and False Futility
Probability (FFP), both terms corresponding directly to their probability
interpretations. Importantly, the sequential error control during the data
analysis based on posterior probabilities will satisfy these numerical criteria
automatically, without need of preliminary computations before the trial is
started. The method contains the option of applying a decision rule for
terminating the trial early if the predicted costs from continuing would exceed
the corresponding gains.",http://arxiv.org/abs/2312.15222v3,"Elja Arjas, Dario Gasbarra"
354,Response rate estimation in single-stage basket trials: A comparison of estimators that allow for borrowing across cohorts,"Therapeutic advancements in oncology have shifted towards targeted therapy
based on genomic aberrations. This necessitates innovative statistical
approaches in clinical trials, particularly in master protocol studies. Basket
trials, a type of master protocol, evaluate a single treatment across cohorts
sharing a genomic aberration but differing in tumor histology. While offering
operational advantages, basket trial analysis presents statistical inference
challenges. These trials help determine for which tumor histology the treatment
is promising enough to advance to confirmatory evaluation and often use
Bayesian designs to support decisions. Beyond decision-making, estimating
cohort-specific response rates is crucial for designing subsequent trials. This
study compares seven Bayesian estimation methods for basket trials with binary
outcomes against the (frequentist) sample proportion estimate through
simulations. The goal is to estimate cohort-specific response rates, focusing
on bias, mean squared error, and information borrowing. Various scenarios are
examined, including homogeneous, heterogeneous, and clustered response rates
across cohorts. Results show trade-offs in bias and precision, highlighting the
importance of method selection. Berry's method performs best with limited
heterogeneity. No clear winner emerges in general cases, with performance
affected by shrinkage, bias, and the choice of priors and tuning parameters.
Challenges include computational complexity, parameter tuning, and the lack of
clear guidance on selection. Researchers should consider these factors when
designing and analyzing basket trials.",http://arxiv.org/abs/2502.07639v1,"Antonios Daletzakis, Rutger van den Bor, Vincent van der Noort, Kit CB Roes"
355,Algorithms for multi-armed bandit problems,"Although many algorithms for the multi-armed bandit problem are
well-understood theoretically, empirical confirmation of their effectiveness is
generally scarce. This paper presents a thorough empirical study of the most
popular multi-armed bandit algorithms. Three important observations can be made
from our results. Firstly, simple heuristics such as epsilon-greedy and
Boltzmann exploration outperform theoretically sound algorithms on most
settings by a significant margin. Secondly, the performance of most algorithms
varies dramatically with the parameters of the bandit problem. Our study
identifies for each algorithm the settings where it performs well, and the
settings where it performs poorly. Thirdly, the algorithms' performance
relative each to other is affected only by the number of bandit arms and the
variance of the rewards. This finding may guide the design of subsequent
empirical evaluations. In the second part of the paper, we turn our attention
to an important area of application of bandit algorithms: clinical trials.
Although the design of clinical trials has been one of the principal practical
problems motivating research on multi-armed bandits, bandit algorithms have
never been evaluated as potential treatment allocation strategies. Using data
from a real study, we simulate the outcome that a 2001-2002 clinical trial
would have had if bandit algorithms had been used to allocate patients to
treatments. We find that an adaptive trial would have successfully treated at
least 50% more patients, while significantly reducing the number of adverse
effects and increasing patient retention. At the end of the trial, the best
treatment could have still been identified with a high level of statistical
confidence. Our findings demonstrate that bandit algorithms are attractive
alternatives to current adaptive treatment allocation strategies.",http://arxiv.org/abs/1402.6028v1,"Volodymyr Kuleshov, Doina Precup"
356,Sample size re-estimation incorporating prior information on a nuisance parameter,"Prior information is often incorporated informally when planning a clinical
trial. Here, we present an approach on how to incorporate prior information,
such as data from historical clinical trials, into the nuisance parameter based
sample size re-estimation in a design with an internal pilot study. We focus on
trials with continuous endpoints in which the outcome variance is the nuisance
parameter. For planning and analyzing the trial frequentist methods are
considered. Moreover, the external information on the variance is summarized by
the Bayesian meta-analytic-predictive (MAP) approach. To incorporate external
information into the sample size re-estimation, we propose to update the MAP
prior based on the results of the internal pilot study and to re-estimate the
sample size using an estimator from the posterior. By means of a simulation
study, we compare the operating characteristics such as power and sample size
distribution of the proposed procedure with the traditional sample size
re-estimation approach which uses the pooled variance estimator. The simulation
study shows that, if no prior-data conflict is present, incorporating external
information into the sample size re-estimation improves the operating
characteristics compared to the traditional approach. In the case of a
prior-data conflict, that is when the variance of the ongoing clinical trial is
unequal to the prior location, the performance of the traditional sample size
re-estimation procedure is in general superior, even when the prior information
is robustified. When considering to include prior information in sample size
re-estimation, the potential gains should be balanced against the risks.",http://arxiv.org/abs/1703.06957v2,"Tobias Mtze, Heinz Schmidli, Tim Friede"
357,P-hacking in clinical trials and how incentives shape the distribution of results across phases,"Clinical research should conform to high standards of ethical and scientific
integrity, given that human lives are at stake. However, economic incentives
can generate conflicts of interest for investigators, who may be inclined to
withhold unfavorable results or even tamper with data in order to achieve
desired outcomes. To shed light on the integrity of clinical trial results,
this paper systematically analyzes the distribution of p-values of primary
outcomes for phase II and phase III drug trials reported to the
ClinicalTrials.gov registry. First, we detect no bunching of results just above
the classical 5% threshold for statistical significance. Second, a density
discontinuity test reveals an upward jump at the 5% threshold for phase III
results by small industry sponsors. Third, we document a larger fraction of
significant results in phase III compared to phase II. Linking trials across
phases, we find that early favorable results increase the likelihood of
continuing into the next phase. Once we take into account this selective
continuation, we can explain almost completely the excess of significant
results in phase III for trials conducted by large industry sponsors. For small
industry sponsors, instead, part of the excess remains unexplained.",http://arxiv.org/abs/1907.00185v3,"Jrme Adda, Christian Decker, Marco Ottaviani"
358,Graphical approaches for the control of generalised error rates,"When simultaneously testing multiple hypotheses, the usual approach in the
context of confirmatory clinical trials is to control the familywise error rate
(FWER), which bounds the probability of making at least one false rejection. In
many trial settings, these hypotheses will additionally have a hierarchical
structure that reflects the relative importance and links between different
clinical objectives. The graphical approach of Bretz et al. (2009) is a
flexible and easily communicable way of controlling the FWER while respecting
complex trial objectives and multiple structured hypotheses. However, the FWER
can be a very stringent criterion that leads to procedures with low power, and
may not be appropriate in exploratory trial settings. This motivates
controlling generalised error rates, particularly when the number of hypotheses
tested is no longer small. We consider the generalised familywise error rate
(k-FWER), which is the probability of making k or more false rejections, as
well as the tail probability of the false discovery proportion (FDP), which is
the probability that the proportion of false rejections is greater than some
threshold. We also consider asymptotic control of the false discovery rate
(FDR), which is the expectation of the FDP. In this paper, we show how to
control these generalised error rates when using the graphical approach and its
extensions. We demonstrate the utility of the resulting graphical procedures on
three clinical trial case studies.",http://arxiv.org/abs/2004.01759v2,"David S Robertson, James M S Wason, Frank Bretz"
359,A Bayesian Precision Response-adaptive Phase II Clinical Trial Design for Radiotherapies with Competing Risk Survival Outcomes,"Many phase II clinical trials have used survival outcomes as the primary
endpoints in recent decades. Suppose the radiotherapy is evaluated in a phase
II trial using survival outcomes. In that case, the competing risk issue often
arises because the time to disease progression can be censored by the time to
normal tissue complications, and vice versa. Besides, much literature has
examined that patients receiving the same radiotherapy dose may yield distinct
responses due to their heterogeneous radiation susceptibility statuses.
Therefore, the ""one-dose-fit-all"" strategy often fails, and it is more relevant
to evaluate the subgroup-specific treatment effect with the subgroup defined by
the radiation susceptibility status. In this paper, we propose a Bayesian
precision phase II trial design evaluating the subgroup-specific treatment
effects of radiotherapy. We use the cause-specific hazard approach to model the
competing risk survival outcomes. We propose restricting the candidate
radiation doses based on each patient's radiation susceptibility status. Only
the clinically feasible personalized dose will be considered, which enhances
the benefit for the patients in the trial. In addition, we propose a stratified
Bayesian adaptive randomization scheme such that more patients will be
randomized to the dose reporting more favorable survival outcomes. Numerical
studies have shown that the proposed design performed well and outperformed the
conventional design ignoring the competing risk issue.",http://arxiv.org/abs/2203.06830v1,"Jina Park, Wenjing Hu, Ick Hoon Jin, Hao Liu, Yong Zang"
360,Increasing the efficiency of randomized trial estimates via linear adjustment for a prognostic score,"Estimating causal effects from randomized experiments is central to clinical
research. Reducing the statistical uncertainty in these analyses is an
important objective for statisticians. Registries, prior trials, and health
records constitute a growing compendium of historical data on patients under
standard-of-care that may be exploitable to this end. However, most methods for
historical borrowing achieve reductions in variance by sacrificing strict
type-I error rate control. Here, we propose a use of historical data that
exploits linear covariate adjustment to improve the efficiency of trial
analyses without incurring bias. Specifically, we train a prognostic model on
the historical data, then estimate the treatment effect using a linear
regression while adjusting for the trial subjects' predicted outcomes (their
prognostic scores). We prove that, under certain conditions, this prognostic
covariate adjustment procedure attains the minimum variance possible among a
large class of estimators. When those conditions are not met, prognostic
covariate adjustment is still more efficient than raw covariate adjustment and
the gain in efficiency is proportional to a measure of the predictive accuracy
of the prognostic model above and beyond the linear relationship with the raw
covariates. We demonstrate the approach using simulations and a reanalysis of
an Alzheimer's Disease clinical trial and observe meaningful reductions in
mean-squared error and the estimated variance. Lastly, we provide a simplified
formula for asymptotic variance that enables power calculations that account
for these gains. Sample size reductions between 10% and 30% are attainable when
using prognostic models that explain a clinically realistic percentage of the
outcome variance.",http://arxiv.org/abs/2012.09935v3,"Alejandro Schuler, David Walsh, Diana Hall, Jon Walsh, Charles Fisher"
361,Sent2Span: Span Detection for PICO Extraction in the Biomedical Text without Span Annotations,"The rapid growth in published clinical trials makes it difficult to maintain
up-to-date systematic reviews, which requires finding all relevant trials. This
leads to policy and practice decisions based on out-of-date, incomplete, and
biased subsets of available clinical evidence. Extracting and then normalising
Population, Intervention, Comparator, and Outcome (PICO) information from
clinical trial articles may be an effective way to automatically assign trials
to systematic reviews and avoid searching and screening - the two most
time-consuming systematic review processes. We propose and test a novel
approach to PICO span detection. The major difference between our proposed
method and previous approaches comes from detecting spans without needing
annotated span data and using only crowdsourced sentence-level annotations.
Experiments on two datasets show that PICO span detection results achieve much
higher results for recall when compared to fully supervised methods with PICO
sentence detection at least as good as human annotations. By removing the
reliance on expert annotations for span detection, this work could be used in
human-machine pipeline for turning low-quality crowdsourced, and sentence-level
PICO annotations into structured information that can be used to quickly assign
trials to relevant systematic reviews.",http://arxiv.org/abs/2109.02254v1,"Shifeng Liu, Yifang Sun, Bing Li, Wei Wang, Florence T Bourgeois, Adam G Dunn"
362,Generating the right evidence at the right time: Principles of a new class of flexible augmented clinical trial designs,"The past few years have seen an increasing number of initiatives aimed at
integrating information generated outside of confirmatory randomised clinical
trials (RCTs) into drug development. However, data generated non-concurrently
and through observational studies can provide results that are difficult to
compare with randomised trial data. Moreover, the scientific questions these
data can serve to answer often remain vague. Our starting point is to use
clearly defined objectives for evidence generation, which are formulated
towards early discussion with health technology assessment (HTA) bodies and are
additional to regulatory requirements for authorisation of a new treatment. We
propose FACTIVE (Flexible Augmented Clinical Trial for Improved eVidencE
generation), a new class of study designs enabling flexible augmentation of
confirmatory randomised controlled trials with concurrent and
close-to-real-world elements. These enabling designs facilitate estimation of
certain treatment effects in the confirmatory part and other, complementary
treatment effects in a concurrent real-world part. Each stakeholder should use
the evidence that is relevant within their own decision-making framework. High
quality data are generated under one single protocol and the use of
randomisation ensures rigorous statistical inference and interpretation within
and between the different parts of the experiment. Evidence for the
decision-making of HTA bodies could be available earlier than is currently the
case.",http://arxiv.org/abs/2210.15264v3,"Cornelia DungerBaldauf, Rob Hemmings, Frank Bretz, Byron Jones, Anja Schiel, Chris Holmes"
363,Precision Dose-finding Cancer Clinical Trials in the Setting of Broadened Eligibility,"Broadening eligibility criteria in cancer trials has been advocated to
represent the true patient population more accurately. While the advantages are
clear in terms of generalizability and recruitment, novel dose-finding designs
are needed to ensure patient safety. These designs should be able to recommend
precise doses for subpopulations if such subpopulations with different toxicity
profiles exist. While dose-finding designs accounting for patient heterogeneity
have been proposed, all existing methods assume the source of heterogeneity is
known and thus pre-specify the subpopulations or only allow inclusion of a few
patient characteristics. We propose a precision dose-finding design to address
the setting of unknown patient heterogeneity in phase I cancer clinical trials
where eligibility is expanded, and multiple eligibility criteria could
potentially lead to different optimal doses for patient subgroups. The design
offers a two-in-one approach to dose-finding by simultaneously selecting
patient criteria that differentiate the maximum tolerated dose (MTD) and
recommending the subpopulation-specific MTD if needed, using marginal models to
sequentially incorporate patient covariates. Our simulation study compares the
proposed design to the naive approach of assuming patient homogeneity and our
design recommends multiple doses when heterogeneity exists and a single dose
when no heterogeneity exists. The proposed dose-finding design addresses the
challenges of broadening eligibility criteria in cancer trials and the desire
for a more precise dose in the context of early phase clinical trials.",http://arxiv.org/abs/2301.04578v1,"Rebecca B Silva, Bin Cheng, Richard D Carvajal, Shing M Lee"
364,Patient stratification in multi-arm trials: a two-stage procedure with Bayesian profile regression,"Precision medicine is an emerging field that takes into account individual
heterogeneity to inform better clinical practice. In clinical trials, the
evaluation of treatment effect heterogeneity is an important component, and
recently, many statistical methods have been proposed for stratifying patients
into different subgroups based on such heterogeneity. However, the majority of
existing methods developed for this purpose focus on the case with a
dichotomous treatment and are not directly applicable to multi-arm trials. In
this paper, we consider the problem of patient stratification in multi-arm
trial settings and propose a two-stage procedure within the Bayesian
nonparametric framework. Specifically, we first use Bayesian additive
regression trees (BART) to predict potential outcomes (treatment responses)
under different treatment options for each patient, and then we leverage
Bayesian profile regression to cluster patients into subgroups according to
their baseline characteristics and predicted potential outcomes. We further
embed a variable selection procedure into our proposed framework to identify
the patient characteristics that actively ""drive"" the clustering structure. We
conduct simulation studies to examine the performance of our proposed method
and demonstrate the method by applying it to a UK-based multi-arm blood
donation trial, wherein our method uncovers five clinically meaningful donor
subgroups.",http://arxiv.org/abs/2302.11647v1,"Yuejia Xu, Angela M Wood, Brian D M Tom"
365,SECRETS: Subject-Efficient Clinical Randomized Controlled Trials using Synthetic Intervention,"The randomized controlled trial (RCT) is the gold standard for estimating the
average treatment effect (ATE) of a medical intervention but requires
100s-1000s of subjects, making it expensive and difficult to implement. While a
cross-over trial can reduce sample size requirements by measuring the treatment
effect per individual, it is only applicable to chronic conditions and
interventions whose effects dissipate rapidly. Another approach is to replace
or augment data collected from an RCT with external data from prospective
studies or prior RCTs, but it is vulnerable to confounders in the external or
augmented data. We propose to simulate the cross-over trial to overcome its
practical limitations while exploiting its strengths. We propose a novel
framework, SECRETS, which, for the first time, estimates the individual
treatment effect (ITE) per patient in the RCT study without using any external
data by leveraging a state-of-the-art counterfactual estimation algorithm,
called synthetic intervention. It also uses a new hypothesis testing strategy
to determine whether the treatment has a clinically significant ATE based on
the estimated ITEs. We show that SECRETS can improve the power of an RCT while
maintaining comparable significance levels; in particular, on three real-world
clinical RCTs (Phase-3 trials), SECRETS increases power over the baseline
method by $\boldsymbol{6}$-$\boldsymbol{54\%}$ (average: 21.5%, standard
deviation: 15.8%).",http://arxiv.org/abs/2305.05078v1,"Sayeri Lala, Niraj K Jha"
366,Study Duration Prediction for Clinical Trials with Time-to-Event Endpoints Using Mixture Distributions Accounting for Heterogeneous Population,"In the era of precision medicine, more and more clinical trials are now
driven or guided by biomarkers, which are patient characteristics objectively
measured and evaluated as indicators of normal biological processes, pathogenic
processes, or pharmacologic responses to therapeutic interventions. With the
overarching objective to optimize and personalize disease management,
biomarker-guided clinical trials increase the efficiency by appropriately
utilizing prognostic or predictive biomarkers in the design. However, the
efficiency gain is often not quantitatively compared to the traditional
all-comers design, in which a faster enrollment rate is expected (e.g. due to
no restriction to biomarker positive patients) potentially leading to a shorter
duration. To accurately predict biomarker-guided trial duration, we propose a
general framework using mixture distributions accounting for heterogeneous
population. Extensive simulations are performed to evaluate the impact of
heterogeneous population and the dynamics of biomarker characteristics and
disease on the study duration. Several influential parameters including median
survival time, enrollment rate, biomarker prevalence and effect size are
identitied. Re-assessments of two publicly available trials are conducted to
empirically validate the prediction accuracy and to demonstrate the practical
utility. The R package \emph{detest} is developed to implement the proposed
method and is publicly available on CRAN.",http://arxiv.org/abs/2401.00540v1,"Hong Zhang, Jie Pu, Shibing Deng, Satrajit Roychoudhury, Haitao Chu, Douglas Robinson"
367,Chauhan Weighted Trajectory Analysis reduces sample size requirements and expedites time-to-efficacy signals in advanced cancer clinical trials,"As Kaplan-Meier (KM) analysis is limited to single unidirectional endpoints,
most advanced cancer randomized clinical trials (RCTs) are powered for either
progression free survival (PFS) or overall survival (OS). This discards
efficacy information carried by partial responses, complete responses, and
stable disease that frequently precede progressive disease and death. Chauhan
Weighted Trajectory Analysis (CWTA) is a generalization of KM that
simultaneously assesses multiple rank-ordered endpoints. We hypothesized that
CWTA could use this efficacy information to reduce sample size requirements and
expedite efficacy signals in advanced cancer trials. We performed 100-fold and
1000-fold simulations of solid tumour systemic therapy RCTs with health
statuses rank ordered from complete response (Stage 0) to death (Stage 4). At
increments of sample size and hazard ratio, we compared KM PFS and OS with CWTA
for (i) sample size requirements to achieve a power of 0.8 and (ii)
time-to-first significant efficacy signal. CWTA consistently demonstrated
greater power, and reduced sample size requirements by 18% to 35% compared to
KM PFS and 14% to 20% compared to KM OS. CWTA also expedited time-to-efficacy
signals 2- to 6-fold. CWTA, by incorporating all efficacy signals in the cancer
treatment trajectory, provides clinically relevant reduction in required sample
size and meaningfully expedites the efficacy signals of cancer treatments
compared to KM PFS and KM OS. Using CWTA rather than KM as the primary trial
outcome has the potential to meaningfully reduce the numbers of patients, trial
duration, and costs to evaluate therapies in advanced cancer.",http://arxiv.org/abs/2405.02529v4,"Utkarsh Chauhan, Daylen Mackey, John R Mackey"
368,Optimal Bayesian predictive probability for delayed response in single-arm clinical trials with binary efficacy outcome,"In oncology, phase II or multiple expansion cohort trials are crucial for
clinical development plans. This is because they aid in identifying potent
agents with sufficient activity to continue development and confirm the proof
of concept. Typically, these clinical trials are single-arm trials, with the
primary endpoint being short-term treatment efficacy. Despite the development
of several well-designed methodologies, there may be a practical impediment in
that the endpoints may be observed within a sufficient time such that adaptive
go/no-go decisions can be made in a timely manner at each interim monitoring.
Specifically, Response Evaluation Criteria in Solid Tumors guideline defines a
confirmed response and necessitates it in non-randomized trials, where the
response is the primary endpoint. However, obtaining the confirmed outcome from
all participants entered at interim monitoring may be time-consuming as
non-responders should be followed up until the disease progresses. Thus, this
study proposed an approach to accelerate the decision-making process that
incorporated the outcome without confirmation by discounting its contribution
to the decision-making framework using the generalized Bayes' theorem. Further,
the behavior of the proposed approach was evaluated through a simple simulation
study. The results demonstrated that the proposed approach made appropriate
interim go/no-go decisions.",http://arxiv.org/abs/2405.14166v1,"Takuya Yoshimoto, Satoru Shinoda, Kouji Yamamoto, Kouji Tahata"
369,MEXA-CTP: Mode Experts Cross-Attention for Clinical Trial Outcome Prediction,"Clinical trials are the gold standard for assessing the effectiveness and
safety of drugs for treating diseases. Given the vast design space of drug
molecules, elevated financial cost, and multi-year timeline of these trials,
research on clinical trial outcome prediction has gained immense traction.
Accurate predictions must leverage data of diverse modes such as drug
molecules, target diseases, and eligibility criteria to infer successes and
failures. Previous Deep Learning approaches for this task, such as HINT, often
require wet lab data from synthesized molecules and/or rely on prior knowledge
to encode interactions as part of the model architecture. To address these
limitations, we propose a light-weight attention-based model, MEXA-CTP, to
integrate readily-available multi-modal data and generate effective
representations via specialized modules dubbed ""mode experts"", while avoiding
human biases in model design. We optimize MEXA-CTP with the Cauchy loss to
capture relevant interactions across modes. Our experiments on the Trial
Outcome Prediction (TOP) benchmark demonstrate that MEXA-CTP improves upon
existing approaches by, respectively, up to 11.3% in F1 score, 12.2% in PR-AUC,
and 2.5% in ROC-AUC, compared to HINT. Ablation studies are provided to
quantify the effectiveness of each component in our proposed method.",http://arxiv.org/abs/2501.06823v1,"Yiqing Zhang, Xiaozhong Liu, Fabricio Murai"
370,Generation of Drug-Induced Cardiac Reactions towards Virtual Clinical Trials,"Clinical trials are pivotal in cardiac drug development, yet they often fail
due to inadequate efficacy and unexpected safety issues, leading to significant
financial losses. Using in-silico trials to replace a part of physical clinical
trials, e.g., leveraging advanced generative models to generate drug-influenced
electrocardiograms (ECGs), seems an effective method to reduce financial risk
and potential harm to trial participants. While existing generative models have
demonstrated progress in ECG generation, they fall short in modeling drug
reactions due to limited fidelity and inability to capture individualized drug
response patterns. In this paper, we propose a Drug-Aware Diffusion Model
(DADM), which could simulate individualized drug reactions while ensuring
fidelity. To ensure fidelity, we construct a set of ordinary differential
equations to provide external physical knowledge (EPK) of the realistic ECG
morphology. The EPK is used to adaptively constrain the morphology of the
generated ECGs through a dynamic cross-attention (DCA) mechanism. Furthermore,
we propose an extension of ControlNet to incorporate demographic and drug data,
simulating individual drug reactions. We compare DADM with the other eight
state-of-the-art ECG generative models on two real-world databases covering 8
types of drug regimens. The results demonstrate that DADM can more accurately
simulate drug-induced changes in ECGs, improving the accuracy by at least 5.79%
and recall by 8%.",http://arxiv.org/abs/2502.07297v1,"Qian Shao, Bang Du, Zepeng Li, Qiyuan Chen, Hongxia Xu, Jimeng Sun, Jian Wu, Jintai Chen"
371,BaySize: Bayesian Sample Size Planning for Phase I Dose-Finding Trials,"We propose BaySize, a sample size calculator for phase I clinical trials
using Bayesian models. BaySize applies the concept of effect size in dose
finding, assuming the MTD is defined based on an equivalence interval.
Leveraging a decision framework that involves composite hypotheses, BaySize
utilizes two prior distributions, the fitting prior (for model fitting) and
sampling prior (for data generation), to conduct sample size calculation under
desirable statistical power. Look-up tables are generated to facilitate
practical applications. To our knowledge, BaySize is the first sample size tool
that can be applied to a broad range of phase I trial designs.",http://arxiv.org/abs/2103.06421v1,"Xiaolei Lin, Jiaying Lyu, Shijie Yuan, SueJane Wang, Yuan Ji"
372,Treatments for pregestational chronic conditions during pregnancy: emulating a target trial with a treatment decision design,"As a solution to methodologic challenges inherent to estimating causal
effects of exposures in early pregnancy, we suggest emulating a target trial
using a treatment decision design, wherein time zero is centered around
clinical landmarks where treatment decisions may occur, such as the date of
preconception counseling or prenatal care initiation. These ideas are
illustrated via protocols for two target trials in large administrative
databases, antidepressant use for pre-existing depressive disorder and
antihypertensive medication use for mild-to-moderate chronic hypertension.
Careful consideration of these issues is critical to the identification of the
causal effects of early-pregnancy pharmacotherapies on pregnancy outcomes.",http://arxiv.org/abs/2305.13540v1,"Mollie E Wood, Chase D Latour, Lucia C Petito"
373,Immersive virtual reality methods in cognitive neuroscience and neuropsychology: Meeting the criteria of the National Academy of Neuropsychology and American Academy of Clinical Neuropsychology,"Clinical tools involving immersive virtual reality (VR) may bring several
advantages to cognitive neuroscience and neuropsychology. However, there are
some technical and methodological pitfalls. The American Academy of Clinical
Neuropsychology (AACN) and the National Academy of Neuropsychology (NAN) raised
8 key issues pertaining to Computerized Neuropsychological Assessment Devices.
These issues pertain to: (1) the safety and effectivity; (2) the identity of
the end-user; (3) the technical hardware and software features; (4) privacy and
data security; (5) the psychometric properties; (6) examinee issues; (7) the
use of reporting services; and (8) the reliability of the responses and
results. The VR Everyday Assessment Lab (VR-EAL) is the first immersive VR
neuropsychological battery with enhanced ecological validity for the assessment
of everyday cognitive functions by offering a pleasant testing experience
without inducing cybersickness. The VR-EAL meets the criteria of the NAN and
AACN, addresses the methodological pitfalls, and brings advantages for
neuropsychological testing. However, there are still shortcomings of the
VR-EAL, which should be addressed. Future iterations should strive to improve
the embodiment illusion in VR-EAL and the creation of an open access VR
software library should be attempted. The discussed studies demonstrate the
utility of VR methods in cognitive neuroscience and neuropsychology.",http://arxiv.org/abs/2105.11909v2,"Panagiotis Kourtesis, Sarah E MacPherson"
374,A Simple Cellular Automaton Model for Influenza A Viral Infections,"Viral kinetics have been extensively studied in the past through the use of
spatially homogeneous ordinary differential equations describing the time
evolution of the diseased state. However, spatial characteristics such as
localized populations of dead cells might adversely affect the spread of
infection, similar to the manner in which a counter-fire can stop a forest fire
from spreading. In order to investigate the influence of spatial
heterogeneities on viral spread, a simple 2-D cellular automaton (CA) model of
a viral infection has been developed. In this initial phase of the
investigation, the CA model is validated against clinical immunological data
for uncomplicated influenza A infections. Our results will be shown and
discussed.",http://arxiv.org/abs/q-bio/0402012v1,"Catherine Beauchemin, John Samuel, Jack Tuszynski"
375,Depressed serum IgM levels in SLE are restricted to defined subgroups,"Natural IgM autoantibodies have been proposed to convey protection from
autoimmune pathogenesis. Herein, we investigated the IgM responses in 396
systemic lupus erythematosus (SLE) patients, divided into subgroups based on
distinct autoantibody profiles. Depressed IgM levels were more common in SLE
than in matched population controls. Strikingly, an autoreactivity profile
defined by IgG anti-Ro/La was associated with reduced levels of specific
natural IgM anti-phosphorylcholine (PC) antigens and anti-malondialdehyde (MDA)
modified-protein, as well total IgM, while no differences were detected in SLE
patients with an autoreactivity profile defined by
anti-cardiolipin/Beta2glycoprotein-I. We also observed an association of
reduced IgM levels with the HLA-DRB1*03 allelic variant amongst SLE patients
and controls. Associations of low IgM anti-PC with cardiovascular disease were
primarily found in patients without antiphospholipid antibodies. These studies
further highlight the clinical relevance of depressed IgM. Our results suggest
that low IgM levels in SLE patients reflect immunological and genetic
differences between SLE subgroups.",http://arxiv.org/abs/1710.10872v1,"Caroline Gronwall, Uta Hardt, Johanna T Gustafsson, Kerstin Elvin, Kerstin JensenUrstad, Marika Kvarnstrom, Giorgia Grosso, Johan Ronnelid, Leonid Padyukov, Iva Gunnarsson, Gregg J Silverman, Elisabet Svenungsson"
376,Rank the triplets: A ranking-based multiple instance learning framework for detecting HPV infection in head and neck cancers using routine H&E images,"The aetiology of head and neck squamous cell carcinoma (HNSCC) involves
multiple carcinogens such as alcohol, tobacco and infection with human
papillomavirus (HPV). As the HPV infection influences the prognosis, treatment
and survival of patients with HNSCC, it is important to determine the HPV
status of these tumours. In this paper, we propose a novel triplet-ranking loss
function and a multiple instance learning pipeline for HPV status prediction.
This achieves a new state-of-the-art performance in HPV detection using only
the routine H&E stained WSIs on two HNSCC cohorts. Furthermore, a comprehensive
tumour microenvironment profiling was performed, which characterised the unique
patterns between HPV+/- HNSCC from genomic, immunology and cellular
perspectives. Positive correlations of the proposed score with different
subtypes of T cells (e.g. T cells follicular helper, CD8+ T cells), and
negative correlations with macrophages and connective cells (e.g. fibroblast)
were identified, which is in line with clinical findings. Unique gene
expression profiles were also identified with respect to HPV infection status,
and is in line with existing findings.",http://arxiv.org/abs/2206.08275v1,"Ruoyu Wang, Syed Ali Khurram, Amina Asif, Lawrence Young, Nasir Rajpoot"
377,Item response models for the longitudinal analysis of health-related quality of life in cancer clinical trials,"Statistical research regarding health-related quality of life (HRQoL) is a
major challenge to better evaluate the impact of the treatments on their
everyday life and to improve patients' care. Among the models that are used for
the longitudinal analysis of HRQoL, we focused on the mixed models from the
item response theory to analyze directly the raw data from questionnaires.
Using a recent classification of generalized linear models for categorical
data, we discussed about a conceptual selection of these models for the
longitudinal analysis of HRQoL in cancer clinical trials. Through
methodological and practical arguments, the adjacent and cumulative models seem
particularly suitable for this {context}. Specially in cancer clinical trials
and for the comparison between two groups, the cumulative models has the
advantage of providing intuitive illustrations of results. To complete the
comparison studies already performed in literature, a simulation study based on
random part of the mixed models is then carried out to compare the linear mixed
model classically used to the discussed item response models. As expected, the
sensitivity of item response models to detect random effect with lower variance
is better than the linear mixed model sensitivity. Finally, a longitudinal
analysis of HRQoL data from cancer clinical trial is carried out using an item
response cumulative model.",http://arxiv.org/abs/1611.06851v1,"Antoine Barbieri, Jean Peyhardi, Thierry Conroy, Sophie Gourgou, Christian Lavergne, Caroline Mollevi"
378,Group sequential designs for negative binomial outcomes,"Count data and recurrent events in clinical trials, such as the number of
lesions in magnetic resonance imaging in multiple sclerosis, the number of
relapses in multiple sclerosis, the number of hospitalizations in heart
failure, and the number of exacerbations in asthma or in chronic obstructive
pulmonary disease (COPD) are often modeled by negative binomial distributions.
In this manuscript we study planning and analyzing clinical trials with group
sequential designs for negative binomial outcomes. We propose a group
sequential testing procedure for negative binomial outcomes based on Wald
statistics using maximum likelihood estimators. The asymptotic distribution of
the proposed group sequential tests statistics are derived. The finite sample
size properties of the proposed group sequential test for negative binomial
outcomes and the methods for planning the respective clinical trials are
assessed in a simulation study. The simulation scenarios are motivated by
clinical trials in chronic heart failure and relapsing multiple sclerosis,
which cover a wide range of practically relevant settings. Our research assures
that the asymptotic normal theory of group sequential designs can be applied to
negative binomial outcomes when the hypotheses are tested using Wald statistics
and maximum likelihood estimators. We also propose two methods, one based on
Student's t-distribution and one based on resampling, to improve type I error
rate control in small samples. The statistical methods studied in this
manuscript are implemented in the R package \textit{gscounts}, which is
available for download on the Comprehensive R Archive Network (CRAN).",http://arxiv.org/abs/1707.04612v2,"Tobias Mtze, Ekkehard Glimm, Heinz Schmidli, Tim Friede"
379,Using clinical trial registries to inform Copas selection model for publication bias in meta-analysis,"Prospective registration of study protocols in clinical trial registries is a
useful way to minimize the risk of publication bias in meta-analysis, and
several clinical trial registries are available nowadays. However, they are
mainly used as a tool for searching studies and information submitted to the
registries has not been utilized as efficiently as it could. In addressing
publication bias in meta-analyses, sensitivity analysis with the Copas
selection model is a more objective alternative to widely-used graphical
methods such as the funnel-plot and the trim-and-fill method. Despite its
ability to quantify the potential impact of publication bias, a drawback of the
model is that some parameters not to be specified. This may result in some
difficulty in interpreting the results of the sensitivity analysis. In this
paper, we propose an alternative inference procedure for the Copas selection
model by utilizing information from clinical trial registries. Our method
provides a simple and accurate way to estimate all unknown parameters in the
Copas selection model. A simulation study revealed that our proposed method
resulted in smaller biases and more accurate confidence intervals than existing
methods. Furthermore, two published meta-analyses had been re-analysed to
demonstrate how to implement the proposed method in practice.",http://arxiv.org/abs/2005.14396v1,"Ao Huang, Sho Komukai, Tim Friede, Satoshi Hattori"
380,Assessing the Impact of COVID-19 on the Objective and Analysis of Oncology Clinical Trials -- Application of the Estimand Framework,"COVID-19 outbreak has rapidly evolved into a global pandemic. The impact of
COVID-19 on patient journeys in oncology represents a new risk to
interpretation of trial results and its broad applicability for future clinical
practice. We identify key intercurrent events that may occur due to COVID-19 in
oncology clinical trials with a focus on time-to-event endpoints and discuss
considerations pertaining to the other estimand attributes introduced in the
ICH E9 addendum. We propose strategies to handle COVID-19 related intercurrent
events, depending on their relationship with malignancy and treatment and the
interpretability of data after them. We argue that the clinical trial objective
from a world without COVID-19 pandemic remains valid. The estimand framework
provides a common language to discuss the impact of COVID-19 in a structured
and transparent manner. This demonstrates that the applicability of the
framework may even go beyond what it was initially intended for.",http://arxiv.org/abs/2006.04480v2,"Evgeny Degtyarev, Kaspar Rufibach, Yue Shentu, Godwin Yung, Michelle Casey, Stefan Englert, Feng Liu, Yi Liu, Oliver Sailer, Jonathan Siegel, Steven Sun, Rui Tang, Jiangxiu Zhou"
381,Drug and Disease Interpretation Learning with Biomedical Entity Representation Transformer,"Concept normalization in free-form texts is a crucial step in every
text-mining pipeline. Neural architectures based on Bidirectional Encoder
Representations from Transformers (BERT) have achieved state-of-the-art results
in the biomedical domain. In the context of drug discovery and development,
clinical trials are necessary to establish the efficacy and safety of drugs. We
investigate the effectiveness of transferring concept normalization from the
general biomedical domain to the clinical trials domain in a zero-shot setting
with an absence of labeled data. We propose a simple and effective two-stage
neural approach based on fine-tuned BERT architectures. In the first stage, we
train a metric learning model that optimizes relative similarity of mentions
and concepts via triplet loss. The model is trained on available labeled
corpora of scientific abstracts to obtain vector embeddings of concept names
and entity mentions from texts. In the second stage, we find the closest
concept name representation in an embedding space to a given clinical mention.
We evaluated several models, including state-of-the-art architectures, on a
dataset of abstracts and a real-world dataset of trial records with
interventions and conditions mapped to drug and disease terminologies.
Extensive experiments validate the effectiveness of our approach in knowledge
transfer from the scientific literature to clinical trials.",http://arxiv.org/abs/2101.09311v1,"Zulfat Miftahutdinov, Artur Kadurin, Roman Kudrin, Elena Tutubalina"
382,Long-term effect estimation when combining clinical trial and observational follow-up datasets,"Combining experimental and observational follow-up datasets has received a
lot of attention lately. In a time-to-event setting, recent work has used
medicare claims to extend the follow-up period for participants in a prostate
cancer clinical trial. This allows the estimation of the long-term effect that
cannot be estimated by clinical trial data alone. In this paper, we study the
estimation of long-term effect when participants in a clinical trial are linked
to an observational follow-up dataset with incomplete data. Such data linkages
are often incomplete for various reasons. We formulate incomplete linkages as a
missing data problem with careful considerations of the relationship between
the linkage status and the missing data mechanism. We use the popular Cox
proportional hazard model as a working model to define the long-term effect. We
propose a conditional linking at random (CLAR) assumption and an inverse
probability of linkage weighting (IPLW) partial likelihood estimator. We show
that our IPLW partial likelihood estimator is consistent and asymptotically
normal. We further extend our approach to incorporate time-dependent
covariates. Simulations results confirm the validity of our method, and we
further apply our methods to the SWOG study.",http://arxiv.org/abs/2204.04309v1,"Gang Cheng, YenChi Chen, Joseph M Unger, Cathee Till, YingQi Zhao"
383,Using Targeted Maximum Likelihood Estimation to Estimate Treatment Effect with Longitudinal Continuous or Binary Data: A Systematic Evaluation of 28 Diabetes Clinical Trials,"The primary analysis of clinical trials in diabetes therapeutic area often
involves a mixed-model repeated measure (MMRM) approach to estimate the average
treatment effect for longitudinal continuous outcome, and a generalized linear
mixed model (GLMM) approach for longitudinal binary outcome. In this paper, we
considered another estimator of the average treatment effect, called targeted
maximum likelihood estimator (TMLE). This estimator can be a one-step
alternative to model either continuous or binary outcome. We compared those
estimators by simulation studies and by analyzing real data from 28 diabetes
clinical trials. The simulations involved different missing data scenarios, and
the real data sets covered a wide range of possible distributions of the
outcome and covariates in real-life clinical trials for diabetes drugs with
different mechanisms of action. For all the settings, adjusted estimators
tended to be more efficient than the unadjusted one. In the setting of
longitudinal continuous outcome, the MMRM approach with visits and baseline
variables interaction appeared to dominate the performance of the MMRM
considering the main effects only for the baseline variables while showing
better or comparable efficiency to the TMLE estimator in both simulations and
data applications. For modeling longitudinal binary outcome, TMLE generally
outperformed GLMM in terms of relative efficiency, and its avoidance of the
cumbersome covariance fitting procedure from GLMM makes TMLE a more
advantageous estimator.",http://arxiv.org/abs/2208.01168v1,"Lingjing Jiang, Michael Rosenblum, Yu Du"
384,Covariate-adjusted Group Sequential Comparisons of Survival Probabilities,"In confirmatory clinical trials, survival outcomes are frequently studied and
interim analyses for efficacy and/or futility are often desirable. Methods such
as the log rank test and Cox regression model are commonly used to compare
treatments in this setting. They rely on a proportional hazards (PH) assumption
and are subject to type I error rate inflation and loss of power when PH are
violated. Such violations may be expected a priori, particularly when the
mechanisms of treatments differ such as immunotherapy vs. chemotherapy for
treating cancer. We develop group sequential tests for comparing survival
curves with covariate adjustment that allow for interim analyses in the
presence of non-PH and offer easily interpreted, clinically meaningful summary
measures of the treatment effect. The joint distribution of repeatedly computed
test statistics converges to the canonical joint distribution with a Markov
structure. The asymptotic distribution of the test statistics allows marginal
comparisons of survival probabilities at multiple fixed time points and
facilitates both critical value specification to maintain type I error control
and sample size/power determination. Simulations demonstrate that the achieved
type I error rate and power of the proposed tests meet targeted levels and are
robust to the PH assumption and covariate influence. The proposed tests are
illustrated using a clinical trial dataset from the Blood and Marrow Transplant
Clinical Trials Network 1101 trial.",http://arxiv.org/abs/2403.17117v1,"Peter Zhang, Brent Logan, Michael Martens"
385,AlpaPICO: Extraction of PICO Frames from Clinical Trial Documents Using LLMs,"In recent years, there has been a surge in the publication of clinical trial
reports, making it challenging to conduct systematic reviews. Automatically
extracting Population, Intervention, Comparator, and Outcome (PICO) from
clinical trial studies can alleviate the traditionally time-consuming process
of manually scrutinizing systematic reviews. Existing approaches of PICO frame
extraction involves supervised approach that relies on the existence of
manually annotated data points in the form of BIO label tagging. Recent
approaches, such as In-Context Learning (ICL), which has been shown to be
effective for a number of downstream NLP tasks, require the use of labeled
examples. In this work, we adopt ICL strategy by employing the pretrained
knowledge of Large Language Models (LLMs), gathered during the pretraining
phase of an LLM, to automatically extract the PICO-related terminologies from
clinical trial documents in unsupervised set up to bypass the availability of
large number of annotated data instances. Additionally, to showcase the highest
effectiveness of LLM in oracle scenario where large number of annotated samples
are available, we adopt the instruction tuning strategy by employing Low Rank
Adaptation (LORA) to conduct the training of gigantic model in low resource
environment for the PICO frame extraction task. Our empirical results show that
our proposed ICL-based framework produces comparable results on all the version
of EBM-NLP datasets and the proposed instruction tuned version of our framework
produces state-of-the-art results on all the different EBM-NLP datasets. Our
project is available at \url{https://github.com/shrimonmuke0202/AlpaPICO.git}.",http://arxiv.org/abs/2409.09704v1,"Madhusudan Ghosh, Shrimon Mukherjee, Asmit Ganguly, Partha Basuchowdhuri, Sudip Kumar Naskar, Debasis Ganguly"
386,Fragility Index for Time-to-Event Endpoints in Single-Arm Clinical Trials,"The reliability of clinical trial outcomes is crucial, especially in guiding
medical decisions. In this paper, we introduce the Fragility Index (FI) for
time-to-event endpoints in single-arm clinical trials - a novel metric designed
to quantify the robustness of study conclusions. The FI represents the smallest
number of censored observations that, when reclassified as uncensored events,
causes the posterior probability of the median survival time exceeding a
specified threshold to fall below a predefined confidence level. While drug
effectiveness is typically assessed by determining whether the posterior
probability exceeds a specified confidence level, the FI offers a complementary
measure, indicating how robust these conclusions are to potential shifts in the
data. Using a Bayesian approach, we develop a practical framework for computing
the FI based on the exponential survival model. To facilitate the application
of our method, we developed an R package fi, which provides a tool to compute
the Fragility Index. Through real world case studies involving time to event
data from single arms clinical trials, we demonstrate the utility of this
index. Our findings highlight how the FI can be a valuable tool for assessing
the robustness of survival analyses in single-arm studies, aiding researchers
and clinicians in making more informed decisions.",http://arxiv.org/abs/2411.16938v1,"Arnab Kumar Maity, Jhanvi Garg, Cynthia Basu"
387,Subgroup identification in individual patient data meta-analysis using model-based recursive partitioning,"Model-based recursive partitioning (MOB) can be used to identify subgroups
with differing treatment effects. The detection rate of treatment-by-covariate
interactions and the accuracy of identified subgroups using MOB depend strongly
on the sample size. Using data from multiple randomized controlled clinical
trials can overcome the problem of too small samples. However, naively pooling
data from multiple trials may result in the identification of spurious
subgroups as differences in study design, subject selection and other sources
of between-trial heterogeneity are ignored. In order to account for
between-trial heterogeneity in individual participant data (IPD) meta-analysis
random-effect models are frequently used. Commonly, heterogeneity in the
treatment effect is modelled using random effects whereas heterogeneity in the
baseline risks is modelled by either fixed effects or random effects. In this
article, we propose metaMOB, a procedure using the generalized mixed-effects
model tree (GLMM tree) algorithm for subgroup identification in IPD
meta-analysis. Although the application of metaMOB is potentially wider, e.g.
randomized experiments with participants in social sciences or preclinical
experiments in life sciences, we focus on randomized controlled clinical
trials. In a simulation study, metaMOB outperformed GLMM trees assuming a
random intercept only and model-based recursive partitioning (MOB), whose
algorithm is the basis for GLMM trees, with respect to the false discovery
rates, accuracy of identified subgroups and accuracy of estimated treatment
effect. The most robust and therefore most promising method is metaMOB with
fixed effects for modelling the between-trial heterogeneity in the baseline
risks.",http://arxiv.org/abs/2009.10518v1,"Cynthia Huber, Norbert Benda, Tim Friede"
388,Oncology Dose Finding Using Approximate Bayesian Computation Design,"In the development of new cancer treatment, an essential step is to determine
the maximum tolerated dose (MTD) via phase I clinical trials. Generally
speaking, phase I trial designs can be classified as either model-based or
algorithm-based approaches. Model-based phase I designs are typically more
efficient by using all observed data, while there is a potential risk of model
misspecification that may lead to unreliable dose assignment and incorrect MTD
identification. In contrast, most of the algorithm-based designs are less
efficient in using cumulative information, because they tend to focus on the
observed data in the neighborhood of the current dose level for dose movement.
To use the data more efficiently yet without any model assumption, we propose a
novel approximate Bayesian computation (ABC) approach for phase I trial design.
Not only is the ABC design free of any dose--toxicity curve assumption, but it
can also aggregate all the available information accrued in the trial for dose
assignment. Extensive simulation studies demonstrate its robustness and
efficiency compared with other phase I designs. We apply the ABC design to the
MEK inhibitor selumetinib trial to demonstrate its satisfactory performance.
The proposed design can be a useful addition to the family of phase I clinical
trial designs due to its simplicity, efficiency and robustness.",http://arxiv.org/abs/2203.00173v1,"Huaqing Jin, Wenbin Du, Guosheng Yin"
389,Vaccine efficacy for binary post-infection outcomes under misclassification without monotonicity,"In order to meet regulatory approval, pharmaceutical companies often must
demonstrate that new vaccines reduce the total risk of a post-infection outcome
like transmission, symptomatic disease, severe illness, or death in randomized,
placebo-controlled trials. Given that infection is a necessary precondition for
a post-infection outcome, one can use principal stratification to partition the
total causal effect of vaccination into two causal effects: vaccine efficacy
against infection, and the principal effect of vaccine efficacy against a
post-infection outcome in the patients that would be infected under both
placebo and vaccination. Despite the importance of such principal effects to
policymakers, these estimands are generally unidentifiable, even under strong
assumptions that are rarely satisfied in real-world trials. We develop a novel
method to nonparametrically point identify these principal effects while
eliminating the monotonicity assumption and allowing for measurement error.
Furthermore, our results allow for multiple treatments, and are general enough
to be applicable outside of vaccine efficacy. Our method relies on the fact
that many vaccine trials are run at geographically disparate health centers,
and measure biologically-relevant categorical pretreatment covariates. We show
that our method can be applied to a variety of clinical trial settings where
vaccine efficacy against infection and a post-infection outcome can be jointly
inferred. This can yield new insights from existing vaccine efficacy trial data
and will aid researchers in designing new multi-arm clinical trials.",http://arxiv.org/abs/2211.16502v5,"Rob Trangucci, Yang Chen, Jon Zelner"
390,Information-adaptive clinical trials: a selective recruitment design,"We propose a novel adaptive design for clinical trials with time-to-event
outcomes and covariates (which may consist of or include biomarkers). Our
method is based on the expected entropy of the posterior distribution of a
proportional hazards model. The expected entropy is evaluated as a function of
a patient's covariates, and the information gained due to a patient is defined
as the decrease in the corresponding entropy. Candidate patients are only
recruited onto the trial if they are likely to provide sufficient information.
Patients with covariates that are deemed uninformative are filtered out. A
special case is where all patients are recruited, and we determine the optimal
treatment arm allocation. This adaptive design has the advantage of potentially
elucidating the relationship between covariates, treatments, and survival
probabilities using fewer patients, albeit at the cost of rejecting some
candidates. We assess the performance of our adaptive design using data from
the German Breast Cancer Study group and numerical simulations of a biomarker
validation trial.",http://arxiv.org/abs/1502.03813v3,James E Barrett
391,Combining Survival Trials Using Aggregate Data Based on Misspecified Models,"The treatment effects of the same therapy observed from multiple clinical
trials can often be very different. Yet the patient characteristics accounting
for these differences may not be identifiable in real world practice. There
needs to be an unbiased way to combine the results from multiple trials and
report the overall treatment effect for the general population during the
development and validation of a new therapy. The non-linear structure of the
maximum partial likelihood estimates for the (log) hazard ratio defined with a
Cox proportional hazard model leads to challenges in the statistical analyses
for combining such clinical trials. In this paper, we formulated the expected
overall treatment effects using various modeling assumptions. Thus we are
proposing efficient estimates and a version of Wald test for the combined
hazard ratio using only aggregate data. Interpretation of the methods are
provided in the framework of robust data analyses involving misspecified
models.",http://arxiv.org/abs/1503.05852v1,"Tinghui Yu, Yabing Mai, Sherry Liu, Xiaofei Hu"
392,A Stopped Negative Binomial Distribution,"This paper introduces a new discrete distribution suggested by curtailed
sampling rules common in early-stage clinical trials. We derive the
distribution of the smallest number of independent Bernoulli(p) trials needed
in order to observe either s successes or t failures. The closed form
expression for the distribution as well as the compound distribution are
derived. Properties of the distribution are shown and discussed. A case study
is presented showing how the distribution can be used to monitor sequential
enrollment of clinical trials with binary outcomes as well as providing
post-hoc analysis of completed trials.",http://arxiv.org/abs/1508.01264v8,"Michelle DeVeaux, Michael J Kane, Daniel Zelterman"
393,Information-adaptive clinical trials with selective recruitment and binary outcomes,"Selective recruitment designs preferentially recruit individuals that are
estimated to be statistically informative onto a clinical trial. Individuals
that are expected to contribute less information have a lower probability of
recruitment. Furthermore, in an information-adaptive design recruits are
allocated to treatment arms in a manner that maximises information gain. The
informativeness of an individual depends on their covariate (or biomarker)
values and how information is defined is a critical element of
information-adaptive designs. In this paper we define and evaluate four
different methods for quantifying statistical information. Using both
experimental data and numerical simulations we show that selective recruitment
designs can offer a substantial increase in statistical power compared to
randomised designs. In trials without selective recruitment we find that
allocating individuals to treatment arms according to information-adaptive
protocols also leads to an increase in statistical power. Consequently,
selective recruitment designs can potentially achieve successful trials using
fewer recruits thereby offering economic and ethical advantages.",http://arxiv.org/abs/1509.01058v3,James E Barrett
394,AAA: Triple-adaptive Bayesian designs for the identification of optimal dose combinations in dual-agent dose-finding trials,"We propose a flexible design for the identification of optimal dose
combinations in dual-agent dose-finding clinical trials. The design is called
AAA, standing for three adaptations: adaptive model selection, adaptive dose
insertion, and adaptive cohort divi- sion. The adaptations highlight the need
and opportunity for innovation for dual-agent dose finding, and are supported
by the numerical results presented in the proposed simulation studies. To our
knowledge, this is the first design that allows for all three adaptations at
the same time. We find that AAA improves the statistical inference, enhances
the chance of finding the optimal dose combinations, and shortens the trial
duration. A clinical trial is being planned to apply the AAA design.",http://arxiv.org/abs/1706.03278v1,"Jiaying Lyu, Yuan Ji, Naiqing Zhao, Daniel V T Catenacci"
395,Power Priors Based on Multiple Historical Studies for Binary Outcomes,"Incorporating historical information into the design and analysis of a new
clinical trial has been the subject of much recent discussion. For example, in
the context of clinical trials of antibiotics for drug resistant infections,
where patients with specific infections can be difficult to recruit, there is
often only limited and heterogeneous information available from the historical
trials. To make the best use of the combined information at hand, we consider
an approach based on the multiple power prior which allows the prior weight of
each historical study to be chosen adaptively by empirical Bayes. This choice
of weight has advantages in that it varies commensurably with differences in
the historical and current data and can choose weights near 1 if the data from
the corresponding historical study are similar enough to the data from the
current study. Fully Bayesian approaches are also considered. The methods are
applied to data from antibiotics trials. An analysis of the operating
characteristics in a binomial setting shows that the proposed empirical Bayes
adaptive method works well, compared to several alternative approaches,
including the meta-analytic prior.",http://arxiv.org/abs/1708.08239v3,"Isaac Gravestock, Leonhard Held"
396,Interim recruitment prediction for multi-centre clinical trials,"We introduce a general framework for monitoring, modelling, and predicting
the recruitment to multi-centre clinical trials. The work is motivated by
overly optimistic and narrow prediction intervals produced by existing
time-homogeneous recruitment models for multi-centre recruitment. We first
present two tests for detection of decay in recruitment rates, together with a
power study. We then introduce a model based on the inhomogeneous Poisson
process with monotonically decaying intensity, motivated by recruitment trends
observed in oncology trials. The general form of the model permits adaptation
to any parametric curve-shape. A general method for constructing sensible
parameter priors is provided and Bayesian model averaging is used for making
predictions which account for the uncertainty in both the parameters and the
model. The validity of the method and its robustness to misspecification are
tested using simulated datasets. The new methodology is then applied to
oncology trial data, where we make interim accrual predictions, comparing them
to those obtained by existing methods, and indicate where unexpected changes in
the accrual pattern occur.",http://arxiv.org/abs/1910.07965v4,"Szymon Urbas, Chris Sherlock, Paul Metcalfe"
397,Statistical Issues and Recommendations for Clinical Trials Conducted During the COVID-19 Pandemic,"The COVID-19 pandemic has had and continues to have major impacts on planned
and ongoing clinical trials. Its effects on trial data create multiple
potential statistical issues. The scale of impact is unprecedented, but when
viewed individually, many of the issues are well defined and feasible to
address. A number of strategies and recommendations are put forward to assess
and address issues related to estimands, missing data, validity and
modifications of statistical analysis methods, need for additional analyses,
ability to meet objectives and overall trial interpretability.",http://arxiv.org/abs/2005.10248v1,"R Daniel Meyer, Bohdana Ratitch, Marcel Wolbers, Olga Marchenko, Hui Quan, Daniel Li, Chrissie Fletcher, Xin Li, David Wright, Yue Shentu, Stefan Englert, Wei Shen, Jyotirmoy Dey, Thomas Liu, Ming Zhou, Norman Bohidar, PengLiang Zhao, Michael Hale"
398,Survival analysis under non-proportional hazards: investigating non-inferiority or equivalence in time-to-event data,"The classical approach to analyze time-to-event data, e.g. in clinical
trials, is to fit Kaplan-Meier curves yielding the treatment effect as the
hazard ratio between treatment groups. Afterwards commonly a log-rank test is
performed in order to investigate whether there is a difference in survival,
or, depending on additional covariates, a Cox proportional hazard model is
used. However, in numerous trials these approaches fail due to the presence of
non-proportional hazards, resulting in difficulties of interpreting the hazard
ratio and a loss of power. When considering equivalence or non-inferiority
trials, the commonly performed log-rank based tests are similarly affected by a
violation of this assumption. Here we propose a parametric framework to assess
equivalence or non-inferiority for survival data. We derive pointwise
confidence bands for both, the hazard ratio and the difference of the survival
curves. Further we propose a test procedure addressing non-inferiority and
equivalence by directly comparing the survival functions at certain time points
or over an entire range of time. We demonstrate the validity of the methods by
a clinical trial example and by numerous simulation results.",http://arxiv.org/abs/2009.06699v1,"Kathrin Mllenhoff, Achim Tresch"
399,Application of Multi-Armed Bandits to Model-assisted designs for Dose-Finding Clinical Trials,"We consider applying multi-armed bandits to model-assisted designs for
dose-finding clinical trials. Multi-armed bandits are very simple and powerful
methods to determine actions to maximize a reward in a limited number of
trials. Among the multi-armed bandits, we first consider the use of Thompson
sampling which determines actions based on random samples from a posterior
distribution. In the small sample size, as shown in dose-finding trials,
because the tails of posterior distribution are heavier and random samples are
too much variability, we also consider an application of regularized Thompson
sampling and greedy algorithm. The greedy algorithm determines a dose based on
a posterior mean. In addition, we also propose a method to determine a dose
based on a posterior median. We evaluate the performance of our proposed
designs for six scenarios via simulation studies.",http://arxiv.org/abs/2201.05268v1,Masahiro Kojima
400,Optimizing Interim Analysis Timing for Bayesian Adaptive Commensurate Designs,"In developing products for rare diseases, statistical challenges arise due to
the limited number of patients available for participation in drug trials and
other clinical research. Bayesian adaptive clinical trial designs offer the
possibility of increased statistical efficiency, reduced development cost and
ethical hazard prevention via their incorporation of evidence from external
sources (historical data, expert opinions, and real-world evidence), and
flexibility in the specification of interim looks. In this paper, we propose a
novel Bayesian adaptive commensurate design that borrows adaptively from
historical information and also uses a particular payoff function to optimize
the timing of the study's interim analysis. The trial payoff is a function of
how many samples can be saved via early stopping and the probability of making
correct early decisions for either futility or efficacy. We calibrate our
Bayesian algorithm to have acceptable long-run frequentist properties (Type I
error and power) via simulation at the design stage. We illustrate our approach
using a pediatric trial design setting testing the effect of a new drug for a
rare genetic disease. The optimIA R package available at
https://github.com/wxwx1993/Bayesian_IA_Timing provides an easy-to-use
implementation of our approach.",http://arxiv.org/abs/1905.07456v2,"Xiao Wu, Yi Xu, Bradley P Carlin"
401,Finding and assessing treatment effect sweet spots in clinical trial data,"Identifying heterogeneous treatment effects (HTEs) in randomized controlled
trials is an important step toward understanding and acting on trial results.
However, HTEs are often small and difficult to identify, and HTE modeling
methods which are very general can suffer from low power. We present a method
that exploits any existing relationship between illness severity and treatment
effect, and identifies the ""sweet spot"", the contiguous range of illness
severity where the estimated treatment benefit is maximized. We further compute
a bias-corrected estimate of the conditional average treatment effect (CATE) in
the sweet spot, and a $p$-value. Because we identify a single sweet spot and
$p$-value, we believe our method to be straightforward to interpret and
actionable: results from our method can inform future clinical trials and help
clinicians make personalized treatment recommendations.",http://arxiv.org/abs/2011.10157v2,"Erin Craig, Donald A Redelmeier, Robert J Tibshirani"
402,Bayesian prognostic covariate adjustment,"Historical data about disease outcomes can be integrated into the analysis of
clinical trials in many ways. We build on existing literature that uses
prognostic scores from a predictive model to increase the efficiency of
treatment effect estimates via covariate adjustment. Here we go further,
utilizing a Bayesian framework that combines prognostic covariate adjustment
with an empirical prior distribution learned from the predictive performances
of the prognostic model on past trials. The Bayesian approach interpolates
between prognostic covariate adjustment with strict type I error control when
the prior is diffuse, and a single-arm trial when the prior is sharply peaked.
This method is shown theoretically to offer a substantial increase in
statistical power, while limiting the type I error rate under reasonable
conditions. We demonstrate the utility of our method in simulations and with an
analysis of a past Alzheimer's disease clinical trial.",http://arxiv.org/abs/2012.13112v1,"David Walsh, Alejandro Schuler, Diana Hall, Jon Walsh, Charles Fisher"
403,Lessons Learned from the Bayesian Design and Analysis for the BNT162b2 COVID-19 Vaccine Phase 3 Trial,"The phase III BNT162b2 mRNA COVID-19 vaccine trial is based on a Bayesian
design and analysis, and the main evidence of vaccine efficacy is presented in
Bayesian statistics. Confusion and mistakes are produced in the presentation of
the Bayesian results. Some key statistics, such as Bayesian credible intervals,
are mislabeled and stated as confidence intervals. Posterior probabilities of
the vaccine efficacy are not reported as the main results. We illustrate the
main differences in the reporting of Bayesian analysis results for a clinical
trial and provide four recommendations. We argue that statistical evidence from
a Bayesian trial, when presented properly, is easier to interpret and directly
addresses the main clinical questions, thereby better supporting regulatory
decision making. We also recommend using abbreviation ""BI"" to represent
Bayesian credible intervals as a differentiation to ""CI"" which stands for
confidence interval.",http://arxiv.org/abs/2103.05499v1,"Yuan Ji, Shijie Yuan"
404,Priors from Envisioned Posterior Judgments: A Novel Elicitation Approach With Application to Bayesian Clinical Trials,"The uptake of formalized prior elicitation from experts in Bayesian clinical
trials has been limited, largely due to the challenges associated with complex
statistical modeling, the lack of practical tools, and the cognitive burden on
experts required to quantify their uncertainty using probabilistic language.
Additionally, existing methods do not address prior-posterior coherence, i.e.,
does the posterior distribution, obtained mathematically from combining the
estimated prior with the trial data, reflect the expert's actual posterior
beliefs? We propose a new elicitation approach that seeks to ensure
prior-posterior coherence and reduce the expert's cognitive burden. This is
achieved by eliciting responses about the expert's envisioned posterior
judgments under various potential data outcomes and inferring the prior
distribution by minimizing the discrepancies between these responses and the
expected responses obtained from the posterior distribution. The feasibility
and potential value of the new approach are illustrated through an application
to a real trial currently underway.",http://arxiv.org/abs/2409.05271v2,"Yongdong Ouyang, Janice J Eng, Denghuang Zhan, Hubert Wong"
405,Assessing the Role of Volumetric Brain Information in Multiple Sclerosis Progression,"Multiple sclerosis is a chronic autoimmune disease that affects the central
nervous system. Understanding multiple sclerosis progression and identifying
the implicated brain structures is crucial for personalized treatment
decisions. Deformation-based morphometry utilizes anatomical magnetic resonance
imaging to quantitatively assess volumetric brain changes at the voxel level,
providing insight into how each brain region contributes to clinical
progression with regards to neurodegeneration. Utilizing such voxel-level data
from a relapsing multiple sclerosis clinical trial, we extend a model-agnostic
feature importance metric to identify a robust and predictive feature set that
corresponds to clinical progression. These features correspond to brain regions
that are clinically meaningful in MS disease research, demonstrating their
scientific relevance. When used to predict progression using classical survival
models and 3D convolutional neural networks, the identified regions led to the
best-performing models, demonstrating their prognostic strength. We also find
that these features generalize well to other definitions of clinical
progression and can compensate for the omission of highly prognostic clinical
features, underscoring the predictive power and clinical relevance of
deformation-based morphometry as a regional identification tool.",http://arxiv.org/abs/2412.09497v1,"Andy A Shen, Aidan McLoughlin, Zoe Vernon, Jonathan Lin, Richard A D Carano, Peter J Bickel, Zhuang Song, Haiyan Huang"
406,Handling Covariates in the Design of Clinical Trials,"There has been a split in the statistics community about the need for taking
covariates into account in the design phase of a clinical trial. There are many
advocates of using stratification and covariate-adaptive randomization to
promote balance on certain known covariates. However, balance does not always
promote efficiency or ensure more patients are assigned to the better
treatment. We describe these procedures, including model-based procedures, for
incorporating covariates into the design of clinical trials, and give examples
where balance, efficiency and ethical considerations may be in conflict. We
advocate a new class of procedures, covariate-adjusted response-adaptive (CARA)
randomization procedures that attempt to optimize both efficiency and ethical
considerations, while maintaining randomization. We review all these
procedures, present a few new simulation studies, and conclude with our
philosophy.",http://arxiv.org/abs/1102.3773v1,"William F Rosenberger, Oleksandr Sverdlov"
407,An Empirical Comparison of Parametric and Permutation Tests for Regression Analysis of Randomized Experiments,"Hypothesis tests based on linear models are widely accepted by organizations
that regulate clinical trials. These tests are derived using strong assumptions
about the data-generating process so that the resulting inference can be based
on parametric distributions. Because these methods are well understood and
robust, they are sometimes applied to data that depart from assumptions, such
as ordinal integer scores. Permutation tests are a nonparametric alternative
that require minimal assumptions which are often guaranteed by the
randomization that was conducted. We compare analysis of covariance (ANCOVA), a
special case of linear regression that incorporates stratification, to several
permutation tests based on linear models that control for pretreatment
covariates. In simulations of randomized experiments using models which violate
some of the parametric regression assumptions, the permutation tests maintain
power comparable to ANCOVA. We illustrate the use of these permutation tests
alongside ANCOVA using data from a clinical trial comparing the effectiveness
of two treatments for gastroesophageal reflux disease. Given the considerable
costs and scientific importance of clinical trials, an additional nonparametric
method, such as a linear model permutation test, may serve as a robustness
check on the statistical inference for the main study endpoints.",http://arxiv.org/abs/1702.04851v2,"Kellie Ottoboni, Fraser Lewis, Luigi Salmaso"
408,Using Recursive Partitioning to Find and Estimate Heterogenous Treatment Effects In Randomized Clinical Trials,"Heterogeneous treatment effects can be very important in the analysis of
randomized clinical trials. Heightened risks or enhanced benefits may exist for
particular subsets of study subjects. When the heterogeneous treatment effects
are specified as the research is being designed, there are proper and readily
available analysis techniques. When the heterogeneous treatment effects are
inductively obtained as an experiment's data are analyzed, significant
complications are introduced. There can be a need for special loss functions
designed to find local average treatment effects and for techniques that
properly address post selection statistical inference. In this paper, we tackle
both while undertaking a recursive partitioning analysis of a randomized
clinical trial testing whether individuals on probation, who are low risk, can
be minimally supervised with no increase in recidivism.",http://arxiv.org/abs/1807.04164v1,"Richard Berk, Matthew Olson, Andreas Buja, Aurelie Ouss"
409,Knowledge-guided Text Structuring in Clinical Trials,"Clinical trial records are variable resources or the analysis of patients and
diseases. Information extraction from free text such as eligibility criteria
and summary of results and conclusions in clinical trials would better support
computer-based eligibility query formulation and electronic patient screening.
Previous research has focused on extracting information from eligibility
criteria, with usually a single pair of medical entity and attribute, but
seldom considering other kinds of free text with multiple entities, attributes
and relations that are more complex for parsing. In this paper, we propose a
knowledge-guided text structuring framework with an automatically generated
knowledge base as training corpus and word dependency relations as context
information to transfer free text into formal, computer-interpretable
representations. Experimental results show that our method can achieve overall
high precision and recall, demonstrating the effectiveness and efficiency of
the proposed method.",http://arxiv.org/abs/1912.12380v1,"Yingcheng Sun, Kenneth Loparo"
410,A practical Response Adaptive Block Randomization (RABR) design with analytic type I error protection,"Response adaptive randomization (RAR) is appealing from methodological,
ethical, and pragmatic perspectives in the sense that subjects are more likely
to be randomized to better performing treatment groups based on accumulating
data. However, applications of RAR in confirmatory drug clinical trials with
multiple active arms are limited largely due to its complexity, and lack of
control of randomization ratios to different treatment groups. To address the
aforementioned issues, we propose a Response Adaptive Block Randomization
(RABR) design allowing arbitrarily pre-specified randomization ratios for the
control and high-performing groups to meet clinical trial objectives. We show
the validity of the conventional unweighted test in RABR with a controlled type
I error rate based on the weighted combination test for sample size adaptive
design invoking no large sample approximation. The advantages of the proposed
RABR in terms of robustly reaching target final sample size to meet regulatory
requirements and increasing statistical power as compared with the popular
Doubly Adaptive Biased Coin Design (DBCD) are demonstrated by statistical
simulations and a practical clinical trial design example.",http://arxiv.org/abs/2004.07356v2,"Tianyu Zhan, Lu Cui, Ziqian Geng, Lanju Zhang, Yihua Gu, Ivan S F Chan"
411,Response-adaptive randomization in clinical trials: from myths to practical considerations,"Response-Adaptive Randomization (RAR) is part of a wider class of
data-dependent sampling algorithms, for which clinical trials are typically
used as a motivating application. In that context, patient allocation to
treatments is determined by randomization probabilities that change based on
the accrued response data in order to achieve experimental goals. RAR has
received abundant theoretical attention from the biostatistical literature
since the 1930's and has been the subject of numerous debates. In the last
decade, it has received renewed consideration from the applied and
methodological communities, driven by well-known practical examples and its
widespread use in machine learning. Papers on the subject present different
views on its usefulness, and these are not easy to reconcile. This work aims to
address this gap by providing a unified, broad and fresh review of
methodological and practical issues to consider when debating the use of RAR in
clinical trials.",http://arxiv.org/abs/2005.00564v4,"David S Robertson, Kim May Lee, Boryana C LopezKolkovska, Sofia S Villar"
412,A novel estimand to adjust for rescue treatment in clinical trials,"The interpretation of randomised clinical trial results is often complicated
by intercurrent events. For instance, rescue medication is sometimes given to
patients in response to worsening of their disease, either in addition to the
randomised treatment or in its place. The use of such medication complicates
the interpretation of the intention-to-treat analysis. In view of this, we
propose a novel estimand defined as the intention-to-treat effect that would
have been observed, had patients on the active arm been switched to rescue
medication if and only if they would have been switched when randomised to
control. This enables us to disentangle the treatment effect from the effect of
rescue medication on a patient's outcome, while avoiding the strong
extrapolations that are typically needed when inferring what the
intention-to-treat effect would have been in the absence of rescue medication.
We develop an inverse probability weighting method to estimate this estimand
under specific untestable assumptions, in view of which we propose a
sensitivity analysis. We use the method for the analysis of a clinical trial
conducted by Janssen Pharmaceuticals, in which chronically ill patients can
switch to rescue medication for ethical reasons. Monte Carlo simulations
confirm that the proposed estimator is unbiased in moderate sample sizes.",http://arxiv.org/abs/2009.12052v1,"Hege Michiels, Cristina Sotto, An Vandebosch, Stijn Vansteelandt"
413,Improving Adaptive Seamless Designs through Bayesian optimization,"We propose to use Bayesian optimization (BO) to improve the efficiency of the
design selection process in clinical trials. BO is a method to optimize
expensive black-box functions, by using a regression as a surrogate to guide
the search. In clinical trials, planning test procedures and sample sizes is a
crucial task. A common goal is to maximize the test power, given a set of
treatments, corresponding effect sizes, and a total number of samples. From a
wide range of possible designs we aim to select the best one in a short time to
allow quick decisions. The standard approach to simulate the power for each
single design can become too time-consuming. When the number of possible
designs becomes very large, either large computational resources are required
or an exhaustive exploration of all possible designs takes too long. Here, we
propose to use BO to quickly find a clinical trial design with high power from
a large number of candidate designs. We demonstrate the effectiveness of our
approach by optimizing the power of adaptive seamless designs for different
sets of treatment effect sizes. Comparing BO with an exhaustive evaluation of
all candidate designs shows that BO finds competitive designs in a fraction of
the time.",http://arxiv.org/abs/2105.09223v1,"Jakob Richter, Tim Friede, Jrg Rahnenfhrer"
414,Modelling and forecasting patient recruitment in clinical trials with patients' dropout,"This paper focuses on statistical modelling and prediction of patient
recruitment in clinical trials accounting for patients dropout. The recruitment
model is based on a Poisson-gamma model introduced by Anisimov and Fedorov
(2007), where the patients arrive at different centres according to Poisson
processes with rates viewed as gamma-distributed random variables. Each patient
can drop the study during some screening period. Managing the dropout process
is of a major importance but data related to dropout are rarely correctly
collected. In this paper, a few models of dropout are proposed. The technique
for estimating parameters and predicting the number of recruited patients over
time and the recruitment time is developed. Simulation results confirm the
applicability of the technique and thus, the necessity to account for patients
dropout at the stage of forecasting recruitment in clinical trials.",http://arxiv.org/abs/2202.06779v1,"Vladimir Anisimov, Guillaume Mijoule, Armando Turchetta, Nicolas Savy"
415,Missing data imputation for a multivariate outcome of mixed variable types,"Data collected in clinical trials are often composed of multiple types of
variables. For example, laboratory measurements and vital signs are
longitudinal data of continuous or categorical variables, adverse events may be
recurrent events, and death is a time-to-event variable. Missing data due to
patients' discontinuation from the study or as a result of handling
intercurrent events using a hypothetical strategy almost always occur during
any clinical trial. Imputing these data with mixed types of variables
simultaneously is a challenge that has not been studied extensively. In this
article, we propose using an approximate fully conditional specification to
impute the missing data. Simulation shows the proposed method provides
satisfactory results under the assumption of missing at random. Finally, real
data from a clinical trial evaluating treatments for diabetes are analyzed to
illustrate the potential benefit of the proposed method.",http://arxiv.org/abs/2206.01873v2,"Tuo Wang, Rachel Zilinskas, Ying Li, Yongming Qu"
416,Quantifying and Detecting Individual Level `Always Survivor' Causal Effects Under `Truncation by Death' and Censoring Through Time,"The analysis of causal effects when the outcome of interest is possibly
truncated by death has a long history in statistics and causal inference. The
survivor average causal effect is commonly identified with more assumptions
than those guaranteed by the design of a randomized clinical trial or using
sensitivity analysis. This paper demonstrates that individual level causal
effects in the `always survivor' principal stratum can be identified with no
stronger identification assumptions than randomization. We illustrate the
practical utility of our methods using data from a clinical trial on patients
with prostate cancer. Our methodology is the first and, as of yet, only
proposed procedure that enables detecting causal effects in the presence of
truncation by death using only the assumptions that are guaranteed by design of
the clinical trial. This methodology is applicable to all types of outcomes.",http://arxiv.org/abs/1905.11300v3,"Jaffer M Zaidi, Eric J Tchetgen Tchetgen, Tyler J VanderWeele"
417,A Simulation Study Evaluating Phase I Clinical Trial Designs for Combinational Agents,"Nowadays, more and more clinical trials choose combinational agents as the
intervention to achieve better therapeutic responses. However, dose-finding for
combinational agents is much more complicated than single agent as the full
order of combination dose toxicity is unknown. Therefore, regular phase I
designs are not able to identify the maximum tolerated dose (MTD) of
combinational agents. Motivated by such needs, plenty of novel phase I clinical
trial designs for combinational agents were proposed. With so many available
designs, research that compare their performances, explore parameters' impacts,
and provide recommendations is very limited. Therefore, we conducted a
simulation study to evaluate multiple phase I designs that proposed to identify
single MTD for combinational agents under various scenarios. We also explored
influences of different design parameters. In the end, we summarized the pros
and cons of each design, and provided a general guideline in design selection.",http://arxiv.org/abs/2103.07746v2,"Shu Wang, JiHyun Lee"
418,The Predictive Individual Effect for Survival Data,"The call for patient-focused drug development is loud and clear, as expressed
in the 21st Century Cures Act and in recent guidelines and initiatives of
regulatory agencies. Among the factors contributing to modernized drug
development and improved health-care activities are easily interpretable
measures of clinical benefit. In addition, special care is needed for cancer
trials with time-to-event endpoints if the treatment effect is not constant
over time. We propose the predictive individual effect which is a
patient-centric and tangible measure of clinical benefit under a wide variety
of scenarios. It can be obtained by standard predictive calculations under a
rank preservation assumption that has been used previously in trials with
treatment switching. We discuss four recent Oncology trials that cover
situations with proportional as well as non-proportional hazards (delayed
treatment effect or crossing of survival curves). It is shown that the
predictive individual effect offers valuable insights beyond p-values,
estimates of hazard ratios or differences in median survival. Compared to
standard statistical measures, the predictive individual effect is a direct,
easily interpretable measure of clinical benefit. It facilitates communication
among clinicians, patients, and other parties and should therefore be
considered in addition to standard statistical results.",http://arxiv.org/abs/2112.10404v1,"Beat Neuenschwander, Satrajit Roychoudhury, Simon Wandel, Kannan Natarajan, Emmanuel Zuber"
419,UNIMIB at TREC 2021 Clinical Trials Track,"This contribution summarizes the participation of the UNIMIB team to the TREC
2021 Clinical Trials Track. We have investigated the effect of different query
representations combined with several retrieval models on the retrieval
performance. First, we have implemented a neural re-ranking approach to study
the effectiveness of dense text representations. Additionally, we have
investigated the effectiveness of a novel decision-theoretic model for
relevance estimation. Finally, both of the above relevance models have been
compared with standard retrieval approaches. In particular, we combined a
keyword extraction method with a standard retrieval process based on the BM25
model and a decision-theoretic relevance model that exploits the
characteristics of this particular search task. The obtained results show that
the proposed keyword extraction method improves 84% of the queries over the
TREC's median NDCG@10 measure when combined with either traditional or
decision-theoretic relevance models. Moreover, regarding RPEC@10, the employed
decision-theoretic model improves 85% of the queries over the reported TREC's
median value.",http://arxiv.org/abs/2207.13514v1,"Georgios Peikos, Oscar Espitia, Gabriella Pasi"
420,Natural cubic splines for the analysis of Alzheimer's clinical trials,"Mixed model repeated measures (MMRM) is the most common analysis approach
used in clinical trials for Alzheimer's disease and other progressive diseases
measured with continuous outcomes measured over time. The model treats time as
a categorical variable, which allows an unconstrained estimate of the mean for
each study visit in each randomized group. Categorizing time in this way can be
problematic when assessments occur off-schedule, as including off-schedule
visits can induce bias, and excluding them ignores valuable information and
violates the intention to treat principle. This problem has been exacerbated by
clinical trial visits which have been delayed due to the COVID19 pandemic. As
an alternative to MMRM, we propose a constrained longitudinal data analysis
with natural cubic splines that treats time as continuous and uses test version
effects to model the mean over time. The spline model is shown to be superior,
in practice and simulation studies, to categorical-time models like MMRM and
models that assume a proportional treatment effect.",http://arxiv.org/abs/2208.08077v1,"M C Donohue, O Langford, P Insel, C H van Dyck, R C Petersen, S Craft, G Sethuraman, R Raman, P S Aisen"
421,Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports,"With the increasing number of clinical trial reports generated every day, it
is becoming hard to keep up with novel discoveries that inform evidence-based
healthcare recommendations. To help automate this process and assist medical
experts, NLP solutions are being developed. This motivated the SemEval-2023
Task 7, where the goal was to develop an NLP system for two tasks: evidence
retrieval and natural language inference from clinical trial data. In this
paper, we describe our two developed systems. The first one is a pipeline
system that models the two tasks separately, while the second one is a joint
system that learns the two tasks simultaneously with a shared representation
and a multi-task learning approach. The final system combines their outputs in
an ensemble system. We formalize the models, present their characteristics and
challenges, and provide an analysis of achieved results. Our system ranked 3rd
out of 40 participants with a final submission.",http://arxiv.org/abs/2304.13180v2,"Juraj Vladika, Florian Matthes"
422,A note on stratification errors in the analysis of clinical trials,"Stratification in both the design and analysis of randomized clinical trials
is common. Despite features in automated randomization systems to re-confirm
the stratifying variables, incorrect values of these variables may be entered.
These errors are often detected during subsequent data collection and
verification. Questions remain about whether to use the mis-reported initial
stratification or the corrected values in subsequent analyses. It is shown that
the likelihood function resulting from the design of randomized clinical trials
supports the use of the corrected values. New definitions are proposed that
characterize misclassification errors as `ignorable' and `non-ignorable'.
Ignorable errors may depend on the correct strata and any other modeled
baseline covariates, but they are otherwise unrelated to potential treatment
outcomes. Data management review suggests most misclassification errors are
arbitrarily produced by distracted investigators, so they are ignorable or at
most weakly dependent on measured and unmeasured baseline covariates. Ignorable
misclassification errors may produce a small increase in standard errors, but
other properties of the planned analyses are unchanged (e.g., unbiasedness,
confidence interval coverage). It is shown that unbiased linear estimation in
the absence of misclassification errors remains unbiased when there are
non-ignorable misclassification errors, and the corresponding confidence
intervals based on the corrected strata values are conservative.",http://arxiv.org/abs/2304.14538v2,Neal Thomas
423,Modern approaches for evaluating treatment effect heterogeneity from clinical trials and observational data,"In this paper we review recent advances in statistical methods for the
evaluation of the heterogeneity of treatment effects (HTE), including subgroup
identification and estimation of individualized treatment regimens, from
randomized clinical trials and observational studies. We identify several types
of approaches using the features introduced in Lipkovich, Dmitrienko and
D'Agostino (2017) that distinguish the recommended principled methods from
basic methods for HTE evaluation that typically rely on rules of thumb and
general guidelines (the methods are often referred to as common practices). We
discuss the advantages and disadvantages of various principled methods as well
as common measures for evaluating their performance. We use simulated data and
a case study based on a historical clinical trial to illustrate several new
approaches to HTE evaluation.",http://arxiv.org/abs/2311.14889v3,"Ilya Lipkovich, David Svensson, Bohdana Ratitch, Alex Dmitrienko"
424,Group Sequential Design Under Non-proportional Hazards,"Non-proportional hazards (NPH) are often observed in clinical trials with
time-to-event endpoints. A common example is a long-term clinical trial with a
delayed treatment effect in immunotherapy for cancer. When designing clinical
trials with time-to-event endpoints, it is crucial to consider NPH scenarios to
gain a complete understanding of design operating characteristics. In this
paper, we focus on group sequential design for three NPH methods: the average
hazard ratio, the weighted logrank test, and the MaxCombo combination test. For
each of these approaches, we provide analytic forms of design characteristics
that facilitate sample size calculation and bound derivation for group
sequential designs. Examples are provided to illustrate the proposed methods.
To facilitate statisticians in designing and comparing group sequential designs
under NPH, we have implemented the group sequential design methodology in the
gsDesign2 R package at https://cran.r-project.org/web/packages/gsDesign2/.",http://arxiv.org/abs/2312.01723v1,"Yujie Zhao, Yilong Zhang, Larry Leon, Keaven M Anderson"
425,Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials,"Large Language Models have revolutionized various fields and industries, such
as Conversational AI, Content Generation, Information Retrieval, Business
Intelligence, and Medical, to name a few. One major application in the field of
medical is to analyze and investigate clinical trials for entailment
tasks.However, It has been observed that Large Language Models are susceptible
to shortcut learning, factual inconsistency, and performance degradation with
little variation in context. Adversarial and robust testing is performed to
ensure the integrity of models output. But, ambiguity still persists. In order
to ensure the integrity of the reasoning performed and investigate the model
has correct syntactic and semantic understanding probing is used. Here, I used
mnestic probing to investigate the Sci-five model, trained on clinical trial. I
investigated the model for feature learnt with respect to natural logic. To
achieve the target, I trained task specific probes. Used these probes to
investigate the final layers of trained model. Then, fine tuned the trained
model using iterative null projection. The results shows that model accuracy
improved. During experimentation, I observed that size of the probe has affect
on the fine tuning process.",http://arxiv.org/abs/2402.02558v1,Ata Mustafa
426,IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe Biomedical Natural Language Inference for Clinical Trials,"Large Language models (LLMs) have demonstrated state-of-the-art performance
in various natural language processing (NLP) tasks across multiple domains, yet
they are prone to shortcut learning and factual inconsistencies. This research
investigates LLMs' robustness, consistency, and faithful reasoning when
performing Natural Language Inference (NLI) on breast cancer Clinical Trial
Reports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural
Language Inference for Clinical Trials. We examine the reasoning capabilities
of LLMs and their adeptness at logical problem-solving. A comparative analysis
is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro
under zero-shot settings using Retrieval-Augmented Generation (RAG) framework,
integrating various reasoning chains. The evaluation yields an F1 score of
0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test
dataset.",http://arxiv.org/abs/2404.04510v1,"Shreyasi Mandal, Ashutosh Modi"
427,An Evaluation Benchmark for Adverse Drug Event Prediction from Clinical Trial Results,"Adverse drug events (ADEs) are a major safety issue in clinical trials. Thus,
predicting ADEs is key to developing safer medications and enhancing patient
outcomes. To support this effort, we introduce CT-ADE, a dataset for multilabel
ADE prediction in monopharmacy treatments. CT-ADE encompasses 2,497 drugs and
168,984 drug-ADE pairs from clinical trial results, annotated using the MedDRA
ontology. Unlike existing resources, CT-ADE integrates treatment and target
population data, enabling comparative analyses under varying conditions, such
as dosage, administration route, and demographics. In addition, CT-ADE
systematically collects all ADEs in the study population, including positive
and negative cases. To provide a baseline for ADE prediction performance using
the CT-ADE dataset, we conducted analyses using large language models (LLMs).
The best LLM achieved an F1-score of 56%, with models incorporating treatment
and patient information outperforming by 21%-38% those relying solely on the
chemical structure. These findings underscore the importance of contextual
information in ADE prediction and establish CT-ADE as a robust resource for
safety risk assessment in pharmaceutical research and development.",http://arxiv.org/abs/2404.12827v3,"Anthony Yazdani, Alban Bornet, Philipp Khlebnikov, Boya Zhang, Hossein Rouhizadeh, Poorya Amini, Douglas Teodoro"
428,Considerations for Single-Arm Trials to Support Accelerated Approval of Oncology Drugs,"In the last two decades, single-arm trials (SATs) have been effectively used
to study anticancer therapies in well-defined patient populations using durable
response rates as an objective and interpretable clinical endpoints. With a
growing trend of regulatory accelerated approval (AA) requiring randomized
controlled trials (RCTs), some confusions have arisen about the roles of SATs
in AA. This paper is intended to elucidate conditions under which an SAT may be
considered reasonable for AA. Specifically, the paper describes (1) two
necessary conditions for designing an SAT, (2) three sufficient conditions that
help either optimize the study design or interpret the study results, (3) four
conditions that demonstrate substantial evidence of clinical benefits of the
drug, and (4) a plan of a confirmatory RCT to verify the clinical benefits.
Some further considerations are discussed to help design a scientifically sound
SAT and communicate with regulatory agencies. Conditions presented in this
paper may serve as a set of references for sponsors using SATs for regulatory
decision.",http://arxiv.org/abs/2405.12437v2,"Feinan Lu, Tao Wang, Ying Lu, Jie Chen"
429,Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials,"The integration of Large Language Models (LLMs) into the drug discovery and
development field marks a significant paradigm shift, offering novel
methodologies for understanding disease mechanisms, facilitating drug
discovery, and optimizing clinical trial processes. This review highlights the
expanding role of LLMs in revolutionizing various stages of the drug
development pipeline. We investigate how these advanced computational models
can uncover target-disease linkage, interpret complex biomedical data, enhance
drug molecule design, predict drug efficacy and safety profiles, and facilitate
clinical trial processes. Our paper aims to provide a comprehensive overview
for researchers and practitioners in computational biology, pharmacology, and
AI4Science by offering insights into the potential transformative impact of
LLMs on drug discovery and development.",http://arxiv.org/abs/2409.04481v1,"Yizhen Zheng, Huan Yee Koh, Maddie Yang, Li Li, Lauren T May, Geoffrey I Webb, Shirui Pan, George Church"
430,Improve Sensitivity Analysis Synthesizing Randomized Clinical Trials With Limited Overlap,"Randomized clinical trials are the gold standard when estimating the average
treatment effect. However, they are usually not a random sample from the
real-world population because of the inclusion/exclusion rules. Meanwhile,
observational studies typically consist of representative samples from the
real-world population. However, due to unmeasured confounding, sensitivity
analysis is often used to estimate bounds for the average treatment effect
without relying on stringent assumptions of other existing methods. This
article introduces a synthesis estimator that improves sensitivity analysis in
observational studies by incorporating randomized clinical trial data, even
when overlap in covariate distribution is limited due to inclusion/exclusion
criteria. We show that the proposed estimator will give a tighter bound when a
""separability"" condition holds for the sensitivity parameter. Theoretical
proofs and simulations show that this method provides a tighter bound than the
sensitivity analysis using only observational study. We apply this method to
combine an observational study on drug effectiveness with a partially
overlapping RCT dataset, yielding improved average treatment effect bounds.",http://arxiv.org/abs/2409.07391v3,"Kuan Jiang, Wenjie Hu, Shu Yang, Xinxing Lai, Xiaohua Zhou"
431,Comparison of g-estimation approaches for handling symptomatic medication at multiple timepoints in Alzheimer's Disease with a hypothetical strategy,"For handling intercurrent events in clinical trials, one of the strategies
outlined in the ICH E9(R1) addendum targets the hypothetical scenario of
non-occurrence of the intercurrent event. While this strategy is often
implemented by setting data after the intercurrent event to missing even if
they have been collected, g-estimation allows for a more efficient estimation
by using the information contained in post-IE data. As the g-estimation methods
have largely developed outside of randomised clinical trials, optimisations for
the application in clinical trials are possible. In this work, we describe and
investigate the performance of modifications to the established g-estimation
methods, leveraging the assumption that some intercurrent events are expected
to have the same impact on the outcome regardless of the timing of their
occurrence. In a simulation study in Alzheimer disease, the modifications show
a substantial efficiency advantage for the estimation of an estimand that
applies the hypothetical strategy to the use of symptomatic treatment while
retaining unbiasedness and adequate type I error control.",http://arxiv.org/abs/2409.10943v1,"Florian Lasch, Lorenzo Guizzaro, Wen Wei Loh"
432,Direct Estimation for Commonly Used Pattern-Mixture Models in Clinical Trials,"Pattern-mixture models have received increasing attention as they are
commonly used to assess treatment effects in primary or sensitivity analyses
for clinical trials with nonignorable missing data. Pattern-mixture models have
traditionally been implemented using multiple imputation, where the variance
estimation may be a challenge because the Rubin's approach of combining
between- and within-imputation variance may not provide consistent variance
estimation while bootstrap methods may be time-consuming. Direct
likelihood-based approaches have been proposed in the literature and
implemented for some pattern-mixture models, but the assumptions are sometimes
restrictive, and the theoretical framework is fragile. In this article, we
propose an analytical framework for an efficient direct likelihood estimation
method for commonly used pattern-mixture models corresponding to
return-to-baseline, jump-to-reference, placebo washout, and retrieved dropout
imputations. A parsimonious tipping point analysis is also discussed and
implemented. Results from simulation studies demonstrate that the proposed
methods provide consistent estimators. We further illustrate the utility of the
proposed methods using data from a clinical trial evaluating a treatment for
type 2 diabetes.",http://arxiv.org/abs/2410.06939v1,"Jitong Lou, Mallikarjuna Rettiganti, Yongming Qu"
433,"PubMed knowledge graph 2.0: Connecting papers, patents, and clinical trials in biomedical science","Papers, patents, and clinical trials are indispensable types of scientific
literature in biomedicine, crucial for knowledge sharing and dissemination.
However, these documents are often stored in disparate databases with varying
management standards and data formats, making it challenging to form
systematic, fine-grained connections among them. To address this issue, we
introduce PKG2.0, a comprehensive knowledge graph dataset encompassing over 36
million papers, 1.3 million patents, and 0.48 million clinical trials in the
biomedical field. PKG2.0 integrates these previously dispersed resources
through various links, including biomedical entities, author networks, citation
relationships, and research projects. Fine-grained biomedical entity
extraction, high-performance author name disambiguation, and multi-source
citation integration have played a crucial role in the construction of the PKG
dataset. Additionally, project data from the NIH Exporter enriches the dataset
with metadata of NIH-funded projects and their scholarly outputs. Data
validation demonstrates that PKG2.0 excels in key tasks such as author
disambiguation and biomedical entity recognition. This dataset provides
valuable resources for biomedical researchers, bibliometric scholars, and those
engaged in literature mining.",http://arxiv.org/abs/2410.07969v1,"Jian Xu, Chao Yu, Jiawei Xu, Ying Ding, Vetle I Torvik, Jaewoo Kang, Mujeen Sung, Min Song"
434,A novel longitudinal rank-sum test for multiple primary endpoints in clinical trials: Applications to neurodegenerative disorders,"Neurodegenerative disorders such as Alzheimer's disease (AD) present a
significant global health challenge, characterized by cognitive decline,
functional impairment, and other debilitating effects. Current AD clinical
trials often assess multiple longitudinal primary endpoints to comprehensively
evaluate treatment efficacy. Traditional methods, however, may fail to capture
global treatment effects, require larger sample sizes due to multiplicity
adjustments, and may not fully exploit multivariate longitudinal data. To
address these limitations, we introduce the Longitudinal Rank Sum Test (LRST),
a novel nonparametric rank-based omnibus test statistic. The LRST enables a
comprehensive assessment of treatment efficacy across multiple endpoints and
time points without multiplicity adjustments, effectively controlling Type I
error while enhancing statistical power. It offers flexibility against various
data distributions encountered in AD research and maximizes the utilization of
longitudinal data. Extensive simulations and real-data applications demonstrate
the LRST's performance, underscoring its potential as a valuable tool in AD
clinical trials.",http://arxiv.org/abs/2410.19190v2,"Xiaoming Xu, Dhrubajyoti Ghosh, Sheng Luo"
435,G-computation for increasing performances of clinical trials with individual randomization and binary response,"In a clinical trial, the random allocation aims to balance prognostic factors
between arms, preventing true confounders. However, residual differences due to
chance may introduce near-confounders. Adjusting on prognostic factors is
therefore recommended, especially because the related increase of the power. In
this paper, we hypothesized that G-computation associated with machine learning
could be a suitable method for randomized clinical trials even with small
sample sizes. It allows for flexible estimation of the outcome model, even when
the covariates' relationships with outcomes are complex. Through simulations,
penalized regressions (Lasso, Elasticnet) and algorithm-based methods (neural
network, support vector machine, super learner) were compared. Penalized
regressions reduced variance but may introduce a slight increase in bias. The
associated reductions in sample size ranged from 17\% to 54\%. In contrast,
algorithm-based methods, while effective for larger and more complex data
structures, underestimated the standard deviation, especially with small sample
sizes. In conclusion, G-computation with penalized models, particularly
Elasticnet with splines when appropriate, represents a relevant approach for
increasing the power of RCTs and accounting for potential near-confounders.",http://arxiv.org/abs/2411.10089v1,"Joe de Keizer, Rmi Lenain, Raphal Porcher, Sarah Zoha, Arthur Chatton, Yohann Foucher"
436,Fast Learning-based Registration of Sparse 3D Clinical Images,"We introduce SparseVM, a method that registers clinical-quality 3D MR scans
both faster and more accurately than previously possible. Deformable alignment,
or registration, of clinical scans is a fundamental task for many clinical
neuroscience studies. However, most registration algorithms are designed for
high-resolution research-quality scans. In contrast to research-quality scans,
clinical scans are often sparse, missing up to 86% of the slices available in
research-quality scans. Existing methods for registering these sparse images
are either inaccurate or extremely slow. We present a learning-based
registration method, SparseVM, that is more accurate and orders of magnitude
faster than the most accurate clinical registration methods. To our knowledge,
it is the first method to use deep learning specifically tailored to
registering clinical images. We demonstrate our method on a clinically-acquired
MRI dataset of stroke patients and on a simulated sparse MRI dataset. Our code
is available as part of the VoxelMorph package at http://voxelmorph.mit.edu/.",http://arxiv.org/abs/1812.06932v3,"Kathleen M Lewis, Natalia S Rost, John Guttag, Adrian V Dalca"
437,Optimal Patient Allocation in Multi-Arm Clinical Trials,"A multi-arm multi-stage trial is a multi-arm trial which includes interim
analyses - analysing the data at certain specified points, generally
discontinuing treatments which are concluded to not work and proceeding with
the remainder.
  It is possible that the advantages of multi-arm trials over single-arm trials
may be enhanced further by considering the allocation ratio, R. For an R:1
allocation ratio, Rn patients are allocated to the control arm and n patients
allocated to each active treatment arm. In this study, the optimal allocation
ratio will be defined as the allocation ratio which results in the smallest
total sample size satisfying some required power and probability of type I
error. This is an intuitive definition in the context of clinical trials, as a
smaller trial will in general be more ethical and less expensive than a larger
one satisfying the same error rates. The purpose of this paper is to
investigate the optimal allocation ratio in the case of multiple active
treatment arms.
  The setup for a single stage trial with K active treatment arms is described
in Section 2, along with a brief exposition of Dunnett's statement regarding
the optimal allocation ratio in such circumstances. Equations for type I error
and power are derived, and the methodology used to investigate how total sample
size may be minimised using allocation ratio is described. A two-stage trial is
then considered, using the same methodology. Figures and tables showing how
total sample size changes with allocation ratio, for a range of type I error
and power values, are given in Section 3. The possible ethical and financial
benefits of changing allocation ratio, including a simple example, is also
included in Section 3. The results, and what they could mean in practical
terms, are discussed in Section 4.",http://arxiv.org/abs/2211.06342v1,Martin Law
438,Sex Differences in Severity and Mortality Among Patients With COVID-19: Evidence from Pooled Literature Analysis and Insights from Integrated Bioinformatic Analysis,"Objective: To conduct a meta-analysis of current studies that examined sex
differences in severity and mortality in patients with COVID-19, and identify
potential mechanisms underpinning these differences. Methods: We performed a
systematic review to collate data from observational studies examining
associations of sex differences with clinical outcomes of COVID-19. PubMed, Web
of Science and four preprint servers were searched for relevant studies. Data
were extracted and analyzed using meta-analysis where possible, with summary
data presented otherwise. Publicly available bulk RNA sequencing (RNA-seq),
single-cell RNA sequencing (scRNA-seq), and chromatin immunoprecipitation
sequencing (ChIP-seq) data were analyzed to explore the potential mechanisms
underlying the observed association. Results: 39 studies met inclusion
criteria, representing 77932 patients, of which 41510 (53.3%) were males. Men
were at a markedly increased risk of developing severe cases compared with
women. Furthermore, the pooled odds ratio (OR) of mortality for male group
compared with the female group indicated significant higher mortality rate for
male. Data from scRNA-seq suggest that men have a higher amount of
ACE2-expressing pulmonary alveolar type II cells than women. Sex-based
immunological differences exist. The expression of androgen receptor (AR) is
positively correlated with ACE2, and there is evidence that AR may directly
regulate the expression of ACE2. Conclusions: This meta-analysis detected an
increased severity and mortality rate in the male populations with COVID-19,
which might be attributable to the sex-based differences in cellular
compositions and immunological microenvironments of the lung. The host cell
receptor ACE2 is likely regulated by AR signaling pathway, which is identified
as a potential target for prevention and treatment of SARS-Cov-2 infections in
men.",http://arxiv.org/abs/2003.13547v1,"Xiyi Wei, YuTian Xiao, Jian Wang, Rui Chen, Wei Zhang, Yue Yang, Daojun Lv, Chao Qin, Di Gu, Bo Zhang, Weidong Chen, Jianquan Hou, Ninghong Song, Guohua Zeng, Shancheng Ren"
439,LeafAI: query generator for clinical cohort discovery rivaling a human programmer,"Objective: Identifying study-eligible patients within clinical databases is a
critical step in clinical research. However, accurate query design typically
requires extensive technical and biomedical expertise. We sought to create a
system capable of generating data model-agnostic queries while also providing
novel logical reasoning capabilities for complex clinical trial eligibility
criteria.
  Materials and Methods: The task of query creation from eligibility criteria
requires solving several text-processing problems, including named entity
recognition and relation extraction, sequence-to-sequence transformation,
normalization, and reasoning. We incorporated hybrid deep learning and
rule-based modules for these, as well as a knowledge base of the Unified
Medical Language System (UMLS) and linked ontologies. To enable data-model
agnostic query creation, we introduce a novel method for tagging database
schema elements using UMLS concepts. To evaluate our system, called LeafAI, we
compared the capability of LeafAI to a human database programmer to identify
patients who had been enrolled in 8 clinical trials conducted at our
institution. We measured performance by the number of actual enrolled patients
matched by generated queries.
  Results: LeafAI matched a mean 43% of enrolled patients with 27,225 eligible
across 8 clinical trials, compared to 27% matched and 14,587 eligible in
queries by a human database programmer. The human programmer spent 26 total
hours crafting queries compared to several minutes by LeafAI.
  Conclusions: Our work contributes a state-of-the-art data model-agnostic
query generation system capable of conditional reasoning using a knowledge
base. We demonstrate that LeafAI can rival an experienced human programmer in
finding patients eligible for clinical trials.",http://arxiv.org/abs/2304.06203v2,"Nicholas J Dobbins, Bin Han, Weipeng Zhou, Kristine Lan, H Nina Kim, Robert Harrington, Ozlem Uzuner, Meliha Yetisgen"
440,Integrating Phase 2 into Phase 3 based on an Intermediate Endpoint While Accounting for a Cure Proportion -- with an Application to the Design of a Clinical Trial in Acute Myeloid Leukemia,"For a trial with primary endpoint overall survival for a molecule with
curative potential, statistical methods that rely on the proportional hazards
assumption may underestimate the power and the time to final analysis. We show
how a cure proportion model can be used to get the necessary number of events
and appropriate timing via simulation. If Phase 1 results for the new drug are
exceptional and/or the medical need in the target population is high, a Phase 3
trial might be initiated after Phase 1. Building in a futility interim analysis
into such a pivotal trial may mitigate the uncertainty of moving directly to
Phase 3. However, if cure is possible, overall survival might not be mature
enough at the interim to support a futility decision. We propose to base this
decision on an intermediate endpoint that is sufficiently associated with
survival. Planning for such an interim can be interpreted as making a
randomized Phase 2 trial a part of the pivotal trial: if stopped at the
interim, the trial data would be analyzed and a decision on a subsequent Phase
3 trial would be made. If the trial continues at the interim then the Phase 3
trial is already underway. To select a futility boundary, a mechanistic
simulation model that connects the intermediate endpoint and survival is
proposed. We illustrate how this approach was used to design a pivotal
randomized trial in acute myeloid leukemia, discuss historical data that
informed the simulation model, and operational challenges when implementing it.",http://arxiv.org/abs/1901.01308v2,"Kaspar Rufibach, Dominik Heinzmann, Annabelle Monnet"
441,A Semi-parametric Bayesian Approach to Population Finding with Time-to-Event and Toxicity Data in a Randomized Clinical Trial,"A utility-based Bayesian population finding (BaPoFi) method was proposed by
Morita and M\""uller (2017, Biometrics, 1355-1365) to analyze data from a
randomized clinical trial with the aim of identifying good predictive baseline
covariates for optimizing the target population for a future study. The
approach casts the population finding process as a formal decision problem
together with a flexible probability model using a random forest to define a
regression mean function. BaPoFi is constructed to handle a single continuous
or binary outcome variable. In this paper, we develop BaPoFi-TTE as an
extension of the earlier approach for clinically important cases of
time-to-event (TTE) data with censoring, and also accounting for a toxicity
outcome. We model the association of TTE data with baseline covariates using a
semi-parametric failure time model with a P\'olya tree prior for an unknown
error term and a random forest for a flexible regression mean function. We
define a utility function that addresses a trade-off between efficacy and
toxicity as one of the important clinical considerations for population
finding. We examine the operating characteristics of the proposed method in
extensive simulation studies. For illustration, we apply the proposed method to
data from a randomized oncology clinical trial. Concerns in a preliminary
analysis of the same data based on a parametric model motivated the proposed
more general approach.",http://arxiv.org/abs/1910.12174v1,"Satoshi Morita, Peter Mller, Hiroyasu Abe"
442,Three dimensional simulations of embolic stroke: clinical comparisons and an equation for sizing emboli from imaging,"There is a need to develop Monte Carlo simulations of stroke to run in-silico
trials to replace animal models, explore clinical scenarios to develop
hypotheses for clinical studies and for interpreting clinical monitoring. We
perform three-dimensional (3D) stroke simulations, carrying out in-silico
trials to relate lesion volume to embolus diameter and calculate probabilistic
lesion overlap maps, building on our previous Monte Carlo method. Simulated
emboli were released into a 3D in silico vasculature, supplying gray and white
matter brain volumes, to generate individual lesion estimates and probabilistic
lesion overlap maps. Computer generated lesions were assessed by clinicians and
compared with real world radiological images. Simulations of large single
emboli reproduced similar middle cerebral artery (MCA), posterior cerebral
artery (PCA) and anterior cerebral artery (ACA) lesions to those observed
clinically. A proof-of-concept in-silico trial led to a conjecture relating
estimated infarct volume as a percentage of total brain volume to relative
embolus diameter: $\mathrm{relative diameter} = [\% \mathrm{infarct volume} /
a]^{1/b}$, where $a= 104.2 \pm 0.98$, $b=3.380 \pm 0.030$. Probabilistic lesion
overlap maps were created, confirming the MCA territory as the most probable
resting place of emboli in the computational vasculature, followed by the PCA
then ACA. The article shows proof of concept for developing a 3D stroke model
from an automatically constructed vasculature.",http://arxiv.org/abs/2110.15141v2,"James P Hague, Jonathan Keelan, Lucy Beishon, David Swienton, Thompson G Robinson, Emma M L Chung"
443,Misclassification of Vaccination Status in Electronic Health Records: A Bayesian Approach in Cluster Randomized Trials,"Misclassification in binary outcomes is not uncommon and statistical methods
to investigate its impact on policy-driving study results are lacking. While
misclassifying binary outcomes is a statistically ubiquitous phenomena, we
focus on misclassification in a public health application: vaccinations. One
such study design in public health that addresses policy is the cluster
controlled randomized trial (CCRT). A CCRT that measures the impact of a novel
behavioral intervention on increasing vaccine uptake can be severely biased
when the supporting data are incomplete vaccination records. In particular,
these vaccine records more often may be prone to negative misclassification,
that is, a clinic's record of an individual patient's vaccination status may be
unvaccinated when, in reality, this patient was vaccinated outside of the
clinic. With large nation-wide endeavors to encourage vaccinations without a
gold-standard vaccine record system, sensitivity analyses that incorporate
misclassification rates are promising for robust inference. In this work we
introduce a novel extension of Bayesian logistic regression where we perturb
the clinic size and vaccination count with random draws from expert-elicited
prior distributions. These prior distributions represent the misclassification
rates for each clinic that stochastically add unvaccinated counts to the
observed vaccinated counts. These prior distributions are assigned for each
clinic (the first level in a group-level randomized trial). We demonstrate this
method with a data application from a CCRT evaluating the influence of a
behavioral intervention on vaccination uptake among U.S. veterans. A simulation
study is carried out demonstrating its estimation properties.",http://arxiv.org/abs/2411.05215v1,"Adam Kaplan, Collin Calvert, Bridget C Griffith, Daniel Bertenthal, Natalie Purcell, Karen Seal, Jeffrey M Pyne, Karen Anderson Oliver, Denise Esserman, David Nelson"
444,Patient recruitment forecasting in clinical trials using time-dependent Poisson-gamma model and homogeneity testing criteria,"Clinical trials in the modern era are characterized by their complexity and
high costs and usually involve hundreds/thousands of patients to be recruited
across multiple clinical centres in many countries, as typically a rather large
sample size is required in order to prove the efficiency of a particular drug.
As the imperative to recruit vast numbers of patients across multiple clinical
centres has become a major challenge, an accurate forecasting of patient
recruitment is one of key factors for the operational success of clinical
trials. A classic Poisson-gamma (PG) recruitment model assumes time-homogeneous
recruitment rates. However, there can be potential time-trends in the
recruitment driven by various factors, e.g. seasonal changes, exhaustion of
patients on particular treatments in some centres, etc. Recently a few authors
considered some extensions of the PG model to time-dependent rates under some
particular assumptions. In this paper, a natural generalization of the original
PG model to a PG model with non-homogeneous time-dependent rates is introduced.
It is also proposed a new analytic methodology for modelling/forecasting
patient recruitment using a Poisson-gamma approximation of recruitment
processes in different countries and globally. The properties of some tests on
homogeneity of the rates (non-parametric one using a Poisson model and two
parametric tests using Poisson and PG model) are investigated. The techniques
for modeling and simulation of the recruitment using time-dependent model are
discussed. For re-projection of the remaining recruitment it is proposed to use
a moving window and re-estimating parameters at every interim time. The results
are supported by simulation of some artificial data sets.",http://arxiv.org/abs/2411.17393v1,"Volodymyr Anisimov, Lucas Oliver"
445,Power contours: optimising sample size and precision in experimental psychology and human neuroscience,"When designing experimental studies with human participants, experimenters
must decide how many trials each participant will complete, as well as how many
participants to test. Most discussion of statistical power (the ability of a
study design to detect an effect) has focussed on sample size, and assumed
sufficient trials. Here we explore the influence of both factors on statistical
power, represented as a two-dimensional plot on which iso-power contours can be
visualised. We demonstrate the conditions under which the number of trials is
particularly important, i.e. when the within-participant variance is large
relative to the between-participants variance. We then derive power contour
plots using existing data sets for eight experimental paradigms and
methodologies (including reaction times, sensory thresholds, fMRI, MEG, and
EEG), and provide example code to calculate estimates of the within- and
between-participant variance for each method. In all cases, the
within-participant variance was larger than the between-participants variance,
meaning that the number of trials has a meaningful influence on statistical
power in commonly used paradigms. An online tool is provided
(https://shiny.york.ac.uk/powercontours/) for generating power contours, from
which the optimal combination of trials and participants can be calculated when
designing future studies.",http://arxiv.org/abs/1902.06122v5,"Daniel H Baker, Greta Vilidaite, Freya A Lygo, Anika K Smith, Tessa R Flack, Andre D Gouws, Timothy J Andrews"
446,Time-frequency analysis of event-related brain recordings: Connecting power of evoked potential and inter-trial coherence,"Objective. In neuroscience, time-frequency analysis has been used to get
insight into brain rhythms from brain recordings. In event-related protocols,
one applies it to investigate how the brain responds to a stimulation repeated
over many trials. In this framework, three measures have been considered: the
amplitude of the transform for each single trial averaged across trials,
avgAMP; inter-trial phase coherence, ITC; and the power of the evoked potential
transform, POWavg. These three measures are sensitive to different aspects of
event-related responses, ITC and POWavg sharing a common sensitivity to phase
resetting phenomena. Methods. In the present manuscript, we further
investigated the connection between ITC and POWavg using theoretical
calculations, a simulation study and analysis of experimental data. Results. We
derived exact expressions for the relationship between POWavg and ITC in the
particular case of the S-transform of an oscillatory signal. In the more
general case, we showed that POWavg and ITC are connected through a
relationship that roughly reads $\mathrm{POWavg} \approx \mathrm{avgAMP}^2
\times \mathrm{ITC}^2$. This result was confirmed on simulations. We finally
compared the theoretical prediction with results from real data. Conclusion. We
showed that POWavg and ITC are related through an approximate, simple
relationship that also involves avgAMP. Significance. The presented
relationship between POWavg, ITC, and avgAMP confirms previous empirical
evidence and provides a novel perspective to investigate evoked brain rhythms.
It may provide a significant refinement to the neuroscientific toolbox for
studying evoked oscillations.",http://arxiv.org/abs/2211.08811v1,"Jonas Benhamou, Michel Le Van Quyen, Guillaume Marrelec"
447,Unraveling Post-COVID-19 Immune Dysregulation Using Machine Learning-based Immunophenotyping,"The COVID-19 pandemic has left a significant mark on global healthcare, with
many individuals experiencing lingering symptoms long after recovering from the
acute phase of the disease, a condition often referred to as ""long COVID."" This
study delves into the intricate realm of immune dysregulation that ensues in
509 post-COVID-19 patients across multiple Iraqi regions during the years 2022
and 2023. Utilizing advanced machine learning techniques for immunophenotyping,
this research aims to shed light on the diverse immune dysregulation patterns
present in long COVID patients. By analyzing a comprehensive dataset
encompassing clinical, immunological, and demographic information, the study
provides valuable insights into the complex interplay of immune responses
following COVID-19 infection. The findings reveal that long COVID is associated
with a spectrum of immune dysregulation phenomena, including persistent
inflammation, altered cytokine profiles, and abnormal immune cell subsets.
These insights highlight the need for personalized interventions and tailored
treatment strategies for individuals suffering from long COVID-19. This
research represents a significant step forward in our understanding of the
post-COVID-19 immune landscape and opens new avenues for targeted therapies and
clinical management of long COVID patients. As the world grapples with the
long-term implications of the pandemic, these findings offer hope for improving
the quality of life for those affected by this enigmatic condition.",http://arxiv.org/abs/2310.01428v1,"Maitham G Yousif, Ghizal Fatima, Hector J Castro, Fadhil G AlAmran, Salman Rawaf"
448,CardioLab: Laboratory Values Estimation and Monitoring from Electrocardiogram Signals -- A Multimodal Deep Learning Approach,"Background: Laboratory values are fundamental to medical diagnosis and
management, but acquiring these values can be costly, invasive, and
time-consuming. While electrocardiogram (ECG) patterns have been linked to
certain laboratory abnormalities, the comprehensive modeling of these
relationships remains underexplored.
  Methods: We utilize MIMIC-IV dataset to develop multimodal deep-learning
models to demonstrate the feasibility of estimating (real-time) and monitoring
(predict at future intervals) laboratory value abnormalities from ECG
waveforms, demographics, biometrics, and vital signs.
  Results: The models exhibit a strong predictive performance with AUROC scores
above 0.70 in a statistically significant manner for 23 laboratory values in
the estimation setting and up to 26 values in the monitoring setting. Most
notably, the accurately predictable values encompassing abnormalities across
diverse physiological categories such as cardiac, renal, hematological,
metabolic, immunological and coagulation. To name examples, for estimation
NTproBNP (>353 pg/mL) with 0.882, whereas for monitoring at 30 minutes Urea
nitrogen (<6 mg/dL) with 0.851, at 60 minutes creatinine (<0.5 mg/dL) with
0.85, and at 120 minutes hemoglobin (>17.5 g/dL) with 0.821.
  Conclusions: This study provides first evidence for the feasibility of using
ECG data alongside clinical routine data for the real-time estimation and
monitoring of laboratory value abnormalities, which could provide a
non-invasive, cost-effective supplement to traditional laboratory testing, with
strong implications for enhanced patient monitoring and early intervention.
Further validation could facilitate their integration into routine clinical
practice.",http://arxiv.org/abs/2411.14886v1,"Juan Miguel Lopez Alcaraz, Nils Strodthoff"
449,Endpoints for randomized controlled clinical trials for COVID-19 treatments,"Introduction: Endpoint choice for randomized controlled trials of treatments
for COVID-19 is complex. A new disease brings many uncertainties, but trials
must start rapidly. COVID-19 is heterogeneous, ranging from mild disease that
improves within days to critical disease that can last weeks and can end in
death. While improvement in mortality would provide unquestionable evidence
about clinical significance of a treatment, sample sizes for a study evaluating
mortality are large and may be impractical. Furthermore, patient states in
between ""cure"" and ""death"" represent meaningful distinctions. Clinical severity
scores have been proposed as an alternative. However, the appropriate summary
measure for severity scores has been the subject of debate, particularly in
relating to the uncertainty about the time-course of COVID-19. Outcomes
measured at fixed time-points may risk missing the time of clinical benefit. An
endpoint such as time-to-improvement (or recovery), avoids the timing problem.
However, some have argued that power losses will result from reducing the
ordinal scale to a binary state of ""recovered"" vs ""not recovered.""
  Methods: We evaluate statistical power for possible trial endpoints for
COVID-19 treatment trials using simulation models and data from two recent
COVID-19 treatment trials.
  Results: Power for fixed-time point methods depends heavily on the time
selected for evaluation. Time-to-improvement (or recovery) analyses do not
specify a time-point. Time-to-event approaches have reasonable statistical
power, even when compared to a fixed time-point method evaluated at the optimal
time.
  Discussion: Time-to-event analyses methods have advantages in the COVID-19
setting, unless the optimal time for evaluating treatment effect is known in
advance. Even when the optimal time is known, a time-to-event approach may
increase power for interim analyses.",http://arxiv.org/abs/2006.10533v1,"Lori E Dodd, Dean Follmann, Jing Wang, Franz Koenig, Lisa L Korn, Christian Schoergenhofer, Michael Proschan, Sally Hunsberger, Tyler Bonnett, Mat Makowski, Drifa Belhadi, Yeming Wang, Bin Cao, France Mentre, Thomas Jaki"
450,Incidence of the Bertillon and Gompertz effects on the outcome of clinical trials,"The accounts of medical trials provide very detailed information about the
patients' health conditions. In contrast, only minimal data are usually given
about demographic factors. Yet, some of these factors can have a notable impact
on the overall death rate, thereby changing the outcome and conclusions of the
trial. This paper focuses on two of these variables. The first is marital
status; this effect, which will be referred to as the Bertillon effect, may
change death rates by over 100%. The second is the age of the oldest patients;
because of the exponential nature of Gompertz's law, changes in the
distribution of ages in the oldest age group can have dramatic consequences on
the overall number of deaths. It will be seen that randomization alone can
hardly take care of these problems. Appropriate remedies are easy to formulate
however. First, the marital status of patients as well as the age distribution
of those over 65 should be documented for both study groups. Then, thanks to
these data and based on the Bertillon and Gompertz laws, it will become
possible to perform appropriate corrections. Such corrections will notably
improve the reliability and accuracy of the trial's conclusions, especially for
trials which include a large proportion of elderly subjects.",http://arxiv.org/abs/1306.5186v1,Bertrand M Roehner
451,Retrospective analysis of a fatal dose-finding trial,"The commonplace description of phase 1 clinical trials in oncology as
""primarily concerned with safety"" is belied by their near universal adoption of
dose-escalation practices which are inherently unsafe. In contrast with dose
titration, cohort-wise dose escalation regards patients as exchangeable, an
indefensible assumption in the face of widely appreciated inter-individual
heterogeneity in pharmacokinetics and pharmacodynamics (PKPD). I have
previously advanced this argument in terms of a precautionary coherence
principle that brings the well-known coherence notion of Cheung (2005) into
contact with modern imperatives of patient-centeredness and precision dosing.
Here, however, I explore these matters in some mechanistic detail by analyzing
a trial of the bispecific T cell engager AFM11, in which a fatal toxicity
occurred. To this end, I develop a Bayesian dose-response model for a single
ordinal toxicity. By constructing this model's priors to align with the AFM11
trial as designed and conducted, I demonstrate the incompatibility of that
design with any reasonable expectation of safety. Indeed, the model readily
yields prospective estimates of toxic response probabilities that suggest the
fatality in this trial could have been foreseen as likely.",http://arxiv.org/abs/2004.12755v1,David C Norris
452,Approximating the Operating Characteristics of Bayesian Uncertainty Directed Trial Designs,"Bayesian response adaptive clinical trials are currently evaluating
experimental therapies for several diseases. Adaptive decisions, such as
pre-planned variations of the randomization probabilities, attempt to
accelerate the development of new treatments. The design of response adaptive
trials, in most cases, requires time consuming simulation studies to describe
operating characteristics, such as type I/II error rates, across plausible
scenarios. We investigate large sample approximations of pivotal operating
characteristics in Bayesian Uncertainty directed trial Designs (BUDs). A BUD
trial utilizes an explicit metric u to quantify the information accrued during
the study on parameters of interest, for example the treatment effects. The
randomization probabilities vary during time to minimize the uncertainty
summary u at completion of the study. We provide an asymptotic analysis (i) of
the allocation of patients to treatment arms and (ii) of the randomization
probabilities. For BUDs with outcome distributions belonging to the natural
exponential family with quadratic variance function, we illustrate the
asymptotic normality of the number of patients assigned to each arm and of the
randomization probabilities. We use these results to approximate relevant
operating characteristics such as the power of the BUD. We evaluate the
accuracy of the approximations through simulations under several scenarios for
binary, time-to-event and continuous outcome models.",http://arxiv.org/abs/2105.11177v1,"Marta Bonsaglio, Sandra Fortini, Steffen Ventz, Lorenzo Trippa"
453,What Were They Thinking? Pharmacologic priors implicit in a choice of 3+3 dose-escalation design,"If explicit, formal consideration of clinical pharmacology at all informs the
design and conduct of modern oncology dose-finding trials, the designs
themselves hardly attest to this. Yet in conducting a trial, investigators
affirm that they hold reasonable expectations of participant safety -
expectations that necessarily depend on beliefs about how certain pharmacologic
parameters are distributed in the study population. Thus, these beliefs are
implicit in a trial's presumed conformance to a community standard of safety,
and may therefore to some extent be reverse-engineered from trial designs. For
one popular form of dose-escalation trial design, I demonstrate here how this
may be done.",http://arxiv.org/abs/2012.05301v2,David C Norris
454,Modeling synergism in early phase cancer trials with drug combination with continuous dose levels: is there an added value?,"In parametric Bayesian designs of early phase cancer clinical trials with
drug combinations exploring a discrete set of partially ordered doses, several
authors claimed that there is no added value in including an interaction term
to model synergism between the two drugs. In this paper, we investigate these
claims in the setting of continuous dose levels of the two agents. Parametric
models will be used to describe the relationship between the doses of the two
agents and the probability of dose limiting toxicity and efficacy. Trial design
proceeds by treating cohorts of two patients simultaneously receiving different
dose combinations and response adaptive randomization. We compare trial safety
and efficiency of the estimated maximum tolerated dose (MTD) curve between
models that include an interaction term with models without the synergism
parameter with extensive simulations. Under a selected class of dose-toxicity
models and dose escalation algorithm, we found that not including an
interaction term in the model can compromise the safety of the trial and reduce
the pointwise reliability of the estimated MTD curve.",http://arxiv.org/abs/2208.05726v1,"Mourad Tighiouart, Jos L Jimnez, Marcio A Diniz, Andr Rogatko"
455,The stochastic digital human is now enrolling for in silico imaging trials -- Methods and tools for generating digital cohorts,"Randomized clinical trials, while often viewed as the highest evidentiary bar
by which to judge the quality of a medical intervention, are far from perfect.
In silico imaging trials are computational studies that seek to ascertain the
performance of a medical device by collecting this information entirely via
computer simulations. The benefits of in silico trials for evaluating new
technology include significant resource and time savings, minimization of
subject risk, the ability to study devices that are not achievable in the
physical world, allow for the rapid and effective investigation of new
technologies and ensure representation from all relevant subgroups. To conduct
in silico trials, digital representations of humans are needed. We review the
latest developments in methods and tools for obtaining digital humans for in
silico imaging studies. First, we introduce terminology and a classification of
digital human models. Second, we survey available methodologies for generating
digital humans with healthy and diseased status and examine briefly the role of
augmentation methods. Finally, we discuss the trade-offs of four approaches for
sampling digital cohorts and the associated potential for study bias with
selecting specific patient distributions.",http://arxiv.org/abs/2301.08719v1,"A Badano, M Lago, E Sizikova, JG Delfino, S Guan, MA Anastasio, B Sahiner"
456,Frequentist analysis of basket trials with one-sample Mantel-Haenszel procedures,"Recent substantial advances of molecular targeted oncology drug development
is requiring new paradigms for early-phase clinical trial methodologies to
enable us to evaluate efficacy of several subtypes simultaneously and
efficiently. The concept of the basket trial is getting of much attention to
realize this requirement borrowing information across subtypes, which are
called baskets. Bayesian approach is a natural approach to this end and indeed
the majority of the existing proposals relies on it. On the other hand, it
required complicated modeling and may not necessarily control the type 1 error
probabilities at the nominal level. In this paper, we develop a purely
frequentist approach for basket trials based on one-sample Mantel-Haenszel
procedure relying on a very simple idea for borrowing information under the
common treatment effect assumption over baskets. We show that the proposed
estimator is consistent under two limiting models of the large strata and
sparse data limiting models (dually consistent) and propose dually consistent
variance estimators. The proposed Mantel-Haenszel estimators are interpretable
even if the common treatment assumptions are violated. Then, we can design
basket trials in a confirmatory matter. We also propose an information
criterion approach to identify effective subclass of baskets.",http://arxiv.org/abs/2302.08308v1,"Satoshi Hattori, Satoshi Morita"
457,Implementing Response-Adaptive Randomisation in Stratified Rare-disease Trials: Design Challenges and Practical Solutions,"Although response-adaptive randomisation (RAR) has gained substantial
attention in the literature, it still has limited use in clinical trials.
Amongst other reasons, the implementation of RAR in the real world raises
important practical questions, often neglected. Motivated by an innovative
phase-II stratified RAR trial, this paper addresses two challenges: (1) How to
ensure that RAR allocations are both desirable and faithful to target
probabilities, even in small samples? and (2) What adaptations to trigger after
interim analyses in the presence of missing data? We propose a Mapping strategy
that discretises the randomisation probabilities into a vector of allocation
ratios, resulting in improved frequentist errors. Under the implementation of
Mapping, we analyse the impact of missing data on operating characteristics by
examining selected scenarios. Finally, we discuss additional concerns
including: pooling data across trial strata, analysing the level of blinding in
the trial, and reporting safety results.",http://arxiv.org/abs/2410.03346v1,"Rajenki Das, Nina Deliu, Mark Toshner, Sofa S Villar"
458,A Comparative Evaluation of Bayesian Model-Assisted Two-Stage Designs for Phase I/II Clinical Trials,"The primary goal of a two-stage Phase I/II trial is to identify the optimal
dose for the following large-scale Phase III trial. Recently, Phase I
dose-finding designs have shifted from identifying the maximum tolerated dose
(MTD) to the optimal biological dose (OBD). Typically, several doses are
selected as recommended Phase II doses (RP2D) for further evaluation. In Phase
II dose optimization trials, each RP2D is evaluated independently to determine
its ""go/no-go"" decision. The optimal RP2D is then chosen from the remaining
RP2Ds as the recommended Phase III dose (RP3D). The effectiveness of both
dose-finding and dose optimization designs at two stages impacts RP3D
selection. This paper reviews and compares fifteen Bayesian model-assisted
two-stage designs, combining five Phase I dose-finding designs (BOIN,
TITE-BOIN, BF-BOIN, BOIN12, and TITE-BOIN12) with three Phase II dose
optimization designs (TS, BOP2, and TOP). We conduct extensive simulation
studies to evaluate their performance under different dose-response scenarios,
with and without the existence of the OBD. Based on our results, we recommend
the TITE-BOIN12 + TOP combination as the optimal two-stage design for Phase
I/II trials.",http://arxiv.org/abs/2501.08410v1,"Hao Sun, Jerry Li"
459,An Automated Computational Pipeline for Generating Large-Scale Cohorts of Patient-Specific Ventricular Models in Electromechanical In Silico Trials,"In recent years, human in silico trials have gained significant traction as a
powerful approach to evaluate the effects of drugs, clinical interventions, and
medical devices. In silico trials not only minimise patient risks but also
reduce reliance on animal testing. However, the implementation of in silico
trials presents several time-consuming challenges. It requires the creation of
large cohorts of virtual patients. Each virtual patient is described by their
anatomy with a volumetric mesh and electrophysiological and mechanical dynamics
through mathematical equations and parameters. Furthermore, simulated
conditions need definition including stimulation protocols and therapy
evaluation. For large virtual cohorts, this requires automatic and efficient
pipelines for generation of corresponding files. In this work, we present a
computational pipeline to automatically create large virtual patient cohort
files to conduct large-scale in silico trials through cardiac electromechanical
simulations. The pipeline generates the files describing meshes, labels, and
data required for the simulations directly from unprocessed surface meshes. We
applied the pipeline to generate over 100 virtual patients from various
datasets and performed simulations to demonstrate capacity to conduct in silico
trials for virtual patients using verified and validated electrophysiology and
electromechanics models for the context of use. The proposed pipeline is
adaptable to accommodate different types of ventricular geometries and mesh
processing tools, ensuring its versatility in handling diverse clinical
datasets. By establishing an automated framework for large scale simulation
studies as required for in silico trials and providing open-source code, our
work aims to support scalable, personalised cardiac simulations in research and
clinical applications.",http://arxiv.org/abs/2503.03706v1,"Ruben Doste, Julia Camps, Zhinuo Jenny Wang, Lucas Arantes Berg, Maxx Holmes, Hannah Smith, Marcel Beetz, Lei Li, Abhirup Banerjee, Vicente Grau, Blanca Rodriguez"
460,Stochastic Approximation and Modern Model-Based Designs for Dose-Finding Clinical Trials,"In 1951 Robbins and Monro published the seminal article on stochastic
approximation and made a specific reference to its application to the
""estimation of a quantal using response, nonresponse data."" Since the 1990s,
statistical methodology for dose-finding studies has grown into an active area
of research. The dose-finding problem is at its core a percentile estimation
problem and is in line with what the Robbins--Monro method sets out to solve.
In this light, it is quite surprising that the dose-finding literature has
developed rather independently of the older stochastic approximation
literature. The fact that stochastic approximation has seldom been used in
actual clinical studies stands in stark contrast with its constant application
in engineering and finance. In this article, I explore similarities and
differences between the dose-finding and the stochastic approximation
literatures. This review also sheds light on the present and future relevance
of stochastic approximation to dose-finding clinical trials. Such connections
will in turn steer dose-finding methodology on a rigorous course and extend its
ability to handle increasingly complex clinical situations.",http://arxiv.org/abs/1011.6240v1,Ying Kuen Cheung
461,Implementation of Tripartite Estimands Using Adherence Causal Estimators Under the Causal Inference Framework,"Intercurrent events (ICEs) and missing values are inevitable in clinical
trials of any size and duration, making it difficult to assess the treatment
effect for all patients in randomized clinical trials. Defining the appropriate
estimand that is relevant to the clinical research question is the first step
in analyzing data. The tripartite estimands, which evaluate the treatment
differences in the proportion of patients with ICEs due to adverse events, the
proportion of patients with ICEs due to lack of efficacy, and the primary
efficacy outcome for those who can adhere to study treatment under the causal
inference framework, are of interest to many stakeholders in understanding the
totality of treatment effects. In this manuscript, we discuss the details of
how to estimate tripartite estimands based on a causal inference framework and
how to interpret tripartite estimates through a phase 3 clinical study
evaluating a basal insulin treatment for patients with type 1 diabetes.",http://arxiv.org/abs/2005.14624v1,"Yongming Qu, Junxiang Luo, Stephen J Ruberg"
462,A Nonparametric Method for Value Function Guided Subgroup Identification via Gradient Tree Boosting for Censored Survival Data,"In randomized clinical trials with survival outcome, there has been an
increasing interest in subgroup identification based on baseline genomic,
proteomic markers or clinical characteristics. Some of the existing methods
identify subgroups that benefit substantially from the experimental treatment
by directly modeling outcomes or treatment effect. When the goal is to find an
optimal treatment for a given patient rather than finding the right patient for
a given treatment, methods under the individualized treatment regime framework
estimate an individualized treatment rule that would lead to the best expected
clinical outcome as measured by a value function. Connecting the concept of
value function to subgroup identification, we propose a nonparametric method
that searches for subgroup membership scores by maximizing a value function
that directly reflects the subgroup-treatment interaction effect based on
restricted mean survival time. A gradient tree boosting algorithm is proposed
to search for the individual subgroup membership scores. We conduct simulation
studies to evaluate the performance of the proposed method and an application
to an AIDS clinical trial is performed for illustration.",http://arxiv.org/abs/2006.08807v1,"Pingye Zhang, Junshui Ma, Xinqun Chen, Yue Shentu"
463,Utilizing Win Ratio Approaches and Two-Stage Enrichment Designs for Small-Sized Clinical Trials,"Conventional methods for analyzing composite endpoints in clinical trials
often only focus on the time to the first occurrence of all events in the
composite. Therefore, they have inherent limitations because the individual
patients' first event can be the outcome of lesser clinical importance. To
overcome this limitation, the concept of the win ratio (WR), which accounts for
the relative priorities of the components and gives appropriate priority to the
more clinically important event, was examined. For example, because mortality
has a higher priority than hospitalization, it is reasonable to give a higher
priority when obtaining the WR. In this paper, we evaluate three innovative WR
methods (stratified matched, stratified unmatched, and unstratified unmatched)
for two and multiple components under binary and survival composite endpoints.
We compare these methods to traditional ones, including the Cox regression,
O'Brien's rank-sum-type test, and the contingency table for controlling study
Type I error rate. We also incorporate these approaches into two-stage
enrichment designs with the possibility of sample size adaptations to gain
efficiency for rare disease studies.",http://arxiv.org/abs/2211.14996v1,"Jialu Wang, YehFong Chen, Thomas Gwise"
464,Extracting Factual Min/Max Age Information from Clinical Trial Studies,"Population age information is an essential characteristic of clinical trials.
In this paper, we focus on extracting minimum and maximum (min/max) age values
for the study samples from clinical research articles. Specifically, we
investigate the use of a neural network model for question answering to address
this information extraction task. The min/max age QA model is trained on the
massive structured clinical study records from ClinicalTrials.gov. For each
article, based on multiple min and max age values extracted from the QA model,
we predict both actual min/max age values for the study samples and filter out
non-factual age expressions. Our system improves the results over (i) a passage
retrieval based IE system and (ii) a CRF-based system by a large margin when
evaluated on an annotated dataset consisting of 50 research papers on smoking
cessation.",http://arxiv.org/abs/1904.03262v1,"Yufang Hou, Debasis Ganguly, Lea A Deleris, Francesca Bonin"
465,Sequential knockoffs for continuous and categorical predictors: with application to a large Psoriatic Arthritis clinical trial pool,"Knockoffs provide a general framework for controlling the false discovery
rate when performing variable selection. Much of the Knockoffs literature
focuses on theoretical challenges and we recognize a need for bringing some of
the current ideas into practice. In this paper we propose a sequential
algorithm for generating knockoffs when underlying data consists of both
continuous and categorical (factor) variables. Further, we present a heuristic
multiple knockoffs approach that offers a practical assessment of how robust
the knockoff selection process is for a given data set. We conduct extensive
simulations to validate performance of the proposed methodology. Finally, we
demonstrate the utility of the methods on a large clinical data pool of more
than $2,000$ patients with psoriatic arthritis evaluated in 4 clinical trials
with an IL-17A inhibitor, secukinumab (Cosentyx), where we determine prognostic
factors of a well established clinical outcome. The analyses presented in this
paper could provide a wide range of applications to commonly encountered data
sets in medical practice and other fields where variable selection is of
particular interest.",http://arxiv.org/abs/2010.14026v1,"Matthias Kormaksson, Luke J Kelly, Xuan Zhu, Sibylle Haemmerle, Luminita Pricop, David Ohlssen"
466,Optimal personalised treatment computation through in silico clinical trials on patient digital twins,"In Silico Clinical Trials (ISTC), i.e., clinical experimental campaigns
carried out by means of computer simulations, hold the promise to decrease time
and cost for the safety and efficacy assessment of pharmacological treatments,
reduce the need for animal and human testing, and enable precision medicine. In
this paper we present methods and an algorithm that, by means of extensive
computer simulation--based experimental campaigns (ISTC) guided by intelligent
search, optimise a pharmacological treatment for an individual patient
(precision medicine). e show the effectiveness of our approach on a case study
involving a real pharmacological treatment, namely the downregulation phase of
a complex clinical protocol for assisted reproduction in humans.",http://arxiv.org/abs/2106.10684v1,"Stefano Sinisi, Vadim Alimguzhin, Toni Mancini, Enrico Tronci, Federico Mari, Brigitte Leeners"
467,An automated approach to extracting positive and negative clinical research results,"Failure is common in clinical trials since the successful failures presented
in negative results always indicate the ways that should not be taken. In this
paper, we proposed an automated approach to extracting positive and negative
clinical research results by introducing a PICOE (Population, Intervention,
Comparation, Outcome, and Effect) framework to represent randomized controlled
trials (RCT) reports, where E indicates the effect between a specific I and O.
We developed a pipeline to extract and assign the corresponding statistical
effect to a specific I-O pair from natural language RCT reports. The extraction
models achieved a high degree of accuracy for ICO and E descriptive words
extraction through two rounds of training. By defining a threshold of p-value,
we find in all Covid-19 related intervention-outcomes pairs with statistical
tests, negative results account for nearly 40%. We believe that this
observation is noteworthy since they are extracted from the published
literature, in which there is an inherent risk of reporting bias, preferring to
report positive results rather than negative results. We provided a tool to
systematically understand the current level of clinical evidence by
distinguishing negative results from the positive results.",http://arxiv.org/abs/2212.03464v1,"Xuanyu Shi, Shiyao Xie, Wenjia Wang, Ting Chen, Jian Du"
468,Estimands in Real-World Evidence Studies,"A Real-World Evidence (RWE) Scientific Working Group (SWG) of the American
Statistical Association Biopharmaceutical Section (ASA BIOP) has been reviewing
statistical considerations for the generation of RWE to support regulatory
decision-making. As part of the effort, the working group is addressing
estimands in RWE studies. Constructing the right estimand -- the target of
estimation -- which reflects the research question and the study objective, is
one of the key components in formulating a clinical study. ICH E9(R1) describes
statistical principles for constructing estimands in clinical trials with a
focus on five attributes -- population, treatment, endpoints, intercurrent
events, and population-level summary. However, defining estimands for clinical
studies using real-world data (RWD), i.e., RWE studies, requires additional
considerations due to, for example, heterogeneity of study population,
complexity of treatment regimes, different types and patterns of intercurrent
events, and complexities in choosing study endpoints. This paper reviews the
essential components of estimands and causal inference framework, discusses
considerations in constructing estimands for RWE studies, highlights
similarities and differences in traditional clinical trial and RWE study
estimands, and provides a roadmap for choosing appropriate estimands for RWE
studies.",http://arxiv.org/abs/2307.00190v1,"Jie Chen, Daniel Scharfstein, Hongwei Wang, Binbing Yu, Yang Song, Weili He, John Scott, Xiwu Lin, Hana Lee"
469,An Integrated e-science Analysis Base for Computation Neuroscience Experiments and Analysis,"Recent developments in data management and imaging technologies have
significantly affected diagnostic and extrapolative research in the
understanding of neurodegenerative diseases. However, the impact of these new
technologies is largely dependent on the speed and reliability with which the
medical data can be visualised, analysed and interpreted. The EUs neuGRID for
Users (N4U) is a follow-on project to neuGRID, which aims to provide an
integrated environment to carry out computational neuroscience experiments.
This paper reports on the design and development of the N4U Analysis Base and
related Information Services, which addresses existing research and practical
challenges by offering an integrated medical data analysis environment with the
necessary building blocks for neuroscientists to optimally exploit neuroscience
workflows, large image datasets and algorithms in order to conduct analyses.
The N4U Analysis Base enables such analyses by indexing and interlinking the
neuroimaging and clinical study datasets stored on the N4U Grid infrastructure,
algorithms and scientific workflow definitions along with their associated
provenance information.",http://arxiv.org/abs/1402.5757v1,"Kamran Munir, Saad Liaquat Kiani, Khawar Hasham, Richard McClatchey, Andrew Branson, Jetendr Shamdasani, the NU Consortium"
470,"Combining Covariate Adjustment with Group Sequential, Information Adaptive Designs to Improve Randomized Trial Efficiency","In clinical trials, there is potential to improve precision and reduce the
required sample size by appropriately adjusting for baseline variables in the
statistical analysis. This is called covariate adjustment. Despite
recommendations by regulatory agencies in favor of covariate adjustment, it
remains underutilized leading to inefficient trials. We address two obstacles
that make it challenging to use covariate adjustment. A first obstacle is the
incompatibility of many covariate adjusted estimators with commonly used
boundaries in group sequential designs (GSDs). A second obstacle is the
uncertainty at the design stage about how much precision gain will result from
covariate adjustment. We propose a method that modifies the original estimator
so that it becomes compatible with GSDs, while increasing or leaving unchanged
the estimator's precision. Our approach allows the use of any asymptotically
linear estimator, which covers many estimators used in randomized trials.
Building on this, we propose using an information adaptive design, that is,
continuing the trial until the required information level is achieved. Such a
design adapts to the amount of precision gain and can lead to faster, more
efficient trials, without sacrificing validity or power. We evaluate estimator
performance in simulations that mimic features of a completed stroke trial.",http://arxiv.org/abs/2201.12921v3,"Kelly Van Lancker, Joshua Betz, Michael Rosenblum"
471,Bayesian sample size determination in basket trials borrowing information between subsets,"Basket trials are increasingly used for the simultaneous evaluation of a new
treatment in various patient subgroups under one overarching protocol. We
propose a Bayesian approach to sample size determination in basket trials that
permit borrowing of information between commensurate subsets. Specifically, we
consider a randomised basket trial design where patients are randomly assigned
to the new treatment or a control within each trial subset (`subtrial' for
short). Closed-form sample size formulae are derived to ensure each subtrial
has a specified chance of correctly deciding whether the new treatment is
superior to or not better than the control by some clinically relevant
difference. Given pre-specified levels of pairwise (in)commensurability, the
subtrial sample sizes are solved simultaneously. The proposed Bayesian approach
resembles the frequentist formulation of the problem in yielding comparable
sample sizes for circumstances of no borrowing. When borrowing is enabled
between commensurate subtrials, a considerably smaller trial sample size is
required compared to the widely implemented approach of no borrowing. We
illustrate the use of our sample size formulae with two examples based on real
basket trials. A comprehensive simulation study further shows that the proposed
methodology can maintain the true positive and false positive rates at desired
levels.",http://arxiv.org/abs/2205.12227v1,"Haiyan Zheng, Michael J Grayling, Pavel Mozgunov, Thomas Jaki, James M S Wason"
472,Quantifying efficiency gains of innovative designs of two-arm vaccine trials for COVID-19 using an epidemic simulation model,"Clinical trials of a vaccine during an epidemic face particular challenges,
such as the pressure to identify an effective vaccine quickly to control the
epidemic, and the effect that time-space-varying infection incidence has on the
power of a trial. We illustrate how the operating characteristics of different
trial design elements may be evaluated using a network epidemic and trial
simulation model, based on COVID-19 and individually randomised two-arm trials
with a binary outcome. We show that ""ring"" recruitment strategies, prioritising
participants at high risk of infection, can result in substantial improvement
in terms of power, if sufficiently many contacts of observed cases are at high
risk. In addition, we introduce a novel method to make more efficient use of
the data from the earliest cases of infection observed in the trial, whose
infection may have been too early to be vaccine-preventable. Finally, we
compare several methods of response-adaptive randomisation, discussing their
advantages and disadvantages in this two-arm context and identifying particular
adaptation strategies that preserve power and estimation properties, while
slightly reducing the number of infections, given an effective vaccine.",http://arxiv.org/abs/2104.00546v2,"Rob Johnson, Chris Jackson, Anne Presanis, Sofia S Villar, Daniela De Angelis"
473,Designing efficient randomized trials: power and sample size calculation when using semiparametric efficient estimators,"Trials enroll a large number of subjects in order to attain power, making
them expensive and time-consuming. Sample size calculations are often performed
with the assumption of an unadjusted analysis, even if the trial analysis plan
specifies a more efficient estimator (e.g. ANCOVA). This leads to conservative
estimates of required sample sizes and an opportunity for savings. Here we show
that a relatively simple formula can be used to estimate the power of any
two-arm, single-timepoint trial analyzed with a semiparametric efficient
estimator, regardless of the domain of the outcome or kind of treatment effect
(e.g. odds ratio, mean difference). Since an efficient estimator attains the
minimum possible asymptotic variance, this allows for the design of trials that
are as small as possible while still attaining design power and control of type
I error. The required sample size calculation is parsimonious and requires the
analyst to provide only a small number of population parameters. We verify in
simulation that the large-sample properties of trials designed this way attain
their nominal values. Lastly, we demonstrate how to use this formula in the
""design"" (and subsequent reanalysis) of a real clinical trial and show that
fewer subjects are required to attain the same design power when a
semiparametric efficient estimator is accounted for at the design stage.",http://arxiv.org/abs/2104.10784v2,Alejandro Schuler
474,Transporting survival of an HIV clinical trial to the external target populations,"Due to the heterogeneity of the randomized controlled trial (RCT) and
external target populations, the estimated treatment effect from the RCT is not
directly applicable to the target population. For example, the patient
characteristics of the ACTG 175 HIV trial are significantly different from that
of the three external target populations of interest: US early-stage HIV
patients, Thailand HIV patients, and southern Ethiopia HIV patients. This paper
considers several methods to transport the treatment effect from the ACTG 175
HIV trial to the target populations beyond the trial population. Most transport
methods focus on continuous and binary outcomes; on the contrary, we derive and
discuss several transport methods for survival outcomes: an outcome regression
method based on a Cox proportional hazard (PH) model, an inverse probability
weighting method based on the models for treatment assignment, sampling score,
and censoring, and a doubly robust method that combines both methods, called
the augmented calibration weighting (ACW) method. However, as the PH assumption
was found to be incorrect for the ACTG 175 trial, the methods that depend on
the PH assumption may lead to the biased quantification of the treatment
effect. To account for the violation of the PH assumption, we extend the ACW
method with the linear spline-based hazard regression model that does not
require the PH assumption. Applying the aforementioned methods for
transportability, we explore the effect of PH assumption, or the violation
thereof, on transporting the survival results from the ACTG 175 trial to
various external populations.",http://arxiv.org/abs/2210.02571v1,"Dasom Lee, Sujit Ghosh, Shu Yang"
475,Using Limited Trial Evidence to Credibly Choose Treatment Dosage when Efficacy and Adverse Effects Weakly Increase with Dose,"In medical treatment and elsewhere, it has become standard to base treatment
intensity (dosage) on evidence in randomized trials. Yet it has been rare to
study how outcomes vary with dosage. In trials to obtain drug approval, the
norm has been to specify some dose of a new drug and compare it with an
established therapy or placebo. Design-based trial analysis views each trial
arm as qualitatively different, but it may be highly credible to assume that
efficacy and adverse effects (AEs) weakly increase with dosage. Optimization of
patient care requires joint attention to both, as well as to treatment cost.
This paper develops methodology to credibly use limited trial evidence to
choose dosage when efficacy and AEs weakly increase with dose. I suppose that
dosage is an integer choice t in (0, 1, . . . , T), T being a specified maximum
dose. I study dosage choice when trial evidence on outcomes is available for
only K dose levels, where K < T + 1. Then the population distribution of dose
response is partially rather than point identified. The identification region
is a convex polygon determined by linear equalities and inequalities. I
characterize clinical and public-health decision making using the
minimax-regret criterion. A simple analytical solution exists when T = 2 and
computation is tractable when T is larger.",http://arxiv.org/abs/2305.17206v1,Charles F Manski
476,Design Considerations for a Phase II platform trial in Major Depressive Disorder,"Major Depressive Disorder (MDD) is one of the most common causes of
disability worldwide. Unfortunately, about one-third of patients do not benefit
sufficiently from available treatments and not many new drugs have been
developed in this area in recent years. We thus need better and faster ways to
evaluate many different treatment options quickly. Platform trials are a
possible remedy - they facilitate the evaluation of more investigational
treatments in a shorter period of time by sharing controls, as well as reducing
clinical trial activation and recruitment times. We discuss design
considerations for a platform trial in MDD, taking into account the unique
disease characteristics, and present the results of extensive simulations to
investigate the operating characteristics under various realistic scenarios. To
allow the testing of more treatments, interim futility analyses should be
performed to eliminate treatments that have either no or negligible treatment
effect. Furthermore, we investigate different randomisation and allocation
strategies as well as the impact of the per-treatment arm sample size. We
compare the operating characteristics of such platform trials to those of
traditional randomised controlled trials and highlight the potential advantages
of platform trials.",http://arxiv.org/abs/2310.02080v1,"Michaela Maria Freitag, Dario Zocholl, Elias Laurin Meyer, Stefan M Gold, Marta Bofill Roig, Heidi De Smedt, Martin Posch, Franz Knig"
477,Multiscale modelling of replicated nonstationary time series,"Within the neurosciences, to observe variability across time in the dynamics
of an underlying brain process is neither new nor unexpected. Wavelets are
essential in analyzing brain signals because, even within a single trial, brain
signals exhibit nonstationary behaviour. However, neurological signals
generated within an experiment may also potentially exhibit evolution across
trials (replicates). As neurologists consider localised spectra of brain
signals to be most informative, here we develop a novel wavelet-based tool
capable to formally represent process nonstationarities across both time and
replicate dimensions. Specifically, we propose the Replicate Locally Stationary
Wavelet (RLSW) process, that captures the potential nonstationary behaviour
within and across trials. Estimation using wavelets gives a natural desired
time- and replicate-localisation of the process dynamics. We develop the
associated spectral estimation framework and establish its asymptotic
properties. By means of thorough simulation studies, we demonstrate the
theoretical estimator properties hold in practice. A real data investigation
into the evolutionary dynamics of the hippocampus and nucleus accumbens during
an associative learning experiment, demonstrate the applicability of our
proposed methodology, as well as the new insights it provides.",http://arxiv.org/abs/2005.09440v1,"Jonathan Embleton, Marina I Knight, Hernando Ombao"
478,A symbolic information approach to characterize response-related differences in cortical activity during a Go/No-Go task,"How the brain processes information from external stimuli in order to
perceive the world and act on it is one of the greatest questions in
neuroscience. To address this question different time series analyzes
techniques have been employed to characterize the statistical properties of
brain signals during cognitive tasks. Typically response-specific processes are
addressed by comparing the time course of average event-related potentials in
different trials type. Here we analyze monkey Local Field Potentials data
during visual pattern discrimination called Go/No-Go task in the light of
information theory quantifiers. We show that the Bandt-Pompe symbolization
methodology to calculate entropy and complexity of data is a useful tool to
distinguish response-related differences between Go and No-Go trials. We
propose to use an asymmetry index to statistically validate trial type
differences. Moreover, by using the multi-scale approach and embedding time
delays to downsample the data we can estimate the important time scales in
which the relevant information is been processed.",http://arxiv.org/abs/2101.08905v1,"Helena B Lucas, Steven L Bressler, Fernanda S Matias, Osvaldo A Rosso"
479,A Prospective Randomized Clinical Trial for Measuring Radiology Study Reporting Time on Artificial Intelligence-Based Detection of Intracranial Hemorrhage in Emergent Care Head CT,"We propose Artificial Intelligence Prospective Randomized Observer Blinding
Evaluation (AI-PROBE) for quantitative clinical performance evaluation of
radiology AI systems within prospective randomized clinical trials. AI-PROBE
encompasses a study design and a matching radiology IT infrastructure that
randomly blinds radiologists for results provided by AI-based image analysis.
To demonstrate the applicability of our evaluation framework, we present a
first prospective randomized clinical trial on the effect of Intra-Cranial
Hemorrhage (ICH) detection in emergent care head CT on radiology study
Turn-Around Time (TAT). Here, we acquired 620 non-contrast head CT scans from
inpatient and emergency room patients at a large academic hospital. Following
acquisition, scans were automatically analyzed for the presence of ICH using
commercially available software (Aidoc, Tel Aviv, Israel). Cases identified
positive for ICH by AI (ICH-AI+) were flagged in radiologists' reading
worklists, where flagging was randomly switched off with probability 50%. TAT
was measured as time difference between study completion and first clinically
communicated reporting, with time stamps automatically retrieved from various
IT systems. TATs for flagged cases (73+/-143 min) were significantly lower than
TATs for non-flagged (132+/-193 min) cases (p<0.05, one-sided t-test), where
105 of 122 ICH-AI+ cases were true positive. Total sensitivity, specificity,
and accuracy over all analyzed cases were 95.0%, 96.7%, and 96.4%,
respectively. We conclude that automatic identification of ICH reduces TAT for
ICH in emergent care head CT, which carries the potential for improving timely
clinical management of ICH. Our results suggest that AI-PROBE can contribute to
systematic quantitative evaluation of AI systems in clinical practice using
clinically meaningful quantities, such as TAT or diagnostic accuracy.",http://arxiv.org/abs/2002.12515v1,"Axel Wismller, Larry Stockmaster"
480,Clinical connectivity map for drug repurposing: using laboratory tests to bridge drugs and diseases,"Drug repurposing has attracted increasing attention from both the
pharmaceutical industry and the research community. Many existing computational
drug repurposing methods rely on preclinical data (e.g., chemical structures,
drug targets), resulting in translational problems for clinical trials. In this
study, we propose a clinical connectivity map framework for drug repurposing by
leveraging laboratory tests to analyze complementarity between drugs and
diseases. We establish clinical drug effect vectors (i.e., drug-laboratory test
associations) by applying a continuous self-controlled case series model on a
longitudinal electronic health record data. We establish clinical disease sign
vectors (i.e., disease-laboratory test associations) by applying a Wilcoxon
rank sum test on a large-scale national survey data. Finally, we compute a
repurposing possibility score for each drug-disease pair by applying a dot
product-based scoring function on clinical disease sign vectors and clinical
drug effect vectors. We comprehensively evaluate 392 drugs for 6 important
chronic diseases (e.g., asthma, coronary heart disease, type 2 diabetes, etc.).
We discover not only known associations between diseases and drugs but also
many hidden drug-disease associations. Moreover, we are able to explain the
predicted drug-disease associations via the corresponding complementarity
between laboratory tests of drug effect vectors and disease sign vectors. The
proposed clinical connectivity map framework uses laboratory tests from
electronic clinical information to bridge drugs and diseases, which is
explainable and has better translational power than existing computational
methods. Experimental results demonstrate the effectiveness of the proposed
framework and suggest that our method could help identify drug repurposing
opportunities, which will benefit patients by offering more effective and safer
treatments.",http://arxiv.org/abs/2007.07886v2,"Qianlong Wen, Ruoqi Liu, Ping Zhang"
481,Predicting the clinical citation count of biomedical papers using multilayer perceptron neural network,"The number of clinical citations received from clinical guidelines or
clinical trials has been considered as one of the most appropriate indicators
for quantifying the clinical impact of biomedical papers. Therefore, the early
prediction of the clinical citation count of biomedical papers is critical to
scientific activities in biomedicine, such as research evaluation, resource
allocation, and clinical translation. In this study, we designed a four-layer
multilayer perceptron neural network (MPNN) model to predict the clinical
citation count of biomedical papers in the future by using 9,822,620 biomedical
papers published from 1985 to 2005. We extracted ninety-one paper features from
three dimensions as the input of the model, including twenty-one features in
the paper dimension, thirty-five in the reference dimension, and thirty-five in
the citing paper dimension. In each dimension, the features can be classified
into three categories, i.e., the citation-related features, the clinical
translation-related features, and the topic-related features. Besides, in the
paper dimension, we also considered the features that have previously been
demonstrated to be related to the citation counts of research papers. The
results showed that the proposed MPNN model outperformed the other five
baseline models, and the features in the reference dimension were the most
important.",http://arxiv.org/abs/2210.06346v3,"Xin Li, Xuli Tang, Qikai Cheng"
482,Fusing Trial Data for Treatment Comparisons: Single versus Multi-Span Bridging,"While randomized controlled trials (RCTs) are critical for establishing the
efficacy of new therapies, there are limitations regarding what comparisons can
be made directly from trial data. RCTs are limited to a small number of
comparator arms and often compare a new therapeutic to a standard of care which
has already proven efficacious. It is sometimes of interest to estimate the
efficacy of the new therapy relative to a treatment that was not evaluated in
the same trial, such as a placebo or an alternative therapy that was evaluated
in a different trial. Such multi-study comparisons are challenging because of
potential differences between trial populations that can affect the outcome. In
this paper, two bridging estimators are considered that allow for comparisons
of treatments evaluated in different trials using data fusion methods to
account for measured differences in trial populations. A ""multi-span''
estimator leverages a shared arm between two trials, while a ""single-span''
estimator does not require a shared arm. A diagnostic statistic that compares
the outcome in the standardized shared arms is provided. The two estimators are
compared in simulations, where both estimators demonstrate minimal empirical
bias and nominal confidence interval coverage when the identification
assumptions are met. The estimators are applied to data from the AIDS Clinical
Trials Group 320 and 388 to compare the efficacy of two-drug versus four-drug
antiretroviral therapy on CD4 cell counts among persons with advanced HIV. The
single-span approach requires fewer identification assumptions and was more
efficient in simulations and the application.",http://arxiv.org/abs/2305.00845v1,"Bonnie E ShookSa, Paul N Zivich, Samuel P Rosin, Jessie K Edwards, Adaora A Adimora, Michael G Hudgens, Stephen R Cole"
483,The R.O.A.D. to clinical trial emulation,"Observational studies provide the only evidence on the effectiveness of
interventions when randomized controlled trials (RCTs) are impractical due to
cost, ethical concerns, or time constraints. While many methodologies aim to
draw causal inferences from observational data, there is a growing trend to
model observational study designs after RCTs, a strategy known as ""target trial
emulation."" Despite its potential, causal inference through target trial
emulation cannot fully address the confounding bias in real-world data due to
the lack of randomization. In this work, we present a novel framework for
target trial emulation that aims to overcome several key limitations, including
confounding bias. The framework proceeds as follows: First, we apply the
eligibility criteria of a specific trial to an observational cohort. We then
""correct"" this cohort by extracting a subset that matches both the distribution
of covariates and the baseline prognosis of the control group in the target
RCT. Next, we address unmeasured confounding by adjusting the prognosis
estimates of the treated group to align with those observed in the trial.
Following trial emulation, we go a step further by leveraging the emulated
cohort to train optimal decision trees, to identify subgroups of patients with
heterogeneity in treatment effects (HTE). The absence of confounding is
verified using two external models, and the validity of the treatment
recommendations is independently confirmed by the team responsible for the
original trial we emulate. To our knowledge, this is the first framework to
successfully address both observed and unobserved confounding, a challenge that
has historically limited the use of randomized trial emulation and causal
inference. Additionally, our framework holds promise in advancing precision
medicine by identifying patient subgroups that benefit most from specific
treatments.",http://arxiv.org/abs/2412.03528v1,"Dimitris Bertsimas, Angelos G Koulouras, Hiroshi Nagata, Carol Gao, Junki Mizusawa, Yukihide Kanemitsu, Georgios Antonios Margonis"
484,Leveraging contact network structure in the design of cluster randomized trials,"Background: In settings where proof-of-principle trials have succeeded but
the effectiveness of different forms of implementation remains uncertain,
trials that not only generate information about intervention effects but also
provide public health benefit would be useful. Cluster randomized trials (CRT)
capture both direct and indirect intervention effects; the latter depends
heavily on contact networks within and across clusters. We propose a novel
class of connectivity-informed trial designs that leverages information about
such networks in order to improve public health impact and preserve ability to
detect intervention effects.
  Methods: We consider CRTs in which the order of enrollment is based on the
total number of ties between individuals across clusters (based either on the
total number of inter-cluster connections or on connections only to untreated
clusters). We include options analogous both to traditional Parallel and
Stepped Wedge designs. We also allow for control clusters to be ""held-back""
from re-randomization for some period. We investigate the performance epidemic
control and power to detect vaccine effect performance of these designs by
simulating vaccination trials during an SEIR-type epidemic using a
network-structured agent-based model.
  Results: In our simulations, connectivity-informed designs have lower peak
infectiousness than comparable traditional designs and reduce cumulative
incidence by 20%, but with little impact on time to end of epidemic and reduced
power to detect differences in incidence across clusters. However even a brief
""holdback"" period restores most of the power lost compared to traditional
approaches.
  Conclusion: Incorporating information about cluster connectivity in design of
CRTs can increase their public health impact, especially in acute outbreak
settings, with modest cost in power to detect an effective intervention.",http://arxiv.org/abs/1610.09926v1,"Guy Harling, Rui Wang, JukkaPekka Onnela, Victor De Gruttola"
485,"CROCS: Clustering and Retrieval of Cardiac Signals Based on Patient Disease Class, Sex, and Age","The process of manually searching for relevant instances in, and extracting
information from, clinical databases underpin a multitude of clinical tasks.
Such tasks include disease diagnosis, clinical trial recruitment, and
continuing medical education. This manual search-and-extract process, however,
has been hampered by the growth of large-scale clinical databases and the
increased prevalence of unlabelled instances. To address this challenge, we
propose a supervised contrastive learning framework, CROCS, where
representations of cardiac signals associated with a set of patient-specific
attributes (e.g., disease class, sex, age) are attracted to learnable
embeddings entitled clinical prototypes. We exploit such prototypes for both
the clustering and retrieval of unlabelled cardiac signals based on multiple
patient attributes. We show that CROCS outperforms the state-of-the-art method,
DTC, when clustering and also retrieves relevant cardiac signals from a large
database. We also show that clinical prototypes adopt a semantically meaningful
arrangement based on patient attributes and thus confer a high degree of
interpretability.",http://arxiv.org/abs/2011.14230v2,"Dani Kiyasseh, Tingting Zhu, David A Clifton"
486,Hidden Markov models for alcoholism treatment trial data,"In a clinical trial of a treatment for alcoholism, a common response variable
of interest is the number of alcoholic drinks consumed by each subject each
day, or an ordinal version of this response, with levels corresponding to
abstinence, light drinking and heavy drinking. In these trials, within-subject
drinking patterns are often characterized by alternating periods of heavy
drinking and abstinence. For this reason, many statistical models for time
series that assume steady behavior over time and white noise errors do not fit
alcohol data well. In this paper we propose to describe subjects' drinking
behavior using Markov models and hidden Markov models (HMMs), which are better
suited to describe processes that make sudden, rather than gradual, changes
over time. We incorporate random effects into these models using a hierarchical
Bayes structure to account for correlated responses within subjects over time,
and we estimate the effects of covariates, including a randomized treatment, on
the outcome in a novel way. We illustrate the models by fitting them to a large
data set from a clinical trial of the drug Naltrexone. The HMM, in particular,
fits this data well and also contains unique features that allow for useful
clinical interpretations of alcohol consumption behavior.",http://arxiv.org/abs/1010.1410v1,"Kenneth E Shirley, Dylan S Small, Kevin G Lynch, Stephen A Maisto, David W Oslin"
487,Small-Sample Behavior of Novel Phase I Cancer Trial Designs,"Novel dose-finding designs, using estimation to assign the best estimated
maximum- tolerated-dose (MTD) at each point in the experiment, most commonly
via Bayesian techniques, have recently entered large-scale implementation in
Phase I cancer clinical trials. We examine the small-sample behavior of these
""Bayesian Phase I"" (BP1) designs, and also of non-Bayesian designs sharing the
same main ""long-memory"" traits (hereafter: LMP1s).
  For all LMP1s examined, the number of cohorts treated at the true MTD
(denoted here as n*) was highly variable between numerical runs drawn from the
same toxicity-threshold distribution, especially when compared with
""up-and-down"" (U&D) short-memory designs. Further investigation using the same
set of thresholds in permuted order, produced a nearly-identical magnitude of
variability in n*. Therefore, this LMP1 behavior is driven by a strong
sensitivity to the order in which toxicity thresholds appear in the experiment.
We suggest that the sensitivity is related to LMP1's tendency to ""settle"" early
on a specific dose level - a tendency caused by the repeated likelihood-based
""winner-takes-all"" dose assignment rule, which grants the early cohorts a
disproportionately large influence upon experimental trajectories.
  Presently, U&D designs offer a simpler and more stable alternative, with
roughly equivalent MTD estimation performance. A promising direction for
combining the two approaches is briefly discussed (note: the '3+3' protocol is
not a U&D design).",http://arxiv.org/abs/1202.4962v2,"Assaf P Oron, Peter D Hoff"
488,Mathematical Model of Colorectal Cancer with Monoclonal Antibody Treatments,"We present a new mathematical model of colorectal cancer growth and its
response to monoclonal-antibody (mAb) therapy. Although promising, most mAb
drugs are still in trial phases, and the possible variations in the dosing
schedules of those currently approved for use have not yet been thoroughly
explored. To investigate the effectiveness of current mAb treatment schedules,
and to test hypothetical treatment strategies, we have created a system of
nonlinear ordinary differential equations (ODE) to model colorectal cancer
growth and treatment. The model includes tumor cells, elements of the host's
immune response, and treatments. Model treatments include the chemotherapy
agent irinotecan and one of two monoclonal antibodies - cetuximab, which is
FDA-approved for colorectal cancer, and panitumumab, which is still being
evaluated in clinical trials. The model incorporates patient-specific
parameters to account for individual variations in immune system strength and
in medication efficacy against the tumor. We have simulated outcomes for groups
of virtual patients on treatment protocols for which clinical trial data are
available, using a range of biologically reasonable patient-specific parameter
values. Our results closely match clinical trial results for these protocols.
We also simulated experimental dosing schedules, and have found new schedules
which, in our simulations, reduce tumor size more effectively than current
treatment schedules. Additionally, we examined the system's equilibria and
sensitivity to parameter values. In the absence of treatment, tumor evolution
is most affected by the intrinsic tumor growth rate and carrying capacity. When
treatment is introduced, tumor growth is most affected by drug-specific PK/PD
parameters.",http://arxiv.org/abs/1312.3023v1,"L G dePillis, H Savage, A E Radunskaya"
489,Response adaptive designs for binary responses: how to offer patient benefit while being robust to time trends?,"Response-adaptive randomisation (RAR) can considerably improve the chances of
a successful treatment outcome for patients in a clinical trial by skewing the
allocation probability towards better performing treatments as data
accumulates. There is considerable interest in using RAR designs in drug
development for rare diseases, where traditional designs are not feasible or
ethically objectionable. In this paper we discuss and address a major criticism
of RAR: the undesirable type I error inflation due to unknown time trends in
the trial. Time trends can appear because of changes in the characteristics of
recruited patients - so-called ""patient drift"". Patient drift is a realistic
concern for clinical trials in rare diseases because these typically recruit
patients over a very long period of time. We compute by simulations how large
the type I error inflation is as a function of the time trend magnitude in
order to determine in which contexts a potentially costly correction is
actually necessary. We then assess the ability of different correction methods
to preserve type I error in this context and their performance in terms of
other operating characteristics, including patient benefit and power. We make
recommendations of which correction methods are most suitable in the rare
disease context for several RAR rules, differentiating between the two-armed
and the multi-armed case. We further propose a RAR design for multi-armed
clinical trials, which is computationally cheap and robust to several time
trends considered.",http://arxiv.org/abs/1703.04341v1,"Sofia S Villar, Jack Bowden, James Wason"
490,An information-theoretic Phase I/II design for molecularly targeted agents that does not require an assumption of monotonicity,"For many years Phase I and Phase II clinical trials were conducted
separately, but there was a recent shift to combine these Phases. While a
variety of Phase~I/II model-based designs for cytotoxic agents were proposed in
the literature, methods for molecularly targeted agents (TA) are just starting
to develop. The main challenge of the TA setting is the unknown dose-efficacy
relation that can have either an increasing, plateau or umbrella shape. To
capture these, approaches with more parameters are needed to model the
dose-efficacy relationship or, alternatively, more orderings of the
dose-efficacy relationship are required to account for the uncertainty in the
curve shape. As a result, designs for more complex clinical trials, for
example, trials looking at schedules of a combination treatment involving TA,
have not been extensively studied yet. We propose a novel regimen-finding
design which is based on a derived efficacy-toxicity trade-off function. Due to
its special properties, an accurate regimen selection can be achieved without
any parametric or monotonicity assumptions. We illustrate how this design can
be applied in the context of a complex combination-schedule clinical trial. We
discuss practical and ethical issues such as coherence, delayed and missing
efficacy responses, safety and futility constraints.",http://arxiv.org/abs/1803.04397v2,"Pavel Mozgunov, Thomas Jaki"
491,Generation of digital patients for the simulation of tuberculosis with UISS-TB,"EC funded STriTuVaD project aims to test, through a phase IIb clinical trial,
two of the most advanced therapeutic vaccines against tuberculosis. In
parallel, we have extended the Universal Immune System Simulator to include all
relevant determinants of such clinical trial, to establish its predictive
accuracy against the individual patients recruited in the trial, to use it to
generate digital patients and predict their response to the HRT being tested,
and to combine them to the observations made on physical patients using a new
in silico-augmented clinical trial approach that uses a Bayesian adaptive
design. This approach, where found effective could drastically reduce the cost
of innovation in this critical sector of public healthcare. One of the most
challenging task is to develop a methodology to reproduce biological diversity
of the subjects that have to be simulated, i.e., provide an appropriate
strategy for the generation of libraries of digital patients. This has been
achieved through the the creation of the initial immune system repertoire in a
stochastic way, and though the identification of a ""vector of features"" that
combines both biological and pathophysiological parameters that personalize the
digital patient to reproduce the physiology and the pathophysiology of the
subject.",http://arxiv.org/abs/1910.12293v1,"Marzio Pennisi, Miguel A Juarez, Giulia Russo, Marco Viceconti, Francesco Pappalardo"
492,A Bayesian hierarchical model for bridging across patient subgroups in phase I clinical trials with animal data,"Incorporating preclinical animal data, which can be regarded as a special
kind of historical data, into phase I clinical trials can improve decision
making when very little about human toxicity is known. In this paper, we
develop a robust hierarchical modelling approach to leverage animal data into
new phase I clinical trials, where we bridge across non-overlapping,
potentially heterogeneous patient subgroups. Translation parameters are used to
bring both historical and contemporary data onto a common dosing scale. This
leads to feasible exchangeability assumptions that the parameter vectors, which
underpin the dose-toxicity relationship per study, are assumed to be drawn from
a common distribution. Moreover, human dose-toxicity parameter vectors are
assumed to be exchangeable either with the standardised, animal study-specific
parameter vectors, or between themselves. Possibility of non-exchangeability
for each parameter vector is considered to avoid inferences for extreme
subgroups being overly influenced by the other. We illustrate the proposed
approach with several trial data examples, and evaluate the operating
characteristics of our model compared with several alternatives in a simulation
study. Numerical results show that our approach yields robust inferences in
circumstances, where data from multiple sources are inconsistent and/or the
bridging assumptions are incorrect.",http://arxiv.org/abs/1911.05592v1,"Haiyan Zheng, Lisa V Hampson, Thomas Jaki"
493,"Advanced models for predicting event occurrence in event-driven clinical trials accounting for patient dropout, cure and ongoing recruitment","We consider event-driven clinical trials, where the analysis is performed
once a pre-determined number of clinical events has been reached. For example,
these events could be progression in oncology or a stroke in cardiovascular
trials. At the interim stage, one of the main tasks is predicting the number of
events over time and the time to reach specific milestones, where we need to
account for events that may occur not only in patients already recruited and
are followed-up but also in patients yet to be recruited. Therefore, in such
trials we need to model patient recruitment and event counts together. In the
paper we develop a new analytic approach which accounts for the opportunity of
patients to be cured, as well as for them to dropout and be lost to follow-up.
Recruitment is modelled using a Poisson-gamma model developed in previous
publications. When considering the occurrence of events, we assume that the
time to the main event and the time to dropout are independent random
variables, and we have developed a few advanced models with cure using
exponential, Weibull and log-normal distributions. This technique is supported
by well developed, tested and documented software. The results are illustrated
using simulation and a real dataset with reference to the developed software.",http://arxiv.org/abs/2108.09196v1,"Vladimir Anisimov, Stephen Gormley, Rosalind Baverstock, Cynthia Kineza"
494,Covariate Adjustment in Randomized Clinical Trials with Missing Covariate and Outcome Data,"When analyzing data from randomized clinical trials, covariate adjustment can
be used to account for chance imbalance in baseline covariates and to increase
precision of the treatment effect estimate. A practical barrier to covariate
adjustment is the presence of missing data. In this paper, in the light of
recent theoretical advancement, we first review several covariate adjustment
methods with incomplete covariate data. We investigate the implications of the
missing data mechanism on estimating the average treatment effect in randomized
clinical trials with continuous or binary outcomes. In parallel, we consider
settings where the outcome data are fully observed or are missing at random; in
the latter setting, we propose a full weighting approach that combines inverse
probability weighting for adjusting missing outcomes and overlap weighting for
covariate adjustment. We highlight the importance of including the interaction
terms between the missingness indicators and covariates as predictors in the
models. We conduct comprehensive simulation studies to examine the
finite-sample performance of the proposed methods and compare with a range of
common alternatives. We find that conducting the proposed adjustment methods
generally improves the precision of treatment effect estimates regardless of
the imputation methods when the adjusted covariate is associated with the
outcome. We apply the methods to the Childhood Adenotonsillectomy Trial to
assess the effect of adenotonsillectomy on neurocognitive functioning scores.",http://arxiv.org/abs/2207.07890v2,"ChiaRui Chang, Yue Song, Fan Li, Rui Wang"
495,Sensitivity Analyses of Clinical Trial Designs: Selecting Scenarios and Summarizing Operating Characteristics,"The use of simulation-based sensitivity analyses is fundamental to evaluate
and compare candidate designs for future clinical trials. In this context,
sensitivity analyses are especially useful to assess the dependence of
important design operating characteristics (OCs) with respect to various
unknown parameters (UPs). Typical examples of OCs include the likelihood of
detecting treatment effects and the average study duration, which depend on UPs
that are not known until after the onset of the clinical study, such as the
distributions of the primary outcomes and patient profiles. Two crucial
components of sensitivity analyses are (i) the choice of a set of plausible
simulation scenarios $\{\boldsymbol{\theta}_1,...,\boldsymbol{\theta}_K\}$ and
(ii) the list of OCs of interest. We propose a new approach to choose the set
of scenarios for inclusion in design sensitivity analyses. Our approach
balances the need for simplicity and interpretability of OCs computed across
several scenarios with the need to faithfully summarize -- through simulations
-- how the OCs vary across all plausible values of the UPs. Our proposal also
supports the selection of the number of simulation scenarios to be included in
the final sensitivity analysis report. To achieve these goals, we minimize a
loss function $\mathcal{L}(\boldsymbol{\theta}_1,...,\boldsymbol{\theta}_K)$
that formalizes whether a specific set of $K$ sensitivity scenarios
$\{\boldsymbol{\theta}_1,...,\boldsymbol{\theta}_K\}$ is adequate to summarize
how the OCs of the trial design vary across all plausible values of the UPs.
Then, we use optimization techniques to select the best set of simulation
scenarios to exemplify the OCs of the trial design.",http://arxiv.org/abs/2208.03887v1,"Larry Han, Andrea Arfe, Lorenzo Trippa"
496,Adaptive Identification of Populations with Treatment Benefit in Clinical Trials: Machine Learning Challenges and Solutions,"We study the problem of adaptively identifying patient subpopulations that
benefit from a given treatment during a confirmatory clinical trial. This type
of adaptive clinical trial has been thoroughly studied in biostatistics, but
has been allowed only limited adaptivity so far. Here, we aim to relax
classical restrictions on such designs and investigate how to incorporate ideas
from the recent machine learning literature on adaptive and online
experimentation to make trials more flexible and efficient. We find that the
unique characteristics of the subpopulation selection problem -- most
importantly that (i) one is usually interested in finding subpopulations with
any treatment benefit (and not necessarily the single subgroup with largest
effect) given a limited budget and that (ii) effectiveness only has to be
demonstrated across the subpopulation on average -- give rise to interesting
challenges and new desiderata when designing algorithmic solutions. Building on
these findings, we propose AdaGGI and AdaGCPI, two meta-algorithms for
subpopulation construction. We empirically investigate their performance across
a range of simulation scenarios and derive insights into their (dis)advantages
across different settings.",http://arxiv.org/abs/2208.05844v2,"Alicia Curth, Alihan Hyk, Mihaela van der Schaar"
497,Enrollment Forecast for Clinical Trials at the Portfolio Planning Phase Based on Site-Level Historical Data,"Accurate forecast of a clinical trial enrollment timeline at the planning
phase is of great importance to both corporate strategic planning and trial
operational excellence. While predictions of key milestones such as last
subject first dose date can inform strategic decision-making, detailed
predictive insights (e.g., median number of enrolled subjects by month for a
country) can facilitate the planning of clinical trial operation activities and
promote execution excellence. The naive approach often calculates an average
enrollment rate from historical data and generates an inaccurate prediction
based on a linear trend with the average rate. The traditional statistical
approach utilizes the simple Poisson-Gamma model that assumes time-invariant
site activation rates and it can fail to capture the underlying nonlinear
patterns (e.g., up and down site activation pattern). We present a novel
statistical approach based on generalized linear mixed-effects models and the
use of non-homogeneous Poisson processes through Bayesian framework to model
the country initiation, site activation and subject enrollment sequentially in
a systematic fashion. We validate the performance of our proposed enrollment
modeling framework based on a set of preselected 25 studies from four
therapeutic areas. Our modeling framework shows a substantial improvement in
prediction accuracy in comparison to the traditional statistical approach.
Furthermore, we show that our modeling and simulation approach calibrates the
data variability appropriately and gives correct coverage rates for prediction
intervals of various nominal levels. Finally, we demonstrate the use of our
approach to generate the predicted enrollment curves through time with
confidence bands overlaid.",http://arxiv.org/abs/2301.01351v1,"Sheng Zhong, Yunzhao Xing, Mengjia Yu, Li Wang"
498,The Clinical Trials Puzzle: How Network Effects Limit Drug Discovery,"The depth of knowledge offered by post-genomic medicine has carried the
promise of new drugs, and cures for multiple diseases. To explore the degree to
which this capability has materialized, we extract meta-data from 356,403
clinical trials spanning four decades, aiming to offer mechanistic insights
into the innovation practices in drug discovery. We find that convention
dominates over innovation, as over 96% of the recorded trials focus on
previously tested drug targets, and the tested drugs target only 12% of the
human interactome. If current patterns persist, it would take 170 years to
target all druggable proteins. We uncover two network-based fundamental
mechanisms that currently limit target discovery: preferential attachment,
leading to the repeated exploration of previously targeted proteins; and local
network effects, limiting exploration to proteins interacting with highly
explored proteins. We build on these insights to develop a quantitative
network-based model of drug discovery. We demonstrate that the model is able to
accurately recreate the exploration patterns observed in clinical trials. Most
importantly, we show that a network-based search strategy can widen the scope
of drug discovery by guiding exploration to novel proteins that are part of
under explored regions in the human interactome.",http://arxiv.org/abs/2301.10709v1,"Kishore Vasan, Deisy Gysi, AlbertLaszlo Barabasi"
499,PULSAR Effect: Revealing Potential Synergies in Combined Radiation Therapy and Immunotherapy via Differential Equations,"PULSAR (personalized ultrafractionated stereotactic adaptive radiotherapy) is
a form of radiotherapy method where a patient is given a large dose or pulse of
radiation a couple of weeks apart rather than daily small doses. The tumor
response is then monitored to determine when the subsequent pulse should be
given. Pre-clinical trials have shown better tumor response in mice that
received immunotherapy along with pulses spaced 10 days apart. However, this
was not the case when the pulses were 1 day apart. Therefore, a synergistic
effect between immunotherapy and PULSAR is observed when the pulses are spaced
out by a certain number of days. In our study, we aimed to develop a
mathematical model that can capture the synergistic effect by considering a
time-dependent weight function that takes into account the spacing between
pulses. By determining feasible parameters, and applying reasonable conditions,
we utilize our model to simulate murine trials with varying sequencing of
pulses. We successfully demonstrate that our model is simple to implement and
can generate tumor volume data that is consistent with the pre-clinical trial
data. Our model has the potential to aid in the development of clinical trials
of PULSAR therapy.",http://arxiv.org/abs/2402.06101v1,"Samiha Rouf, Casey Moore, Debabrata Saha, Dan Nguyen, MaryLena Bleile, Robert Timmerman, Hao Peng, Steve Jiang"
