docid|title|abstract|url|authors
0|Bayesian Models and Decision Algorithms for Complex Early Phase Clinical Trials|An early phase clinical trial is the first step in evaluating the effects in humans of a potential new anti-disease agent or combination of agents. Usually called "phase I" or "phase I/II" trials, these experiments typically have the nominal scientific goal of determining an acceptable dose, most often based on adverse event probabilities. This arose from a tradition of phase I trials to evaluate cytotoxic agents for treating cancer, although some methods may be applied in other medical settings, such as treatment of stroke or immunological diseases. Most modern statistical designs for early phase trials include model-based, outcome-adaptive decision rules that choose doses for successive patient cohorts based on data from previous patients in the trial. Such designs have seen limited use in clinical practice, however, due to their complexity, the requirement of intensive, computer-based data monitoring, and the medical community's resistance to change. Still, many actual applications of model-based outcome-adaptive designs have been remarkably successful in terms of both patient benefit and scientific outcome. In this paper I will review several Bayesian early phase trial designs that were tailored to accommodate specific complexities of the treatment regime and patient outcomes in particular clinical settings.|http://arxiv.org/abs/1011.6494v1|Peter F. Thall
1|Deep Historical Borrowing Framework to Prospectively and Simultaneously Synthesize Control Information in Confirmatory Clinical Trials with Multiple Endpoints|In current clinical trial development, historical information is receiving more attention as it provides utility beyond sample size calculation. Meta-analytic-predictive (MAP) priors and robust MAP priors have been proposed for prospectively borrowing historical data on a single endpoint. To simultaneously synthesize control information from multiple endpoints in confirmatory clinical trials, we propose to approximate posterior probabilities from a Bayesian hierarchical model and estimate critical values by deep learning to construct pre-specified strategies for hypothesis testing. This feature is important to ensure study integrity by establishing prospective decision functions before the trial conduct. Simulations are performed to show that our method properly controls family-wise error rate (FWER) and preserves power as compared with a typical practice of choosing constant critical values given a subset of null space. Satisfactory performance under prior-data conflict is also demonstrated. We further illustrate our method using a case study in Immunology.|http://arxiv.org/abs/2008.12774v2|Tianyu Zhan,Yiwang Zhou,Ziqian Geng,Yihua Gu,Jian Kang,Li Wang,Xiaohong Huang,Elizabeth H. Slate
2|Parametric Resonance May Explain Virologic Failure to HIV Treatment Interruptions|Pilot studies of structured treatment interruptions (STI) in HIV therapy have shown that patients can maintain low viral loads whilst benefiting from reduced treatment toxicity. However, a recent STI clinical trial reported a high degree of virologic failure. Here we present a novel hypothesis that could explain virologic failure to STI and provides new insights of great clinical relevance. We analyze a classic mathematical model of HIV within-host viral dynamics and find that nonlinear parametric resonance occurs when STI are added to the model; resonance is observed as virologic failure. We use the model to simulate clinical trial data and to calculate patient-specific resonant spectra. We gain two important insights. Firstly, within an STI trial, we determine that patients who begin with similar viral loads can be expected to show extremely different virologic responses as a result of resonance. Thus, high heterogeneity of patient response within a STI clinical trial is to be expected. Secondly and more importantly, we determine that virologic failure is not simply due to STI or patient characteristics; rather it is the result of a complex dynamic interaction between STI and patient viral dynamics. Hence, our analyses demonstrate that no universal regimen with periodic interruptions will be effective for all patients. On the basis of our results, we suggest that immunologic and virologic parameters should be used to design patient-specific STI regimens.|http://arxiv.org/abs/q-bio/0504031v1|Romulus Breban,Sally Blower
3|Efficient nonparametric inference on the effects of stochastic interventions under two-phase sampling, with applications to vaccine efficacy trials|The advent and subsequent widespread availability of preventive vaccines has altered the course of public health over the past century. Despite this success, effective vaccines to prevent many high-burden diseases, including HIV, have been slow to develop. Vaccine development can be aided by the identification of immune response markers that serve as effective surrogates for clinically significant infection or disease endpoints. However, measuring immune response marker activity is often costly, which has motivated the usage of two-phase sampling for immune response evaluation in clinical trials of preventive vaccines. In such trials, the measurement of immunological markers is performed on a subset of trial participants, where enrollment in this second phase is potentially contingent on the observed study outcome and other participant-level information. We propose nonparametric methodology for efficiently estimating a counterfactual parameter that quantifies the impact of a given immune response marker on the subsequent probability of infection. Along the way, we fill in theoretical gaps pertaining to the asymptotic behavior of nonparametric efficient estimators in the context of two-phase sampling, including a multiple robustness property enjoyed by our estimators. Techniques for constructing confidence intervals and hypothesis tests are presented, and an open source software implementation of the methodology, the txshift R package, is introduced. We illustrate the proposed techniques using data from a recent preventive HIV vaccine efficacy trial.|http://arxiv.org/abs/2003.13771v2|Nima S. Hejazi,Mark J. van der Laan,Holly E. Janes,Peter B. Gilbert,David C. Benkeser
4|DEMO: Dose Exploration, Monitoring, and Optimization Using a Biological Mediator for Clinical Outcomes|Phase 1-2 designs provide a methodological advance over phase 1 designs for dose finding by using both clinical response and toxicity. A phase 1-2 trial still may fail to select a truly optimal dose. because early response is not a perfect surrogate for long term therapeutic success. To address this problem, a generalized phase 1-2 design first uses a phase 1-2 design's components to identify a set of candidate doses, adaptively randomizes patients among the candidates, and after longer follow up selects a dose to maximize long-term success rate. In this paper, we extend this paradigm by proposing a design that exploits an early treatment-related, real-valued biological outcome, such as pharmacodynamic activity or an immunological effect, that may act as a mediator between dose and clinical outcomes, including tumor response, toxicity, and survival time. We assume multivariate dose-outcome models that include effects appearing in causal pathways from dose to the clinical outcomes. Bayesian model selection is used to identify and eliminate biologically inactive doses. At the end of the trial, a therapeutically optimal dose is chosen from the set of doses that are acceptably safe, clinically effective, and biologically active to maximize restricted mean survival time. Results of a simulation study show that the proposed design may provide substantial improvements over designs that ignore the biological variable.|http://arxiv.org/abs/2404.02120v1|Cheng-Han Yang,Peter F. Thall,Ruitao Lin
5|Unlocking Historical Clinical Trial Data with ALIGN: A Compositional Large Language Model System for Medical Coding|The reuse of historical clinical trial data has significant potential to accelerate medical research and drug development. However, interoperability challenges, particularly with missing medical codes, hinders effective data integration across studies. While Large Language Models (LLMs) offer a promising solution for automated coding without labeled data, current approaches face challenges on complex coding tasks. We introduce ALIGN, a novel compositional LLM-based system for automated, zero-shot medical coding. ALIGN follows a three-step process: (1) diverse candidate code generation; (2) self-evaluation of codes and (3) confidence scoring and uncertainty estimation enabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing medication terms into Anatomical Therapeutic Chemical (ATC) and medical history terms into Medical Dictionary for Regulatory Activities (MedDRA) codes extracted from 22 immunology trials. ALIGN outperformed the LLM baselines, while also providing capabilities for trustworthy deployment. For MedDRA coding, ALIGN achieved high accuracy across all levels, matching RAG and excelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN demonstrated superior performance, particularly at lower hierarchy levels (ATC Level 4), with 72-73% overall accuracy and 86-89% accuracy for common medications, outperforming baselines by 7-22%. ALIGN's uncertainty-based deferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably enhancing performance on uncommon medications. ALIGN achieves this cost-efficiently at \$0.0007 and \$0.02 per code for GPT-4o-mini and GPT-4o, reducing barriers to clinical adoption. ALIGN advances automated medical coding for clinical trial data, contributing to enhanced data interoperability and reusability, positioning it as a promising tool to improve clinical research and accelerate drug development.|http://arxiv.org/abs/2411.13163v1|Nabeel Seedat,Caterina Tozzi,Andrea Hita Ardiaca,Mihaela van der Schaar,James Weatherall,Adam Taylor
6|An immunological autobiography: my year as a COVID-19 vaccine trial participant|I present here longitudinal evaluation of T and B cell immunity to SARS-CoV2 and variants of concern (VOC) from a single subject (me) over an entire year post vaccination. After enrolling in the Moderna phase III clinical trial, I collected my own biological samples pre- and post-immunization in the event of being a recipient of the experimental vaccine. The evidence strongly supports the conclusion that I did not receive the placebo. The analysis is admittedly limited to an n of 1, but the results fit well with data taken from published works and represent one of the more comprehensive longitudinal evaluations of vaccine-elicited immunity within a single individual yet to be undertaken. Though the data amount to a well-documented anecdote, given its granularity, it is not without its insights and may be of further use in directing future longitudinal studies that have actual statistical significance.|http://arxiv.org/abs/2111.01282v2|Ross M. Kedl
7|What Radio Waves Tell Us about Sleep|The ability to assess sleep at home, capture sleep stages, and detect the occurrence of apnea (without on-body sensors) simply by analyzing the radio waves bouncing off people's bodies while they sleep is quite powerful. Such a capability would allow for longitudinal data collection in patients' homes, informing our understanding of sleep and its interaction with various diseases and their therapeutic responses, both in clinical trials and routine care. In this article, we develop an advanced machine learning algorithm for passively monitoring sleep and nocturnal breathing from radio waves reflected off people while asleep. Validation results in comparison with the gold standard (i.e., polysomnography) (n=849) demonstrate that the model captures the sleep hypnogram (with an accuracy of 81% for 30-second epochs categorized into Wake, Light Sleep, Deep Sleep, or REM), detects sleep apnea (AUROC = 0.88), and measures the patient's Apnea-Hypopnea Index (ICC=0.95; 95% CI = [0.93, 0.97]). Notably, the model exhibits equitable performance across race, sex, and age. Moreover, the model uncovers informative interactions between sleep stages and a range of diseases including neurological, psychiatric, cardiovascular, and immunological disorders. These findings not only hold promise for clinical practice and interventional trials but also underscore the significance of sleep as a fundamental component in understanding and managing various diseases.|http://arxiv.org/abs/2405.11739v2|Hao He,Chao Li,Wolfgang Ganglberger,Kaileigh Gallagher,Rumen Hristov,Michail Ouroutzoglou,Haoqi Sun,Jimeng Sun,Brandon Westover,Dina Katabi
8|Distinguishing immunological and behavioral effects of vaccination|The interpretation of vaccine efficacy estimands is subtle, even in randomized trials designed to quantify immunological effects of vaccination. In this article, we introduce terminology to distinguish between different vaccine efficacy estimands and clarify their interpretations. This allows us to explicitly consider immunological and behavioural effects of vaccination, and establish that policy-relevant estimands can differ substantially from those commonly reported in vaccine trials. We further show that a conventional vaccine trial allows identification and estimation of different vaccine estimands under plausible conditions, if one additional post-treatment variable is measured. Specifically, we utilize a ``belief variable'' that indicates the treatment an individual believed they had received. The belief variable is similar to ``blinding assessment'' variables that are occasionally collected in placebo-controlled trials in other fields. We illustrate the relations between the different estimands, and their practical relevance, in numerical examples based on an influenza vaccine trial.|http://arxiv.org/abs/2311.08335v1|Mats Stensrud,Daniel Nevo,Uri Obolski
9|Panacea: A foundation model for clinical trial search, summarization, design, and recruitment|Clinical trials are fundamental in developing new drugs, medical devices, and treatments. However, they are often time-consuming and have low success rates. Although there have been initial attempts to create large language models (LLMs) for clinical trial design and patient-trial matching, these models remain task-specific and not adaptable to diverse clinical trial tasks. To address this challenge, we propose a clinical trial foundation model named Panacea, designed to handle multiple tasks, including trial search, trial summarization, trial design, and patient-trial matching. We also assemble a large-scale dataset, named TrialAlign, of 793,279 trial documents and 1,113,207 trial-related scientific papers, to infuse clinical knowledge into the model by pre-training. We further curate TrialInstruct, which has 200,866 of instruction data for fine-tuning. These resources enable Panacea to be widely applicable for a range of clinical trial tasks based on user requirements.   We evaluated Panacea on a new benchmark, named TrialPanorama, which covers eight clinical trial tasks. Our method performed the best on seven of the eight tasks compared to six cutting-edge generic or medicine-specific LLMs. Specifically, Panacea showed great potential to collaborate with human experts in crafting the design of eligibility criteria, study arms, and outcome measures, in multi-round conversations. In addition, Panacea achieved 14.42% improvement in patient-trial matching, 41.78% to 52.02% improvement in trial search, and consistently ranked at the top for five aspects of trial summarization. Our approach demonstrates the effectiveness of Panacea in clinical trials and establishes a comprehensive resource, including training data, model, and benchmark, for developing clinical trial foundation models, paving the path for AI-based clinical trial development.|http://arxiv.org/abs/2407.11007v1|Jiacheng Lin,Hanwen Xu,Zifeng Wang,Sheng Wang,Jimeng Sun
10|Assessment of Immune Correlates of Protection via Controlled Vaccine Efficacy and Controlled Risk|Immune correlates of protection (CoPs) are immunologic biomarkers accepted as a surrogate for an infectious disease clinical endpoint and thus can be used for traditional or provisional vaccine approval. To study CoPs in randomized, placebo-controlled trials, correlates of risk (CoRs) are first assessed in vaccine recipients. This analysis does not assess causation, as a CoR may fail to be a CoP. We propose a causal CoP analysis that estimates the controlled vaccine efficacy curve across biomarker levels $s$, $CVE(s)$, equal to one minus the ratio of the controlled-risk curve $r_C(s)$ at $s$ and placebo risk, where $r_C(s)$ is causal risk if all participants are assigned vaccine and the biomarker is set to $s$. The criterion for a useful CoP is wide variability of $CVE(s)$ in $s$. Moreover, estimation of $r_C(s)$ is of interest in itself, especially in studies without a placebo arm. For estimation of $r_C(s)$, measured confounders can be adjusted for by any regression method that accommodates missing biomarkers, to which we add sensitivity analysis to quantify robustness of CoP evidence to unmeasured confounding. Application to two harmonized phase 3 trials supports that 50% neutralizing antibody titer has value as a controlled vaccine efficacy CoP for virologically confirmed dengue (VCD): in CYD14 the point estimate (95% confidence interval) for $CVE(s)$ accounting for measured confounders and building in conservative margin for unmeasured confounding increases from 29.6% (95% CI 3.5 to 45.9) at titer 1:36 to 78.5% (95% CI 67.9 to 86.8) at titer 1:1200; these estimates are 17.4% (95% CI -14.4 to 36.5) and 84.5% (95% CI 79.6 to 89.1) for CYD15.|http://arxiv.org/abs/2107.05734v1|Peter B. Gilbert,Youyi Fong,Marco Carone
11|Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision|Clinical trials are essential for drug development but are extremely expensive and time-consuming to conduct. It is beneficial to study similar historical trials when designing a clinical trial. However, lengthy trial documents and lack of labeled data make trial similarity search difficult. We propose a zero-shot clinical trial retrieval method, Trial2Vec, which learns through self-supervision without annotating similar clinical trials. Specifically, the meta-structure of trial documents (e.g., title, eligibility criteria, target disease) along with clinical knowledge (e.g., UMLS knowledge base https://www.nlm.nih.gov/research/umls/index.html) are leveraged to automatically generate contrastive samples. Besides, Trial2Vec encodes trial documents considering meta-structure thus producing compact embeddings aggregating multi-aspect information from the whole document. We show that our method yields medically interpretable embeddings by visualization and it gets a 15% average improvement over the best baselines on precision/recall for trial retrieval, which is evaluated on our labeled 1600 trial pairs. In addition, we prove the pre-trained embeddings benefit the downstream trial outcome prediction task over 240k trials. Software ias available at https://github.com/RyanWangZf/Trial2Vec.|http://arxiv.org/abs/2206.14719v2|Zifeng Wang,Jimeng Sun
12|SPOT: Sequential Predictive Modeling of Clinical Trial Outcome with Meta-Learning|Clinical trials are essential to drug development but time-consuming, costly, and prone to failure. Accurate trial outcome prediction based on historical trial data promises better trial investment decisions and more trial success. Existing trial outcome prediction models were not designed to model the relations among similar trials, capture the progression of features and designs of similar trials, or address the skewness of trial data which causes inferior performance for less common trials.   To fill the gap and provide accurate trial outcome prediction, we propose Sequential Predictive mOdeling of clinical Trial outcome (SPOT) that first identifies trial topics to cluster the multi-sourced trial data into relevant trial topics. It then generates trial embeddings and organizes them by topic and time to create clinical trial sequences. With the consideration of each trial sequence as a task, it uses a meta-learning strategy to achieve a point where the model can rapidly adapt to new tasks with minimal updates. In particular, the topic discovery module enables a deeper understanding of the underlying structure of the data, while sequential learning captures the evolution of trial designs and outcomes. This results in predictions that are not only more accurate but also more interpretable, taking into account the temporal patterns and unique characteristics of each trial topic. We demonstrate that SPOT wins over the prior methods by a significant margin on trial outcome benchmark data: with a 21.5\% lift on phase I, an 8.9\% lift on phase II, and a 5.5\% lift on phase III trials in the metric of the area under precision-recall curve (PR-AUC).|http://arxiv.org/abs/2304.05352v1|Zifeng Wang,Cao Xiao,Jimeng Sun
13|Automatically Labeling $200B Life-Saving Datasets: A Large Clinical Trial Outcome Benchmark|The global cost of drug discovery and development exceeds $200 billion annually. The main results of drug discovery and development are the outcomes of clinical trials, which directly influence the regulatory approval of new drug candidates and ultimately affect patient outcomes. Despite their significance, large-scale, high-quality clinical trial outcome data are not readily available to the public. Suppose a large clinical trial outcome dataset is provided; machine learning researchers can potentially develop accurate prediction models using past trials and outcome labels, which could help prioritize and optimize therapeutic programs, ultimately benefiting patients. This paper introduces Clinical Trial Outcome (CTO) dataset, the largest trial outcome dataset with around 479K clinical trials, aggregating outcomes from multiple sources of weakly supervised labels, minimizing the noise from individual sources, and eliminating the need for human annotation. These sources include large language model (LLM) decisions on trial-related documents, news headline sentiments, stock prices of trial sponsors, trial linkages across phases, and other signals such as patient dropout rates and adverse events. CTO's labels show unprecedented agreement with supervised clinical trial outcome labels from test split of the supervised TOP dataset, with a 91 F1.|http://arxiv.org/abs/2406.10292v1|Chufan Gao,Jathurshan Pradeepkumar,Trisha Das,Shivashankar Thati,Jimeng Sun
14|Smooth and probabilistic PARAFAC model with auxiliary covariates|In immunological and clinical studies, matrix-valued time-series data clustering is increasingly popular. Researchers are interested in finding low-dimensional embedding of subjects based on potentially high-dimensional longitudinal features and investigating relationships between static clinical covariates and the embedding. These studies are often challenging due to high dimensionality, as well as the sparse and irregular nature of sample collection along the time dimension. We propose a smoothed probabilistic PARAFAC model with covariates (SPACO) to tackle these two problems while utilizing auxiliary covariates of interest. We provide intensive simulations to test different aspects of SPACO and demonstrate its use on an immunological data set from patients with SARs-CoV-2 infection.|http://arxiv.org/abs/2104.05184v3|Leying Guan
15|Optimal Allocation of Gold Standard Testing under Constrained Availability: Application to Assessment of HIV Treatment Failure|The World Health Organization (WHO) guidelines for monitoring the effectiveness of HIV treatment in resource-limited settings (RLS) are mostly based on clinical and immunological markers (e.g., CD4 cell counts). Recent research indicates that the guidelines are inadequate and can result in high error rates. Viral load (VL) is considered the "gold standard", yet its widespread use is limited by cost and infrastructure. In this paper, we propose a diagnostic algorithm that uses information from routinely-collected clinical and immunological markers to guide a selective use of VL testing for diagnosing HIV treatment failure, under the assumption that VL testing is available only at a certain portion of patient visits. Our algorithm identifies the patient subpopulation, such that the use of limited VL testing on them minimizes a pre-defined risk (e.g., misdiagnosis error rate). Diagnostic properties of our proposal algorithm are assessed by simulations. For illustration, data from the Miriam Hospital Immunology Clinic (RI, USA) are analyzed.|http://arxiv.org/abs/2010.00692v1|Tao Liu,Joseph W. Hogan,Lisa Wang,Shangxuan Zhang,Rami Kantor
16|Exploring the Generalization of Cancer Clinical Trial Eligibility Classifiers Across Diseases|Clinical trials are pivotal in medical research, and NLP can enhance their success, with application in recruitment. This study aims to evaluate the generalizability of eligibility classification across a broad spectrum of clinical trials. Starting with phase 3 cancer trials, annotated with seven eligibility exclusions, then to determine how well models can generalize to non-cancer and non-phase 3 trials. To assess this, we have compiled eligibility criteria data for five types of trials: (1) additional phase 3 cancer trials, (2) phase 1 and 2 cancer trials, (3) heart disease trials, (4) type 2 diabetes trials, and (5) observational trials for any disease, comprising 2,490 annotated eligibility criteria across seven exclusion types. Our results show that models trained on the extensive cancer dataset can effectively handle criteria commonly found in non-cancer trials, such as autoimmune diseases. However, they struggle with criteria disproportionately prevalent in cancer trials, like prior malignancy. We also experiment with few-shot learning, demonstrating that a limited number of disease-specific examples can partially overcome this performance gap. We are releasing this new dataset of annotated eligibility statements to promote the development of cross-disease generalization in clinical trial classification.|http://arxiv.org/abs/2403.17135v1|Yumeng Yang,Ashley Gilliam,Ethan B Ludmir,Kirk Roberts
17|TrialSynth: Generation of Synthetic Sequential Clinical Trial Data|Analyzing data from past clinical trials is part of the ongoing effort to optimize the design, implementation, and execution of new clinical trials and more efficiently bring life-saving interventions to market. While there have been recent advances in the generation of static context synthetic clinical trial data, due to both limited patient availability and constraints imposed by patient privacy needs, the generation of fine-grained synthetic time-sequential clinical trial data has been challenging. Given that patient trajectories over an entire clinical trial are of high importance for optimizing trial design and efforts to prevent harmful adverse events, there is a significant need for the generation of high-fidelity time-sequence clinical trial data. Here we introduce TrialSynth, a Variational Autoencoder (VAE) designed to address the specific challenges of generating synthetic time-sequence clinical trial data. Distinct from related clinical data VAE methods, the core of our method leverages Hawkes Processes (HP), which are particularly well-suited for modeling event-type and time gap prediction needed to capture the structure of sequential clinical trial data. Our experiments demonstrate that TrialSynth surpasses the performance of other comparable methods that can generate sequential clinical trial data at varying levels of fidelity / privacy tradeoff, enabling the generation of highly accurate event sequences across multiple real-world sequential event datasets with small patient source populations. Notably, our empirical findings highlight that TrialSynth not only outperforms existing clinical sequence-generating methods but also produces data with superior utility while empirically preserving patient privacy.|http://arxiv.org/abs/2409.07089v2|Chufan Gao,Mandis Beigi,Afrah Shafquat,Jacob Aptekar,Jimeng Sun
18|Clinical Trial Information Extraction with BERT|Natural language processing (NLP) of clinical trial documents can be useful in new trial design. Here we identify entity types relevant to clinical trial design and propose a framework called CT-BERT for information extraction from clinical trial text. We trained named entity recognition (NER) models to extract eligibility criteria entities by fine-tuning a set of pre-trained BERT models. We then compared the performance of CT-BERT with recent baseline methods including attention-based BiLSTM and Criteria2Query. The results demonstrate the superiority of CT-BERT in clinical trial NLP.|http://arxiv.org/abs/2110.10027v1|Xiong Liu,Greg L. Hersch,Iya Khalil,Murthy Devarakonda
19|CTP-LLM: Clinical Trial Phase Transition Prediction Using Large Language Models|New medical treatment development requires multiple phases of clinical trials. Despite the significant human and financial costs of bringing a drug to market, less than 20% of drugs in testing will make it from the first phase to final approval. Recent literature indicates that the design of the trial protocols significantly contributes to trial performance. We investigated Clinical Trial Outcome Prediction (CTOP) using trial design documents to predict phase transitions automatically. We propose CTP-LLM, the first Large Language Model (LLM) based model for CTOP. We also introduce the PhaseTransition (PT) Dataset; which labels trials based on their progression through the regulatory process and serves as a benchmark for CTOP evaluation. Our fine-tuned GPT-3.5-based model (CTP-LLM) predicts clinical trial phase transition by analyzing the trial's original protocol texts without requiring human-selected features. CTP-LLM achieves a 67% accuracy rate in predicting trial phase transitions across all phases and a 75% accuracy rate specifically in predicting the transition from Phase~III to final approval. Our experimental performance highlights the potential of LLM-powered applications in forecasting clinical trial outcomes and assessing trial design.|http://arxiv.org/abs/2408.10995v1|Michael Reinisch,Jianfeng He,Chenxi Liao,Sauleh Ahmad Siddiqui,Bei Xiao
20|Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial Generation|Machine learning (ML) exhibits promise in the clinical domain. However, it is constrained by data scarcity and ethical considerations, as the generation of clinical trials presents significant challenges due to stringent privacy regulations, high costs, and the extended duration required for conducting studies with human participants. Despite the advancements of large language models (LLMs) in general generation tasks, their potential in facilitating the generation of synthetic clinical trials is under-explored. To address this gap, we introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs to generate artificial yet realistic and diverse clinical trials with binary success/failure labels. Experiments conducted on real clinical trials from the \url{ClinicalTrials.gov} database demonstrate that our synthetic data can effectively augment real datasets. Furthermore, by fine-tuning a pre-trained model as a binary classifier on synthetic clinical trial datasets, we demonstrate that this augmentation enhances model training for downstream tasks such as trial outcome prediction. Our findings suggest that LLMs for synthetic clinical trial generation hold promise for accelerating clinical research and upholding ethical standards for patient privacy. The code is publicly available at https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4.|http://arxiv.org/abs/2410.12476v2|Zerui Xu,Fang Wu,Yuanyuan Zhang,Yue Zhao
21|From Bench to Bedside: A Review of Clinical Trials in Drug Discovery and Development|Clinical trials are an indispensable part of the drug development process, bridging the gap between basic research and clinical application. During the development of new drugs, clinical trials are used not only to evaluate the safety and efficacy of the drug but also to explore its dosage, treatment regimens, and potential side effects. This review discusses the various stages of clinical trials, including Phase I (safety assessment), Phase II (preliminary efficacy evaluation), Phase III (large-scale validation), and Phase IV (post-marketing surveillance), highlighting the characteristics of each phase and their interrelationships. Additionally, the paper addresses the major challenges encountered in clinical trials, such as ethical issues, subject recruitment difficulties, diversity and representativeness concerns, and proposes strategies for overcoming these challenges. With the advancement of technology, innovative technologies such as artificial intelligence, big data, and digitalization are gradually transforming clinical trial design and implementation, improving trial efficiency and data quality. The article also looks forward to the future of clinical trials, particularly the impact of emerging therapies such as gene therapy and immunotherapy on trial design, as well as the importance of regulatory reforms and global collaboration. In conclusion, the core role of clinical trials in drug development will continue to drive the progress of innovative drug development and clinical treatment.|http://arxiv.org/abs/2412.09378v2|Tianyang Wang,Ming Liu,Benji Peng,Xinyuan Song,Charles Zhang,Xintian Sun,Qian Niu,Junyu Liu,Silin Chen,Keyu Chen,Ming Li,Pohsun Feng,Ziqian Bi,Yunze Wang,Yichao Zhang,Cheng Fei,Lawrence KQ Yan
22|Estimating Design Operating Characteristics in Bayesian Adaptive Clinical Trials|Bayesian adaptive designs have gained popularity in all phases of clinical trials with numerous new developments in the past few decades. During the COVID-19 pandemic, the need to establish evidence for the effectiveness of vaccines, therapeutic treatments and policies that could resolve or control the crisis emphasized the advantages offered by efficient and flexible clinical trial designs. In many COVID-19 clinical trials, due to the high level of uncertainty, Bayesian adaptive designs were considered advantageous. Designing Bayesian adaptive trials, however, requires extensive simulation studies that are generally considered challenging, particularly in time-sensitive settings such as a pandemic. In this article, we propose a set of methods for efficient estimation and uncertainty quantification for design operating characteristics of Bayesian adaptive trials. Specifically, we model the sampling distribution of Bayesian probability statements that are commonly used as the basis of decision making. To showcase the implementation and performance of the proposed approach, we use a clinical trial design with an ordinal disease-progression scale endpoint that was popular among COVID-19 trial. However, the proposed methodology may be applied generally in clinical trial context where design operating characteristics cannot be obtained analytically.|http://arxiv.org/abs/2105.03022v2|Shirin Golchi
23|Artificial Intelligence for In Silico Clinical Trials: A Review|A clinical trial is an essential step in drug development, which is often costly and time-consuming. In silico trials are clinical trials conducted digitally through simulation and modeling as an alternative to traditional clinical trials. AI-enabled in silico trials can increase the case group size by creating virtual cohorts as controls. In addition, it also enables automation and optimization of trial design and predicts the trial success rate. This article systematically reviews papers under three main topics: clinical simulation, individualized predictive modeling, and computer-aided trial design. We focus on how machine learning (ML) may be applied in these applications. In particular, we present the machine learning problem formulation and available data sources for each task. We end with discussing the challenges and opportunities of AI for in silico trials in real-world applications.|http://arxiv.org/abs/2209.09023v1|Zifeng Wang,Chufan Gao,Lucas M. Glass,Jimeng Sun
24|Language Interaction Network for Clinical Trial Approval Estimation|Clinical trial outcome prediction seeks to estimate the likelihood that a clinical trial will successfully reach its intended endpoint. This process predominantly involves the development of machine learning models that utilize a variety of data sources such as descriptions of the clinical trials, characteristics of the drug molecules, and specific disease conditions being targeted. Accurate predictions of trial outcomes are crucial for optimizing trial planning and prioritizing investments in a drug portfolio. While previous research has largely concentrated on small-molecule drugs, there is a growing need to focus on biologics-a rapidly expanding category of therapeutic agents that often lack the well-defined molecular properties associated with traditional drugs. Additionally, applying conventional methods like graph neural networks to biologics data proves challenging due to their complex nature. To address these challenges, we introduce the Language Interaction Network (LINT), a novel approach that predicts trial outcomes using only the free-text descriptions of the trials. We have rigorously tested the effectiveness of LINT across three phases of clinical trials, where it achieved ROC-AUC scores of 0.770, 0.740, and 0.748 for phases I, II, and III, respectively, specifically concerning trials involving biologic interventions.|http://arxiv.org/abs/2405.06662v1|Chufan Gao,Tianfan Fu,Jimeng Sun
25|Speed and accuracy in a visual motion discrimination task as performed by rats|We find that rats, like primates and humans, perform better on the random dot motion task when they take more time to respond. We provide evidence that this improvement is due to stimulus integration. Rats increase their response latency modestly as a function of trial difficulty. Rats can modulate response latency more strongly on a trial by trial basis, apparently on the basis of reward-related parameters.|http://arxiv.org/abs/1206.0311v1|Pamela Reinagel,Emily Mankin,Adam Calhoun
26|A Latent Gaussian Process Model with Application to Monitoring Clinical Trials|In many clinical trials treatments need to be repeatedly applied as diseases relapse frequently after remission over a long period of time (e.g., 35 weeks). Most research in statistics focuses on the overall trial design, such as sample size and power calculation, or on the data analysis after trials are completed. Little is done to improve the efficiency of trial monitoring, such as early termination of trials due to futility. The challenge faced in such trial monitoring is mostly caused by the need to properly model repeated outcomes from patients. We propose a Bayesian trial monitoring scheme for clinical trials with repeated and potentially cyclic binary outcomes. We construct a latent Gaussian process (LGP) to model discrete longitudinal data in those trials. LGP describes the underlying latent process that gives rise to the observed longitudinal binary outcomes. The posterior consistency property of the proposed model is studied. Posterior inference is conducted with a hybrid Monte Carlo algorithm. Simulation studies are conducted under various clinical scenarios, and a case study is reported based on a real-life trial.|http://arxiv.org/abs/1403.7853v1|Yanxun Xu,Yuan Ji
27|Clinical trial site matching with improved diversity using fair policy learning|The ongoing pandemic has highlighted the importance of reliable and efficient clinical trials in healthcare. Trial sites, where the trials are conducted, are chosen mainly based on feasibility in terms of medical expertise and access to a large group of patients. More recently, the issue of diversity and inclusion in clinical trials is gaining importance. Different patient groups may experience the effects of a medical drug/ treatment differently and hence need to be included in the clinical trials. These groups could be based on ethnicity, co-morbidities, age, or economic factors. Thus, designing a method for trial site selection that accounts for both feasibility and diversity is a crucial and urgent goal. In this paper, we formulate this problem as a ranking problem with fairness constraints. Using principles of fairness in machine learning, we learn a model that maps a clinical trial description to a ranked list of potential trial sites. Unlike existing fairness frameworks, the group membership of each trial site is non-binary: each trial site may have access to patients from multiple groups. We propose fairness criteria based on demographic parity to address such a multi-group membership scenario. We test our method on 480 real-world clinical trials and show that our model results in a list of potential trial sites that provides access to a diverse set of patients while also ensuing a high number of enrolled patients.|http://arxiv.org/abs/2204.06501v1|Rakshith S Srinivasa,Cheng Qian,Brandon Theodorou,Jeffrey Spaeder,Cao Xiao,Lucas Glass,Jimeng Sun
28|Ongoing Vaccine and Monoclonal Antibody HIV Prevention Efficacy Trials and Considerations for Sequel Efficacy Trial Designs|Four randomized placebo-controlled efficacy trials of a candidate vaccine or passively infused monoclonal antibody for prevention of HIV-1 infection are underway (HVTN 702 in South African men and women; HVTN 705 in sub-Saharan African women; HVTN 703/HPTN 081 in sub-Saharan African women; HVTN 704/HPTN 085 in U.S., Peruvian, Brazilian, and Swiss men or transgender persons who have sex with men). Several challenges are posed to the optimal design of the sequel efficacy trials, including: (1) how to account for the evolving mosaic of effective prevention interventions that may be part of the trial design or standard of prevention; (2) how to define viable and optimal sequel trial designs depending on the primary efficacy results and secondary 'correlates of protection' results of each of the ongoing trials; and (3) how to define the primary objective of sequel efficacy trials if HIV-1 incidence is expected to be very low in all study arms such that a standard trial design has a steep opportunity cost. After summarizing the ongoing trials, I discuss statistical science considerations for sequel efficacy trial designs, both generally and specifically to each trial listed above. One conclusion is that the results of 'correlates of protection' analyses, which ascertain how different host immunological markers and HIV-1 viral features impact HIV-1 risk and prevention efficacy, have an important influence on sequel trial design. This influence is especially relevant for the monoclonal antibody trials because of the focused pre-trial hypothesis that potency and coverage of serum neutralization constitutes a surrogate endpoint for HIV-1 infection... (see manuscript for the full abstract)|http://arxiv.org/abs/1906.08409v1|Peter B. Gilbert
29|Augmenting adaptive immunity: progress and challenges in the quantitative engineering and analysis of adaptive immune receptor repertoires|The adaptive immune system is a natural diagnostic and therapeutic. It recognizes threats earlier than clinical symptoms manifest and neutralizes antigen with exquisite specificity. Recognition specificity and broad reactivity is enabled via adaptive B- and T-cell receptors: the immune receptor repertoire. The human immune system, however, is not omnipotent. Our natural defense system sometimes loses the battle to parasites and microbes and even turns against us in the case of cancer, autoimmune and inflammatory disease. A long-standing dream of immunoengineers has been, therefore, to mechanistically understand how the immune system sees, reacts and remembers antigens. Only very recently, experimental and computational methods have achieved sufficient quantitative resolution to start querying and engineering adaptive immunity with great precision. In specific, these innovations have been applied with the greatest fervency and success in immunotherapy, autoimmunity and vaccine design. The work here highlights advances, challenges and future directions of quantitative approaches which seek to advance the fundamental understanding of immunological phenomena, and reverse engineer the immune system to produce auspicious biopharmaceutical drugs and immunodiagnostics. Our review indicates that the merger of fundamental immunology, computational immunology and digital-biotechnology minimizes black box engineering, thereby advancing both immunological knowledge and as well immunoengineering methodologies.|http://arxiv.org/abs/1904.04105v2|Alex J. Brown,Igor Snapkov,Rahmad Akbar,Milena Pavlovi,Enkelejda Miho,Geir K. Sandve,Victor Greiff
30|Sample size for a non-inferiority clinical trial with time-to-event data in the presence of competing risks|The analysis and planning methods for competing risks model have been described in the literatures in recent decades, and non-inferiority clinical trials are helpful in current pharmaceutical practice. Analytical methods for non-inferiority clinical trials in the presence of competing risks were investigated by Parpia et al., who indicated that the proportional sub-distribution hazard model is appropriate in the context of biological studies. However, the analytical methods of competing risks model differ from those appropriate for analyzing non-inferiority clinical trials with a single outcome; thus, a corresponding method for planning such trials is necessary. A sample size formula for non-inferiority clinical trials in the presence of competing risks based on the proportional sub-distribution hazard model is presented in this paper. The primary endpoint relies on the sub-distribution hazard ratio. A total of 120 simulations and an example based on a randomized controlled trial verified the empirical performance of the presented formula. The results demonstrate that the empirical power of sample size formulas based on the Weibull distribution for non-inferiority clinical trials with competing risks can reach the targeted power.|http://arxiv.org/abs/1802.10245v1|Dong Han,Zheng Chen,Yawen Hou
31|Clinical Trial Recommendations Using Semantics-Based Inductive Inference and Knowledge Graph Embeddings|Designing a new clinical trial entails many decisions, such as defining a cohort and setting the study objectives to name a few, and therefore can benefit from recommendations based on exhaustive mining of past clinical trial records. Here, we propose a novel recommendation methodology, based on neural embeddings trained on a first-of-a-kind knowledge graph of clinical trials. We addressed several important research questions in this context, including designing a knowledge graph (KG) for clinical trial data, effectiveness of various KG embedding (KGE) methods for it, a novel inductive inference using KGE, and its use in generating recommendations for clinical trial design. We used publicly available data from clinicaltrials.gov for the study. Results show that our recommendations approach achieves relevance scores of 70%-83%, measured as the text similarity to actual clinical trial elements, and the most relevant recommendation can be found near the top of list. Our study also suggests potential improvement in training KGE using node semantics.|http://arxiv.org/abs/2309.15979v1|Murthy V. Devarakonda,Smita Mohanty,Raja Rao Sunkishala,Nag Mallampalli,Xiong Liu
32|MatchMiner-AI: An Open-Source Solution for Cancer Clinical Trial Matching|Clinical trials drive improvements in cancer treatments and outcomes. However, most adults with cancer do not participate in trials, and trials often fail to enroll enough patients to answer their scientific questions. Artificial intelligence could accelerate matching of patients to appropriate clinical trials. Here, we describe the development and evaluation of the MatchMiner-AI pipeline for clinical trial searching and ranking. MatchMiner-AI focuses on matching patients to potential trials based on core criteria describing clinical "spaces," or disease contexts, targeted by a trial. It aims to accelerate the human work of identifying potential matches, not to fully automate trial screening. The pipeline includes modules for extraction of key information from a patient's longitudinal electronic health record; rapid ranking of candidate trial-patient matches based on embeddings in vector space; and classification of whether a candidate match represents a reasonable clinical consideration. Code and synthetic data are available at https://huggingface.co/ksg-dfci/MatchMiner-AI . Model weights based on synthetic data are available at https://huggingface.co/ksg-dfci/TrialSpace and https://huggingface.co/ksg-dfci/TrialChecker . A simple cancer clinical trial search engine to demonstrate pipeline components is available at https://huggingface.co/spaces/ksg-dfci/trial_search_alpha .|http://arxiv.org/abs/2412.17228v1|Ethan Cerami,Pavel Trukhanov,Morgan A. Paul,Michael J. Hassett,Irbaz B. Riaz,James Lindsay,Emily Mallaber,Harry Klein,Gufran Gungor,Matthew Galvin,Stephen C. Van Nostrand,Joyce Yu,Tali Mazor,Kenneth L. Kehl
33|TrialDura: Hierarchical Attention Transformer for Interpretable Clinical Trial Duration Prediction|The clinical trial process, a critical phase in drug development, is essential for developing new treatments. The primary goal of interventional clinical trials is to evaluate the safety and efficacy of drug-based treatments for specific diseases. However, these trials are often lengthy, labor-intensive, and expensive. The duration of a clinical trial significantly impacts overall costs, making efficient timeline management crucial for controlling budgets and ensuring the economic feasibility of research. To address this issue, We propose TrialDura, a machine learning-based method that estimates the duration of clinical trials using multimodal data, including disease names, drug molecules, trial phases, and eligibility criteria. Then, we encode them into Bio-BERT embeddings specifically tuned for biomedical contexts to provide a deeper and more relevant semantic understanding of clinical trial data. Finally, the model's hierarchical attention mechanism connects all of the embeddings to capture their interactions and predict clinical trial duration. Our proposed model demonstrated superior performance with a mean absolute error (MAE) of 1.04 years and a root mean square error (RMSE) of 1.39 years compared to the other models, indicating more accurate clinical trial duration prediction. Publicly available code can be found at: https://anonymous.4open.science/r/TrialDura-F196.|http://arxiv.org/abs/2404.13235v2|Ling Yue,Jonathan Li,Sixue Xing,Md Zabirul Islam,Bolun Xia,Tianfan Fu,Jintai Chen
34|A general Bayesian approach to design adaptive clinical trials with time-to-event outcomes|Clinical trials are an integral component of medical research. Trials require careful design to, for example, maintain the safety of participants, use resources efficiently and allow clinically meaningful conclusions to be drawn. Adaptive clinical trials (i.e. trials that can be altered based on evidence that has accrued) are often more efficient, informative and ethical than standard or non-adaptive trials because they require fewer participants, target more promising treatments, and can stop early with sufficient evidence of effectiveness or harm. The design of adaptive trials requires the pre-specification of adaptions that are permissible throughout the conduct of the trial. Proposed adaptive designs are then usually evaluated through simulation which provides indicative metrics of performance (e.g. statistical power and type-1 error) under different scenarios. Trial simulation requires assumptions about the data generating process to be specified but correctly specifying these in practice can be difficult, particularly for new and emerging diseases. To address this, we propose an approach to design adaptive clinical trials without needing to specify the complete data generating process. To facilitate this, we consider a general Bayesian framework where inference about the treatment effect on a time-to-event outcome can be performed via the partial likelihood. As a consequence, the proposed approach to evaluate trial designs is robust to the specific form of the baseline hazard function. The benefits of this approach are demonstrated through the redesign of a recent clinical trial to evaluate whether a third dose of a vaccine provides improved protection against gastroenteritis in Australian Indigenous infants.|http://arxiv.org/abs/2303.00901v1|James M. McGree,Antony M. Overstall,Mark Jones,Robert K. Mahar
35|Comparing Biomarkers as Trial Level General Surrogates|An intermediate response measure that accurately predicts efficacy in a new setting can reduce trial cost and time to product licensure. In this paper, we define a trial level general surrogate as a trial level intermediate response that accurately predicts trial level clinical responses. Methods for evaluating trial level general surrogates have been developed previously. Many methods in the literature use trial level intermediate responses for prediction. However, all existing methods focus on surrogate evaluation and prediction in new settings, rather than comparison of candidate trial level surrogates, and few formalize the use of cross validation to quantify the expected prediction error. Our proposed method uses Bayesian non-parametric modeling and cross-validation to estimate the absolute prediction error for use in evaluating and comparing candidate trial level general surrogates. Simulations show that our method performs well across a variety of scenarios. We use our method to evaluate and to compare candidate trial level general surrogates in several multi-national trials of a pentavalent rotavirus vaccine. We identify two immune measures that have potential value as trial level general surrogates and use the measures to predict efficacy in a trial with no clinical outcomes measured.|http://arxiv.org/abs/1507.01825v1|Erin E. Gabriel,Michael J. Daniels,M. Elizabeth Halloran
36|Dynamic borrowing from historical controls via the synthetic prior with covariates in randomized clinical trials|Motivated by a rheumatoid arthritis clinical trial, we propose a new Bayesian method called SPx, standing for synthetic prior with covariates, to borrow information from historical trials to reduce the control group size in a new trial. The method involves a novel use of Bayesian model averaging to balance between multiple possible relationships between the historical and new trial data, allowing the historical data to be dynamically trusted or discounted as appropriate. We require only trial-level summary statistics, which are available more often than patient-level data. Through simulations and an application to the rheumatoid arthritis trial we show that SPx can substantially reduce the control group size while maintaining Frequentist properties.|http://arxiv.org/abs/2410.07242v1|Daniel E. Schwartz,Yuan Ji,Li Wang
37|Adversity Index for Clinical Trials: An Inclusive Approach for Analysis of Safety Data|This paper proposes a new method for analysis of adverse event data in clinical trials. The method is illustrated by application to data on 4 phase III clinical trials, two on breast cancer and two on diabetes mellitus.|http://arxiv.org/abs/1806.00204v1|Sharayu Paranjpe,Anil Gore
38|MiStImm: a simulation tool to compare classical nonsef-centered immune models with a novel self-centered model|Our main purpose is to compare classical nonself-centered, two-signal theoretical models of the adaptive immune system with a novel, self-centered, one-signal model developed by our research group. Our model hypothesizes that the immune system of a fetus is capable learning the limited set of self antigens but unable to prepare itself for the unlimited variety of nonself antigens. We have built a computational model that simulates the development of the adaptive immune system. For simplicity, we concentrated on humoral immunity and its major components: T cells, B cells, antibodies, interleukins, non-immune self cells, and foreign antigens. Our model is a microscopic one, similar to the interacting particle models of statistical physics and agent-based models in immunology. Furthermore, our model is stochastic: events are considered random and modeled by a continuous time, finite state Markov process, that is, they are controlled by finitely many independent exponential clocks.   We investigate under what conditions can an immune memory be created that results in a more effective immune response to a repeated infection. The simulations show that our self-centered model is realistic. Moreover, in case of a primary adaptive immune reaction, it can destroy infections more efficiently than a classical nonself-centered model.   Predictions of our theoretical model were clinically supported by autoimmune-related adverse events in high-dose immune checkpoint inhibitor immunotherapy trials and also by safe and successful low-dose immune checkpoint inhibitor combination treatment of heavily pretreated stage IV cancer patients who had exhausted all conventional treatments.   The MiStImm simulation tool and source codes are available at the address https://github.com/kerepesi/MiStImm.|http://arxiv.org/abs/1507.00950v2|Tams Szabados,Csaba Kerepesi,Tibor Bakcs
39|Relational Dynamics in Perception: Impacts on trial-to-trial variation|We show that trial-to-trial variability in sensory detection of a weak visual stimulus is dramatically diminished when rather than presenting a fixed stimulus contrast, fluctuations in a subject's judgment are matched by fluctuations in stimulus contrast. This attenuation of fluctuations does not involve a change in the subject's psychometric function. The result is consistent with the interpretation of trial-to-trial variability in this sensory detection task being a high-level meta-cognitive control process that explores for something that our brains are so used to: subject-object relational dynamics.|http://arxiv.org/abs/1102.1384v1|Shimon Marom,Avner Wallach
40|PyTrial: Machine Learning Software and Benchmark for Clinical Trial Applications|Clinical trials are conducted to test the effectiveness and safety of potential drugs in humans for regulatory approval. Machine learning (ML) has recently emerged as a new tool to assist in clinical trials. Despite this progress, there have been few efforts to document and benchmark ML4Trial algorithms available to the ML research community. Additionally, the accessibility to clinical trial-related datasets is limited, and there is a lack of well-defined clinical tasks to facilitate the development of new algorithms.   To fill this gap, we have developed PyTrial that provides benchmarks and open-source implementations of a series of ML algorithms for clinical trial design and operations. In this paper, we thoroughly investigate 34 ML algorithms for clinical trials across 6 different tasks, including patient outcome prediction, trial site selection, trial outcome prediction, patient-trial matching, trial similarity search, and synthetic data generation. We have also collected and prepared 23 ML-ready datasets as well as their working examples in Jupyter Notebooks for quick implementation and testing.   PyTrial defines each task through a simple four-step process: data loading, model specification, model training, and model evaluation, all achievable with just a few lines of code. Furthermore, our modular API architecture empowers practitioners to expand the framework to incorporate new algorithms and tasks effortlessly. The code is available at https://github.com/RyanWangZf/PyTrial.|http://arxiv.org/abs/2306.04018v2|Zifeng Wang,Brandon Theodorou,Tianfan Fu,Cao Xiao,Jimeng Sun
41|Advancing clinical trial outcomes using deep learning and predictive modelling: bridging precision medicine and patient-centered care|The integration of artificial intelligence [AI] into clinical trials has revolutionized the process of drug development and personalized medicine. Among these advancements, deep learning and predictive modelling have emerged as transformative tools for optimizing clinical trial design, patient recruitment, and real-time monitoring. This study explores the application of deep learning techniques, such as convolutional neural networks [CNNs] and transformerbased models, to stratify patients, forecast adverse events, and personalize treatment plans. Furthermore, predictive modelling approaches, including survival analysis and time-series forecasting, are employed to predict trial outcomes, enhancing efficiency and reducing trial failure rates. To address challenges in analysing unstructured clinical data, such as patient notes and trial protocols, natural language processing [NLP] techniques are utilized for extracting actionable insights. A custom dataset comprising structured patient demographics, genomic data, and unstructured text is curated for training and validating these models. Key metrics, including precision, recall, and F1 scores, are used to evaluate model performance, while trade-offs between accuracy and computational efficiency are examined to identify the optimal model for clinical deployment. This research underscores the potential of AI-driven methods to streamline clinical trial workflows, improve patient-centric outcomes, and reduce costs associated with trial inefficiencies. The findings provide a robust framework for integrating predictive analytics into precision medicine, paving the way for more adaptive and efficient clinical trials. By bridging the gap between technological innovation and real-world applications, this study contributes to advancing the role of AI in healthcare, particularly in fostering personalized care and improving overall trial success rates.|http://arxiv.org/abs/2412.07050v1|Sydney Anuyah,Mallika K Singh,Hope Nyavor
42|TrialBench: Multi-Modal Artificial Intelligence-Ready Clinical Trial Datasets|Clinical trials are pivotal for developing new medical treatments, yet they typically pose some risks such as patient mortality, adverse events, and enrollment failure that waste immense efforts spanning over a decade. Applying artificial intelligence (AI) to forecast or simulate key events in clinical trials holds great potential for providing insights to guide trial designs. However, complex data collection and question definition requiring medical expertise and a deep understanding of trial designs have hindered the involvement of AI thus far. This paper tackles these challenges by presenting a comprehensive suite of meticulously curated AIready datasets covering multi-modal data (e.g., drug molecule, disease code, text, categorical/numerical features) and 8 crucial prediction challenges in clinical trial design, encompassing prediction of trial duration, patient dropout rate, serious adverse event, mortality rate, trial approval outcome, trial failure reason, drug dose finding, design of eligibility criteria. Furthermore, we provide basic validation methods for each task to ensure the datasets' usability and reliability. We anticipate that the availability of such open-access datasets will catalyze the development of advanced AI approaches for clinical trial design, ultimately advancing clinical trial research and accelerating medical solution development. The curated dataset, metrics, and basic models are publicly available at https://github.com/ML2Health/ML2ClinicalTrials/tree/main/AI4Trial.|http://arxiv.org/abs/2407.00631v2|Jintai Chen,Yaojun Hu,Yue Wang,Yingzhou Lu,Xu Cao,Miao Lin,Hongxia Xu,Jian Wu,Cao Xiao,Jimeng Sun,Lucas Glass,Kexin Huang,Marinka Zitnik,Tianfan Fu
43|AutoTrial: Prompting Language Models for Clinical Trial Design|Clinical trials are critical for drug development. Constructing the appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for patient recruitment) is essential for the trial's success. Proper design of clinical trial protocols should consider similar precedent trials and their eligibility criteria to ensure sufficient patient coverage. In this paper, we present a method named AutoTrial to aid the design of clinical eligibility criteria using language models. It allows (1) controllable generation under instructions via a hybrid of discrete and neural prompting, (2) scalable knowledge incorporation via in-context learning, and (3) explicit reasoning chains to provide rationales for understanding the outputs. Experiments on over 70K clinical trials verify that AutoTrial generates high-quality criteria texts that are fluent and coherent and with high accuracy in capturing the relevant clinical concepts to the target trial. It is noteworthy that our method, with a much smaller parameter size, gains around 60% winning rate against the GPT-3.5 baselines via human evaluations.|http://arxiv.org/abs/2305.11366v2|Zifeng Wang,Cao Xiao,Jimeng Sun
44|Can artificial intelligence predict clinical trial outcomes?|The increasing complexity and cost of clinical trials, particularly in the context of oncology and advanced therapies, pose significant challenges for drug development. This study evaluates the predictive capabilities of large language models (LLMs) such as GPT-3.5, GPT-4, and HINT in determining clinical trial outcomes. By leveraging a curated dataset of trials from ClinicalTrials.gov, we compare the models' performance using metrics including balanced accuracy, specificity, recall, and Matthews Correlation Coefficient (MCC). Results indicate that GPT-4o demonstrates robust performance in early trial phases, achieving high recall but facing limitations in specificity. Conversely, the HINT model excels in recognizing negative outcomes, particularly in later trial phases, offering a balanced approach across diverse endpoints. Oncology trials, characterized by high complexity, remain challenging for all models. Additionally, trial duration and disease categories influence predictive performance, with longer durations and complex diseases such as neoplasms reducing accuracy. This study highlights the complementary strengths of LLMs and HINT, providing insights into optimizing predictive tools for clinical trial design and risk management. Future advancements in LLMs are essential to address current gaps in handling negative outcomes and complex domains.|http://arxiv.org/abs/2411.17595v1|Shuyi Jin,Lu Chen,Hongru Ding,Meijie Wang,Lun Yu
45|Three mechanistically different variability and noise sources in the trial-to-trial fluctuations of responses to brain stimulation|Motor-evoked potentials (MEPs) are among the few directly observable responses to external brain stimulation and serve a variety of applications, often in the form of input-output (IO) curves. Previous statistical models with two variability sources inherently consider the small MEPs at the low-side plateau as part of the neural recruitment properties. However, recent studies demonstrated that small MEP responses under resting conditions are contaminated and over-shadowed by background noise of mostly technical quality, e.g., caused by the amplifier, and suggested that the neural recruitment curve should continue below this noise level. This work intends to separate physiological variability from background noise and improve the description of recruitment behaviour. We developed a triple-variability-source model around a logarithmic logistic function without a lower plateau and incorporated an additional source for background noise. Compared to models with two or fewer variability sources, our approach better described IO characteristics, evidenced by lower Bayesian Information Criterion scores across all subjects and pulse shapes. The model independently extracted hidden variability information across the stimulated neural system and isolated it from background noise, which led to an accurate estimation of the IO curve parameters. This new model offers a robust tool to analyse brain stimulation IO curves in clinical and experimental neuroscience and reduces the risk of spurious results from inappropriate statistical methods. The presented model together with the corresponding calibration method provides a more accurate representation of MEP responses and variability sources, advances our understanding of cortical excitability, and may improve the assessment of neuromodulation effects.|http://arxiv.org/abs/2412.16997v1|Ke Ma,Siwei Liu,Mengjie Qin,Stefan Goetz
46|Predicting Clinical Trial Results by Implicit Evidence Integration|Clinical trials provide essential guidance for practicing Evidence-Based Medicine, though often accompanying with unendurable costs and risks. To optimize the design of clinical trials, we introduce a novel Clinical Trial Result Prediction (CTRP) task. In the CTRP framework, a model takes a PICO-formatted clinical trial proposal with its background as input and predicts the result, i.e. how the Intervention group compares with the Comparison group in terms of the measured Outcome in the studied Population. While structured clinical evidence is prohibitively expensive for manual collection, we exploit large-scale unstructured sentences from medical literature that implicitly contain PICOs and results as evidence. Specifically, we pre-train a model to predict the disentangled results from such implicit evidence and fine-tune the model with limited data on the downstream datasets. Experiments on the benchmark Evidence Integration dataset show that the proposed model outperforms the baselines by large margins, e.g., with a 10.7% relative gain over BioBERT in macro-F1. Moreover, the performance improvement is also validated on another dataset composed of clinical trials related to COVID-19.|http://arxiv.org/abs/2010.05639v1|Qiao Jin,Chuanqi Tan,Mosha Chen,Xiaozhong Liu,Songfang Huang
47|The Leaf Clinical Trials Corpus: a new resource for query generation from clinical trial eligibility criteria|Identifying cohorts of patients based on eligibility criteria such as medical conditions, procedures, and medication use is critical to recruitment for clinical trials. Such criteria are often most naturally described in free-text, using language familiar to clinicians and researchers. In order to identify potential participants at scale, these criteria must first be translated into queries on clinical databases, which can be labor-intensive and error-prone. Natural language processing (NLP) methods offer a potential means of such conversion into database queries automatically. However they must first be trained and evaluated using corpora which capture clinical trials criteria in sufficient detail. In this paper, we introduce the Leaf Clinical Trials (LCT) corpus, a human-annotated corpus of over 1,000 clinical trial eligibility criteria descriptions using highly granular structured labels capturing a range of biomedical phenomena. We provide details of our schema, annotation process, corpus quality, and statistics. Additionally, we present baseline information extraction results on this corpus as benchmarks for future work.|http://arxiv.org/abs/2207.13757v1|Nicholas J Dobbins,Tony Mullen,Ozlem Uzuner,Meliha Yetisgen
48|Towards quantum computing for clinical trial design and optimization: A perspective on new opportunities and challenges|Clinical trials are pivotal in the drug discovery process to determine the safety and efficacy of a drug candidate. The high failure rates of these trials are attributed to deficiencies in clinical model development and protocol design. Improvements in the clinical drug design process could therefore yield significant benefits for all stakeholders involved. This paper examines the current challenges faced in clinical trial design and optimization, reviews established classical computational approaches, and introduces quantum algorithms aimed at enhancing these processes. Specifically, the focus is on three critical aspects: clinical trial simulations, site selection, and cohort identification. This study aims to provide a comprehensive framework that leverages quantum computing to innovate and refine the efficiency and effectiveness of clinical trials.|http://arxiv.org/abs/2404.13113v1|Hakan Doga,M. Emre Sahin,Joao Bettencourt-Silva,Anh Pham,Eunyoung Kim,Alan Andress,Sudhir Saxena,Aritra Bose,Laxmi Parida,Jan Lukas Robertus,Hideaki Kawaguchi,Radwa Soliman,Daniel Blankenberg
49|Adaptive Cohort Size Determination Method for Bayesian Optimal Interval Phase I/II Design to Shorten Clinical Trial Duration|Recently, the strategy for dose optimization in oncology has shifted to conduct Phase 2 randomized controlled trials with multiple doses. Optimal biologic dose selection from Phase 1 trial data to determine candidate doses for Phase 2 trials has been gaining attention. This study proposes a novel adaptive cohort size determination method for optimal biologic dose-finding to accelerate trials. The cohort size expansion is determined adaptively depending on the toxicity and efficacy data of the ongoing trial. In a simulation, the proposed method shortened the trial duration and maintained accuracy. The trial duration was reduced by an average of approximately 20% compared with the non-adaptive cohort size determination design. The cohort size expansion is demonstrated using a simple example.|http://arxiv.org/abs/2302.06088v1|Masahiro Kojima
50|ClinicalAgent: Clinical Trial Multi-Agent System with Large Language Model-based Reasoning|Large Language Models (LLMs) and multi-agent systems have shown impressive capabilities in natural language tasks but face challenges in clinical trial applications, primarily due to limited access to external knowledge. Recognizing the potential of advanced clinical trial tools that aggregate and predict based on the latest medical data, we propose an integrated solution to enhance their accessibility and utility. We introduce Clinical Agent System (ClinicalAgent), a clinical multi-agent system designed for clinical trial tasks, leveraging GPT-4, multi-agent architectures, LEAST-TO-MOST, and ReAct reasoning technology. This integration not only boosts LLM performance in clinical contexts but also introduces novel functionalities. The proposed method achieves competitive predictive performance in clinical trial outcome prediction (0.7908 PR-AUC), obtaining a 0.3326 improvement over the standard prompt Method. Publicly available code can be found at https://anonymous.4open.science/r/ClinicalAgent-6671.|http://arxiv.org/abs/2404.14777v2|Ling Yue,Sixue Xing,Jintai Chen,Tianfan Fu
51|Predictive Directions for Individualized Treatment Selection in Clinical Trials|In many clinical trials, individuals in different subgroups have experience differential treatment effects. This leads to individualized differences in treatment benefit. In this article, we introduce the general concept of predictive directions, which are risk scores motivated by potential outcomes considerations. These techniques borrow heavily from sufficient dimension reduction (SDR) and causal inference methodology. Under some conditions, one can use existing methods from the SDR literature to estimate the directions assuming an idealized complete data structure, which subsequently yields an obvious extension to clinical trial datasets. In addition, we generalize the direction idea to a nonlinear setting that exploits support vector machines. The methodology is illustrated with application to a series of colorectal cancer clinical trials.|http://arxiv.org/abs/1807.03375v1|Debashis Ghosh,Youngjoo Cho
52|Clinical Trials Ontology Engineering with Large Language Models|Managing clinical trial information is currently a significant challenge for the medical industry, as traditional methods are both time-consuming and costly. This paper proposes a simple yet effective methodology to extract and integrate clinical trial data in a cost-effective and time-efficient manner. Allowing the medical industry to stay up-to-date with medical developments. Comparing time, cost, and quality of the ontologies created by humans, GPT3.5, GPT4, and Llama3 (8b & 70b). Findings suggest that large language models (LLM) are a viable option to automate this process both from a cost and time perspective. This study underscores significant implications for medical research where real-time data integration from clinical trials could become the norm.|http://arxiv.org/abs/2412.14387v1|Berkan akr
53|Combining Evidence from Clinical Trials in Conditional or Accelerated Approval|Conditional (European Medicines Agency) or accelerated (U.S. Food and Drug Administration) approval of drugs allow earlier access to promising new treatments that address unmet medical needs. Certain post-marketing requirements must typically be met in order to obtain full approval, such as conducting a new post-market clinical trial. We study the applicability of the recently developed harmonic mean Chi-squared test to this conditional or accelerated approval framework. The proposed approach can be used both to support the design of the post-market trial and the analysis of the combined evidence provided by both trials. Other methods considered are the two-trials rule, Fisher's criterion and Stouffer's method. In contrast to some of the traditional methods, the harmonic mean Chi-squared test always requires a post-market clinical trial. If the p-value from the pre-market clinical trial is << 0.025, a smaller sample size for the post-market clinical trial is needed than with the two-trials rule. For illustration, we apply the harmonic mean Chi-squared test to a drug which received conditional (and later full) market licensing by the EMA. A simulation study is conducted to study the operating characteristics of the harmonic mean Chi-squared test and two-trials rule in more detail. We finally investigate the applicability of these two methods to compute the power at interim of an ongoing post-market trial. These results are expected to aid in the design and assessment of the required post-market studies in terms of the level of evidence required for full approval.|http://arxiv.org/abs/2112.11898v2|Manja Deforth,Charlotte Micheloud,Kit C Roes,Leonhard Held
54|Forum on immune digital twins: a meeting report|Medical digital twins are computational models of human biology relevant to a given medical condition, which can be tailored to an individual patient, thereby predicting the course of disease and individualized treatments, an important goal of personalized medicine. The immune system, which has a central role in many diseases, is highly heterogeneous between individuals, and thus poses a major challenge for this technology. If medical digital twins are to faithfully capture the characteristics of a patient's immune system, we need to answer many questions, such as: What do we need to know about the immune system to build mathematical models that reflect features of an individual? What data do we need to collect across the different scales of immune system action? What are the right modeling paradigms to properly capture immune system complexity? In February 2023, an international group of experts convened in Lake Nona, FL for two days to discuss these and other questions related to digital twins of the immune system. The group consisted of clinicians, immunologists, biologists, and mathematical modelers, representative of the interdisciplinary nature of medical digital twin development. A video recording of the entire event is available. This paper presents a synopsis of the discussions, brief descriptions of ongoing digital twin projects at different stages of progress. It also proposes a 5-year action plan for further developing this technology. The main recommendations are to identify and pursue a small number of promising use cases, to develop stimulation-specific assays of immune function in a clinical setting, and to develop a database of existing computational immune models, as well as advanced modeling technology and infrastructure.|http://arxiv.org/abs/2310.18374v1|Reinhard Laubenbacher,Fred Adler,Gary An,Filippo Castiglione,Stephen Eubank,Luis L. Fonseca,James Glazier,Tomas Helikar,Marti Jett-Tilton,Denise Kirschner,Paul Macklin,Borna Mehrad,Beth Moore,Virginia Pasour,Ilya Shmulevich,Amber Smith,Isabel Voigt,Thomas E. Yankeelov,Tjalf Ziemssen
55|Multi-disciplinary fairness considerations in machine learning for clinical trials|While interest in the application of machine learning to improve healthcare has grown tremendously in recent years, a number of barriers prevent deployment in medical practice. A notable concern is the potential to exacerbate entrenched biases and existing health disparities in society. The area of fairness in machine learning seeks to address these issues of equity; however, appropriate approaches are context-dependent, necessitating domain-specific consideration. We focus on clinical trials, i.e., research studies conducted on humans to evaluate medical treatments. Clinical trials are a relatively under-explored application in machine learning for healthcare, in part due to complex ethical, legal, and regulatory requirements and high costs. Our aim is to provide a multi-disciplinary assessment of how fairness for machine learning fits into the context of clinical trials research and practice. We start by reviewing the current ethical considerations and guidelines for clinical trials and examine their relationship with common definitions of fairness in machine learning. We examine potential sources of unfairness in clinical trials, providing concrete examples, and discuss the role machine learning might play in either mitigating potential biases or exacerbating them when applied without care. Particular focus is given to adaptive clinical trials, which may employ machine learning. Finally, we highlight concepts that require further investigation and development, and emphasize new approaches to fairness that may be relevant to the design of clinical trials.|http://arxiv.org/abs/2205.08875v1|Isabel Chien,Nina Deliu,Richard E. Turner,Adrian Weller,Sofia S. Villar,Niki Kilbertus
56|Model-based Pre-clinical Trials for Medical Devices Using Statistical Model Checking|Clinical trials are considered as the golden standard for medical device validation. However, many sacrifices have to be made during the design and conduction of the trials due to cost considerations and partial information, which may compromise the significance of the trial results. In this paper, we proposed a model-based pre-clinical trial framework using statistical model checking. Physiological models represent disease mechanism, which enable automated adjudication of simulation results. Sampling of the patient parameters and hypothesis testing are performed by statistical model checker. The framework enables a broader range of hypothesis to be tested with guaranteed statistical significance, which are useful complements to the clinical trials. We demonstrated our framework with a pre-clinical trial on implantable cardioverter defibrillators.|http://arxiv.org/abs/2106.11917v1|Haochen Yang,Jicheng Gu,Zhihao Jiang
57|Next generation clinical trials: Seamless designs and master protocols|Background: Drug development is often inefficient, costly and lengthy, yet it is essential for evaluating the safety and efficacy of new interventions. Compared with other disease areas, this is particularly true for Phase II / III cancer clinical trials where high attrition rates and reduced regulatory approvals are being seen. In response to these challenges, seamless clinical trials and master protocols have emerged to streamline the drug development process. Methods: Seamless clinical trials, characterized by their ability to transition seamlessly from one phase to another, can lead to accelerating the development of promising therapies while Master protocols provide a framework for investigating multiple treatment options and patient subgroups within a single trial. Results: We discuss the advantages of these methods through real trial examples and the principals that lead to their success while also acknowledging the associated regulatory considerations and challenges. Conclusion: Seamless designs and Master protocols have the potential to improve confirmatory clinical trials. In the disease area of cancer, this ultimately means that patients can receive life-saving treatments sooner.|http://arxiv.org/abs/2405.06353v1|Abigail Burdon,Thomas Jaki,Xijin Chen,Pavel Mozgunov,Haiyan Zheng,Richard Baird
58|HINT: Hierarchical Interaction Network for Trial Outcome Prediction Leveraging Web Data|Clinical trials are crucial for drug development but are time consuming, expensive, and often burdensome on patients. More importantly, clinical trials face uncertain outcomes due to issues with efficacy, safety, or problems with patient recruitment. If we were better at predicting the results of clinical trials, we could avoid having to run trials that will inevitably fail more resources could be devoted to trials that are likely to succeed. In this paper, we propose Hierarchical INteraction Network (HINT) for more general, clinical trial outcome predictions for all diseases based on a comprehensive and diverse set of web data including molecule information of the drugs, target disease information, trial protocol and biomedical knowledge. HINT first encode these multi-modal data into latent embeddings, where an imputation module is designed to handle missing data. Next, these embeddings will be fed into the knowledge embedding module to generate knowledge embeddings that are pretrained using external knowledge on pharmaco-kinetic properties and trial risk from the web. Then the interaction graph module will connect all the embedding via domain knowledge to fully capture various trial components and their complex relations as well as their influences on trial outcomes. Finally, HINT learns a dynamic attentive graph neural network to predict trial outcome. Comprehensive experimental results show that HINT achieves strong predictive performance, obtaining 0.772, 0.607, 0.623, 0.703 on PR-AUC for Phase I, II, III, and indication outcome prediction, respectively. It also consistently outperforms the best baseline method by up to 12.4\% on PR-AUC.|http://arxiv.org/abs/2102.04252v3|Tianfan Fu,Kexin Huang,Cao Xiao,Lucas M. Glass,Jimeng Sun
59|Text Classification of Cancer Clinical Trial Eligibility Criteria|Automatic identification of clinical trials for which a patient is eligible is complicated by the fact that trial eligibility is stated in natural language. A potential solution to this problem is to employ text classification methods for common types of eligibility criteria. In this study, we focus on seven common exclusion criteria in cancer trials: prior malignancy, human immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness, drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase III cancer trials with these exclusions annotated at the trial level. We experiment with common transformer models as well as a new pre-trained clinical trial BERT model. Our results demonstrate the feasibility of automatically classifying common exclusion criteria. Additionally, we demonstrate the value of a pre-trained language model specifically for clinical trials, which yields the highest average performance across all criteria.|http://arxiv.org/abs/2309.07812v2|Yumeng Yang,Soumya Jayaraj,Ethan B Ludmir,Kirk Roberts
60|Clinical Trial Active Learning|This paper presents a novel approach to active learning that takes into account the non-independent and identically distributed (non-i.i.d.) structure of a clinical trial setting. There exists two types of clinical trials: retrospective and prospective. Retrospective clinical trials analyze data after treatment has been performed; prospective clinical trials collect data as treatment is ongoing. Typically, active learning approaches assume the dataset is i.i.d. when selecting training samples; however, in the case of clinical trials, treatment results in a dependency between the data collected at the current and past visits. Thus, we propose prospective active learning to overcome the limitations present in traditional active learning methods and apply it to disease detection in optical coherence tomography (OCT) images, where we condition on the time an image was collected to enforce the i.i.d. assumption. We compare our proposed method to the traditional active learning paradigm, which we refer to as retrospective in nature. We demonstrate that prospective active learning outperforms retrospective active learning in two different types of test settings.|http://arxiv.org/abs/2307.11209v1|Zoe Fowler,Kiran Kokilepersaud,Mohit Prabhushankar,Ghassan AlRegib
61|Improving clinical trial interpretation with ACCEPT analyses|Effective decision making from randomised controlled clinical trials relies on robust interpretation of the numerical results. However, the language we use to describe clinical trials can cause confusion both in trial design and in comparing results across trials. ACceptability Curve Estimation using Probability Above Threshold (ACCEPT) aids comparison between trials (even where of different designs) by harmonising reporting of results, acknowledging different interpretations of the results may be valid in different situations, and moving the focus from comparison to a pre-specified value to interpretation of the trial data. ACCEPT can be applied to historical trials or incorporated into statistical analysis plans for future analyses. An online tool enables ACCEPT on up to three trials simultaneously.|http://arxiv.org/abs/2203.11164v2|Michelle N. Clements,Ian R. White,Andrew J. Copas,Victoria Cornelius,Suzie Cro,David T Dunn,Matteo Quartagno,Rebecca M. Turner,Conor D. Tweed,A. Sarah Walker
62|Equipoise calibration of clinical trial design|Statistical methods for clinical trial design are currently unable to rely on a sufficiently precise and general definition of what is an adequately powered study. Operationally, this definition is needed to ensure an alignment by design between statistical significance and clinical interpretability. To address this gap, this paper shows how to calibrate randomised trial designs to establishing strong clinical equipoise imbalance. Among several equipoise models, the least informed population distribution of the pre-trial odds of the design hypotheses is recommended here as the most practical calibrator. Under this model, primary analysis outcomes of common phase 3 superiority designs are shown to provide at least 90% evidence of equipoise imbalance. Designs carrying 95% power at 5% false positive rate are shown to demonstrate even stronger equipoise imbalance, providing an operational definition of a robustly powered study. Equipoise calibration is then applied to design of clinical development plans comprising randomised phase 2 and phase 3 studies. Development plans using oncology clinical endpoints are shown to provide strong equipoise imbalance when positive outcomes are observed in phase 2 and in phase 3. Establishing equipoise imbalance on a statistical basis when a positive phase 2 is not confirmed in phase 3 is shown to require large sample sizes unlikely to be associated with clinically meaningful effect sizes. Equipoise calibration is proposed as an effective clinical trial methodology ensuring that the statistical properties of clinical trial outcomes are clinically interpretable. Strong equipoise imbalance is achieved for designs carrying 95% power at 5% false positive rate, regardless of whether the primary outcome is positive or negative. Sponsors should consider raising power of their designs beyond current practice to achieve more conclusive results.|http://arxiv.org/abs/2501.03009v1|Fabio Rigat
63|Combining Real-World and Randomized Control Trial Data Using Data-Adaptive Weighting via the On-Trial Score|Clinical trials with a hybrid control arm (a control arm constructed from a combination of randomized patients and real-world data on patients receiving usual care in standard clinical practice) have the potential to decrease the cost of randomized trials while increasing the proportion of trial patients given access to novel therapeutics. However, due to stringent trial inclusion criteria and differences in care and data quality between trials and community practice, trial patients may have systematically different outcomes compared to their real-world counterparts. We propose a new method for analyses of trials with a hybrid control arm that efficiently controls bias and type I error. Under our proposed approach, selected real-world patients are weighted by a function of the "on-trial score," which reflects their similarity to trial patients. In contrast to previously developed hybrid control designs that assign the same weight to all real-world patients, our approach upweights of real-world patients who more closely resemble randomized control patients while dissimilar patients are discounted. Estimates of the treatment effect are obtained via Cox proportional hazards models. We compare our approach to existing approaches via simulations and apply these methods to a study using electronic health record data. Our proposed method is able to control type I error, minimize bias, and decrease variance when compared to using only trial data in nearly all scenarios examined. Therefore, our new approach can be used when conducting clinical trials by augmenting the standard-of-care arm with weighted patients from the EHR to increase power without inducing bias.|http://arxiv.org/abs/2108.08756v1|Joanna Harton,Brian Segal,Ronac Mamtani,Nandita Mitra,Rebecca Hubbard
64|CliniDigest: A Case Study in Large Language Model Based Large-Scale Summarization of Clinical Trial Descriptions|A clinical trial is a study that evaluates new biomedical interventions. To design new trials, researchers draw inspiration from those current and completed. In 2022, there were on average more than 100 clinical trials submitted to ClinicalTrials.gov every day, with each trial having a mean of approximately 1500 words [1]. This makes it nearly impossible to keep up to date. To mitigate this issue, we have created a batch clinical trial summarizer called CliniDigest using GPT-3.5. CliniDigest is, to our knowledge, the first tool able to provide real-time, truthful, and comprehensive summaries of clinical trials. CliniDigest can reduce up to 85 clinical trial descriptions (approximately 10,500 words) into a concise 200-word summary with references and limited hallucinations. We have tested CliniDigest on its ability to summarize 457 trials divided across 27 medical subdomains. For each field, CliniDigest generates summaries of $\mu=153,\ \sigma=69 $ words, each of which utilizes $\mu=54\%,\ \sigma=30\% $ of the sources. A more comprehensive evaluation is planned and outlined in this paper.|http://arxiv.org/abs/2307.14522v2|Renee D. White,Tristan Peng,Pann Sripitak,Alexander Rosenberg Johansen,Michael Snyder
65|PRISM: Patient Records Interpretation for Semantic Clinical Trial Matching using Large Language Models|Clinical trial matching is the task of identifying trials for which patients may be potentially eligible. Typically, this task is labor-intensive and requires detailed verification of patient electronic health records (EHRs) against the stringent inclusion and exclusion criteria of clinical trials. This process is manual, time-intensive, and challenging to scale up, resulting in many patients missing out on potential therapeutic options. Recent advancements in Large Language Models (LLMs) have made automating patient-trial matching possible, as shown in multiple concurrent research studies. However, the current approaches are confined to constrained, often synthetic datasets that do not adequately mirror the complexities encountered in real-world medical data. In this study, we present the first, end-to-end large-scale empirical evaluation of clinical trial matching using real-world EHRs. Our study showcases the capability of LLMs to accurately match patients with appropriate clinical trials. We perform experiments with proprietary LLMs, including GPT-4 and GPT-3.5, as well as our custom fine-tuned model called OncoLLM and show that OncoLLM, despite its significantly smaller size, not only outperforms GPT-3.5 but also matches the performance of qualified medical doctors. All experiments were carried out on real-world EHRs that include clinical notes and available clinical trials from a single cancer center in the United States.|http://arxiv.org/abs/2404.15549v2|Shashi Kant Gupta,Aditya Basu,Mauro Nievas,Jerrin Thomas,Nathan Wolfrath,Adhitya Ramamurthi,Bradley Taylor,Anai N. Kothari,Regina Schwind,Therica M. Miller,Sorena Nadaf-Rahrov,Yanshan Wang,Hrituraj Singh
66|TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model|Clinical trials are indispensable for medical research and the development of new treatments. However, clinical trials often involve thousands of participants and can span several years to complete, with a high probability of failure during the process. Recently, there has been a burgeoning interest in virtual clinical trials, which simulate real-world scenarios and hold the potential to significantly enhance patient safety, expedite development, reduce costs, and contribute to the broader scientific knowledge in healthcare. Existing research often focuses on leveraging electronic health records (EHRs) to support clinical trial outcome prediction. Yet, trained with limited clinical trial outcome data, existing approaches frequently struggle to perform accurate predictions. Some research has attempted to generate EHRs to augment model development but has fallen short in personalizing the generation for individual patient profiles. Recently, the emergence of large language models has illuminated new possibilities, as their embedded comprehensive clinical knowledge has proven beneficial in addressing medical issues. In this paper, we propose a large language model-based digital twin creation approach, called TWIN-GPT. TWIN-GPT can establish cross-dataset associations of medical information given limited data, generating unique personalized digital twins for different patients, thereby preserving individual patient characteristics. Comprehensive experiments show that using digital twins created by TWIN-GPT can boost the clinical trial outcome prediction, exceeding various previous prediction approaches.|http://arxiv.org/abs/2404.01273v2|Yue Wang,Tianfan Fu,Yinlong Xu,Zihan Ma,Hongxia Xu,Yingzhou Lu,Bang Du,Honghao Gao,Jian Wu
67|Dynamic causal modelling of immune heterogeneity|An interesting inference drawn by some Covid-19 epidemiological models is that there exists a proportion of the population who are not susceptible to infection -- even at the start of the current pandemic. This paper introduces a model of the immune response to a virus. This is based upon the same sort of mean-field dynamics as used in epidemiology. However, in place of the location, clinical status, and other attributes of people in an epidemiological model, we consider the state of a virus, B and T-lymphocytes, and the antibodies they generate. Our aim is to formalise some key hypotheses as to the mechanism of resistance. We present a series of simple simulations illustrating changes to the dynamics of the immune response under these hypotheses. These include attenuated viral cell entry, pre-existing cross-reactive humoral (antibody-mediated) immunity, and enhanced T-cell dependent immunity. Finally, we illustrate the potential application of this sort of model by illustrating variational inversion (using simulated data) of this model to illustrate its use in testing hypotheses. In principle, this furnishes a fast and efficient immunological assay--based on sequential serology--that provides a (i) quantitative measure of latent immunological responses and (ii) a Bayes optimal classification of the different kinds of immunological response (c.f., glucose tolerance tests used to test for insulin resistance). This may be especially useful in assessing SARS-CoV-2 vaccines.|http://arxiv.org/abs/2009.08411v1|Thomas Parr,Anjali Bhat,Peter Zeidman,Aimee Goel,Alexander J. Billig,Rosalyn Moran,Karl J. Friston
68|Towards More Flexible False Positive Control in Phase III Randomized Clinical Trials|Phase III randomized clinical trials play a monumentally critical role in the evaluation of new medical products. Because of the intrinsic nature of uncertainty embedded in our capability in assessing the efficacy of a medical product, interpretation of trial results relies on statistical principles to control the error of false positives below desirable level. The well-established statistical hypothesis testing procedure suffers from two major limitations, namely, the lack of flexibility in the thresholds to claim success and the lack of capability of controlling the total number of false positives that could be yielded by the large volume of trials. We propose two general theoretical frameworks based on the conventional frequentist paradigm and Bayesian perspectives, which offer realistic, flexible and effective solutions to these limitations. Our methods are based on the distribution of the effect sizes of the population of trials of interest. The estimation of this distribution is practically feasible as clinicaltrials.gov provides a centralized data repository with unbiased coverage of clinical trials. We provide a detailed development of the two frameworks with numerical results obtained for industry sponsored Phase III randomized clinical trials.|http://arxiv.org/abs/1902.08229v1|Changyu Shen,Xiaochun Li
69|A Machine Learning Approach for Recruitment Prediction in Clinical Trial Design|Significant advancements have been made in recent years to optimize patient recruitment for clinical trials, however, improved methods for patient recruitment prediction are needed to support trial site selection and to estimate appropriate enrollment timelines in the trial design stage. In this paper, using data from thousands of historical clinical trials, we explore machine learning methods to predict the number of patients enrolled per month at a clinical trial site over the course of a trial's enrollment duration. We show that these methods can reduce the error that is observed with current industry standards and propose opportunities for further improvement.|http://arxiv.org/abs/2111.07407v1|Jingshu Liu,Patricia J Allen,Luke Benz,Daniel Blickstein,Evon Okidi,Xiao Shi
70|A web application for the design of multi-arm clinical trials|Multi-arm designs provide an effective means of evaluating several treatments within the same clinical trial. Given the large number of treatments now available for testing in many disease areas, it has been argued that their utilisation should increase. However, for any given clinical trial there are numerous possible multi-arm designs that could be used, and choosing between them can be a difficult task. This task is complicated further by a lack of available easy-to-use software for designing multi-arm trials. To aid the wider implementation of multi-arm clinical trial designs, we have developed a web application for sample size calculation when using a variety of popular multiple comparison corrections. Furthermore, the application supports sample size calculation to control several varieties of power, as well as the determination of optimised arm-wise allocation ratios. It is built using the Shiny package in the R programming language, is free to access on any device with an internet browser, and requires no programming knowledge to use. The application provides the core information required by statisticians and clinicians to review the operating characteristics of a chosen multi-arm clinical trial design. We hope that it will assist with the future utilisation of such designs in practice.|http://arxiv.org/abs/1906.09178v1|Michael J Grayling,James MS Wason
71|Assessing the Validity of a a priori Patient-Trial Generalizability Score using Real-world Data from a Large Clinical Data Research Network: A Colorectal Cancer Clinical Trial Case Study|Existing trials had not taken enough consideration of their population representativeness, which can lower the effectiveness when the treatment is applied in real-world clinical practice. We analyzed the eligibility criteria of Bevacizumab colorectal cancer treatment trials, assessed their a priori generalizability, and examined how it affects patient outcomes when applied in real-world clinical settings. To do so, we extracted patient-level data from a large collection of electronic health records (EHRs) from the OneFlorida consortium. We built a zero-inflated negative binomial model using a composite patient-trial generalizability (cPTG) score to predict patients clinical outcomes (i.e., number of serious adverse events, (SAEs)). Our study results provide a body of evidence that 1) the cPTG scores can predict patient outcomes; and 2) patients who are more similar to the study population in the trials that were used to develop the treatment will have a significantly lower possibility to experience serious adverse events.|http://arxiv.org/abs/1906.10163v1|Qian Li,Zhe He,Yi Guo,Hansi Zhang,Thomas J George Jr,William Hogan,Neil Charness,Jiang Bian
72|On Bayesian Sequential Clinical Trial Designs|Clinical trials usually involve sequential patient entry. When designing a clinical trial, it is often desirable to include a provision for interim analyses of accumulating data with the potential for stopping the trial early. We review Bayesian sequential clinical trial designs based on posterior probabilities, posterior predictive probabilities, and decision-theoretic frameworks. A pertinent question is whether Bayesian sequential designs need to be adjusted for the planning of interim analyses. We answer this question from three perspectives: a frequentist-oriented perspective, a calibrated Bayesian perspective, and a subjective Bayesian perspective. We also provide new insights into the likelihood principle, which is commonly tied to statistical inference and decision making in sequential clinical trials. Some theoretical results are derived, and numerical studies are conducted to illustrate and assess these designs.|http://arxiv.org/abs/2112.09644v3|Tianjian Zhou,Yuan Ji
73|Design Considerations for Factorial Adaptive Multi-Arm Multi-Stage (FAST) Clinical Trials|Multi-Arm, Multi-Stage (MAMS) clinical trial designs allow for multiple therapies to be compared across a spectrum of clinical trial phases. MAMS designs can be categorized into several overarching design groups, including adaptive designs (AD) and multi-arm (MA) designs. Factorial clinical trials designs represent an additional group of designs which can provide increased efficiency relative to fixed, traditional designs. In this work, we explore design choices associated with Factorial Adaptive Multi-Arm Multi-Stage (FAST) designs, which represent the combination of factorial and MAMS designs. This category of trial can potentially offer benefits similar to both MAMS and factorial designs. This work is motivated by a proposed clinical trial under development.|http://arxiv.org/abs/2310.12830v1|Jonathan Beall,Jordan Elm,Mathew W Semler,Li Wang,Todd Rice,Hooman Kamel,William Mack,Akshitkumar M. Mistry
74|Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials|Online reinforcement learning (RL) algorithms offer great potential for personalizing treatment for participants in clinical trials. However, deploying an online, autonomous algorithm in the high-stakes healthcare setting makes quality control and data quality especially difficult to achieve. This paper proposes algorithm fidelity as a critical requirement for deploying online RL algorithms in clinical trials. It emphasizes the responsibility of the algorithm to (1) safeguard participants and (2) preserve the scientific utility of the data for post-trial analyses. We also present a framework for pre-deployment planning and real-time monitoring to help algorithm developers and clinical researchers ensure algorithm fidelity. To illustrate our framework's practical application, we present real-world examples from the Oralytics clinical trial. Since Spring 2023, this trial successfully deployed an autonomous, online RL algorithm to personalize behavioral interventions for participants at risk for dental disease.|http://arxiv.org/abs/2402.17003v2|Anna L. Trella,Kelly W. Zhang,Inbal Nahum-Shani,Vivek Shetty,Iris Yan,Finale Doshi-Velez,Susan A. Murphy
75|Augmented Binary Method for Basket Trials (ABBA)|In several clinical areas, traditional clinical trials often use a responder outcome, a composite endpoint that involves dichotomising a continuous measure. An augmented binary method that improves power whilst retaining the original responder endpoint has previously been proposed. The method leverages information from the the undichotomised component to improve power. We extend this method for basket trials, which are gaining popularity in many clinical areas. For clinical areas where response outcomes are used, we propose the new Augmented Binary method for BAsket trials (ABBA) enhances efficiency by borrowing information on the treatment effect between subtrials. The method is developed within a latent variable framework using a Bayesian hierarchical modelling approach. We investigate the properties of the proposed methodology by analysing point estimates and credible intervals in various simulation scenarios, comparing them to the standard analysis for basket trials that assumes binary outcome. Our method results in a reduction of 95% high density interval of the posterior distribution of the log odds ratio and an increase in power when the treatment effect is consistent across subtrials. We illustrate our approach using real data from two clinical trials in rheumatology.|http://arxiv.org/abs/2408.08636v1|Svetlana Cherlin,James M S Wason
76|On an Approach to Bayesian Sample Sizing in Clinical Trials|This paper explores an approach to Bayesian sample size determination in clinical trials. The approach falls into the category of what is often called "proper Bayesian", in that it does not mix frequentist concepts with Bayesian ones. A criterion for a "successful trial" is defined in terms of a posterior probability, its probability is assessed using the marginal distribution of the data, and this probability forms the basis for choosing sample sizes. We illustrate with a standard problem in clinical trials, that of establishing superiority of a new drug over a control.|http://arxiv.org/abs/1204.4460v1|Robb J. Muirhead,Adina I. Soaita
77|Assurance for clinical trial design with normally distributed outcomes: eliciting uncertainty about variances|The assurance method is growing in popularity in clinical trial planning. The method involves eliciting a prior distribution for the treatment effect, and then calculating the probability that a proposed trial will produce a `successful' outcome. For normally distributed observations, uncertainty about the variance of the normal distribution also needs to be accounted for, but there is little guidance in the literature on how to elicit a distribution for a variance parameter. We present a simple elicitation method, and illustrate how the elicited distribution is incorporated within an assurance calculation. We also consider multi-stage trials, where a decision to proceed with a larger trial will follow from the outcome of a smaller trial; we illustrate the role of the elicted distribution in assessing the information provided by a proposed smaller trial. Free software is available for implementing our methods.|http://arxiv.org/abs/1702.00978v2|Ziyad A. Alhussain,Jeremy E. Oakley
78|Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint|Clinical trials are indispensable in developing new treatments, but they face obstacles in patient recruitment and retention, hindering the enrollment of necessary participants. To tackle these challenges, deep learning frameworks have been created to match patients to trials. These frameworks calculate the similarity between patients and clinical trial eligibility criteria, considering the discrepancy between inclusion and exclusion criteria. Recent studies have shown that these frameworks outperform earlier approaches. However, deep learning models may raise fairness issues in patient-trial matching when certain sensitive groups of individuals are underrepresented in clinical trials, leading to incomplete or inaccurate data and potential harm. To tackle the issue of fairness, this work proposes a fair patient-trial matching framework by generating a patient-criterion level fairness constraint. The proposed framework considers the inconsistency between the embedding of inclusion and exclusion criteria among patients of different sensitive groups. The experimental results on real-world patient-trial and patient-criterion matching tasks demonstrate that the proposed framework can successfully alleviate the predictions that tend to be biased.|http://arxiv.org/abs/2303.13790v1|Chia-Yuan Chang,Jiayi Yuan,Sirui Ding,Qiaoyu Tan,Kai Zhang,Xiaoqian Jiang,Xia Hu,Na Zou
79|Scrybe: A Secure Audit Trail for Clinical Trial Data Fusion|Clinical trials are a multi-billion dollar industry. One of the biggest challenges facing the clinical trial research community is satisfying Part 11 of Title 21 of the Code of Federal Regulations and ISO 27789. These controls provide audit requirements that guarantee the reliability of the data contained in the electronic records. Context-aware smart devices and wearable IoT devices have become increasingly common in clinical trials. Electronic Data Capture (EDC) and Clinical Data Management Systems (CDMS) do not currently address the new challenges introduced using these devices. The healthcare digital threat landscape is continually evolving, and the prevalence of sensor fusion and wearable devices compounds the growing attack surface. We propose Scrybe, a permissioned blockchain, to store proof of clinical trial data provenance. We illustrate how Scrybe addresses each control and the limitations of the Ethereum-based blockchains. Finally, we provide a proof-of-concept integration with REDCap to show tamper resistance.|http://arxiv.org/abs/2109.05649v1|Jon Oakley,Carl Worley,Lu Yu,Richard Brooks,Ilker Ozcelik,Anthony Skjellum,Jihad Obeid
80|Local Explanations for Clinical Search Engine results|Health care professionals rely on treatment search engines to efficiently find adequate clinical trials and early access programs for their patients. However, doctors lose trust in the system if its underlying processes are unclear and unexplained. In this paper, a model-agnostic explainable method is developed to provide users with further information regarding the reasons why a clinical trial is retrieved in response to a query. To accomplish this, the engine generates features from clinical trials using by using a knowledge graph, clinical trial data and additional medical resources. and a crowd-sourcing methodology is used to determine their importance. Grounded on the proposed methodology, the rationale behind retrieving the clinical trials is explained in layman's terms so that healthcare processionals can effortlessly perceive them. In addition, we compute an explainability score for each of the retrieved items, according to which the items can be ranked. The experiments validated by medical professionals suggest that the proposed methodology induces trust in targeted as well as in non-targeted users, and provide them with reliable explanations and ranking of retrieved items.|http://arxiv.org/abs/2110.12891v1|Edeline Contempr,Zoltn Szlvik,Majid Mohammadi,Erick Velazquez,Annette ten Teije,Ilaria Tiddi
81|A data science approach to drug safety: Semantic and visual mining of adverse drug events from clinical trials of pain treatments|Clinical trials are the basis of Evidence-Based Medicine. Trial results are reviewed by experts and consensus panels for producing meta-analyses and clinical practice guidelines. However, reviewing these results is a long and tedious task, hence the meta-analyses and guidelines are not updated each time a new trial is published. Moreover, the independence of experts may be difficult to appraise. On the contrary, in many other domains, including medical risk analysis, the advent of data science, big data and visual analytics allowed moving from expert-based to fact-based knowledge. Since 12 years, many trial results are publicly available online in trial registries. Nevertheless, data science methods have not yet been applied widely to trial data. In this paper, we present a platform for analyzing the safety events reported during clinical trials and published in trial registries. This platform is based on an ontological model including 582 trials on pain treatments, and uses semantic web technologies for querying this dataset at various levels of granularity. It also relies on a 26-dimensional flower glyph for the visualization of the Adverse Drug Events (ADE) rates in 13 categories and 2 levels of seriousness. We illustrate the interest of this platform through several use cases and we were able to find back conclusions that were initially found during meta-analyses. The platform was presented to four experts in drug safety, and is publicly available online, with the ontology of pain treatment ADE.|http://arxiv.org/abs/2006.16910v2|Jean-Baptiste Lamy
82|TAD-SIE: Sample Size Estimation for Clinical Randomized Controlled Trials using a Trend-Adaptive Design with a Synthetic-Intervention-Based Estimator|Phase-3 clinical trials provide the highest level of evidence on drug safety and effectiveness needed for market approval by implementing large randomized controlled trials (RCTs). However, 30-40% of these trials fail mainly because such studies have inadequate sample sizes, stemming from the inability to obtain accurate initial estimates of average treatment effect parameters. To remove this obstacle from the drug development cycle, we present a new algorithm called Trend-Adaptive Design with a Synthetic-Intervention-Based Estimator (TAD-SIE) that appropriately powers a parallel-group trial, a standard RCT design, by leveraging a state-of-the-art hypothesis testing strategy and a novel trend-adaptive design (TAD). Specifically, TAD-SIE uses SECRETS (Subject-Efficient Clinical Randomized Controlled Trials using Synthetic Intervention) for hypothesis testing, which simulates a cross-over trial in order to boost power; doing so, makes it easier for a trial to reach target power within trial constraints (e.g., sample size limits). To estimate sample sizes, TAD-SIE implements a new TAD tailored to SECRETS given that SECRETS violates assumptions under standard TADs. In addition, our TAD overcomes the ineffectiveness of standard TADs by allowing sample sizes to be increased across iterations without any condition while controlling significance level with futility stopping. On a real-world Phase-3 clinical RCT (i.e., a two-arm parallel-group superiority trial with an equal number of subjects per arm), TAD-SIE reaches typical target operating points of 80% or 90% power and 5% significance level in contrast to baseline algorithms that only get at best 59% power and 4% significance level.|http://arxiv.org/abs/2401.03693v1|Sayeri Lala,Niraj K. Jha
83|Biomarker Clustering of Colorectal Cancer Data to Complement Clinical Classification|In this paper, we describe a dataset relating to cellular and physical conditions of patients who are operated upon to remove colorectal tumours. This data provides a unique insight into immunological status at the point of tumour removal, tumour classification and post-operative survival. Attempts are made to cluster this dataset and important subsets of it in an effort to characterize the data and validate existing standards for tumour classification. It is apparent from optimal clustering that existing tumour classification is largely unrelated to immunological factors within a patient and that there may be scope for re-evaluating treatment options and survival estimates based on a combination of tumour physiology and patient histochemistry.|http://arxiv.org/abs/1307.1601v1|Chris Roadknight,Uwe Aickelin,Alex Ladas,Daniele Soria,John Scholefield,Lindy Durrant
84|baskexact: An R package for analytical calculation of basket trial operating characteristics|Basket trials are a new type of clinical trial in which a treatment is investigated in several subgroups. For the analysis of these trials, information is shared between the subgroups based on the observed data to increase the power. Many approaches for the analysis of basket trials have been suggested, but only a few have been implemented in open source software packages. The R package baskexact facilitates the evaluation of two basket trial designs which use empirical Bayes techniques for sharing information. With baskexact, operating characteristics for single-stage and two-stage designs can be calculated analytically and optimal tuning parameters can be selected.|http://arxiv.org/abs/2403.17510v1|Lukas Baumann
85|Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models|Physicians considering clinical trials for their patients are met with the laborious process of checking many text based eligibility criteria. Large Language Models (LLMs) have shown to perform well for clinical information extraction and clinical reasoning, including medical tests, but not yet in real-world scenarios. This paper investigates the use of InstructGPT to assist physicians in determining eligibility for clinical trials based on a patient's summarised medical profile. Using a prompting strategy combining one-shot, selection-inference and chain-of-thought techniques, we investigate the performance of LLMs on 10 synthetically created patient profiles. Performance is evaluated at four levels: ability to identify screenable eligibility criteria from a trial given a medical profile; ability to classify for each individual criterion whether the patient qualifies; the overall classification whether a patient is eligible for a clinical trial and the percentage of criteria to be screened by physician. We evaluated against 146 clinical trials and a total of 4,135 eligibility criteria. The LLM was able to correctly identify the screenability of 72% (2,994/4,135) of the criteria. Additionally, 72% (341/471) of the screenable criteria were evaluated correctly. The resulting trial level classification as eligible or ineligible resulted in a recall of 0.5. By leveraging LLMs with a physician-in-the-loop, a recall of 1.0 and precision of 0.71 on clinical trial level can be achieved while reducing the amount of criteria to be checked by an estimated 90%. LLMs can be used to assist physicians with pre-screening of patients for clinical trials. By forcing instruction-tuned LLMs to produce chain-of-thought responses, the reasoning can be made transparent to and the decision process becomes amenable by physicians, thereby making such a system feasible for use in real-world scenarios.|http://arxiv.org/abs/2304.07396v2|Danny M. den Hamer,Perry Schoor,Tobias B. Polak,Daniel Kapitan
86|Large-scale Virtual Clinical Trials of Closed-loop Treatments for People with Type 1 Diabetes|We propose a virtual clinical trial for assessing the safety and efficacy of closed-loop diabetes treatments prior to an actual clinical trial. Such virtual trials enable rapid and risk-free pretrial testing of algorithms, and they can be used to compare different treatment variations for large and diverse populations. The participants are represented by multiple mathematical models, consisting of stochastic differential equations, and we use Monte Carlo closed-loop simulations to compute detailed statistics of the closed-loop treatments. We implement the virtual clinical trial using high-performance software and hardware, and we present an example trial with two mathematical models of one~million participants over 52~weeks (i.e., two~million simulations), which can be completed in 2~h 9~min.|http://arxiv.org/abs/2205.01332v1|Tobias K. S. Ritschel,Asbjrn Thode Reenberg,John Bagterp Jrgensen
87|Making all pairwise comparisons in multi-arm clinical trials without control treatment|The standard paradigm for confirmatory clinical trials is to compare experimental treatments with a control, for example the standard of care or a placebo. However, it is not always the case that a suitable control exists. Efficient statistical methodology is well studied in the setting of randomised controlled trials. This is not the case if one wishes to compare several experimental with no control arm. We propose hypothesis testing methods suitable for use in such a setting. These methods are efficient, ensuring the error rate is controlled at exactly the desired rate with no conservatism. This in turn yields an improvement in power when compared with standard methods one might otherwise consider using, such as a Bonferroni adjustment. The proposed testing procedure is also highly flexible. We show how it may be extended for use in multi-stage adaptive trials, covering the majority of scenarios in which one might consider the use of such procedures in the clinical trials setting. With such a highly flexible nature, these methods may also be applied more broadly outside of a clinical trials setting.|http://arxiv.org/abs/2410.20908v1|Thomas Burnett,Thomas Jaki
88|Recent advances in methodology for clinical trials in small populations: the InSPiRe project|Where there are a limited number of patients, such as in a rare disease, clinical trials in these small populations present several challenges, including statistical issues. This led to an EU FP7 call for proposals in 2013. One of the three projects funded was the Innovative Methodology for Small Populations Research (InSPiRe) project. This paper summarizes the main results of the project, which was completed in 2017. The InSPiRe project has led to development of novel statistical methodology for clinical trials in small populations in four areas. We have explored new decision-making methods for small population clinical trials using a Bayesian decision-theoretic framework to compare costs with potential benefits, developed approaches for targeted treatment trials, enabling simultaneous identification of subgroups and confirmation of treatment effect for these patients, worked on early phase clinical trial design and on extrapolation from adult to pediatric studies, developing methods to enable use of pharmacokinetics and pharmacodynamics data, and also developed improved robust meta-analysis methods for a small number of trials to support the planning, analysis and interpretation of a trial as well as enabling extrapolation between patient groups. In addition to scientific publications, we have contributed to regulatory guidance and produced free software in order to facilitate implementation of the novel methods.|http://arxiv.org/abs/1811.02504v1|T. Friede,M. Posch,S. Zohar,C. Alberti,N. Benda,E. Comets,S. Day,A. Dmitrenko,A. Graf,B. K. Gnhan,S. W. Hee,F. Lentz,J. Madan,F. Miller,T. Ondra,M. Pearce,C. Rver,A. Tournazi,S. Unkel,M. Ursino,G. Wassmer,N. Stallard
89|Selection Induced Contrast Estimate (SICE) Effect: An Attempt to Quantify the Impact of Some Patient Selection Criteria in Randomized Clinical Trials|Defining the Inclusion/Exclusion (I/E) criteria of a trial is one of the most important steps during a trial design. Increasingly complex I/E criteria potentially create information imbalance and transparency issues between the people who design and run the trials and those who consume the information produced by the trials. In order to better understand and quantify the impact of a category of I/E criteria on observed treatment effects, a concept, named the Selection Induced Contrast Estimate (SICE) effect, is introduced and formulated in this paper. The SICE effect can exist in controlled clinical trials when treatment affects the correlation between a marker used for selection and the response of interest. This effect is demonstrated with both simulations and real clinical trial data. Although the statistical elements behind the SICE effect have been well studied, explicitly formulating and studying this effect can benefit several areas, including better transparency in I/E criteria, meta-analysis of multiple clinical trials, treatment effect interpretation in real-world medical practice, etc.|http://arxiv.org/abs/2001.02036v1|Junshui Ma,Daniel J. Holder
90|Data monitoring committees for clinical trials evaluating treatments of COVID-19|The first cases of coronavirus disease 2019 (COVID-19) were reported in December 2019 and the outbreak of SARS-CoV-2 was declared a pandemic in March 2020 by the World Health Organization. This sparked a plethora of investigations into diagnostics and vaccination for SARS-CoV-2, as well as treatments for COVID-19. Since COVID-19 is a severe disease associated with a high mortality, clinical trials in this disease should be monitored by a data monitoring committee (DMC), also known as data safety monitoring board (DSMB). DMCs in this indication face a number of challenges including fast recruitment requiring an unusually high frequency of safety reviews, more frequent use of complex designs and virtually no prior experience with the disease. In this paper, we provide a perspective on the work of DMCs for clinical trials of treatments for COVID-19. More specifically, we discuss organizational aspects of setting up and running DMCs for COVID-19 trials, in particular for trials with more complex designs such as platform trials or adaptive designs. Furthermore, statistical aspects of monitoring clinical trials of treatments for COVID-19 are considered. Some recommendations are made regarding the presentation of the data, stopping rules for safety monitoring and the use of external data. The proposed stopping boundaries are assessed in a simulation study motivated by clinical trials in COVID-19.|http://arxiv.org/abs/2008.10992v1|Tobias Mtze,Tim Friede
91|Coping with Information Loss and the Use of Auxiliary Sources of Data: A Report from the NISS Ingram Olkin Forum Series on Unplanned Clinical Trial Disruptions|Clinical trials disruption has always represented a non negligible part of the ending of interventional studies. While the SARS-CoV-2 (COVID-19) pandemic has led to an impressive and unprecedented initiation of clinical research, it has also led to considerable disruption of clinical trials in other disease areas, with around 80% of non-COVID-19 trials stopped or interrupted during the pandemic. In many cases the disrupted trials will not have the planned statistical power necessary to yield interpretable results. This paper describes methods to compensate for the information loss arising from trial disruptions by incorporating additional information available from auxiliary data sources. The methods described include the use of auxiliary data on baseline and early outcome data available from the trial itself and frequentist and Bayesian approaches for the incorporation of information from external data sources. The methods are illustrated by application to the analysis of artificial data based on the Primary care pediatrics Learning Activity Nutrition (PLAN) study, a clinical trial assessing a diet and exercise intervention for overweight children, that was affected by the COVID-19 pandemic. We show how all of the methods proposed lead to an increase in precision relative to use of complete case data only.|http://arxiv.org/abs/2206.11238v1|Silvia Calderazzo,Sergey Tarima,Carissa Reid,Nancy Flournoy,Tim Friede,Nancy Geller,James L Rosenberger,Nigel Stallard,Moreno Ursino,Marc Vandemeulebroecke,Kelly Van Lancker,Sarah Zohar
92|Oncology clinical trial design planning based on a multistate model that jointly models progression-free and overall survival endpoints|When planning an oncology clinical trial, the usual approach is to assume proportional hazards and even an exponential distribution for time-to-event endpoints. Often, besides the gold-standard endpoint overall survival (OS), progression-free survival (PFS) is considered as a second confirmatory endpoint. We use a survival multistate model to jointly model these two endpoints and find that neither exponential distribution nor proportional hazards will typically hold for both endpoints simultaneously. The multistate model provides a stochastic process approach to model the dependency of such endpoints neither requiring latent failure times nor explicit dependency modelling such as copulae. We use the multistate model framework to simulate clinical trials with endpoints OS and PFS and show how design planning questions can be answered using this approach. In particular, non-proportional hazards for at least one of the endpoints are naturally modelled as well as their dependency to improve planning. We consider an oncology trial on non-small-cell lung cancer as a motivating example from which we derive relevant trial design questions. We then illustrate how clinical trial design can be based on simulations from a multistate model. Key applications are co-primary endpoints and group-sequential designs. Simulations for these applications show that the standard simplifying approach may very well lead to underpowered or overpowered clinical trials. Our approach is quite general and can be extended to more complex trial designs, further endpoints, and other therapeutic areas. An R package is available on CRAN.|http://arxiv.org/abs/2301.10059v2|Alexandra Erdmann,Jan Beyersmann,Kaspar Rufibach
93|A computational hierarchy in human cortex|Hierarchies feature prominently in anatomical accounts of cortical organisation. An open question is which computational (algorithmic) processes are implemented by these hierarchies. One renowned hypothesis is that cortical hierarchies implement a model of the world's causal structure and serve to infer environmental states from sensory inputs. This view, which casts perception as hierarchical Bayesian inference, has become a highly influential concept in both basic and clinical neuroscience. So far, however, a direct correspondence between the predicted order of hierarchical Bayesian computations and the sequence of evoked neuronal activity has not been demonstrated. Here, we present evidence for this correspondence from neuroimaging and electrophysiological data in healthy volunteers. Trial-wise sequences of hierarchical computations were inferred from participants' behaviour during a social learning task that required multi-level inference about intentions. We found that the temporal sequence of neuronal activity matched the order of computations as predicted by the theory. These findings provide strong evidence for the operation of hierarchical Bayesian inference in human cortex. Furthermore, our approach offers a novel strategy for the combined computational-physiological phenotyping of patients with disorders of perception, such as schizophrenia or autism.|http://arxiv.org/abs/1709.02323v1|Andreea O. Diaconescu,Vladimir Litvak,Christoph Mathys,Lars Kasper,Karl J. Friston,Klaas E. Stephan
94|Bayesian deep neural networks for low-cost neurophysiological markers of Alzheimer's disease severity|As societies around the world are ageing, the number of Alzheimer's disease (AD) patients is rapidly increasing. To date, no low-cost, non-invasive biomarkers have been established to advance the objectivization of AD diagnosis and progression assessment. Here, we utilize Bayesian neural networks to develop a multivariate predictor for AD severity using a wide range of quantitative EEG (QEEG) markers. The Bayesian treatment of neural networks both automatically controls model complexity and provides a predictive distribution over the target function, giving uncertainty bounds for our regression task. It is therefore well suited to clinical neuroscience, where data sets are typically sparse and practitioners require a precise assessment of the predictive uncertainty. We use data of one of the largest prospective AD EEG trials ever conducted to demonstrate the potential of Bayesian deep learning in this domain, while comparing two distinct Bayesian neural network approaches, i.e., Monte Carlo dropout and Hamiltonian Monte Carlo.|http://arxiv.org/abs/1812.04994v2|Wolfgang Fruehwirt,Adam D. Cobb,Martin Mairhofer,Leonard Weydemann,Heinrich Garn,Reinhold Schmidt,Thomas Benke,Peter Dal-Bianco,Gerhard Ransmayr,Markus Waser,Dieter Grossegger,Pengfei Zhang,Georg Dorffner,Stephen Roberts
95|A Contextual-bandit-based Approach for Informed Decision-making in Clinical Trials|Clinical trials involving multiple treatments utilize randomization of the treatment assignments to enable the evaluation of treatment efficacies in an unbiased manner. Such evaluation is performed in post hoc studies that usually use supervised-learning methods that rely on large amounts of data collected in a randomized fashion. That approach often proves to be suboptimal in that some participants may suffer and even die as a result of having not received the most appropriate treatments during the trial. Reinforcement-learning methods improve the situation by making it possible to learn the treatment efficacies dynamically during the course of the trial, and to adapt treatment assignments accordingly. Recent efforts using \textit{multi-arm bandits}, a type of reinforcement-learning methods, have focused on maximizing clinical outcomes for a population that was assumed to be homogeneous. However, those approaches have failed to account for the variability among participants that is becoming increasingly evident as a result of recent clinical-trial-based studies. We present a contextual-bandit-based online treatment optimization algorithm that, in choosing treatments for new participants in the study, takes into account not only the maximization of the clinical outcomes but also the patient characteristics. We evaluated our algorithm using a real clinical trial dataset from the International Stroke Trial. The results of our retrospective analysis indicate that the proposed approach performs significantly better than either a random assignment of treatments (the current gold standard) or a multi-arm-bandit-based approach, providing substantial gains in the percentage of participants who are assigned the most suitable treatments. The contextual-bandit and multi-arm bandit approaches provide 72.63% and 64.34% gains, respectively, compared to a random assignment.|http://arxiv.org/abs/1809.00258v1|Yogatheesan Varatharajah,Brent Berry,Sanmi Koyejo,Ravishankar Iyer
96|Contextual Constrained Learning for Dose-Finding Clinical Trials|Clinical trials in the medical domain are constrained by budgets. The number of patients that can be recruited is therefore limited. When a patient population is heterogeneous, this creates difficulties in learning subgroup specific responses to a particular drug and especially for a variety of dosages. In addition, patient recruitment can be difficult by the fact that clinical trials do not aim to provide a benefit to any given patient in the trial. In this paper, we propose C3T-Budget, a contextual constrained clinical trial algorithm for dose-finding under both budget and safety constraints. The algorithm aims to maximize drug efficacy within the clinical trial while also learning about the drug being tested. C3T-Budget recruits patients with consideration of the remaining budget, the remaining time, and the characteristics of each group, such as the population distribution, estimated expected efficacy, and estimation credibility. In addition, the algorithm aims to avoid unsafe dosages. These characteristics are further illustrated in a simulated clinical trial study, which corroborates the theoretical analysis and demonstrates an efficient budget usage as well as a balanced learning-treatment trade-off.|http://arxiv.org/abs/2001.02463v2|Hyun-Suk Lee,Cong Shen,James Jordon,Mihaela van der Schaar
97|Improving efficiency of inference in clinical trials with external control data|Suppose we are interested in the effect of a treatment in a clinical trial. The efficiency of inference may be limited due to small sample size. However, external control data are often available from historical studies. Motivated by an application to Helicobacter pylori infection, we show how to borrow strength from such data to improve efficiency of inference in the clinical trial. Under an exchangeability assumption about the potential outcome mean, we show that the semiparametric efficiency bound for estimating the average treatment effect can be reduced by incorporating both the clinical trial data and external controls. We then derive a doubly robust and locally efficient estimator. The improvement in efficiency is prominent especially when the external control dataset has a large sample size and small variability. Our method allows for a relaxed overlap assumption, and we illustrate with the case where the clinical trial only contains a treated group. We also develop doubly robust and locally efficient approaches that extrapolate the causal effect in the clinical trial to the external population and the overall population. Our results also offer a meaningful implication for trial design and data collection. We evaluate the finite-sample performance of the proposed estimators via simulation. In the Helicobacter pylori infection application, our approach shows that the combination treatment has potential efficacy advantages over the triple therapy.|http://arxiv.org/abs/2011.07234v2|Xinyu Li,Wang Miao,Fang Lu,Xiao-Hua Zhou
98|Applying the Estimand and Target Trial frameworks to external control analyses using observational data: a case study in the solid tumor setting|In causal inference, the correct formulation of the scientific question of interest is a crucial step. Here we apply the estimand framework to a comparison of the outcomes of patient-level clinical trials and observational data to help structure the clinical question. In addition, we complement the estimand framework with the target trial framework to address specific issues in defining the estimand attributes using observational data and discuss synergies and differences of the two frameworks. Whereas the estimand framework proves useful to address the challenge that in clinical trials and routine clinical practice patients may switch to subsequent systemic therapies after the initially assigned systematic treatment, the target trial framework supports addressing challenges around baseline confounding and the index date. We apply the combined framework to compare long-term outcomes of a pooled set of three previously reported randomized phase 3 trials studying patients with metastatic non-small cell lung cancer receiving front-line chemotherapy (randomized clinical trial cohort) and similar patients treated with front-line chemotherapy as part of routine clinical care (observational comparative cohort). We illustrate the process to define the estimand attributes and select the estimator to estimate the estimand of interest while accounting for key baseline confounders, index date, and receipt of subsequent therapies. The proposed combined framework provides more clarity on the causal contrast of interest and the estimator to adopt and thus facilitates design and interpretation of the analyses.|http://arxiv.org/abs/2208.06707v1|Letizia Polito,Qixing Liang,Navdeep Pal,Philani Mpofu,Ahmed Sawas,Olivier Humblet,Kaspar Rufibach,Dominik Heinzmann
99|Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology|Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially in structuring patient information from longitudinal medical records.|http://arxiv.org/abs/2308.02180v3|Cliff Wong,Sheng Zhang,Yu Gu,Christine Moung,Jacob Abel,Naoto Usuyama,Roshanthi Weerasinghe,Brian Piening,Tristan Naumann,Carlo Bifulco,Hoifung Poon
100|Negative Spillover: A Potential Source of Bias in Pragmatic Clinical Trials|Pragmatic clinical trials evaluate the effectiveness of health interventions in real-world settings. Negative spillover can arise in a pragmatic trial if the study intervention affects how scarce resources are allocated between patients in the intervention and comparison groups. This can harm patients assigned to the control group and lead to overestimation of treatment effect. While this type of negative spillover is often addressed in trials of social welfare and public health interventions, there is little recognition of this source of bias in the medical literature. In this article, I examine what causes negative spillover and how it may have led clinical trial investigators to overestimate the effect of patient navigation, AI-based physiological alarms, and elective induction of labor. I also suggest ways to detect negative spillover and design trials that avoid this potential source of bias.|http://arxiv.org/abs/2309.10978v4|Sean Mann
101|Incorporating external data for analyzing randomized clinical trials: A transfer learning approach|Randomized clinical trials are the gold standard for analyzing treatment effects, but high costs and ethical concerns can limit recruitment, potentially leading to invalid inferences. Incorporating external trial data with similar characteristics into the analysis using transfer learning appears promising for addressing these issues. In this paper, we present a formal framework for applying transfer learning to the analysis of clinical trials, considering three key perspectives: transfer algorithm, theoretical foundation, and inference method. For the algorithm, we adopt a parameter-based transfer learning approach to enhance the lasso-adjusted stratum-specific estimator developed for estimating treatment effects. A key component in constructing the transfer learning estimator is deriving the regression coefficient estimates within each stratum, accounting for the bias between source and target data. To provide a theoretical foundation, we derive the $l_1$ convergence rate for the estimated regression coefficients and establish the asymptotic normality of the transfer learning estimator. Our results show that when external trial data resembles current trial data, the sample size requirements can be reduced compared to using only the current trial data. Finally, we propose a consistent nonparametric variance estimator to facilitate inference. Numerical studies demonstrate the effectiveness and robustness of our proposed estimator across various scenarios.|http://arxiv.org/abs/2409.04126v1|Yujia Gu,Hanzhong Liu,Wei Ma
102|A Conservative Approach to Leveraging External Evidence for Effective Clinical Trial Design|Prior probabilities of clinical hypotheses are not systematically used for clinical trial design yet, due to a concern that poor priors may lead to poor decisions. To address this concern, a conservative approach to Bayesian trial design is illustrated here, requiring that the operational characteristics of the primary trial outcome are stronger than the prior. This approach is complementary to current Bayesian design methods, in that it insures against prior-data conflict by defining a sample size commensurate to a discrete design prior. This approach is ethical, in that it requires designs appropriate to achieving pre-specified levels of clinical equipoise imbalance. Practical examples are discussed, illustrating design of trials with binary or time to event endpoints. Moderate increases in phase II study sample size are shown to deliver strong levels of overall evidence for go/no-go clinical development decisions. Levels of negative evidence provided by group sequential confirmatory designs are found negligible, highlighting the importance of complementing efficacy boundaries with non-binding futility criteria.|http://arxiv.org/abs/2211.02381v4|Fabio Rigat
103|Leveraging Historical Data for High-Dimensional Regression Adjustment, a Composite Covariate Approach|The amount of data collected from patients involved in clinical trials is continuously growing. All patient characteristics are potential covariates that could be used to improve clinical trial analysis and power. However, the restricted number of patients in phases I and II studies limits the possible number of covariates included in the analyses. In this paper, we investigate the cost/benefit ratio of including covariates in the analysis of clinical trials. Within this context, we address the long-running question "What is the optimum number of covariates to include in a clinical trial?" To further improve the cost/benefit ratio of covariates, historical data can be leveraged to pre-specify the covariate weights, which can be viewed as the definition of a new composite covariate. We analyze the use of a composite covariate while estimating the treatment effect in small clinical trials. A composite covariate limits the loss of degrees of freedom and the risk of overfitting.|http://arxiv.org/abs/2103.14421v1|Samuel Branders,Alvaro Pereira,Guillaume Bernard,Marie Ernst,Adelin Albert
104|Utilizing ChatGPT to Enhance Clinical Trial Enrollment|Clinical trials are a critical component of evaluating the effectiveness of new medical interventions and driving advancements in medical research. Therefore, timely enrollment of patients is crucial to prevent delays or premature termination of trials. In this context, Electronic Health Records (EHRs) have emerged as a valuable tool for identifying and enrolling eligible participants. In this study, we propose an automated approach that leverages ChatGPT, a large language model, to extract patient-related information from unstructured clinical notes and generate search queries for retrieving potentially eligible clinical trials. Our empirical evaluation, conducted on two benchmark retrieval collections, shows improved retrieval performance compared to existing approaches when several general-purposed and task-specific prompts are used. Notably, ChatGPT-generated queries also outperform human-generated queries in terms of retrieval performance. These findings highlight the potential use of ChatGPT to enhance clinical trial enrollment while ensuring the quality of medical service and minimizing direct risks to patients.|http://arxiv.org/abs/2306.02077v1|Georgios Peikos,Symeon Symeonidis,Pranav Kasela,Gabriella Pasi
105|Multivariate Rank-Based Analysis of Multiple Endpoints in Clinical Trials: A Global Test Approach|Clinical trials often involve the assessment of multiple endpoints to comprehensively evaluate the efficacy and safety of interventions. In the work, we consider a global nonparametric testing procedure based on multivariate rank for the analysis of multiple endpoints in clinical trials. Unlike other existing approaches that rely on pairwise comparisons for each individual endpoint, the proposed method directly incorporates the multivariate ranks of the observations. By considering the joint ranking of all endpoints, the proposed approach provides robustness against diverse data distributions and censoring mechanisms commonly encountered in clinical trials. Through extensive simulations, we demonstrate the superior performance of the multivariate rank-based approach in controlling type I error and achieving higher power compared to existing rank-based methods. The simulations illustrate the advantages of leveraging multivariate ranks and highlight the robustness of the approach in various settings. The proposed method offers an effective tool for the analysis of multiple endpoints in clinical trials, enhancing the reliability and efficiency of outcome evaluations.|http://arxiv.org/abs/2306.15380v2|Kexuan Li,Lingli Yang,Shaofei Zhao,Susie Sinks,Luan Lin,Peng Sun
106|Team IELAB at TREC Clinical Trial Track 2023: Enhancing Clinical Trial Retrieval with Neural Rankers and Large Language Models|We describe team ielab from CSIRO and The University of Queensland's approach to the 2023 TREC Clinical Trials Track. Our approach was to use neural rankers but to utilise Large Language Models to overcome the issue of lack of training data for such rankers. Specifically, we employ ChatGPT to generate relevant patient descriptions for randomly selected clinical trials from the corpus. This synthetic dataset, combined with human-annotated training data from previous years, is used to train both dense and sparse retrievers based on PubmedBERT. Additionally, a cross-encoder re-ranker is integrated into the system. To further enhance the effectiveness of our approach, we prompting GPT-4 as a TREC annotator to provide judgments on our run files. These judgments are subsequently employed to re-rank the results. This architecture tightly integrates strong PubmedBERT-based rankers with the aid of SOTA Large Language Models, demonstrating a new approach to clinical trial retrieval.|http://arxiv.org/abs/2401.01566v1|Shengyao Zhuang,Bevan Koopman,Guido Zuccon
107|Using Large Language Models to Generate Clinical Trial Tables and Figures|Tables, figures, and listings (TFLs) are essential tools for summarizing clinical trial data. Creation of TFLs for reporting activities is often a time-consuming task encountered routinely during the execution of clinical trials. This study explored the use of large language models (LLMs) to automate the generation of TFLs through prompt engineering and few-shot transfer learning. Using public clinical trial data in ADaM format, our results demonstrated that LLMs can efficiently generate TFLs with prompt instructions, showcasing their potential in this domain. Furthermore, we developed a conservational agent named Clinical Trial TFL Generation Agent: An app that matches user queries to predefined prompts that produce customized programs to generate specific predefined TFLs.|http://arxiv.org/abs/2409.12046v2|Yumeng Yang,Peter Krusche,Kristyn Pantoja,Cheng Shi,Ethan Ludmir,Kirk Roberts,Gen Zhu
108|Centrality in Collaboration: A Novel Algorithm for Social Partitioning Gradients in Community Detection for Multiple Oncology Clinical Trial Enrollments|Patients at a comprehensive cancer center who do not achieve cure or remission following standard treatments often become candidates for clinical trials. Patients who participate in a clinical trial may be suitable for other studies. A key factor influencing patient enrollment in subsequent clinical trials is the structured collaboration between oncologists and most responsible physicians. Possible identification of these collaboration networks can be achieved through the analysis of patient movements between clinical trial intervention types with social network analysis and community detection algorithms. In the detection of oncologist working groups, the present study evaluates three community detection algorithms: Girvan-Newman, Louvain and an algorithm developed by the author. Girvan-Newman identifies each intervention as their own community, while Louvain groups interventions in a manner that is difficult to interpret. In contrast, the author's algorithm groups interventions in a way that is both intuitive and informative, with a gradient evident in social partitioning that is particularly useful for epidemiological research. This lays the groundwork for future subgroup analysis of clustered interventions.|http://arxiv.org/abs/2411.01394v2|Benjamin Smith,Tyler Pittman,Wei Xu
109|Incorporating external information in analyses of clinical trials with binary outcomes|External information, such as prior information or expert opinions, can play an important role in the design, analysis and interpretation of clinical trials. However, little attention has been devoted thus far to incorporating external information in clinical trials with binary outcomes, perhaps due to the perception that binary outcomes can be treated as normally-distributed outcomes by using normal approximations. In this paper we show that these two types of clinical trials could behave differently, and that special care is needed for the analysis of clinical trials with binary outcomes. In particular, we first examine a simple but commonly used univariate Bayesian approach and observe a technical flaw. We then study the full Bayesian approach using different beta priors and a new frequentist approach based on the notion of confidence distribution (CD). These approaches are illustrated and compared using data from clinical studies and simulations. The full Bayesian approach is theoretically sound, but surprisingly, under skewed prior distributions, the estimate derived from the marginal posterior distribution may not fall between those from the marginal prior and the likelihood of clinical trial data. This counterintuitive phenomenon, which we call the "discrepant posterior phenomenon," does not occur in the CD approach. The CD approach is also computationally simpler and can be applied directly to any prior distribution, symmetric or skewed.|http://arxiv.org/abs/1304.6208v1|Minge Xie,Regina Y. Liu,C. V. Damaraju,William H. Olson
110|Sample size calculation based on the difference in restricted mean time lost for clinical trials with competing risks|Computation of sample size is important when designing clinical trials. The presence of competing risks makes the design of clinical trials with time-to-event endpoints cumbersome. A model based on the subdistribution hazard ratio (SHR) is commonly used for trials under competing risks. However, this approach has some limitations related to model assumptions and clinical interpretation. Considering such limitations, the difference in restricted mean time lost (RMTLd) is recommended as an alternative indicator. In this paper, we propose a sample size calculation method based on the RMTLd for the Weibull distribution (RMTLdWeibull) for clinical trials, which considers experimental conditions such as equal allocation, uniform accrual, uniform loss to follow-up, and administrative censoring. Simulation results show that sample size calculation based on the RMTLdWeibull can generally achieve a predefined power level and maintain relative robustness. Moreover, the performance of the sample size calculation based on the RMTLdWeibull is similar or superior to that based on the SHR. Even if the event time does not follow the Weibull distribution, the sample size calculation based on the RMTLdWeibull still performs well. The results also verify the performance of the sample size calculation method based on the RMTLdWeibull. From the perspective of the results of this study, clinical interpretation, application conditions and statistical performance, we recommend that when designing clinical trials in the presence of competing risks, the RMTLd indicator be applied for sample size calculation and subsequent effect size measurement.|http://arxiv.org/abs/2311.12293v1|Xiang Geng,Zhaojin Li,Chengfeng Zhang,Yanjie Wang,Haoning Shen,Zhiheng Huang,Yawen Hou,Zheng Chen
111|Simulation-based Bayesian predictive probability of success for interim monitoring of clinical trials with competing event data: two case studies|Bayesian predictive probabilities of success (PPoS) use interim trial data to calculate the probability of trial success. These quantities can be used to optimize trial size or to stop for futility. In this paper, we describe a simulation-based approach to compute the PPoS for clinical trials with competing event data, for which no specific methodology is currently available. The proposed procedure hinges on modelling the joint distribution of time to event and event type by specifying Bayesian models for the cause-specific hazards of all event types. This allows the prediction of outcome data at the conclusion of the trial. The PPoS is obtained by numerically averaging the probability of success evaluated at fixed parameter values over the posterior distribution of the parameters. Our work is motivated by two randomised clinical trials: the I-SPY COVID phase II trial for the treatment of severe COVID-19 (NCT04488081) and the STHLM3 prostate cancer diagnostic trial (ISRCTN84445406), both of which are characterised by competing event data. We present different modelling alternatives for the joint distribution of time to event and event type and show how the choice of the prior distributions can be used to assess the PPoS under different scenarios. The role of the PPoS analyses in the decision making process for these two trials is also discussed.|http://arxiv.org/abs/2412.15899v1|Chiara Micoli,Alessio Crippa,Jason T. Connor,I-SPY COVID Consortium,Martin Eklund,Andrea Discacciati
112|How essential are unstructured clinical narratives and information fusion to clinical trial recruitment?|Electronic health records capture patient information using structured controlled vocabularies and unstructured narrative text. While structured data typically encodes lab values, encounters and medication lists, unstructured data captures the physician's interpretation of the patient's condition, prognosis, and response to therapeutic intervention. In this paper, we demonstrate that information extraction from unstructured clinical narratives is essential to most clinical applications. We perform an empirical study to validate the argument and show that structured data alone is insufficient in resolving eligibility criteria for recruiting patients onto clinical trials for chronic lymphocytic leukemia (CLL) and prostate cancer. Unstructured data is essential to solving 59% of the CLL trial criteria and 77% of the prostate cancer trial criteria. More specifically, for resolving eligibility criteria with temporal constraints, we show the need for temporal reasoning and information integration with medical events within and across unstructured clinical narratives and structured data.|http://arxiv.org/abs/1502.04049v1|Preethi Raghavan,James L. Chen,Eric Fosler-Lussier,Albert M. Lai
113|Deep Learning Derived Histopathology Image Score for Increasing Phase 3 Clinical Trial Probability of Success|Failures in Phase 3 clinical trials contribute to expensive cost of drug development in oncology. To drastically reduce such cost, responders to an oncology treatment need to be identified early on in the drug development process with limited amount of patient data before the planning of Phase 3 clinical trials. Despite the challenge of small sample size, we pioneered the use of deep-learning derived digital pathology scores to identify responders based on the immunohistochemistry images of the target antigen expressed in tumor biopsy samples from a Phase 1 Non-small Cell Lung Cancer clinical trial. Based on repeated 10-fold cross validations, the deep-learning derived score on average achieved 4% higher AUC of ROC curve and 6% higher AUC of Precision-Recall curve comparing to the tumor proportion score (TPS) based clinical benchmark. In a small independent testing set of patients, we also demonstrated that the deep-learning derived score achieved numerically at least 25% higher responder rate in the enriched population than the TPS clinical benchmark.|http://arxiv.org/abs/2011.05406v1|Qi Tang,Vardaan Kishore Kumar
114|From RAGs to riches: Using large language models to write documents for clinical trials|Clinical trials require numerous documents to be written -- protocols, consent forms, clinical study reports and others. Large language models (LLMs) offer the potential to rapidly generate first versions of these documents, however there are concerns about the quality of their output Here we report an evaluation of LLMs in generating parts of one such document, clinical trial protocols. We find that an offthe-shelf LLM delivers reasonable results, especially when assessing content relevance and the correct use of terminology. However, deficiencies remain: specifically clinical thinking and logic, and appropriate use of references. To improve performance, we used retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date information. As a result of using RAG, the writing quality of the LLM improves substantially, which has implications for the practical useability of LLMs in clinical trial-related writing.|http://arxiv.org/abs/2402.16406v1|Nigel Markey,Ilyass El-Mansouri,Gaetan Rensonnet,Casper van Langen,Christoph Meier
115|Summary of Information Theoretic Quantities|Information theory is a practical and theoretical framework developed for the study of communication over noisy channels. Its probabilistic basis and capacity to relate statistical structure to function make it ideally suited for studying information flow in the nervous system. As a framework it has a number of useful properties: it provides a general measure sensitive to any relationship, not only linear effects; its quantities have meaningful units which in many cases allow direct comparison between different experiments; and it can be used to study how much information can be gained by observing neural responses in single experimental trials, rather than in averages over multiple trials. A variety of information theoretic quantities are in common use in neuroscience - including the Shannon entropy, Kullback-Leibler divergence, and mutual information. In this entry, we introduce and define these quantities. Further details on how these quantities can be estimated in practice are provided in the entry "Estimation of Information-Theoretic Quantities" and examples of application of these techniques in neuroscience can be found in the entry "Applications of Information-Theoretic Quantities in Neuroscience".|http://arxiv.org/abs/1501.01854v1|Robin A. A. Ince,Stefano Panzeri,Simon R. Schultz
116|Application of Multiple Imputation When Using Propensity Score Methods to Generalize Clinical Trials to Target Populations of Interest|When the distribution of treatment effect modifiers differs between the trial sample and target population, inverse probability weighting (IPSW) can be applied to achieve an unbiased estimate of the population average treatment effect in the target population. The statistical validity of IPSW is threatened when there are missing data in the target population, including potential missingness in trial sample. However, missing data methods have not been adequately discussed in the current literature. We conducted a set of simulation studies to determine how to apply multiple imputation (MI) in the context of IPSW. We specifically addressed questions such as which variables to include in the imputation model and whether they should come from trial or non-trial portion of the target population. Based on our findings, we recommend including all potential effect modifiers and trial indicator from both trial and non-trial populations, as well as treatment and outcome variables from trial sample in the imputation model as main effects. Additionally, we have illustrated ideas by transporting findings from the Frequent Hemodialysis Network (FHN) Daily Trial to the United States Renal Stage System (USRDS) population.|http://arxiv.org/abs/2202.00827v2|Albee Y. Ling,Maria E. Montez-Rath,Kris Kapphahn,Manisha Desai
117|Adaptive Randomization Methods for Sequential Multiple Assignment Randomized Trials (SMARTs) via Thompson Sampling|Response-adaptive randomization (RAR) has been studied extensively in conventional, single-stage clinical trials, where it has been shown to yield ethical and statistical benefits, especially in trials with many treatment arms. However, RAR and its potential benefits are understudied in sequential multiple assignment randomized trials (SMARTs), which are the gold-standard trial design for evaluation of multi-stage treatment regimes. We propose a suite of RAR algorithms for SMARTs based on Thompson Sampling (TS), a widely used RAR method in single-stage trials in which treatment randomization probabilities are aligned with the estimated probability that the treatment is optimal. We focus on two common objectives in SMARTs: (i) comparison of the regimes embedded in the trial, and (ii) estimation of an optimal embedded regime. We develop valid post-study inferential procedures for treatment regimes under the proposed algorithms. This is nontrivial, as (even in single-stage settings) RAR can lead to nonnormal limiting distributions of estimators. Our algorithms are the first for RAR in multi-stage trials that account for nonregularity in the estimand. Empirical studies based on real-world SMARTs show that TS can improve in-trial subject outcomes without sacrificing efficiency for post-trial comparisons.|http://arxiv.org/abs/2401.03268v1|Peter Norwood,Marie Davidian,Eric Laber
118|Conditional Similarity Triplets Enable Covariate-Informed Representations of Single-Cell Data|Single-cell technologies enable comprehensive profiling of diverse immune cell-types through the measurement of multiple genes or proteins per individual cell. In order to translate immune signatures assayed from blood or tissue into powerful diagnostics, machine learning approaches are often employed to compute immunological summaries or per-sample featurizations, which can be used as inputs to models for outcomes of interest. Current supervised learning approaches for computing per-sample representations are trained only to accurately predict a single outcome and do not take into account relevant additional clinical features or covariates that are likely to also be measured for each sample. Here, we introduce a novel approach for incorporating measured covariates in optimizing model parameters to ultimately specify per-sample encodings that accurately affect both immune signatures and additional clinical information. Our introduced method CytoCoSet is a set-based encoding method for learning per-sample featurizations, which formulates a loss function with an additional triplet term penalizing samples with similar covariates from having disparate embedding results in per-sample representations. Overall, incorporating clinical covariates enables the learning of encodings for each individual sample that ultimately improve prediction of clinical outcome.|http://arxiv.org/abs/2406.08638v2|Chi-Jane Chen,Haidong Yi,Natalie Stanley
119|Utilising high-dimensional data in randomised clinical trials: a review of methods and practice|Introduction: Even in effectively conducted randomised trials, the probability of a successful study remains relatively low. With recent advances in the next-generation sequencing technologies, there is a rapidly growing number of high-dimensional data, including genetic, molecular and phenotypic information, that have improved our understanding of driver genes, drug targets, and drug mechanisms of action. The leveraging of high-dimensional data holds promise for increased success of clinical trials. Methods: We provide an overview of methods for utilising high-dimensional data in clinical trials. We also investigate the use of these methods in practice through a review of recently published randomised clinical trials that utilise high-dimensional genetic data. The review includes articles that were published between 2019 and 2021, identified through the PubMed database. Results: Out of 174 screened articles, 100 (57.5%) were randomised clinical trials that collected high-dimensional data. The most common clinical area was oncology (30%), followed by chronic diseases (28%), nutrition and ageing (18%) and cardiovascular diseases (7%). The most common types of data analysed were gene expression data (70%), followed by DNA data (21%). The most common method of analysis (36.3%) was univariable analysis. Articles that described multivariable analyses used standard statistical methods. Most of the clinical trials had two arms. Discussion: New methodological approaches are required for more efficient analysis of the increasing amount of high-dimensional data collected in randomised clinical trials. We highlight the limitations and barriers to the current use of high-dimensional data in trials, and suggest potential avenues for improvement and future work.|http://arxiv.org/abs/2305.10174v2|Svetlana Cherlin,Theophile Bigirumurame,Michael J Grayling,Jrmie Nsengimana,Luke Ouma,Aida Santaolalla,Fang Wan,S Faye Williamson,James M S Wason
120|How biomedical papers accumulated their clinical citations: A large-scale retrospective analysis based on PubMed|This paper explored the temporal characteristics of clinical citations of biomedical papers, including how long it takes to receive its first clinical citation (the initial stage) and how long it takes to receive two or more clinical citations after its first clinical citation (the build-up stage). Over 23 million biomedical papers in PubMed between 1940 and 2013 and their clinical citations are used as the research data. We divide these biomedical papers into three groups and four categories from clinical citation level and translational science perspectives. We compare the temporal characteristics of biomedical papers of different groups or categories. From the perspective of clinical citation level, the results show that highly clinically cited papers had obvious advantages of receiving clinical citations over medium and lowly clinically cited papers in both the initial and build-up stages. Meanwhile, as the number of clinical citations increased in the build-up stage, the difference in the length of time to receive the corresponding number of clinical citations among the three groups of biomedical papers significantly increased. From the perspective of translational science, the results reveal that biomedical papers closer to clinical science more easily receive clinical citations than papers closer to basic science in both the initial and build-up stages. Moreover, we found that highly clinically cited papers had the desperate advantage of receiving clinical citations over even the clinical guidelines or clinical trials. The robustness analysis of the two aspects demonstrates the reliability of our results.|http://arxiv.org/abs/2404.01072v1|Xin Li,Xuli Tang,Wei Lu
121|The IBEX Knowledge-Base: Achieving more together with open science|Iterative Bleaching Extends multipleXity (IBEX) is a versatile method for highly multiplexed imaging of diverse tissues. Based on open science principles, we created the IBEX Knowledge-Base, a resource for reagents, protocols and more, to empower innovation.|http://arxiv.org/abs/2407.19059v1|Andrea J. Radtke,Ifeanyichukwu Anidi,Leanne Arakkal,Armando Arroyo-Mejias,Rebecca T. Beuschel,Katy Borner,Colin J. Chu,Beatrice Clark,Menna R. Clatworthy,Jake Colautti,Joshua Croteau,Saven Denha,Rose Dever,Walderez O. Dutra,Sonja Fritzsche,Spencer Fullam,Michael Y. Gerner,Anita Gola,Kenneth J. Gollob,Jonathan M. Hernandez,Jyh Liang Hor,Hiroshi Ichise,Zhixin Jing,Danny Jonigk,Evelyn Kandov,Wolfgang Kastenmueller,Joshua F. E. Koenig,Aanandita Kothurkar,Alexandra Y. Kreins,Ian Lamborn,Yuri Lin,Katia Luciano Pereira Morais,Aleksandra Lunich,Jean C. S. Luz,Ryan B. MacDonald,Chen Makranz,Vivien I. Maltez,Ryan V. Moriaty,Juan M. Ocampo-Godinez,Vitoria M. Olyntho,Kartika Padhan,Kirsten Remmert,Nathan Richoz,Edward C. Schrom,Wanjing Shang,Lihong Shi,Rochelle M. Shih,Emily Speranza,Salome Stierli,Sarah A. Teichmann,Tibor Z. Veres,Megan Vierhout,Brianna T. Wachter,Adam K. Wade-Vallance,Margaret Williams,Nathan Zangger,Ronald N. Germain,Ziv Yaniv
122|Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data|Understanding the neural implementation of complex human behaviors is one of the major goals in neuroscience. To this end, it is crucial to find a true representation of the neural data, which is challenging due to the high complexity of behaviors and the low signal-to-ratio (SNR) of the signals. Here, we propose a novel unsupervised learning framework, Neural Latent Aligner (NLA), to find well-constrained, behaviorally relevant neural representations of complex behaviors. The key idea is to align representations across repeated trials to learn cross-trial consistent information. Furthermore, we propose a novel, fully differentiable time warping model (TWM) to resolve the temporal misalignment of trials. When applied to intracranial electrocorticography (ECoG) of natural speaking, our model learns better representations for decoding behaviors than the baseline models, especially in lower dimensional space. The TWM is empirically validated by measuring behavioral coherence between aligned trials. The proposed framework learns more cross-trial consistent representations than the baselines, and when visualized, the manifold reveals shared neural trajectories across trials.|http://arxiv.org/abs/2308.06443v1|Cheol Jun Cho,Edward F. Chang,Gopala K. Anumanchipalli
123|Exploring the Human Connectome Topology in Group Studies|Visually comparing brain networks, or connectomes, is an essential task in the field of neuroscience. Especially relevant to the field of clinical neuroscience, group studies that examine differences between populations or changes over time within a population enable neuroscientists to reason about effective diagnoses and treatments for a range of neuropsychiatric disorders. In this paper, we specifically explore how visual analytics tools can be used to facilitate various clinical neuroscience tasks, in which observation and analysis of meaningful patterns in the connectome can support patient diagnosis and treatment. We conduct a survey of visualization tasks that enable clinical neuroscience activities, and further explore how existing connectome visualization tools support or fail to support these tasks. Based on our investigation of these tasks, we introduce a novel visualization tool, NeuroCave, to support group studies analyses. We discuss how our design decisions (the use of immersive visualization, the use of hierarchical clustering and dimensionality reduction techniques, and the choice of visual encodings) are motivated by these tasks. We evaluate NeuroCave through two use cases that illustrate the utility of interactive connectome visualization in clinical neuroscience contexts. In the first use case, we study sex differences using functional connectomes and discover hidden connectome patterns associated with well-known cognitive differences in spatial and verbal abilities. In the second use case, we show how the utility of visualizing the brain in different topological space coupled with clustering information can reveal the brain's intrinsic structure.|http://arxiv.org/abs/1706.10297v1|Johnson J. G. Keiriz,Liang Zhan,Morris Chukhman,Olu Ajilore,Alex D. Leow,Angus G. Forbes
124|Sequential nonparametrics and semiparametrics: Theory, implementation and applications to clinical trials|One of Pranab K. Sen's major research areas is sequential nonparametrics and semiparametrics and their applications to clinical trials, to which he has made many important contributions. Herein we review a number of these contributions and related developments. We also describe some recent work on nonparametric and semiparametric inference and the associated computational methods in time-sequential clinical trials with survival endpoints.|http://arxiv.org/abs/0805.2492v1|Tze Leung Lai,Zheng Su
125|Generalized Likelihood Ratio Statistics and Uncertainty Adjustments in Efficient Adaptive Design of Clinical Trials|A new approach to adaptive design of clinical trials is proposed in a general multiparameter exponential family setting, based on generalized likelihood ratio statistics and optimal sequential testing theory. These designs are easy to implement, maintain the prescribed Type I error probability, and are asymptotically efficient. Practical issues involved in clinical trials allowing mid-course adaptation and the large literature on this subject are discussed, and comparisons between the proposed and existing designs are presented in extensive simulation studies of their finite-sample performance, measured in terms of the expected sample size and power functions.|http://arxiv.org/abs/1105.4667v1|Jay Bartroff,Tze Leung Lai
126|TMU at TREC Clinical Trials Track 2023|This paper describes Toronto Metropolitan University's participation in the TREC Clinical Trials Track for 2023. As part of the tasks, we utilize advanced natural language processing techniques and neural language models in our experiments to retrieve the most relevant clinical trials. We illustrate the overall methodology, experimental settings, and results of our implementation for the run submission as part of Team - V-TorontoMU.|http://arxiv.org/abs/2403.12088v1|Aritra Kumar Lahiri,Emrul Hasan,Qinmin Vivian Hu,Cherie Ding
127|Considerations for the planning, conduct and reporting of clinical trials with interim analyses|Interim analyses are prevalent in clinical trials. Although methodology is well established, there are aspects of how to operationalize and interpret interim analyses which remain unclear to many stakeholders. In this paper, a team of statisticians from the pharmaceutical industry, academia, and regulatory agencies provide a multi-stakeholder perspective on the key concepts behind interim analyses and considerations on terminology. We illustrate our proposals using a hypothetical clinical trial.|http://arxiv.org/abs/2410.01478v2|Elina Asikanius,Benjamin Hofner,Lisa V. Hampson,Gernot Wassmer,Christopher Jennison,Tobias Mielke,Cornelia Ursula Kunz,Kaspar Rufibach
128|A Bayesian seamless phase I-II trial design with two stages for cancer clinical trials with drug combinations|The use of drug combinations in clinical trials is increasingly common during the last years since a more favorable therapeutic response may be obtained by combining drugs. In phase I clinical trials, most of the existing methodology recommends a one unique dose combination as "optimal", which may result in a subsequent failed phase II clinical trial since other dose combinations may present higher treatment efficacy for the same level of toxicity. We are particularly interested in the setting where it is necessary to wait a few cycles of therapy to observe an efficacy outcome and the phase I and II population of patients are different with respect to treatment efficacy. Under these circumstances, it is common practice to implement two-stage designs where a set of maximum tolerated dose combinations is selected in a first stage, and then studied in a second stage for treatment efficacy. In this article we present a new two-stage design for early phase clinical trials with drug combinations. In the first stage, binary toxicity data is used to guide the dose escalation and set the maximum tolerated dose combinations. In the second stage, we take the set of maximum tolerated dose combinations recommended from the first stage, which remains fixed along the entire second stage, and through adaptive randomization, we allocate subsequent cohorts of patients in dose combinations that are likely to have high posterior median time to progression. The methodology is assessed with extensive simulations and exemplified with a real trial.|http://arxiv.org/abs/1809.04348v3|Jos L. Jimnez,Sungjin Kim,Mourad Tighiouart
129|On the relevance of prognostic information for clinical trials: A theoretical quantification|The question of how individual patient data from cohort studies or historical clinical trials can be leveraged for designing more powerful, or smaller yet equally powerful, clinical trials becomes increasingly important in the era of digitalisation. Today, the traditional statistical analyses approaches may seem questionable to practitioners in light of ubiquitous historical covariate information.   Several methodological developments aim at incorporating historical information in the design and analysis of future clinical trials, most importantly Bayesian information borrowing, propensity score methods, stratification, and covariate adjustment. Recently, adjusting the analysis with respect to a prognostic score, which was obtained from some machine learning procedure applied to historical data, has been suggested and we study the potential of this approach for randomised clinical trials.   In an idealised situation of a normal outcome in a two-arm trial with 1:1 allocation, we derive a simple sample size reduction formula as a function of two criteria characterising the prognostic score: (1) The coefficient of determination $R^2$ on historical data and (2) the correlation $\rho$ between the estimated and the true unknown prognostic scores. While maintaining the same power, the original total sample size $n$ planned for the unadjusted analysis reduces to $(1 - R^2 \rho^2) \times n$ in an adjusted analysis. Robustness in less ideal situations was assessed empirically. We conclude that there is potential for substantially more powerful or smaller trials, but only when prognostic scores can be accurately estimated.|http://arxiv.org/abs/2111.03391v1|Sandra Siegfried,Stephen Senn,Torsten Hothorn
130|The use of large language models to enhance cancer clinical trial educational materials|Cancer clinical trials often face challenges in recruitment and engagement due to a lack of participant-facing informational and educational resources. This study investigated the potential of Large Language Models (LLMs), specifically GPT4, in generating patient-friendly educational content from clinical trial informed consent forms. Using data from ClinicalTrials.gov, we employed zero-shot learning for creating trial summaries and one-shot learning for developing multiple-choice questions, evaluating their effectiveness through patient surveys and crowdsourced annotation. Results showed that GPT4-generated summaries were both readable and comprehensive, and may improve patients' understanding and interest in clinical trials. The multiple-choice questions demonstrated high accuracy and agreement with crowdsourced annotators. For both resource types, hallucinations were identified that require ongoing human oversight. The findings demonstrate the potential of LLMs "out-of-the-box" to support the generation of clinical trial education materials with minimal trial-specific engineering, but implementation with a human-in-the-loop is still needed to avoid misinformation risks.|http://arxiv.org/abs/2412.01955v2|Mingye Gao,Aman Varshney,Shan Chen,Vikram Goddla,Jack Gallifant,Patrick Doyle,Claire Novack,Maeve Dillon-Martin,Teresia Perkins,Xinrong Correia,Erik Duhaime,Howard Isenstein,Elad Sharon,Lisa Soleymani Lehmann,David Kozono,Brian Anthony,Dmitriy Dligach,Danielle S. Bitterman
131|TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network|Clinical trials are critical for drug development but often suffer from expensive and inefficient patient recruitment. In recent years, machine learning models have been proposed for speeding up patient recruitment via automatically matching patients with clinical trials based on longitudinal patient electronic health records (EHR) data and eligibility criteria of clinical trials. However, they either depend on trial-specific expert rules that cannot expand to other trials or perform matching at a very general level with a black-box model where the lack of interpretability makes the model results difficult to be adopted.   To provide accurate and interpretable patient trial matching, we introduce a personalized dynamic tree-based memory network model named TREEMENT. It utilizes hierarchical clinical ontologies to expand the personalized patient representation learned from sequential EHR data, and then uses an attentional beam-search query learned from eligibility criteria embedding to offer a granular level of alignment for improved performance and interpretability. We evaluated TREEMENT against existing models on real-world datasets and demonstrated that TREEMENT outperforms the best baseline by 7% in terms of error reduction in criteria-level matching and achieves state-of-the-art results in its trial-level matching ability. Furthermore, we also show TREEMENT can offer good interpretability to make the model results easier for adoption.|http://arxiv.org/abs/2307.09942v1|Brandon Theodorou,Cao Xiao,Jimeng Sun
132|An overview of open source Deep Learning-based libraries for Neuroscience|In recent years, deep learning revolutionized machine learning and its applications, producing results comparable to human experts in several domains, including neuroscience. Each year, hundreds of scientific publications present applications of deep neural networks for biomedical data analysis. Due to the fast growth of the domain, it could be a complicated and extremely time-consuming task for worldwide researchers to have a clear perspective of the most recent and advanced software libraries. This work contributes to clarify the current situation in the domain, outlining the most useful libraries that implement and facilitate deep learning application to neuroscience, allowing scientists to identify the most suitable options for their research or clinical projects. This paper summarizes the main developments in Deep Learning and their relevance to Neuroscience; it then reviews neuroinformatic toolboxes and libraries, collected from the literature and from specific hubs of software projects oriented to neuroscience research. The selected tools are presented in tables detailing key features grouped by domain of application (e.g. data type, neuroscience area, task), model engineering (e.g. programming language, model customization) and technological aspect (e.g. interface, code source). The results show that, among a high number of available software tools, several libraries are standing out in terms of functionalities for neuroscience applications. The aggregation and discussion of this information can help the neuroscience community to devolop their research projects more efficiently and quickly, both by means of readily available tools, and by knowing which modules may be improved, connected or added.|http://arxiv.org/abs/2301.05057v1|Louis Fabrice Tshimanga,Manfredo Atzori,Federico Del Pup,Maurizio Corbetta
133|Multimodal Clinical Trial Outcome Prediction with Large Language Models|The clinical trial is a pivotal and costly process, often spanning multiple years and requiring substantial financial resources. Therefore, the development of clinical trial outcome prediction models aims to exclude drugs likely to fail and holds the potential for significant cost savings. Recent data-driven attempts leverage deep learning methods to integrate multimodal data for predicting clinical trial outcomes. However, these approaches rely on manually designed modal-specific encoders, which limits both the extensibility to adapt new modalities and the ability to discern similar information patterns across different modalities. To address these issues, we propose a multimodal mixture-of-experts (LIFTED) approach for clinical trial outcome prediction. Specifically, LIFTED unifies different modality data by transforming them into natural language descriptions. Then, LIFTED constructs unified noise-resilient encoders to extract information from modal-specific language descriptions. Subsequently, a sparse Mixture-of-Experts framework is employed to further refine the representations, enabling LIFTED to identify similar information patterns across different modalities and extract more consistent representations from those patterns using the same expert model. Finally, a mixture-of-experts module is further employed to dynamically integrate different modality representations for prediction, which gives LIFTED the ability to automatically weigh different modalities and pay more attention to critical information. The experiments demonstrate that LIFTED significantly enhances performance in predicting clinical trial outcomes across all three phases compared to the best baseline, showcasing the effectiveness of our proposed key components.|http://arxiv.org/abs/2402.06512v4|Wenhao Zheng,Liaoyaqi Wang,Dongshen Peng,Hongxia Xu,Yun Li,Hongtu Zhu,Tianfan Fu,Huaxiu Yao
134|Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials|Due to the substantial number of clinicians, patients, and data collection environments involved in clinical trials, gathering data of superior quality poses a significant challenge. In clinical trials, patients are assessed based on their speech data to detect and monitor cognitive and mental health disorders. We propose using these speech recordings to verify the identities of enrolled patients and identify and exclude the individuals who try to enroll multiple times in the same trial. Since clinical studies are often conducted across different countries, creating a system that can perform speaker verification in diverse languages without additional development effort is imperative. We evaluate pre-trained TitaNet, ECAPA-TDNN, and SpeakerNet models by enrolling and testing with speech-impaired patients speaking English, German, Danish, Spanish, and Arabic languages. Our results demonstrate that tested models can effectively generalize to clinical speakers, with less than 2.7% EER for European Languages and 8.26% EER for Arabic. This represents a significant step in developing more versatile and efficient speaker verification systems for cognitive and mental health clinical trials that can be used across a wide range of languages and dialects, substantially reducing the effort required to develop speaker verification systems for multiple languages. We also evaluate how speech tasks and number of speakers involved in the trial influence the performance and show that the type of speech tasks impacts the model performance.|http://arxiv.org/abs/2404.01981v2|Ali Akram,Marija Stanojevic,Malikeh Ehghaghi,Jekaterina Novikova
135|LinkedCT: A Linked Data Space for Clinical Trials|The Linked Clinical Trials (LinkedCT) project aims at publishing the first open semantic web data source for clinical trials data. The database exposed by LinkedCT is generated by (1) transforming existing data sources of clinical trials into RDF, and (2) discovering semantic links between the records in the trials data and several other data sources. In this paper, we discuss several challenges involved in these two steps and present the methodology used in LinkedCT to overcome these challenges. Our approach for semantic link discovery involves using state-of-the-art approximate string matching techniques combined with ontology-based semantic matching of the records, all performed in a declarative and easy-to-use framework. We present an evaluation of the performance of our proposed techniques in several link discovery scenarios in LinkedCT.|http://arxiv.org/abs/0908.0567v1|Oktie Hassanzadeh,Anastasios Kementsietsidis,Lipyeow Lim,Renee J. Miller,Min Wang
136|Active Clinical Trials for Personalized Medicine|Individualized treatment rules (ITRs) tailor treatments according to individual patient characteristics. They can significantly improve patient care and are thus becoming increasingly popular. The data collected during randomized clinical trials are often used to estimate the optimal ITRs. However, these trials are generally expensive to run, and, moreover, they are not designed to efficiently estimate ITRs. In this paper, we propose a cost-effective estimation method from an active learning perspective. In particular, our method recruits only the "most informative" patients (in terms of learning the optimal ITRs) from an ongoing clinical trial. Simulation studies and real-data examples show that our active clinical trial method significantly improves on competing methods. We derive risk bounds and show that they support these observed empirical advantages.|http://arxiv.org/abs/1404.2971v2|Stanislav Minsker,Ying-Qi Zhao,Guang Cheng
137|A Blockchain Framework for Managing and Monitoring Data in Multi-Site Clinical Trials|The cost of conducting multi-site clinical trials has significantly increased over time, with site monitoring, data management, and amendments being key drivers. Clinical trial data management approaches typically rely on a central database, and require manual efforts to encode and maintain data capture and reporting requirements. To reduce the administrative burden, time, and effort of ensuring data integrity and privacy in multi-site trials, we propose a novel data management framework based on permissioned blockchain technology. We demonstrate how our framework, which uses smart contracts and private channels, enables confidential data communication, protocol enforcement, and and an automated audit trail. We compare this framework with the traditional data management approach and evaluate its effectiveness in satisfying the major requirements of multi-site clinical trials. We show that our framework ensures enforcement of IRB-related regulatory requirements across multiple sites and stakeholders.|http://arxiv.org/abs/1902.03975v1|Olivia Choudhury,Noor Fairoza,Issa Sylla,Amar Das
138|Learning for Dose Allocation in Adaptive Clinical Trials with Safety Constraints|Phase I dose-finding trials are increasingly challenging as the relationship between efficacy and toxicity of new compounds (or combination of them) becomes more complex. Despite this, most commonly used methods in practice focus on identifying a Maximum Tolerated Dose (MTD) by learning only from toxicity events. We present a novel adaptive clinical trial methodology, called Safe Efficacy Exploration Dose Allocation (SEEDA), that aims at maximizing the cumulative efficacies while satisfying the toxicity safety constraint with high probability. We evaluate performance objectives that have operational meanings in practical clinical trials, including cumulative efficacy, recommendation/allocation success probabilities, toxicity violation probability, and sample efficiency. An extended SEEDA-Plateau algorithm that is tailored for the increase-then-plateau efficacy behavior of molecularly targeted agents (MTA) is also presented. Through numerical experiments using both synthetic and real-world datasets, we show that SEEDA outperforms state-of-the-art clinical trial designs by finding the optimal dose with higher success rate and fewer patients.|http://arxiv.org/abs/2006.05026v2|Cong Shen,Zhiyang Wang,Sofia S. Villar,Mihaela van der Schaar
139|Predicting Intervention Approval in Clinical Trials through Multi-Document Summarization|Clinical trials offer a fundamental opportunity to discover new treatments and advance the medical knowledge. However, the uncertainty of the outcome of a trial can lead to unforeseen costs and setbacks. In this study, we propose a new method to predict the effectiveness of an intervention in a clinical trial. Our method relies on generating an informative summary from multiple documents available in the literature about the intervention under study. Specifically, our method first gathers all the abstracts of PubMed articles related to the intervention. Then, an evidence sentence, which conveys information about the effectiveness of the intervention, is extracted automatically from each abstract. Based on the set of evidence sentences extracted from the abstracts, a short summary about the intervention is constructed. Finally, the produced summaries are used to train a BERT-based classifier, in order to infer the effectiveness of an intervention. To evaluate our proposed method, we introduce a new dataset which is a collection of clinical trials together with their associated PubMed articles. Our experiments, demonstrate the effectiveness of producing short informative summaries and using them to predict the effectiveness of an intervention.|http://arxiv.org/abs/2204.00290v1|Georgios Katsimpras,Georgios Paliouras
140|Pragmatic Clinical Trials in the Rubric of Structural Causal Models|Explanatory studies, such as randomized controlled trials, are targeted to extract the true causal effect of interventions on outcomes and are by design adjusted for covariates through randomization. On the contrary, observational studies are a representation of events that occurred without intervention. Both can be illustrated using the Structural Causal Model (SCM), and do-calculus can be employed to estimate the causal effects. Pragmatic clinical trials (PCT) fall between these two ends of the trial design spectra and are thus hard to define. Due to its pragmatic nature, no standardized representation of PCT through SCM has been yet established. In this paper, we approach this problem by proposing a generalized representation of PCT under the rubric of structural causal models (SCM). We discuss different analysis techniques commonly employed in PCT using the proposed graphical model, such as intention-to-treat, as-treated, and per-protocol analysis. To show the application of our proposed approach, we leverage an experimental dataset from a pragmatic clinical trial. Our proposition of SCM through PCT creates a pathway to leveraging do-calculus and related mathematical operations on clinical datasets.|http://arxiv.org/abs/2204.13782v1|Riddhiman Adib,Sheikh Iqbal Ahamed,Mohammad Adibuzzaman
141|Robust Detection of Covariate-Treatment Interactions in Clinical Trials|Detection of interactions between treatment effects and patient descriptors in clinical trials is critical for optimizing the drug development process. The increasing volume of data accumulated in clinical trials provides a unique opportunity to discover new biomarkers and further the goal of personalized medicine, but it also requires innovative robust biomarker detection methods capable of detecting non-linear, and sometimes weak, signals. We propose a set of novel univariate statistical tests, based on the theory of random walks, which are able to capture non-linear and non-monotonic covariate-treatment interactions. We also propose a novel combined test, which leverages the power of all of our proposed univariate tests into a single general-case tool. We present results for both synthetic trials as well as real-world clinical trials, where we compare our method with state-of-the-art techniques and demonstrate the utility and robustness of our approach.|http://arxiv.org/abs/1712.08211v1|Baptiste Goujaud,Eric W. Tramel,Pierre Courtiol,Mikhail Zaslavskiy,Gilles Wainrib
142|Automatic prediction of cognitive and functional decline can significantly decrease the number of subjects required for clinical trials in early Alzheimer's disease|INTRODUCTION: Heterogeneity in the progression of Alzheimer's disease makes it challenging to predict the rate of cognitive and functional decline for individual patients. Tools for short-term prediction could help enrich clinical trial designs and focus prevention strategies on the most at-risk patients. METHOD: We built a prognostic model using baseline cognitive scores and MRI-based features to determine which subjects with mild cognitive impairment remained stable and which functionally declined (measured by a two-point increase in CDR-SB) over 2 and 3-year follow-up periods, periods typical of the length of clinical trials. RESULTS: Combining both sets of features yields 77% accuracy (81% sensitivity and 75% specificity) to predict cognitive decline at 2 years (74% accuracy at 3 years with 75% sensitivity and 73% specificity). Using this tool to select trial participants yields a 3.8-fold decrease in the required sample size for a 2-year study (2.8-fold decrease for a 3-year study) for a hypothesized 25% treatment effect to reduce cognitive decline. DISCUSSION: This cohort enrichment tool could accelerate treatment development by increasing power in clinical trials.|http://arxiv.org/abs/2101.08346v1|Neda Shafiee,Mahsa Dadar,Simon Ducharme,D. Louis Collins
143|Incorporating External Data into the Analysis of Clinical Trials via Bayesian Additive Regression Trees|Most clinical trials involve the comparison of a new treatment to a control arm (e.g., the standard of care) and the estimation of a treatment effect. External data, including historical clinical trial data and real-world observational data, are commonly available for the control arm. Borrowing information from external data holds the promise of improving the estimation of relevant parameters and increasing the power of detecting a treatment effect if it exists. In this paper, we propose to use Bayesian additive regression trees (BART) for incorporating external data into the analysis of clinical trials, with a specific goal of estimating the conditional or population average treatment effect. BART naturally adjusts for patient-level covariates and captures potentially heterogeneous treatment effects across different data sources, achieving flexible borrowing. Simulation studies demonstrate that BART compares favorably to a hierarchical linear model and a normal-normal hierarchical model. We illustrate the proposed method with an acupuncture trial.|http://arxiv.org/abs/2103.08754v1|Tianjian Zhou,Yuan Ji
144|High-performance Uncertainty Quantification in Large-scale Virtual Clinical Trials of Closed-loop Diabetes Treatment|In this paper, we propose a virtual clinical trial for assessing the performance and identifying risks in closed-loop diabetes treatments. Virtual clinical trials enable fast and risk-free tests of many treatment variations for large populations of fictive patients (represented by mathematical models). We use closed-loop Monte Carlo simulation, implemented in high-performance software and hardware, to quantify the uncertainty in treatment performance as well as to compare the performance in different scenarios or of different closed-loop treatments. Our software can be used for testing a wide variety of control strategies ranging from heuristical approaches to nonlinear model predictive control. We present an example of a virtual clinical trial with one million patients over 52 weeks, and we use high-performance software and hardware to conduct the virtual trial in 1 h and 22 min.|http://arxiv.org/abs/2202.13927v1|Asbjrn Thode Reenberg,Tobias K. S. Ritschel,Bernd Dammann,John Bagterp Jrgensen
145|Contextual aggregation and rapid updating of trial outcomes within a user-friendly open-source environment|The delayed and incomplete availability of historical findings and the lack of integrative and user-friendly software hampers the reliable interpretation of new clinical data. We developed a free, open, and user-friendly clinical trial aggregation program combining a large and representative sample of existing trial data with the latest classical and Bayesian meta-analytical models, including clear output visualizations. Our software is of particular interest for (post-graduate) educational programs (e.g., medicine, epidemiology) and global health initiatives. We demonstrate the database, interface, and plot functionality with a recent randomized controlled trial on effective epileptic seizure reduction in children treated for a parasitic brain infection. The single trial data is placed into context and we show how to interpret new results against existing knowledge instantaneously. Our program is of particular interest to those working on the contextualizing of medical findings. It may facilitate the advancement of global clinical progress as efficiently and openly as possible and simulate further bridging clinical data with the latest biostatistical models.|http://arxiv.org/abs/2306.14061v1|Frantiek Barto,Eric-Jan Wagenmakers,Christiaan H. Vinkers,Kees P. J. Braun,Willem M. Otte
146|Mathematical programming tools for randomization purposes in small two-arm clinical trials: A case study with real data|Modern randomization methods in clinical trials are invariably adaptive, meaning that the assignment of the next subject to a treatment group uses the accumulated information in the trial. Some of the recent adaptive randomization methods use mathematical programming to construct attractive clinical trials that balance the group features, such as their sizes and covariate distributions of their subjects. We review some of these methods and compare their performance with common covariate-adaptive randomization methods for small clinical trials. We introduce an energy distance measure that compares the discrepancy between the two groups using the joint distribution of the subjects' covariates. This metric is more appealing than evaluating the discrepancy between the groups using their marginal covariate distributions. Using numerical experiments, we demonstrate the advantages of the mathematical programming methods under the new measure. In the supplementary material, we provide R codes to reproduce our study results and facilitate comparisons of different randomization procedures.|http://arxiv.org/abs/2402.06058v1|Alan R. Vazquez,Weng Kee Wong
147|Clinical Trials Protocol Authoring using LLMs|This report embarks on a mission to revolutionize clinical trial protocol development through the integration of advanced AI technologies. With a focus on leveraging the capabilities of generative AI, specifically GPT-4, this initiative aimed to streamline and enhance the efficiency and accuracy of clinical trial protocols. The methodology encompassed a detailed analysis and preparation of comprehensive drug and study level metadata, followed by the deployment of GPT-4 for automated protocol section generation. Results demonstrated a significant improvement in protocol authoring, highlighted by increases in efficiency, accuracy, and the customization of protocols to specific trial requirements. Challenges encountered during model selection and prompt engineering were systematically addressed, leading to refined methodologies that capitalized on the advanced text generation capabilities of GPT-4. This project not only showcases the practical applications and benefits of generative AI in clinical trial design but also sets a foundation for future innovations in the field.|http://arxiv.org/abs/2404.05044v2|Morteza Maleki,SeyedAli Ghahari
148|Design of Bayesian Clinical Trials with Clustered Data and Multiple Endpoints|In the design of clinical trials, it is essential to assess the design operating characteristics (i.e., the probabilities of making correct decisions). Common practice for the evaluation of operating characteristics in Bayesian clinical trials relies on estimating the sampling distribution of posterior summaries via Monte Carlo simulation. It is computationally intensive to repeat this estimation process for each design configuration considered, particularly for clustered data that are analyzed using complex, high-dimensional models. In this paper, we propose an efficient method to assess operating characteristics and determine sample sizes for Bayesian trials with clustered data and multiple endpoints. We prove theoretical results that enable posterior probabilities to be modelled as a function of the sample size. Using these functions, we assess operating characteristics at a range of sample sizes given simulations conducted at only two sample sizes. These theoretical results are also leveraged to quantify the impact of simulation variability on our sample size recommendations. The applicability of our methodology is illustrated using a current clinical trial with clustered data.|http://arxiv.org/abs/2501.13218v1|Luke Hagar,Shirin Golchi
149|Choriocapillaris Flow Signal Impairment in Sorsby Fundus Dystrophy|Purpose: To quantify choriocapillaris flow alterations in early Sorsby Fundus Dystrophy (SFD) and to investigate the relationship of choriocapillaris flow with the choroidal and outer retinal microstructure. Methods: In this prospective case-control study, 18 eyes of 11 patients with early SFD and 32 eyes of 32 controls without ocular pathology underwent multimodal imaging including spectral-domain optical coherence tomography (OCT)followed by deep-learning-based layer segmentation. OCT-angiography (OCT-A) was performed to quantify choriocapillaris flow signal deficits (FDs). Differences in choriocapillaris flow area percentage between SFD patients and controls were determined and a structure-function correlation with outer retinal layer thicknesses were analyzed based on mixed model analysis. Results: SFD patients exhibited a significantly greater choriocapillaris FDs area percentage than controls (estimate [95% CI] 32.05% [24.31-39.80] vs. 23.36% [20.64-26.09], P<0.001), even when adjusting for age. Choroidal thickness was a structural OCT surrogate of the choriocapillaris FD area percentage (-0.82% per 100 micrometer, P=0.017), whereas retinal-pigment-epithelium-drusen-complex thickness was not informative regarding choriocapillaris FDs (P=0.932). The choriocapillaris FD area percentage was associated with an altered microstructure of the overlying photoreceptors (outer-segments, inner-segments and outer-nuclear-layer thinning of -0.31, -0.12 and -0.47 $\mu$m per %FD, respectively, P<0.001). Conclusions: Patients with early SFD exhibit pronounced abnormalities of choriocapillaris flow signal on OCT-A, which are not limited to areas of sub-RPE deposits seen in OCT imaging. Thus, analysis of the choriocapillaris flow may enable clinical trials at earlier disease stages in SFD and possibly in mimicking diseases with an impaired Bruchs membrane including age-related macular degeneration.|http://arxiv.org/abs/2107.11361v1|Kristina Hess,Kristin Raming,Martin Gliem,Peter Charbel Issa,Philipp Herrmann,Frank G. Holz,Maximilian Pfau
150|Estimands in Hematologic Oncology Trials|The estimand framework included in the addendum to the ICH E9 guideline facilitates discussions to ensure alignment between the key question of interest, the analysis, and interpretation. Therapeutic knowledge and drug mechanism play a crucial role in determining the strategy and defining the estimand for clinical trial designs. Clinical trials in patients with hematological malignancies often present unique challenges for trial design due to complexity of treatment options and existence of potential curative but highly risky procedures, e.g. stem cell transplant or treatment sequence across different phases (induction, consolidation, maintenance). Here, we illustrate how to apply the estimand framework in hematological clinical trials and how the estimand framework can address potential difficulties in trial result interpretation.   This paper is a result of a cross-industry collaboration to connect the International Conference on Harmonisation (ICH) E9 addendum concepts to applications. Three randomized phase 3 trials will be used to consider common challenges including intercurrent events in hematologic oncology trials to illustrate different scientific questions and the consequences of the estimand choice for trial design, data collection, analysis, and interpretation. Template language for describing estimand in both study protocols and statistical analysis plans is suggested for statisticians' reference.|http://arxiv.org/abs/2010.00957v1|Steven Sun,Hans-Jochen Weber,Emily Butler,Kaspar Rufibach,Satrajit Roychoudhury
151|How mature are survival data at the time of an interim analysis in a clinical trial with a survival outcome?|In a clinical trial with a survival outcome, an interim analysis is often performed to allow for early stopping for efficacy. If the interim analysis is early in the trial, one might conclude that a new treatment is more effective (compared to e.g.\ a placebo) and stop the trial, whereas the survival curves in the trial arms are not mature for the research question under investigation, for example because the curves are still close to 1 at that time. This means that the decision is based on a small percentage of the events in the long run only; possibly the events of the more frail patients in the trial who may not be representative for the whole group of patients. It may not be sensible to conclude effectiveness based on so little information. Criteria to determine the moment the interim analysis will be performed, should be chosen with care, and include the maturity of the data at the time of the interim analysis. Here, the expected survival rates at the interim analysis play a role. In this paper we will derive the asymptotic distribution of the Kaplan-Meier curves at the (random) moment the interim analysis will be performed for a one and two arm clinical trial. Based on this distribution, an interval in which the Kaplan Meier curves will fall into (with probability 95\%) is derived and could be used to plan the moment of the interim analysis in the design stage of the trial, so before the trial starts.|http://arxiv.org/abs/2305.04103v1|Marianne A Jonker,Steven Teerenstra
152|Active Learning for Developing Personalized Treatment|The personalization of treatment via bio-markers and other risk categories has drawn increasing interest among clinical scientists. Personalized treatment strategies can be learned using data from clinical trials, but such trials are very costly to run. This paper explores the use of active learning techniques to design more efficient trials, addressing issues such as whom to recruit, at what point in the trial, and which treatment to assign, throughout the duration of the trial. We propose a minimax bandit model with two different optimization criteria, and discuss the computational challenges and issues pertaining to this approach. We evaluate our active learning policies using both simulated data, and data modeled after a clinical trial for treating depressed individuals, and contrast our methods with other plausible active learning policies.|http://arxiv.org/abs/1202.3714v1|Kun Deng,Joelle Pineau,Susan A. Murphy
|Charles F. Manski,Aleksey Tetenov
154|Attention-Based LSTM Network for COVID-19 Clinical Trial Parsing|COVID-19 clinical trial design is a critical task in developing therapeutics for the prevention and treatment of COVID-19. In this study, we apply a deep learning approach to extract eligibility criteria variables from COVID-19 trials to enable quantitative analysis of trial design and optimization. Specifically, we train attention-based bidirectional Long Short-Term Memory (Att-BiLSTM) models and use the optimal model to extract entities (i.e., variables) from the eligibility criteria of COVID-19 trials. We compare the performance of Att-BiLSTM with traditional ontology-based method. The result on a benchmark dataset shows that Att-BiLSTM outperforms the ontology model. Att-BiLSTM achieves a precision of 0.942, recall of 0.810, and F1 of 0.871, while the ontology model only achieves a precision of 0.715, recall of 0.659, and F1 of 0.686. Our analyses demonstrate that Att-BiLSTM is an effective approach for characterizing patient populations in COVID-19 clinical trials.|http://arxiv.org/abs/2012.10063v1|Xiong Liu,Luca A. Finelli,Greg L. Hersch,Iya Khalil
155|TrialEnroll: Predicting Clinical Trial Enrollment Success with Deep & Cross Network and Large Language Models|Clinical trials need to recruit a sufficient number of volunteer patients to demonstrate the statistical power of the treatment (e.g., a new drug) in curing a certain disease. Clinical trial recruitment has a significant impact on trial success. Forecasting whether the recruitment process would be successful before we run the trial would save many resources and time. This paper develops a novel deep & cross network with large language model (LLM)-augmented text feature that learns semantic information from trial eligibility criteria and predicts enrollment success. The proposed method enables interpretability by understanding which sentence/word in eligibility criteria contributes heavily to prediction. We also demonstrate the empirical superiority of the proposed method (0.7002 PR-AUC) over a bunch of well-established machine learning methods. The code and curated dataset are publicly available at https://anonymous.4open.science/r/TrialEnroll-7E12.|http://arxiv.org/abs/2407.13115v1|Ling Yue,Sixue Xing,Jintai Chen,Tianfan Fu
156|Unveiling Trail Making Test: Visual and manual trajectories indexing multiple executive processes|The Trail Making Test (TMT) is one of the most popular neuropsychological tests in the clinical assessment of executive functions (EF) and research in a wide range of clinical conditions. In addition to its sensitivity to executive dysfunction, the TMT presents several strengths: it is simple and intuitive, it is easy to understand for patients, and has a short administration. However, it has important limitations. First, the underlying EFs articulated during the task are not well discriminated, which makes it a test with low specificity. Second, the traditional pen-and-paper version presents one trial per condition which introduces high variability. Third, only the total time is quantified, which does not allow for a detailed analysis. Fourth, it has a fixed spatial configuration per condition. In the present study we designed a computerized version of the TMT (cTMT) to overcome its main limitations. Eye and hand positions are simultaneously measured with high resolution, several trials are acquired, and spatial configuration of the targets is controlled. Our results showed a very similar performance profile compared to the traditional TMT. Moreover, it revealed similarities and differences in eye movements between the two parts of the task. Most importantly, we found an internal working memory measure of the cTMT based on hand and eye movements that showed an association to a validated working memory task. Additionally, we found another internal measure of the TMT, also based on hand and eye movements, that we propose as a potential marker of inhibitory control. Our results showed that executive functions can be studied in more detail using traditional tests combined with powerful digital setups. Finally, our study paved the way for a detailed analysis of other complex tasks used for clinical evaluation, providing a deeper understanding of the processes underlying its resolution.|http://arxiv.org/abs/2109.15255v1|Ignacio Linari,Gustavo Juantorena,Agustin Ibaez,Agustin Petroni,Juan E. Kamienkowski
157|Boundary crossing Random Walks, clinical trials and multinomial sequential estimation|A sufficient condition for the uniqueness of multinomial sequential unbiased estimators is provided generalizing a classical result for binomial samples. Unbiased estimators are applied to infer the parameters of multidimensional or multinomial Random Walks which are observed until they reach a boundary. An application to clinical trials is presented.|http://arxiv.org/abs/1101.4038v2|Enrico Bibbona,Alessandro Rubba
158|Discussion of "Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues" by W. DuMouchel|Discussion of "Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues" by W. DuMouchel [arXiv:1210.0385].|http://arxiv.org/abs/1210.0655v1|Bradley W. McEvoy,Ram C. Tiwari
159|Discussion of "Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues" by W. DuMouchel|Discussion of "Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues" by W. DuMouchel [arXiv:1210.0385].|http://arxiv.org/abs/1210.0658v1|Don Berry
160|Rejoinder to "Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues"|Rejoinder to "Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues" by W. DuMouchel [arXiv:1210.0385].|http://arxiv.org/abs/1210.0669v1|William DuMouchel
161|A generic rule-based system for clinical trial patient selection|The n2c2 2018 Challenge task 1 aimed to identify patients who meet lists of heterogeneous inclusion/exclusion criteria for a hypothetical clinical trial. We demonstrate a generic rule-based natural language pipeline can support this task with decent performance (the average F1 score on the test set is 0.89, ranked the 8th out of 45 teams ).|http://arxiv.org/abs/1907.06860v1|Jianlin Shi,Kevin Graves,John F. Hurdle
162|[Invited Discussion] Randomization Tests to Address Disruptions in Clinical Trials: A Report from the NISS Ingram Olkin Forum Series on Unplanned Clinical Trial Disruptions|Disruptions in clinical trials may be due to external events like pandemics, warfare, and natural disasters. Resulting complications may lead to unforeseen intercurrent events (events that occur after treatment initiation and affect the interpretation of the clinical question of interest or the existence of the measurements associated with it). In Uschner et al. (2023), several example clinical trial disruptions are described: treatment effect drift, population shift, change of care, change of data collection, and change of availability of study medication. A complex randomized controlled trial (RCT) setting with (planned or unplanned) intercurrent events is then described, and randomization tests are presented as a means for non-parametric inference that is robust to violations of assumption typically made in clinical trials. While estimation methods like Targeted Learning (TL) are valid in such settings, we do not see where the authors make the case that one should be going for a randomization test in such disrupted RCTs. In this discussion, we comment on the appropriateness of TL and the accompanying TL Roadmap in the context of disrupted clinical trials. We highlight a few key articles related to the broad applicability of TL for RCTs and real-world data (RWD) analyses with intercurrent events. We begin by introducing TL and motivating its utility in Section 2, and then in Section 3 we provide a brief overview of the TL Roadmap. In Section 4 we recite the example clinical trial disruptions presented in Uschner et al. (2023), discussing considerations and solutions based on the principles of TL. We request in an authors' rejoinder a clear theoretical demonstration with specific examples in this setting that a randomization test is the only valid inferential method relative to one based on following the TL Roadmap.|http://arxiv.org/abs/2408.09060v1|Rachael V. Phillips,Mark J. van der Laan
163|Substantial Doubt Remains about the Efficacy of Anti-Amyloid Antibodies|Alzheimer's disease (AD) is a prevalent, progressive, and ultimately fatal neurodegenerative disorder that is defined pathologically by the accumulation of amyloid plaques and tau neurofibrillary tangles in the brain. There remains an unmet need for therapies that can halt or slow the course of AD. To address this need, the FDA has provided a mechanism, under its Accelerated Approval pathway, for potential therapeutics to be approved based in part on their ability to reduce brain amyloid. Through this pathway, two monoclonal anti-amyloid antibodies, aducanumab and lecanemab, have been approved for clinical use. More recently, another amyloid-lowering antibody, donanemab, generated a statistically significant outcome in a phase 3 clinical trial and will shortly come under FDA review. While these monoclonal antibodies are not yet routinely used in clinical practice, the series of recent positive clinical trials has fostered enthusiasm amongst some AD experts. Here, we discuss three key limitations regarding recent anti-amyloid clinical trials: (1) there is little to no evidence that amyloid reduction correlates with clinical outcome, (2) the reported efficacy of anti-amyloid therapies may be partly, or wholly, explained by functional unblinding, and (3) donanemab in its phase 3 trial had no effect on tau burden, the pathological hallmark more closely related to cognition. Taken together, these observations call into question the efficacy of anti-amyloid therapies.|http://arxiv.org/abs/2310.15456v2|Leonardino A. Digma MD,Joseph R. Winer PhD,Michael D. Greicius MD
164|Anytime-valid inference in N-of-1 trials|App-based N-of-1 trials offer a scalable experimental design for assessing the effects of health interventions at an individual level. Their practical success depends on the strong motivation of participants, which, in turn, translates into high adherence and reduced loss to follow-up. One way to maintain participant engagement is by sharing their interim results. Continuously testing hypotheses during a trial, known as "peeking", can also lead to shorter, lower-risk trials by detecting strong effects early. Nevertheless, traditionally, results are only presented upon the trial's conclusion. In this work, we introduce a potential outcomes framework that permits interim peeking of the results and enables statistically valid inferences to be drawn at any point during N-of-1 trials. Our work builds on the growing literature on valid confidence sequences, which enables anytime-valid inference with uniform type-1 error guarantees over time. We propose several causal estimands for treatment effects applicable in an N-of-1 trial and demonstrate, through empirical evaluation, that the proposed approach results in valid confidence sequences over time. We anticipate that incorporating anytime-valid inference into clinical trials can significantly enhance trial participation and empower participants.|http://arxiv.org/abs/2309.07353v1|Ivana Malenica,Yongyi Guo,Kyra Gan,Stefan Konigorski
165|Bayesian leveraging of historical control data for a clinical trial with time-to-event endpoint|The recent 21st Century Cures Act propagates innovations to accelerate the discovery, development, and delivery of 21st century cures. It includes the broader application of Bayesian statistics and the use of evidence from clinical expertise. An example of the latter is the use of trial-external (or historical) data, which promises more efficient or ethical trial designs. We propose a Bayesian meta-analytic approach to leveraging historical data for time-to-event endpoints, which are common in oncology and cardiovascular diseases. The approach is based on a robust hierarchical model for piecewise exponential data. It allows for various degrees of between trial-heterogeneity and for leveraging individual as well as aggregate data. An ovarian carcinoma trial and a non-small-cell cancer trial illustrate methodological and practical aspects of leveraging historical data for the analysis and design of time-to-event trials.|http://arxiv.org/abs/1908.07265v2|Satrajit Roychoudhury,Beat Neuenschwander
166|Trialstreamer: Mapping and Browsing Medical Evidence in Real-Time|We introduce Trialstreamer, a living database of clinical trial reports. Here we mainly describe the evidence extraction component; this extracts from biomedical abstracts key pieces of information that clinicians need when appraising the literature, and also the relations between these. Specifically, the system extracts descriptions of trial participants, the treatments compared in each arm (the interventions), and which outcomes were measured. The system then attempts to infer which interventions were reported to work best by determining their relationship with identified trial outcome measures. In addition to summarizing individual trials, these extracted data elements allow automatic synthesis of results across many trials on the same topic. We apply the system at scale to all reports of randomized controlled trials indexed in MEDLINE, powering the automatic generation of evidence maps, which provide a global view of the efficacy of different interventions combining data from all relevant clinical trials on a topic. We make all code and models freely available alongside a demonstration of the web interface.|http://arxiv.org/abs/2005.10865v1|Benjamin E. Nye,Ani Nenkova,Iain J. Marshall,Byron C. Wallace
167|Clinical Evidence Engine: Proof-of-Concept For A Clinical-Domain-Agnostic Decision Support Infrastructure|Abstruse learning algorithms and complex datasets increasingly characterize modern clinical decision support systems (CDSS). As a result, clinicians cannot easily or rapidly scrutinize the CDSS recommendation when facing a difficult diagnosis or treatment decision in practice. Over-trust or under-trust are frequent. Prior research has explored supporting such assessments by explaining DST data inputs and algorithmic mechanisms. This paper explores a different approach: Providing precisely relevant, scientific evidence from biomedical literature. We present a proof-of-concept system, Clinical Evidence Engine, to demonstrate the technical and design feasibility of this approach across three domains (cardiovascular diseases, autism, cancer). Leveraging Clinical BioBERT, the system can effectively identify clinical trial reports based on lengthy clinical questions (e.g., "risks of catheter infection among adult patients in intensive care unit who require arterial catheters, if treated with povidone iodine-alcohol"). This capability enables the system to identify clinical trials relevant to diagnostic/treatment hypotheses -- a clinician's or a CDSS's. Further, Clinical Evidence Engine can identify key parts of a clinical trial abstract, including patient population (e.g., adult patients in intensive care unit who require arterial catheters), intervention (povidone iodine-alcohol), and outcome (risks of catheter infection). This capability opens up the possibility of enabling clinicians to 1) rapidly determine the match between a clinical trial and a clinical question, and 2) understand the result and contexts of the trial without extensive reading. We demonstrate this potential by illustrating two example use scenarios of the system. We discuss the idea of designing DST explanations not as specific to a DST or an algorithm, but as a domain-agnostic decision support infrastructure.|http://arxiv.org/abs/2111.00621v1|Bojian Hou,Hao Zhang,Gur Ladizhinsky,Gur Ladizhinsky,Stephen Yang,Volodymyr Kuleshov,Fei Wang,Qian Yang
168|COMPOSE: Cross-Modal Pseudo-Siamese Network for Patient Trial Matching|Clinical trials play important roles in drug development but often suffer from expensive, inaccurate and insufficient patient recruitment. The availability of massive electronic health records (EHR) data and trial eligibility criteria (EC) bring a new opportunity to data driven patient recruitment. One key task named patient-trial matching is to find qualified patients for clinical trials given structured EHR and unstructured EC text (both inclusion and exclusion criteria). How to match complex EC text with longitudinal patient EHRs? How to embed many-to-many relationships between patients and trials? How to explicitly handle the difference between inclusion and exclusion criteria? In this paper, we proposed CrOss-Modal PseudO-SiamEse network (COMPOSE) to address these challenges for patient-trial matching. One path of the network encodes EC using convolutional highway network. The other path processes EHR with multi-granularity memory network that encodes structured patient records into multiple levels based on medical ontology. Using the EC embedding as query, COMPOSE performs attentional record alignment and thus enables dynamic patient-trial matching. COMPOSE also introduces a composite loss term to maximize the similarity between patient records and inclusion criteria while minimize the similarity to the exclusion criteria. Experiment results show COMPOSE can reach 98.0% AUC on patient-criteria matching and 83.7% accuracy on patient-trial matching, which leads 24.3% improvement over the best baseline on real-world patient-trial matching tasks.|http://arxiv.org/abs/2006.08765v1|Junyi Gao,Cao Xiao,Lucas M. Glass,Jimeng Sun
169|Doctor2Vec: Dynamic Doctor Representation Learning for Clinical Trial Recruitment|Massive electronic health records (EHRs) enable the success of learning accurate patient representations to support various predictive health applications. In contrast, doctor representation was not well studied despite that doctors play pivotal roles in healthcare. How to construct the right doctor representations? How to use doctor representation to solve important health analytic problems? In this work, we study the problem on {\it clinical trial recruitment}, which is about identifying the right doctors to help conduct the trials based on the trial description and patient EHR data of those doctors. We propose doctor2vec which simultaneously learns 1) doctor representations from EHR data and 2) trial representations from the description and categorical information about the trials. In particular, doctor2vec utilizes a dynamic memory network where the doctor's experience with patients are stored in the memory bank and the network will dynamically assign weights based on the trial representation via an attention mechanism. Validated on large real-world trials and EHR data including 2,609 trials, 25K doctors and 430K patients, doctor2vec demonstrated improved performance over the best baseline by up to $8.7\%$ in PR-AUC. We also demonstrated that the doctor2vec embedding can be transferred to benefit data insufficiency settings including trial recruitment in less populated/newly explored country with $13.7\%$ improvement or for rare diseases with $8.1\%$ improvement in PR-AUC.|http://arxiv.org/abs/1911.10395v1|Siddharth Biswal,Cao Xiao,Lucas M. Glass,Elizabeth Milkovits,Jimeng Sun
170|Inference for natural mediation effects under case-cohort sampling with applications in identifying COVID-19 vaccine correlates of protection|Combating the SARS-CoV2 pandemic will require the fast development of effective preventive vaccines. Regulatory agencies may open accelerated approval pathways for vaccines if an immunological marker can be established as a mediator of a vaccine's protection. A rich source of information for identifying such correlates are large-scale efficacy trials of COVID-19 vaccines, where immune responses are measured subject to a case-cohort sampling design. We propose two approaches to estimation of mediation parameters in the context of case-cohort sampling designs. We establish the theoretical large-sample efficiency of our proposed estimators and evaluate them in a realistic simulation to understand whether they can be employed in the analysis of COVID-19 vaccine efficacy trials.|http://arxiv.org/abs/2103.02643v1|David Benkeser,Ivn Daz,Jialu Ran
171|Mondrian Processes for Flow Cytometry Analysis|Analysis of flow cytometry data is an essential tool for clinical diagnosis of hematological and immunological conditions. Current clinical workflows rely on a manual process called gating to classify cells into their canonical types. This dependence on human annotation limits the rate, reproducibility, and complexity of flow cytometry analysis. In this paper, we propose using Mondrian processes to perform automated gating by incorporating prior information of the kind used by gating technicians. The method segments cells into types via Bayesian nonparametric trees. Examining the posterior over trees allows for interpretable visualizations and uncertainty quantification - two vital qualities for implementation in clinical practice.|http://arxiv.org/abs/1711.07673v2|Disi Ji,Eric Nalisnick,Padhraic Smyth
172|Bayesian Information Criterion for Event-based Multi-trial Ensemble data|Transient recurring phenomena are ubiquitous in many scientific fields like neuroscience and meteorology. Time inhomogenous Vector Autoregressive Models (VAR) may be used to characterize peri-event system dynamics associated with such phenomena, and can be learned by exploiting multi-dimensional data gathering samples of the evolution of the system in multiple time windows comprising, each associated with one occurrence of the transient phenomenon, that we will call "trial". However, optimal VAR model order selection methods, commonly relying on the Akaike or Bayesian Information Criteria (AIC/BIC), are typically not designed for multi-trial data. Here we derive the BIC methods for multi-trial ensemble data which are gathered after the detection of the events. We show using simulated bivariate AR models that the multi-trial BIC is able to recover the real model order. We also demonstrate with simulated transient events and real data that the multi-trial BIC is able to estimate a sufficiently small model order for dynamic system modeling.|http://arxiv.org/abs/2204.14096v1|Kaidi Shao,Nikos K. Logothetis,Michel Besserve
173|Statistical Neuroscience in the Single Trial Limit|Individual neurons often produce highly variable responses over nominally identical trials, reflecting a mixture of intrinsic "noise" and systematic changes in the animal's cognitive and behavioral state. Disentangling these sources of variability is of great scientific interest in its own right, but it is also increasingly inescapable as neuroscientists aspire to study more complex and naturalistic animal behaviors. In these settings, behavioral actions never repeat themselves exactly and may rarely do so even approximately. Thus, new statistical methods that extract reliable features of neural activity using few, if any, repeated trials are needed. Accurate statistical modeling in this severely trial-limited regime is challenging, but still possible if simplifying structure in neural data can be exploited. We review recent works that have identified different forms of simplifying structure -- including shared gain modulations across neural subpopulations, temporal smoothness in neural firing rates, and correlations in responses across behavioral conditions -- and exploited them to reveal novel insights into the trial-by-trial operation of neural circuits.|http://arxiv.org/abs/2103.05075v2|Alex H. Williams,Scott W. Linderman
174|Adding flexibility to clinical trial designs: an example-based guide to the practical use of adaptive designs|Adaptive designs for clinical trials permit alterations to a study in response to accumulating data in order to make trials more flexible, ethical and efficient. These benefits are achieved while preserving the integrity and validity of the trial, through the pre-specification and proper adjustment for the possible alterations during the course of the trial. Despite much research in the statistical literature highlighting the potential advantages of adaptive designs over traditional fixed designs, the uptake of such methods in clinical research has been slow. One major reason for this is that different adaptations to trial designs, as well as their advantages and limitations, remain unfamiliar to large parts of the clinical community. The aim of this paper is to clarify where adaptive designs can be used to address specific questions of scientific interest; we introduce the main features of adaptive designs and commonly used terminology, highlighting their utility and pitfalls, and illustrate their use through case studies of adaptive trials ranging from early-phase dose escalation to confirmatory Phase III studies.|http://arxiv.org/abs/2006.12811v1|Thomas Burnett,Pavel Mozgunov,Philip Pallmann,Sofia S. Villar,Graham M. Wheeler,Thomas Jaki
175|Estimands and their Estimators for Clinical Trials Impacted by the COVID-19 Pandemic: A Report from the NISS Ingram Olkin Forum Series on Unplanned Clinical Trial Disruptions|The COVID-19 pandemic continues to affect the conduct of clinical trials globally. Complications may arise from pandemic-related operational challenges such as site closures, travel limitations and interruptions to the supply chain for the investigational product, or from health-related challenges such as COVID-19 infections. Some of these complications lead to unforeseen intercurrent events in the sense that they affect either the interpretation or the existence of the measurements associated with the clinical question of interest. In this article, we demonstrate how the ICH E9(R1) Addendum on estimands and sensitivity analyses provides a rigorous basis to discuss potential pandemic-related trial disruptions and to embed these disruptions in the context of study objectives and design elements. We introduce several hypothetical estimand strategies and review various causal inference and missing data methods, as well as a statistical method that combines unbiased and possibly biased estimators for estimation. To illustrate, we describe the features of a stylized trial, and how it may have been impacted by the pandemic. This stylized trial will then be re-visited by discussing the changes to the estimand and the estimator to account for pandemic disruptions. Finally, we outline considerations for designing future trials in the context of unforeseen disruptions.|http://arxiv.org/abs/2202.03531v1|Kelly Van Lancker,Sergey Tarima,Jonathan Bartlett,Madeline Bauer,Bharani Bharani-Dharan,Frank Bretz,Nancy Flournoy,Hege Michiels,Camila Olarte Parra,James L Rosenberger,Suzie Cro
176|Decentralized Clinical Trials in the Era of Real-World Evidence: A Statistical Perspective|There has been a growing trend that activities relating to clinical trials take place at locations other than traditional trial sites (hence decentralized clinical trials or DCTs), some of which are at settings of real-world clinical practice. Although there are numerous benefits of DCTs, this also brings some implications on a number of issues relating to the design, conduct, and analysis of DCTs. The Real-World Evidence Scientific Working Group of the American Statistical Association Biopharmaceutical Section has been reviewing the field of DCTs and provides in this paper considerations for decentralized trials from a statistical perspective. This paper first discusses selected critical decentralized elements that may have statistical implications on the trial and then summarizes regulatory guidance, framework, and initiatives on DCTs. More discussions are presented by focusing on the design (including construction of estimand), implementation, statistical analysis plan (including missing data handling), and reporting of safety events. Some additional considerations (e.g., ethical considerations, technology infrastructure, study oversight, data security and privacy, and regulatory compliance) are also briefly discussed. This paper is intended to provide statistical considerations for decentralized trials of medical products to support regulatory decision-making.|http://arxiv.org/abs/2410.06591v1|Jie Chen,Junrui Di,Nadia Daizadeh,Ying Lu,Hongwei Wang,Yuan-Li Shen,Jennifer Kirk,Frank W. Rockhold,Herbert Pang,Jing Zhao,Weili He,Andrew Potter,Hana Lee
177|An Information Extraction Approach to Prescreen Heart Failure Patients for Clinical Trials|To reduce the large amount of time spent screening, identifying, and recruiting patients into clinical trials, we need prescreening systems that are able to automate the data extraction and decision-making tasks that are typically relegated to clinical research study coordinators. However, a major obstacle is the vast amount of patient data available as unstructured free-form text in electronic health records. Here we propose an information extraction-based approach that first automatically converts unstructured text into a structured form. The structured data are then compared against a list of eligibility criteria using a rule-based system to determine which patients qualify for enrollment in a heart failure clinical trial. We show that we can achieve highly accurate results, with recall and precision values of 0.95 and 0.86, respectively. Our system allowed us to significantly reduce the time needed for prescreening patients from a few weeks to a few minutes. Our open-source information extraction modules are available for researchers and could be tested and validated in other cardiovascular trials. An approach such as the one we demonstrate here may decrease costs and expedite clinical trials, and could enhance the reproducibility of trials across institutions and populations.|http://arxiv.org/abs/1609.01594v1|Abhishek Kalyan Adupa,Ravi Prakash Garg,Jessica Corona-Cox,Sanjiv. J. Shah,Siddhartha R. Jonnalagadda
178|Automating the Compilation of Potential Core-Outcomes for Clinical Trials|Due to increased access to clinical trial outcomes and analysis, researchers and scientists are able to iterate or improve upon relevant approaches more effectively. However, the metrics and related results of clinical trials typically do not follow any standardization in their reports, making it more difficult for researchers to parse the results of different trials. The objective of this paper is to describe an automated method utilizing natural language processing in order to describe the probable core outcomes of clinical trials, in order to alleviate the issues around disparate clinical trial outcomes. As the nature of this process is domain specific, BioBERT was employed in order to conduct a multi-class entity normalization task. In addition to BioBERT, an unsupervised feature-based approach making use of only the encoder output embedding representations for the outcomes and labels was utilized. Finally, cosine similarity was calculated across the vectors to obtain the semantic similarity. This method was able to both harness the domain-specific context of each of the tokens from the learned embeddings of the BioBERT model as well as a more stable metric of sentence similarity. Some common outcomes identified using the Jaccard similarity in each of the classifications were compiled, and while some are untenable, a pipeline for which this automation process could be conducted was established.|http://arxiv.org/abs/2101.04076v1|Shwetha Bharadwaj,Melanie Laffin
179|HeCiX: Integrating Knowledge Graphs and Large Language Models for Biomedical Research|Despite advancements in drug development strategies, 90% of clinical trials fail. This suggests overlooked aspects in target validation and drug optimization. In order to address this, we introduce HeCiX-KG, Hetionet-Clinicaltrials neXus Knowledge Graph, a novel fusion of data from ClinicalTrials.gov and Hetionet in a single knowledge graph. HeCiX-KG combines data on previously conducted clinical trials from ClinicalTrials.gov, and domain expertise on diseases and genes from Hetionet. This offers a thorough resource for clinical researchers. Further, we introduce HeCiX, a system that uses LangChain to integrate HeCiX-KG with GPT-4, and increase its usability. HeCiX shows high performance during evaluation against a range of clinically relevant issues, proving this model to be promising for enhancing the effectiveness of clinical research. Thus, this approach provides a more holistic view of clinical trials and existing biological data.|http://arxiv.org/abs/2407.14030v1|Prerana Sanjay Kulkarni,Muskaan Jain,Disha Sheshanarayana,Srinivasan Parthiban
180|Distribution-Based Sub-Population Selection (DSPS): A Method for in-Silico Reproduction of Clinical Trials Outcomes|Background and Objective: Diabetes presents a significant challenge to healthcare due to the negative impact of poor blood sugar control on health and associated complications. Computer simulation platforms, notably exemplified by the UVA/Padova Type 1 Diabetes simulator, has emerged as a promising tool for advancing diabetes treatments by simulating patient responses in a virtual environment. The UVA Virtual Lab (UVLab) is a new simulation platform to mimic the metabolic behavior of people with Type 2 diabetes (T2D) with a large population of 6062 virtual subjects. Methods: The work introduces the Distribution-Based Population Selection (DSPS) method, a systematic approach to identifying virtual subsets that mimic the clinical behavior observed in real trials. The method transforms the sub-population selection task into a Linear Programing problem, enabling the identification of the largest representative virtual cohort. This selection process centers on key clinical outcomes in diabetes research, such as HbA1c and Fasting plasma Glucose (FPG), ensuring that the statistical properties (moments) of the selected virtual sub-population closely resemble those observed in real-word clinical trial. Results: DSPS method was applied to the insulin degludec (IDeg) arm of a phase 3 clinical trial. This method was used to select a sub-population of virtual subjects that closely mirrored the clinical trial data across multiple key metrics, including glycemic efficacy, insulin dosages, and cumulative hypoglycemia events over a 26-week period. Conclusion: The DSPS algorithm is able to select virtual sub-population within UVLab to reproduce and predict the outcomes of a clinical trial. This statistical method can bridge the gap between large population simulation platforms and previously conducted clinical trials.|http://arxiv.org/abs/2409.00232v2|Mohammadreza Ganji,Anas El Fathi,Chiara Fabris,Dayu Lv,Boris Kovatchev,Marc Breton
181|SynRL: Aligning Synthetic Clinical Trial Data with Human-preferred Clinical Endpoints Using Reinforcement Learning|Each year, hundreds of clinical trials are conducted to evaluate new medical interventions, but sharing patient records from these trials with other institutions can be challenging due to privacy concerns and federal regulations. To help mitigate privacy concerns, researchers have proposed methods for generating synthetic patient data. However, existing approaches for generating synthetic clinical trial data disregard the usage requirements of these data, including maintaining specific properties of clinical outcomes, and only use post hoc assessments that are not coupled with the data generation process. In this paper, we propose SynRL which leverages reinforcement learning to improve the performance of patient data generators by customizing the generated data to meet the user-specified requirements for synthetic data outcomes and endpoints. Our method includes a data value critic function to evaluate the quality of the generated data and uses reinforcement learning to align the data generator with the users' needs based on the critic's feedback. We performed experiments on four clinical trial datasets and demonstrated the advantages of SynRL in improving the quality of the generated synthetic data while keeping the privacy risks low. We also show that SynRL can be utilized as a general framework that can customize data generation of multiple types of synthetic data generators. Our code is available at https://anonymous.4open.science/r/SynRL-DB0F/.|http://arxiv.org/abs/2411.07317v1|Trisha Das,Zifeng Wang,Afrah Shafquat,Mandis Beigi,Jason Mezey,Jimeng Sun
182|Validating GAN-BioBERT: A Methodology For Assessing Reporting Trends In Clinical Trials|In the past decade, there has been much discussion about the issue of biased reporting in clinical research. Despite this attention, there have been limited tools developed for the systematic assessment of qualitative statements made in clinical research, with most studies assessing qualitative statements relying on the use of manual expert raters, which limits their size. Also, previous attempts to develop larger scale tools, such as those using natural language processing, were limited by both their accuracy and the number of categories used for the classification of their findings. With these limitations in mind, this study's goal was to develop a classification algorithm that was both suitably accurate and finely grained to be applied on a large scale for assessing the qualitative sentiment expressed in clinical trial abstracts. Additionally, this study seeks to compare the performance of the proposed algorithm, GAN-BioBERT, to previous studies as well as to expert manual rating of clinical trial abstracts. This study develops a three-class sentiment classification algorithm for clinical trial abstracts using a semi-supervised natural language process model based on the Bidirectional Encoder Representation from Transformers (BERT) model, from a series of clinical trial abstracts annotated by a group of experts in academic medicine. Results: The use of this algorithm was found to have a classification accuracy of 91.3%, with a macro F1-Score of 0.92, which is a significant improvement in accuracy when compared to previous methods and expert ratings, while also making the sentiment classification finer grained than previous studies. The proposed algorithm, GAN-BioBERT, is a suitable classification model for the large-scale assessment of qualitative statements in clinical trial literature, providing an accurate, reproducible tool for the large-scale study of clinical publication trends.|http://arxiv.org/abs/2106.00665v1|Joshua J Myszewski,Emily Klossowski,Patrick Meyer,Kristin Bevil,Lisa Klesius,Kristopher M Schroeder
183|A shared latent space matrix factorisation method for recommending new trial evidence for systematic review updates|Clinical trial registries can be used to monitor the production of trial evidence and signal when systematic reviews become out of date. However, this use has been limited to date due to the extensive manual review required to search for and screen relevant trial registrations. Our aim was to evaluate a new method that could partially automate the identification of trial registrations that may be relevant for systematic review updates. We identified 179 systematic reviews of drug interventions for type 2 diabetes, which included 537 clinical trials that had registrations in ClinicalTrials.gov. We tested a matrix factorisation approach that uses a shared latent space to learn how to rank relevant trial registrations for each systematic review, comparing the performance to document similarity to rank relevant trial registrations. The two approaches were tested on a holdout set of the newest trials from the set of type 2 diabetes systematic reviews and an unseen set of 141 clinical trial registrations from 17 updated systematic reviews published in the Cochrane Database of Systematic Reviews. The matrix factorisation approach outperformed the document similarity approach with a median rank of 59 and recall@100 of 60.9%, compared to a median rank of 138 and recall@100 of 42.8% in the document similarity baseline. In the second set of systematic reviews and their updates, the highest performing approach used document similarity and gave a median rank of 67 (recall@100 of 62.9%). The proposed method was useful for ranking trial registrations to reduce the manual workload associated with finding relevant trials for systematic review updates. The results suggest that the approach could be used as part of a semi-automated pipeline for monitoring potentially new evidence for inclusion in a review update.|http://arxiv.org/abs/1709.06758v4|Didi Surian,Adam G. Dunn,Liat Orenstein,Rabia Bashir,Enrico Coiera,Florence T. Bourgeois
184|Hegemonic structure of basic, clinical and patented knowledge on Ebola research: a US army reductionist initiative|Background: In this paper, we present an approach to understand how the basic, clinical and patent knowledge on Ebola is organized and intercommunicated and what leading factor could be shaping the evolution of the knowledge translation process for this disease. Methodology: A combination of citation network analysis; analysis of Medical heading Subject (MeSH) and Gene Ontology (GO) terms, and quantitative content analysis for patents and scientific literature, aimed to map the organization of Ebola research was carried out. Results: We found six putative research fronts (i.e. clusters of high interconnected papers). Three research fronts are basic research on Ebola virus structural proteins: glycoprotein, VP40 and VP35, respectively. There is a fourth research front of basic research papers on pathogenesis, which is the organizing hub of Ebola research. A fifth research front is pre-clinical research focused on vaccines and glycoproteins. Finally, a clinical-epidemiology research front related to the disease outbreaks was identified. The network structure of patent families shows that the dominant design is the use of Ebola virus proteins as targets of vaccines and other immunological treatments. Therefore, patents network organization resembles the organization of the scientific literature. Specifically, the knowledge on Ebola would flow from higher (clinical-epidemiology) to intermediated (cellular-tissular pathogenesis) to lower (molecular interactions) levels of organization. Conclusion: Our results suggest a strong reductionist approach for Ebola research probably influenced by the lethality of the disease. On the other hand, the ownership profile of the patent families network and the main researches relationship with the United State Army suggest a strong involvement of this military institution in Ebola research.|http://arxiv.org/abs/1607.08479v1|David Fajardo-Ortiz,Josw Ortega-Sanchez-de-Tagle,Victor-M Castano
185|Beyond p-values: a phase II dual-criterion design with statistical significance and clinical relevance|Background: Well-designed phase II trials must have acceptable error rates relative to a pre-specified success criterion, usually a statistically significant p-value. Such standard designs may not always suffice from a clinical perspective because clinical relevance may call for more. For example, proof-of-concept in phase II often requires not only statistical significance but also a sufficiently large effect estimate.   Purpose: We propose dual-criterion designs to complement statistical significance with clinical relevance, discuss their methodology, and illustrate their implementation in phase II.   Methods: Clinical relevance requires the effect estimate to pass a clinically motivated threshold (the decision value). In contrast to standard designs, the required effect estimate is an explicit design input whereas study power is implicit. The sample size for a dual-criterion design needs careful considerations of the study's operating characteristics (type-I error, power).   Results: Dual-criterion designs are discussed for a randomized controlled and a single-arm phase II trial, including decision criteria, sample size calculations, decisions under various data scenarios, and operating characteristics. The designs facilitate GO/NO-GO decisions due to their complementary statistical-clinical criterion.   Conclusion: To improve evidence-based decision-making, a formal yet transparent quantitative framework is important. Dual-criterion designs offer an appealing statistical-clinical compromise, which may be preferable to standard designs if evidence against the null hypothesis alone does not suffice for an efficacy claim.|http://arxiv.org/abs/1908.07751v1|Satrajit Roychoudhury,Nicolas Scheuer,Beat Neuenschwander
186|Primary analysis method for incomplete CD4 count data from IMPI trial and other trials with similar setting|The National Research Council panel on prevention and treatment of missing data in clinical trials recommends that primary analysis methods are carefully selected before appropriate sensitivity analysis methods can be chosen. In this paper, we recommend an appropriate primary analysis method for handling CD4 count data from the IMPI trial and trials with similar settings. The estimand of interest in the IMPI trial is the effectiveness estimand hypothesis. We discussed, compared, and contrasted results from complete case analysis and simple imputation methods, with the direct-likelihood and multiple imputation methods. The simple imputation methods produced biased estimates of treatment effect. However, the maximum likelihood and the multiple imputation methods produced consistent estimates of treatment effect. The maximum likelihood or the multiple imputation approaches produced unbiased and consistent estimates. Therefore, either the maximum likelihood or the multiple imputation methods, under the assumption that the data are missing at random can be considered as primary analysis method when one is considering sensitivity analysis to dropout using the CD4 count data from the IMPI trial and other trials with similar settings.|http://arxiv.org/abs/2105.03197v1|Abdul-Karim Iddrisu,Abukari Alhassan
187|Using instrumental variables to disentangle treatment and placebo effects in blinded and unblinded randomized clinical trials influenced by unmeasured confounders|Clinical trials traditionally employ blinding as a design mechanism to reduce the influence of placebo effects. In practice, however, it can be difficult or impossible to blind study participants and unblinded trials are common in medical research. Here we show how instrumental variables can be used to quantify and disentangle treatment and placebo effects in randomized clinical trials comparing control and active treatments in the presence of confounders. The key idea is to use randomization to separately manipulate treatment assignment and psychological encouragement messages that increase the participants' desire for improved symptoms. The proposed approach is able to improve the estimation of treatment effects in blinded studies and, most importantly, opens the doors to account for placebo effects in unblinded trials.|http://arxiv.org/abs/1606.04896v2|Elias Chaibub Neto
188|Clinical trials impacted by the COVID-19 pandemic: Adaptive designs to the rescue?|Very recently the new pathogen severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) was identified and the coronavirus disease 2019 (COVID-19) declared a pandemic by the World Health Organization. The pandemic has a number of consequences for the ongoing clinical trials in non-COVID-19 conditions. Motivated by four currently ongoing clinical trials in a variety of disease areas we illustrate the challenges faced by the pandemic and sketch out possible solutions including adaptive designs. Guidance is provided on (i) where blinded adaptations can help; (ii) how to achieve type I error rate control, if required; (iii) how to deal with potential treatment effect heterogeneity; (iv) how to utilize early readouts; and (v) how to utilize Bayesian techniques. In more detail approaches to resizing a trial affected by the pandemic are developed including considerations to stop a trial early, the use of group-sequential designs or sample size adjustment. All methods considered are implemented in a freely available R shiny app. Furthermore, regulatory and operational issues including the role of data monitoring committees are discussed.|http://arxiv.org/abs/2005.13979v1|Cornelia Ursula Kunz,Silke Jrgens,Frank Bretz,Nigel Stallard,Kelly Van Lancker,Dong Xi,Sarah Zohar,Christoph Gerlinger,Tim Friede
189|Adaptive treatment allocation and selection in multi-arm clinical trials: a Bayesian perspective|Clinical trials are an instrument for making informed decisions based on evidence from well-designed experiments. Here we consider adaptive designs mainly from the perspective of multi-arm Phase II clinical trials, in which one or more experimental treatments are compared to a control. Treatment allocation of individual trial participants is assumed to take place according to a fixed block randomization, albeit with an important twist: The performance of each treatment arm is assessed after every measured outcome, in terms of the posterior distribution of a corresponding model parameter. Different treatments arms are then compared to each other, according to pre-defined criteria and using the joint posterior as the basis for such assessment. If a treatment is found to be sufficiently clearly inferior to the currently best candidate, it can be closed off either temporarily or permanently from further participant accrual. The latter possibility provides a method for adaptive treatment selection, including early stopping of the trial. The main development in the paper is in terms of binary outcomes, but some extensions, notably for handling time-to-event data, are discussed as well. The presentation is to a large extent comparative and expository.|http://arxiv.org/abs/2104.03398v2|Elja Arjas,Dario Gasbarra
190|trialr: Bayesian Clinical Trial Designs in R and Stan|This manuscript introduces an \proglang{R} package called \pkg{trialr} that implements a collection of clinical trial methods in \proglang{Stan} and \proglang{R}. In this article, we explore three methods in detail. The first is the continual reassessment method for conducting phase I dose-finding trials that seek a maximum tolerable dose. The second is EffTox, a dose-finding design that scrutinises doses by joint efficacy and toxicity outcomes. The third is the augmented binary method for modelling the probability of treatment success in phase II oncology trials with reference to repeated measures of continuous tumour size and binary indicators of treatment failure. We emphasise in this article the benefits that stem from having access to posterior samples, including flexible inference and powerful visualisation. We hope that this package encourages the use of Bayesian methods in clinical trials.|http://arxiv.org/abs/1907.00161v1|Kristian Brock
191|The Future will be Different than Today: Model Evaluation Considerations when Developing Translational Clinical Biomarker|Finding translational biomarkers stands center stage of the future of personalized medicine in healthcare. We observed notable challenges in identifying robust biomarkers as some with great performance in one scenario often fail to perform well in new trials (e.g. different population, indications). With rapid development in the clinical trial world (e.g. assay, disease definition), new trials very likely differ from legacy ones in many perspectives and in development of biomarkers this heterogeneity should be considered. In response, we recommend considering building in the heterogeneity when evaluating biomarkers. In this paper, we present one evaluation strategy by using leave-one-study-out (LOSO) in place of conventional cross-validation (cv) methods to account for the potential heterogeneity across trials used for building and testing the biomarkers. To demonstrate the performance of K-fold vs LOSO cv in estimating the effect size of biomarkers, we leveraged data from clinical trials and simulation studies. In our assessment, LOSO cv provided a more objective estimate of the future performance. This conclusion remained true across different evaluation metrics and different statistical methods.|http://arxiv.org/abs/2107.08787v1|Yichen Lu,Jane Fridlyand,Tiffany Tang,Ting Qi,Noah Simon,Ning Leng
192|ITTC @ TREC 2021 Clinical Trials Track|This paper describes the submissions of the Natural Language Processing (NLP) team from the Australian Research Council Industrial Transformation Training Centre (ITTC) for Cognitive Computing in Medical Technologies to the TREC 2021 Clinical Trials Track. The task focuses on the problem of matching eligible clinical trials to topics constituting a summary of a patient's admission notes. We explore different ways of representing trials and topics using NLP techniques, and then use a common retrieval model to generate the ranked list of relevant trials for each topic. The results from all our submitted runs are well above the median scores for all topics, but there is still plenty of scope for improvement.|http://arxiv.org/abs/2202.07858v1|Thinh Hung Truong,Yulia Otmakhova,Rahmad Mahendra,Timothy Baldwin,Jey Han Lau,Trevor Cohn,Lawrence Cavedon,Damiano Spina,Karin Verspoor
193|A matching design for augmenting a randomized clinical trial with external control|The use of information from real world to assess the effectiveness of medical products is becoming increasingly popular and more acceptable by regulatory agencies. According to a strategic real-world evidence framework published by U.S. Food and Drug Administration, a hybrid randomized controlled trial that augments internal control arm with real-world data is a pragmatic approach worth more attention. In this paper, we aim to improve on existing matching designs for such a hybrid randomized controlled trial. In particular, we propose to match the entire concurrent randomized clinical trial (RCT) such that (1) the matched external control subjects used to augment the internal control arm are as comparable as possible to the RCT population, (2) every active treatment arm in an RCT with multiple treatments is compared with the same control group, and (3) matching can be conducted and the matched set locked before treatment unblinding to better maintain the data integrity. Besides a weighted estimator, we also introduce a bootstrap method to obtain its variance estimation. The finite sample performance of the proposed method is evaluated by simulations based on data from a real clinical trial.|http://arxiv.org/abs/2203.10128v1|Jianghao Li,Yu Du,Huayu Liu,Yanyao Yi
194|Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching|The process of matching patients with suitable clinical trials is essential for advancing medical research and providing optimal care. However, current approaches face challenges such as data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. In this paper, we explore the potential of large language models (LLMs) to address these challenges by leveraging their advanced natural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions. We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data. Our experiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method, and the generalizability to new data is improved by 12.12%. Additionally, we present case studies to further illustrate the effectiveness of our approach and provide a deeper understanding of its underlying principles.|http://arxiv.org/abs/2303.16756v2|Jiayi Yuan,Ruixiang Tang,Xiaoqian Jiang,Xia Hu
195|Using Bayesian Statistics in Confirmatory Clinical Trials in the Regulatory Setting|Bayesian statistics plays a pivotal role in advancing medical science by enabling healthcare companies, regulators, and stakeholders to assess the safety and efficacy of new treatments, interventions, and medical procedures. The Bayesian framework offers a unique advantage over the classical framework, especially when incorporating prior information into a new trial with quality external data, such as historical data or another source of co-data. In recent years, there has been a significant increase in regulatory submissions using Bayesian statistics due to its flexibility and ability to provide valuable insights for decision-making, addressing the modern complexity of clinical trials where frequentist trials are inadequate. For regulatory submissions, companies often need to consider the frequentist operating characteristics of the Bayesian analysis strategy, regardless of the design complexity. In particular, the focus is on the frequentist type I error rate and power for all realistic alternatives. This tutorial review aims to provide a comprehensive overview of the use of Bayesian statistics in sample size determination in the regulatory environment of clinical trials. Fundamental concepts of Bayesian sample size determination and illustrative examples are provided to serve as a valuable resource for researchers, clinicians, and statisticians seeking to develop more complex and innovative designs.|http://arxiv.org/abs/2311.16506v2|Se Yoon Lee
196|A non-parametric U-statistic testing approach for multi-arm clinical trials with multivariate longitudinal data|Randomized clinical trials (RCTs) often involve multiple longitudinal primary outcomes to comprehensively assess treatment efficacy. The Longitudinal Rank-Sum Test (LRST), a robust U-statistics-based, non-parametric, rank-based method, effectively controls Type I error and enhances statistical power by leveraging the temporal structure of the data without relying on distributional assumptions. However, the LRST is limited to two-arm comparisons. To address the need for comparing multiple doses against a control group in many RCTs, we extend the LRST to a multi-arm setting. This novel multi-arm LRST provides a flexible and powerful approach for evaluating treatment efficacy across multiple arms and outcomes, with a strong capability for detecting the most effective dose in multi-arm trials. Extensive simulations demonstrate that this method maintains excellent Type I error control while providing greater power compared to the two-arm LRST with multiplicity adjustments. Application to the Bapineuzumab (Bapi) 301 trial further validates the multi-arm LRST's practical utility and robustness, confirming its efficacy in complex clinical trial analyses.|http://arxiv.org/abs/2408.10149v1|Dhrubajyoti Ghosh,Sheng Luo
197|Applications of Information Theory to Analysis of Neural Data|Information theory is a practical and theoretical framework developed for the study of communication over noisy channels. Its probabilistic basis and capacity to relate statistical structure to function make it ideally suited for studying information flow in the nervous system. It has a number of useful properties: it is a general measure sensitive to any relationship, not only linear effects; it has meaningful units which in many cases allow direct comparison between different experiments; and it can be used to study how much information can be gained by observing neural responses in single trials, rather than in averages over multiple trials. A variety of information theoretic quantities are commonly used in neuroscience - (see entry "Definitions of Information-Theoretic Quantities"). In this entry we review some applications of information theory in neuroscience to study encoding of information in both single neurons and neuronal populations.|http://arxiv.org/abs/1501.01860v1|Simon R. Schultz,Robin A. A. Ince,Stefano Panzeri
198|Enhancing spatial auditory attention decoding with neuroscience-inspired prototype training|The spatial auditory attention decoding (Sp-AAD) technology aims to determine the direction of auditory attention in multi-talker scenarios via neural recordings. Despite the success of recent Sp-AAD algorithms, their performance is hindered by trial-specific features in EEG data. This study aims to improve decoding performance against these features. Studies in neuroscience indicate that spatial auditory attention can be reflected in the topological distribution of EEG energy across different frequency bands. This insight motivates us to propose Prototype Training, a neuroscience-inspired method for Sp-AAD. This method constructs prototypes with enhanced energy distribution representations and reduced trial-specific characteristics, enabling the model to better capture auditory attention features. To implement prototype training, an EEGWaveNet that employs the wavelet transform of EEG is further proposed. Detailed experiments indicate that the EEGWaveNet with prototype training outperforms other competitive models on various datasets, and the effectiveness of the proposed method is also validated. As a training method independent of model architecture, prototype training offers new insights into the field of Sp-AAD.|http://arxiv.org/abs/2407.06498v1|Zelin Qiu,Jianjun Gu,Dingding Yao,Junfeng Li
199|Quantifying the Diaspora of Knowledge in the Last Century|Academic research is driven by several factors causing different disciplines to act as "sources" or "sinks" of knowledge. However, how the flow of authors' research interests -- a proxy of human knowledge -- evolved across time is still poorly understood. Here, we build a comprehensive map of such flows across one century, revealing fundamental periods in the raise of interest in areas of human knowledge. We identify and quantify the most attractive topics over time, when a relatively significant number of researchers moved from their original area to another one, causing what we call a "diaspora of the knowledge" towards sinks of scientific interest, and we relate these points to crucial historical and political events. Noticeably, only a few areas -- like Medicine, Physics or Chemistry -- mainly act as sources of the diaspora, whereas areas like Material Science, Chemical Engineering, Neuroscience, Immunology and Microbiology or Environmental Science behave like sinks.|http://arxiv.org/abs/1604.00696v1|Manlio De Domenico,Elisa Omodei,Alex Arenas
200|Generalizing trial evidence to target populations in non-nested designs: Applications to AIDS clinical trials|Comparative effectiveness evidence from randomized trials may not be directly generalizable to a target population of substantive interest when, as in most cases, trial participants are not randomly sampled from the target population. Motivated by the need to generalize evidence from two trials conducted in the AIDS Clinical Trials Group (ACTG), we consider weighting, regression and doubly robust estimators to estimate the causal effects of HIV interventions in a specified population of people living with HIV in the USA. We focus on a non-nested trial design and discuss strategies for both point and variance estimation of the target population average treatment effect. Specifically in the generalizability context, we demonstrate both analytically and empirically that estimating the known propensity score in trials does not increase the variance for each of the weighting, regression and doubly robust estimators. We apply these methods to generalize the average treatment effects from two ACTG trials to specified target populations and operationalize key practical considerations. Finally, we report on a simulation study that investigates the finite-sample operating characteristics of the generalizability estimators and their sandwich variance estimators.|http://arxiv.org/abs/2103.04907v2|Fan Li,Ashley L. Buchanan,Stephen R. Cole
201|Matching Patients to Clinical Trials with Large Language Models|Patient recruitment is challenging for clinical trials. We introduce TrialGPT, an end-to-end framework for zero-shot patient-to-trial matching with large language models. TrialGPT comprises three modules: it first performs large-scale filtering to retrieve candidate trials (TrialGPT-Retrieval); then predicts criterion-level patient eligibility (TrialGPT-Matching); and finally generates trial-level scores (TrialGPT-Ranking). We evaluate TrialGPT on three cohorts of 183 synthetic patients with over 75,000 trial annotations. TrialGPT-Retrieval can recall over 90% of relevant trials using less than 6% of the initial collection. Manual evaluations on 1,015 patient-criterion pairs show that TrialGPT-Matching achieves an accuracy of 87.3% with faithful explanations, close to the expert performance. The TrialGPT-Ranking scores are highly correlated with human judgments and outperform the best-competing models by 43.8% in ranking and excluding trials. Furthermore, our user study reveals that TrialGPT can reduce the screening time by 42.6% in patient recruitment. Overall, these results have demonstrated promising opportunities for patient-to-trial matching with TrialGPT.|http://arxiv.org/abs/2307.15051v5|Qiao Jin,Zifeng Wang,Charalampos S. Floudas,Fangyuan Chen,Changlin Gong,Dara Bracken-Clarke,Elisabetta Xue,Yifan Yang,Jimeng Sun,Zhiyong Lu
202|Comment on ``Quantum noise influencing human behaviour could fake effectiveness of drugs in clinical trials''|Here are discussed some problems concerning quant-ph/0208006.|http://arxiv.org/abs/quant-ph/0209024v1|Alexander Yu. Vlasov
203|Learning Modular Safe Policies in the Bandit Setting with Application to Adaptive Clinical Trials|The stochastic multi-armed bandit problem is a well-known model for studying the exploration-exploitation trade-off. It has significant possible applications in adaptive clinical trials, which allow for dynamic changes in the treatment allocation probabilities of patients. However, most bandit learning algorithms are designed with the goal of minimizing the expected regret. While this approach is useful in many areas, in clinical trials, it can be sensitive to outlier data, especially when the sample size is small. In this paper, we define and study a new robustness criterion for bandit problems. Specifically, we consider optimizing a function of the distribution of returns as a regret measure. This provides practitioners more flexibility to define an appropriate regret measure. The learning algorithm we propose to solve this type of problem is a modification of the BESA algorithm [Baransi et al., 2014], which considers a more general version of regret. We present a regret bound for our approach and evaluate it empirically both on synthetic problems as well as on a dataset from the clinical trial literature. Our approach compares favorably to a suite of standard bandit algorithms.|http://arxiv.org/abs/1903.01026v3|Hossein Aboutalebi,Doina Precup,Tibor Schuster
204|Two-Stage Penalized Regression Screening to Detect Biomarker-Treatment Interactions in Randomized Clinical Trials|High-dimensional biomarkers such as genomics are increasingly being measured in randomized clinical trials. Consequently, there is a growing interest in developing methods that improve the power to detect biomarker-treatment interactions. We adapt recently proposed two-stage interaction detecting procedures in the setting of randomized clinical trials. We also propose a new stage 1 multivariate screening strategy using ridge regression to account for correlations among biomarkers. For this multivariate screening, we prove the asymptotic between-stage independence, required for family-wise error rate control, under biomarker-treatment independence. Simulation results show that in various scenarios, the ridge regression screening procedure can provide substantially greater power than the traditional one-biomarker-at-a-time screening procedure in highly correlated data. We also exemplify our approach in two real clinical trial data applications.|http://arxiv.org/abs/2004.12028v2|Jixiong Wang,Ashish Patel,James M. S. Wason,Paul J. Newcombe
205|BOP2-DC: Bayesian optimal phase II designs with dual-criterion decision making|The conventional phase II trial design paradigm is to make the go/no-go decision based on the hypothesis testing framework. Statistical significance itself alone, however, may not be sufficient to establish that the drug is clinically effective enough to warrant confirmatory phase III trials. We propose the Bayesian optimal phase II trial design with dual-criterion decision making (BOP2-DC), which incorporates both statistical significance and clinical relevance into decision making. Based on the posterior probability that the treatment effect reaches the lower reference value (statistical significance) and the clinically meaningful value (clinical significance), BOP2-DC allows for go/consider/no-go decisions, rather than a binary go/no-go decision, and it is optimized to maximize the probability of a go decision when the treatment is effective or minimize the sample size when the treatment is futile. BOP2-DC is highly flexible and accommodates various types of endpoints, including binary, continuous, time-to-event, multiple, and co-primary endpoints, in single-arm and randomized trials. Simulation studies show that the BOP2-DC design yields desirable operating characteristics. The software to implement BOP2-DC is freely available at \url{www.trialdesign.org}.|http://arxiv.org/abs/2112.10880v1|Yujie Zhao,Daniel Li,Rong Liu,Ying Yuan
206|SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language Models on Natural Language Inference for Clinical Trials|This paper describes our submission to Task 2 of SemEval-2024: Safe Biomedical Natural Language Inference for Clinical Trials. The Multi-evidence Natural Language Inference for Clinical Trial Data (NLI4CT) consists of a Textual Entailment (TE) task focused on the evaluation of the consistency and faithfulness of Natural Language Inference (NLI) models applied to Clinical Trial Reports (CTR). We test 2 distinct approaches, one based on finetuning and ensembling Masked Language Models and the other based on prompting Large Language Models using templates, in particular, using Chain-Of-Thought and Contrastive Chain-Of-Thought. Prompting Flan-T5-large in a 2-shot setting leads to our best system that achieves 0.57 F1 score, 0.64 Faithfulness, and 0.56 Consistency.|http://arxiv.org/abs/2404.03977v1|Mathilde Aguiar,Pierre Zweigenbaum,Nona Naderi
207|Counting Clinical Trials: New Evidence on Pharmaceutical Sector Productivity|We develop a method for assigning high-quality labels to unstructured text. This method is based on fine-tuning an efficient, open-source language model with data extracted from a large, proprietary language model. We apply this method to construct a census of published clinical trials. With these data, we revisit a literature that contends that pharmaceutical sector productivity is declining. Central to this conclusion are measurements of substantial increases in the quantity of clinical trials over time, unmatched by trends in measures of output. In our data, the quantity, quality, and composition of clinical trials are stable since 2010. We show that previous measurements are an artifact of biases introduced by shifts in the composition of other forms of research.|http://arxiv.org/abs/2405.08030v5|Maya M. Durvasula,Sabri Eyuboglu,David M. Ritzwoller
208|Clinical trial cohort selection using Large Language Models on n2c2 Challenges|Clinical trials are a critical process in the medical field for introducing new treatments and innovations. However, cohort selection for clinical trials is a time-consuming process that often requires manual review of patient text records for specific keywords. Though there have been studies on standardizing the information across the various platforms, Natural Language Processing (NLP) tools remain crucial for spotting eligibility criteria in textual reports. Recently, pre-trained large language models (LLMs) have gained popularity for various NLP tasks due to their ability to acquire a nuanced understanding of text. In this paper, we study the performance of large language models on clinical trial cohort selection and leverage the n2c2 challenges to benchmark their performance. Our results are promising with regard to the incorporation of LLMs for simple cohort selection tasks, but also highlight the difficulties encountered by these models as soon as fine-grained knowledge and reasoning are required.|http://arxiv.org/abs/2501.11114v1|Chi-en Amy Tai,Xavier Tannier
209|Maximum Matchings in Graphs for Allocating Kidney Paired Donation|Relatives and friends of an end-stage renal disease patient who offer to donate a kidney are often found to be incompatible with their intended recipients. Kidney paired donation matches one patient and his incompatible donor with another patient and donor in the same situation for an organ exchange. Let patient- donor pairs be the vertices of an undirected graph G, with an edge connecting any two reciprocally compatible vertices. A matching in G is a feasible set of paired donations. We describe various optimization problems on kidney paired donation graphs G and the merits of each in clinical transplantation. Because some matches are geographically undesirable, and the expected lifespan of a transplanted kidney depends on the immunologic concordance of donor and recipient, we weight the edges of G and seek a maximum edge-weight matching. Unfortunately, such matchings might not have the maximum cardinality; there is a risk of an unpredictable trade-off between the quality and quantity of paired donations. We propose an edge-weighting of G which guarantees that every matching with maximum weight also has maximum cardinality, and also maximizes the number of transplants for an exceptional subset of recipients, while reducing travel and favoring immunologic concordance.|http://arxiv.org/abs/1710.00953v3|Sommer Gentry,Michal Mankowski,T. S. Michael,Dorry Segev
210|On Preparing a List of Random treatment Assigns|This paper presents the foundations of a computer oriented approach for preparing a list of random treatment assignments to be adopted in randomised controlled trials. Software is presented which can be applied in the earliest stage of clinical trials and bioequivalence assays. This allocation of patients to treatment in clinical trials ensures exactly equal treatment numbers. The investigation of the randomness properties of an assignment leads to the concept of a 'strong randomised list'. The new approach introduced in this note is based on thresholds and produces a strong randomised list of treatment assignments.|http://arxiv.org/abs/1502.03301v1|N. S. Santos-Magalhaes,H. M. de Oliveira,A. J. Alves
211|Incertus.jl -- The Julia Lego Blocks for Randomized Clinical Trial Designs|In this paper, we present Insertus.jl, the Julia package that can help the user generate a randomization sequence of a given length for a multi-arm trial with a pre-specified target allocation ratio and assess the operating characteristics of the chosen randomization method through Monte Carlo simulations. The developed package is computationally efficient, and it can be invoked in R. Furthermore, the package is open-ended -- it can flexibly accommodate new randomization procedures and evaluate their statistical properties via simulation. It may be also helpful for validating other randomization methods for which software is not readily available. In summary, Insertus.jl can be used as ``Lego Blocks'' to construct a fit-for-purpose randomization procedure for a given clinical trial design.|http://arxiv.org/abs/2407.14248v1|Yevgen Ryeznik,Oleksandr Sverdlov
212|A Clinical Trial Design Approach to Auditing Language Models in Healthcare Setting|We present an audit mechanism for language models, with a focus on models deployed in the healthcare setting. Our proposed mechanism takes inspiration from clinical trial design where we posit the language model audit as a single blind equivalence trial, with the comparison of interest being the subject matter experts. We show that using our proposed method, we can follow principled sample size and power calculations, leading to the requirement of sampling minimum number of records while maintaining the audit integrity and statistical soundness. Finally, we provide a real-world example of the audit used in a production environment in a large-scale public health network.|http://arxiv.org/abs/2411.16702v2|Lovedeep Gondara,Jonathan Simkin
213|Sample-targeted clinical trial adaptation|Clinical trial adaptation refers to any adjustment of the trial protocol after the onset of the trial. The main goal is to make the process of introducing new medical interventions to patients more efficient by reducing the cost and the time associated with evaluating their safety and efficacy. The principal question is how should adaptation be performed so as to minimize the chance of distorting the outcome of the trial. We propose a novel method for achieving this. Unlike previous work our approach focuses on trial adaptation by sample size adjustment. We adopt a recently proposed stratification framework based on collected auxiliary data and show that this information together with the primary measured variables can be used to make a probabilistically informed choice of the particular sub-group a sample should be removed from. Experiments on simulated data are used to illustrate the effectiveness of our method and its application in practice.|http://arxiv.org/abs/1411.3919v1|Ognjen Arandjelovic
214|Principal causal effect identification and surrogate endpoint evaluation by multiple trials|Principal stratification is a causal framework to analyze randomized experiments with a post-treatment variable between the treatment and endpoint variables. Because the principal strata defined by the potential outcomes of the post-treatment variable are not observable, we generally cannot identify the causal effects within principal strata. Motivated by a real data set of phase III adjuvant colon clinical trials, we propose approaches to identifying and estimating the principal causal effects via multiple trials. For the identifiability, we remove the commonly-used exclusion restriction assumption by stipulating that the principal causal effects are homogeneous across these trials. To remove another commonly-used monotonicity assumption, we give a necessary condition for the local identifiability, which requires at least three trials. Applying our approaches to the data from adjuvant colon clinical trials, we find that the commonly-used monotonicity assumption is untenable, and disease-free survival with three-year follow-up is a valid surrogate endpoint for overall survival with five-year follow-up, which satisfies both the causal necessity and the causal sufficiency. We also propose a sensitivity analysis approach based on Bayesian hierarchical models to investigate the impact of the deviation from the homogeneity assumption.|http://arxiv.org/abs/1507.05935v1|Zhichao Jiang,Peng Ding,Zhi Geng
215|Optimizing Trial Designs for Targeted Therapies|An important objective in the development of targeted therapies is to identify the populations where the treatment under consideration has positive benefit risk balance. We consider pivotal clinical trials, where the efficacy of a treatment is tested in an overall population and/or in a pre-specified subpopulation. Based on a decision theoretic framework we derive optimized trial designs by maximizing utility functions. Features to be optimized include the sample size and the population in which the trial is performed (the full population or the targeted subgroup only) as well as the underlying multiple test procedure. The approach accounts for prior knowledge of the efficacy of the drug in the considered populations using a two dimensional prior distribution. The considered utility functions account for the costs of the clinical trial as well as the expected benefit when demonstrating efficacy in the different subpopulations. We model utility functions from a sponsor's as well as from a public health perspective, reflecting actual civil interests. Examples of optimized trial designs obtained by numerical optimization are presented for both perspectives.|http://arxiv.org/abs/1606.03987v1|Thomas Ondra,Sebastian Jobjrnsson,Robert A. Beckman,Carl-Fredrik Burman,Franz Knig,Nigel Stallard,Martin Posch
216|Imbalanced Randomization in Clinical Trials|Randomization is a common technique used in clinical trials to eliminate potential bias and confounders in a patient population. Equal allocation to treatment groups is the standard due to its optimal efficiency in many cases. However, in certain scenarios, unequal allocation can improve efficiency. In superiority trials with more than two groups, the optimal randomization is not always a balanced randomization. In non-inferiority trials, additive margin with equal variance is the only instance with balanced randomization. Optimal randomization for non-inferiority trials can be far from 1:1 and can greatly improve efficiency, a fact which is commonly overlooked. A tool for sample size calculation for non-inferiority trials with additive or multiplicative margin with normal, binomial or Poisson distribution is available at http://www.statlab.wisc.edu/shiny/SSNI/.|http://arxiv.org/abs/1806.06020v3|Thevaa Chandereng,Xiaodan Wei,Rick Chappell
217|Bayesian adaptive N-of-1 trials for estimating population and individual treatment effects|This article proposes a novel adaptive design algorithm that can be used to find optimal treatment allocations in N-of-1 clinical trials. This new methodology uses two Laplace approximations to provide a computationally efficient estimate of population and individual random effects within a repeated measures, adaptive design framework. Given the efficiency of this approach, it is also adopted for treatment selection to target the collection of data for the precise estimation of treatment effects. To evaluate this approach, we consider both a simulated and motivating N-of-1 clinical trial from the literature. For each trial, our methods were compared to the multi-armed bandit approach and a randomised N-of-1 trial design in terms of identifying the best treatment for each patient and the information gained about the model parameters. The results show that our new approach selects designs that are highly efficient in achieving each of these objectives. As such, we propose our Laplace-based algorithm as an efficient approach for designing adaptive N-of-1 trials.|http://arxiv.org/abs/1911.00878v3|S. G. Jagath Senarathne,Antony M. Overstall,James M. McGree
218|A Scalable AI Approach for Clinical Trial Cohort Optimization|FDA has been promoting enrollment practices that could enhance the diversity of clinical trial populations, through broadening eligibility criteria. However, how to broaden eligibility remains a significant challenge. We propose an AI approach to Cohort Optimization (AICO) through transformer-based natural language processing of the eligibility criteria and evaluation of the criteria using real-world data. The method can extract common eligibility criteria variables from a large set of relevant trials and measure the generalizability of trial designs to real-world patients. It overcomes the scalability limits of existing manual methods and enables rapid simulation of eligibility criteria design for a disease of interest. A case study on breast cancer trial design demonstrates the utility of the method in improving trial generalizability.|http://arxiv.org/abs/2109.02808v1|Xiong Liu,Cheng Shi,Uday Deore,Yingbo Wang,Myah Tran,Iya Khalil,Murthy Devarakonda
219|TrialView: An AI-powered Visual Analytics System for Temporal Event Data in Clinical Trials|Randomized controlled trials (RCT) are the gold standards for evaluating the efficacy and safety of therapeutic interventions in human subjects. In addition to the pre-specified endpoints, trial participants' experience reveals the time course of the intervention. Few analytical tools exist to summarize and visualize the individual experience of trial participants. Visual analytics allows integrative examination of temporal event patterns of patient experience, thus generating insights for better care decisions. Towards this end, we introduce TrialView, an information system that combines graph artificial intelligence (AI) and visual analytics to enhance the dissemination of trial data. TrialView offers four distinct yet interconnected views: Individual, Cohort, Progression, and Statistics, enabling an interactive exploration of individual and group-level data. The TrialView system is a general-purpose analytical tool for a broad class of clinical trials. The system is powered by graph AI, knowledge-guided clustering, explanatory modeling, and graph-based agglomeration algorithms. We demonstrate the system's effectiveness in analyzing temporal event data through a case study.|http://arxiv.org/abs/2310.04586v1|Zuotian Li,Xiang Liu,Zelei Cheng,Yingjie Chen,Wanzhu Tu,Jing Su
220|Biomarker-Guided Adaptive Enrichment Design with Threshold Detection for Clinical Trials with Time-to-Event Outcome|Biomarker-guided designs are increasingly used to evaluate personalized treatments based on patients' biomarker status in Phase II and III clinical trials. With adaptive enrichment, these designs can improve the efficiency of evaluating the treatment effect in biomarker-positive patients by increasing their proportion in the randomized trial. While time-to-event outcomes are often used as the primary endpoint to measure treatment effects for a new therapy in severe diseases like cancer and cardiovascular diseases, there is limited research on biomarker-guided adaptive enrichment trials in this context. Such trials almost always adopt hazard ratio methods for statistical measurement of treatment effects. In contrast, restricted mean survival time (RMST) has gained popularity for analyzing time-to-event outcomes because it offers more straightforward interpretations of treatment effects and does not require the proportional hazard assumption. This paper proposes a two-stage biomarker-guided adaptive RMST design with threshold detection and patient enrichment. We develop sophisticated methods for identifying the optimal biomarker threshold, treatment effect estimators in the biomarker-positive subgroup, and approaches for type I error rate, power analysis, and sample size calculation. We present a numerical example of re-designing an oncology trial. An extensive simulation study is conducted to evaluate the performance of the proposed design.|http://arxiv.org/abs/2406.06426v1|Kaiyuan Hua,Hwanhee Hong,Xiaofei Wang
221|Predictive Probabilities Made Simple: A Fast and Accurate Method for Clinical Trial Decision Making|Bayesian predictive probabilities are commonly used for interim monitoring of clinical trials through efficacy and futility stopping rules. Despite their usefulness, calculation of predictive probabilities, particularly in pre-experiment trial simulation, can be a significant challenge. We introduce an approximation for computing predictive probabilities using either a p-value or a posterior probability that significantly reduces this burden. We show the approximation has a high degree of concordance with standard Monte Carlo imputation methods for computing predictive probabilities, and present five simulation studies comparing the approximation to the full predictive probability for a range of primary analysis strategies: dichotomous, time-to-event, and ordinal endpoints, as well as historical borrowing and longitudinal modeling. We find that this faster method of predictive probability approximation works well in all five applications, thus significantly reducing the computational burden of trial simulation, allowing more virtual trials to be simulated to achieve greater precision in estimating trial operating characteristics.|http://arxiv.org/abs/2406.11406v1|Joe Marion,Liz Lorenzi,Cora Allen-Savietta,Scott Berry,Kert Viele
222|Confidence intervals for adaptive trial designs I: A methodological review|Regulatory guidance notes the need for caution in the interpretation of confidence intervals (CIs) constructed during and after an adaptive clinical trial. Conventional CIs of the treatment effects are prone to undercoverage (as well as other undesirable properties) in many adaptive designs, because they do not take into account the potential and realised trial adaptations. This paper is the first in a two-part series that explores CIs for adaptive trials. It provides a comprehensive review of the methods to construct CIs for adaptive designs, while the second paper illustrates how to implement these in practice and proposes a set of guidelines for trial statisticians. We describe several classes of techniques for constructing CIs for adaptive clinical trials, before providing a systematic literature review of available methods, classified by the type of adaptive design. As part of this, we assess, through a proposed traffic light system, which of several desirable features of CIs (such as achieving nominal coverage and consistency with the hypothesis test decision) each of these methods holds.|http://arxiv.org/abs/2411.08495v1|David S. Robertson,Thomas Burnett,Babak Choodari-Oskooei,Munya Dimairo,Michael Grayling,Philip Pallmann,Thomas Jaki
223|Confidence intervals for adaptive trial designs II: Case study and practical guidance|In adaptive clinical trials, the conventional confidence interval (CI) for a treatment effect is prone to undesirable properties such as undercoverage and potential inconsistency with the final hypothesis testing decision. Accordingly, as is stated in recent regulatory guidance on adaptive designs, there is the need for caution in the interpretation of CIs constructed during and after an adaptive clinical trial. However, it may be unclear which of the available CIs in the literature are preferable. This paper is the second in a two-part series that explores CIs for adaptive trials. Part I provided a methodological review of approaches to construct CIs for adaptive designs. In this paper (part II), we present an extended case study based around a two-stage group sequential trial, including a comprehensive simulation study of the proposed CIs for this setting. This facilitates an expanded description of considerations around what makes for an effective CI procedure following an adaptive trial. We show that the CIs can have notably different properties. Finally, we propose a set of guidelines for researchers around the choice of CIs and the reporting of CIs following an adaptive design.|http://arxiv.org/abs/2411.08771v1|David S. Robertson,Thomas Burnett,Babak Choodari-Oskooei,Munya Dimairo,Michael Grayling,Philip Pallmann,Thomas Jaki
224|End-To-End Clinical Trial Matching with Large Language Models|Matching cancer patients to clinical trials is essential for advancing treatment and patient care. However, the inconsistent format of medical free text documents and complex trial eligibility criteria make this process extremely challenging and time-consuming for physicians. We investigated whether the entire trial matching process - from identifying relevant trials among 105,600 oncology-related clinical trials on clinicaltrials.gov to generating criterion-level eligibility matches - could be automated using Large Language Models (LLMs). Using GPT-4o and a set of 51 synthetic Electronic Health Records (EHRs), we demonstrate that our approach identifies relevant candidate trials in 93.3% of cases and achieves a preliminary accuracy of 88.0% when matching patient-level information at the criterion level against a baseline defined by human experts. Utilizing LLM feedback reveals that 39.3% criteria that were initially considered incorrect are either ambiguous or inaccurately annotated, leading to a total model accuracy of 92.7% after refining our human baseline. In summary, we present an end-to-end pipeline for clinical trial matching using LLMs, demonstrating high precision in screening and matching trials to individual patients, even outperforming the performance of qualified medical doctors. Our fully end-to-end pipeline can operate autonomously or with human supervision and is not restricted to oncology, offering a scalable solution for enhancing patient-trial matching in real-world settings.|http://arxiv.org/abs/2407.13463v1|Dyke Ferber,Lars Hilgers,Isabella C. Wiest,Marie-Elisabeth Lemann,Jan Clusmann,Peter Neidlinger,Jiefu Zhu,Georg Wlflein,Jacqueline Lammert,Maximilian Tschochohei,Heiko Bhme,Dirk Jger,Mihaela Aldea,Daniel Truhn,Christiane Hper,Jakob Nikolas Kather
225|Treatment Effect Quantification for Time-to-event Endpoints -- Estimands, Analysis Strategies, and beyond|A draft addendum to ICH E9 has been released for public consultation in August 2017. The addendum focuses on two topics particularly relevant for randomized confirmatory clinical trials: estimands and sensitivity analyses. The need to amend ICH E9 grew out of the realization of a lack of alignment between the objectives of a clinical trial stated in the protocol and the accompanying quantification of the "treatment effect" reported in a regulatory submission. We embed time-to-event endpoints in the estimand framework, and discuss how the four estimand attributes described in the addendum apply to time-to-event endpoints. We point out that if the proportional hazards assumption is not met, the estimand targeted by the most prevalent methods used to analyze time-to-event endpoints, logrank test and Cox regression, depends on the censoring distribution. We discuss for a large randomized clinical trial how the analyses for the primary and secondary endpoints as well as the sensitivity analyses actually performed in the trial can be seen in the context of the addendum. To the best of our knowledge, this is the first attempt to do so for a trial with a time-to-event endpoint. Questions that remain open with the addendum for time-to-event endpoints and beyond are formulated, and recommendations for planning of future trials are given. We hope that this will provide a contribution to developing a common framework based on the final version of the addendum that can be applied to design, protocols, statistical analysis plans, and clinical study reports in the future.|http://arxiv.org/abs/1711.07518v4|Kaspar Rufibach
226|Efficient adaptive designs for clinical trials of interventions for COVID-19|The COVID-19 pandemic has led to an unprecedented response in terms of clinical research activity. An important part of this research has been focused on randomized controlled clinical trials to evaluate potential therapies for COVID-19. The results from this research need to be obtained as rapidly as possible. This presents a number of challenges associated with considerable uncertainty over the natural history of the disease and the number and characteristics of patients affected, and the emergence of new potential therapies. These challenges make adaptive designs for clinical trials a particularly attractive option. Such designs allow a trial to be modified on the basis of interim analysis data or stopped as soon as sufficiently strong evidence has been observed to answer the research question, without compromising the trial's scientific validity or integrity. In this paper we describe some of the adaptive design approaches that are available and discuss particular issues and challenges associated with their use in the pandemic setting. Our discussion is illustrated by details of four ongoing COVID-19 trials that have used adaptive designs.|http://arxiv.org/abs/2005.13309v1|Nigel Stallard,Lisa Hampson,Norbert Benda,Werner Brannath,Tom Burnett,Tim Friede,Peter K. Kimani,Franz Koenig,Johannes Krisam,Pavel Mozgunov,Martin Posch,James Wason,Gernot Wassmer,John Whitehead,S. Faye Williamson,Sarah Zohar,Thomas Jaki
227|Leveraging external data in the analysis of randomized controlled trials: a comparative analysis|The use of patient-level information from previous studies, registries, and other external datasets can support the analysis of single-arm and randomized clinical trials to evaluate and test experimental treatments. However, the integration of external data in the analysis of clinical trials can also compromise the scientific validity of the results due to selection bias, study-to-study differences, unmeasured confounding, and other distortion mechanisms. Therefore, leveraging external data in the analysis of a clinical trial requires the use of appropriate methods that can detect, prevent or mitigate the risks of bias and potential distortion mechanisms. We review several methods that have been previously proposed to leverage external datasets, such as matching procedures or random effects modeling. Different methods present distinct trade-offs between risks and efficiencies. We conduct a comparative analysis of statistical methods to leverage external data and analyze randomized clinical trials. Multiple operating characteristics are discussed, such as the control of false positive results, power, and the bias of the treatment effect estimates, across candidate statistical methods. We compare the statistical methods through a broad set of simulation scenarios. We then compare the methods using a collection of datasets with individual patient-level information from several glioblastoma studies in order to provide recommendations for future glioblastoma trials.|http://arxiv.org/abs/2408.13409v1|Gopal Kotecha,Daniel E. Schwartz,Steffen Ventz,Lorenzo Trippa
228|Designing and evaluating advanced adaptive randomised clinical trials: a practical guide|Background   Advanced adaptive randomised clinical trials are increasingly used. Compared to their conventional counterparts, their flexibility may make them more efficient, increase the probability of obtaining conclusive results without larger samples than necessary, and increase the probability that individual participants are allocated to more promising interventions. However, limited guidance is available on designing and evaluating the performance of advanced adaptive trials.   Methods   We summarise the methodological considerations and provide practical guidance on the entire workflow of planning and evaluating advanced adaptive trials using adaptive stopping, adaptive arm dropping, and response-adaptive randomisation within a Bayesian statistical framework.   Results   This comprehensive practical guide covers the key methodological decisions for advanced adaptive trials and their specification and evaluation using statistical simulation. These considerations include interventions and common control use; outcome type and generation; analysis timing and outcome-data lag; allocation rules; analysis model; adaptation rules for stopping and arm dropping; clinical scenarios assessed; performance metrics; calibration; sensitivity analyses; and reporting. The considerations are covered in the context of realistic examples, along with simulation code using the adaptr R package.   Conclusions   This practical guide will help clinical trialists, methodologists, and biostatisticians design and evaluate advanced adaptive trials.|http://arxiv.org/abs/2501.08765v1|Anders Granholm,Aksel Karl Georg Jensen,Theis Lange,Anders Perner,Morten Hylander Mller,Benjamin Skov Kaas-Hansen
229|Multi-center clinical trials: Randomization and ancillary statistics|The purpose of this paper is to investigate and develop methods for analysis of multi-center randomized clinical trials which only rely on the randomization process as a basis of inference. Our motivation is prompted by the fact that most current statistical procedures used in the analysis of randomized multi-center studies are model based. The randomization feature of the trials is usually ignored. An important characteristic of model based analysis is that it is straightforward to model covariates. Nevertheless, in nearly all model based analyses, the effects due to different centers and, in general, the design of the clinical trials are ignored. An alternative to a model based analysis is to have analyses guided by the design of the trial. Our development of design based methods allows the incorporation of centers as well as other features of the trial design. The methods make use of conditioning on the ancillary statistics in the sample space generated by the randomization process. We have investigated the power of the methods and have found that, in the presence of center variation, there is a significant increase in power. The methods have been extended to group sequential trials with similar increases in power.|http://arxiv.org/abs/0807.4002v1|Lu Zheng,Marvin Zelen
230|FRAMM: Fair Ranking with Missing Modalities for Clinical Trial Site Selection|Despite many efforts to address the disparities, the underrepresentation of gender, racial, and ethnic minorities in clinical trials remains a problem and undermines the efficacy of treatments on minorities. This paper focuses on the trial site selection task and proposes FRAMM, a deep reinforcement learning framework for fair trial site selection. We focus on addressing two real-world challenges that affect fair trial sites selection: the data modalities are often not complete for many potential trial sites, and the site selection needs to simultaneously optimize for both enrollment and diversity since the problem is necessarily a trade-off between the two with the only possible way to increase diversity post-selection being through limiting enrollment via caps. To address the missing data challenge, FRAMM has a modality encoder with a masked cross-attention mechanism for handling missing data, bypassing data imputation and the need for complete data in training. To handle the need for making efficient trade-offs, FRAMM uses deep reinforcement learning with a specifically designed reward function that simultaneously optimizes for both enrollment and fairness.   We evaluate FRAMM using 4,392 real-world clinical trials ranging from 2016 to 2021 and show that FRAMM outperforms the leading baseline in enrollment-only settings while also achieving large gains in diversity. Specifically, it is able to produce a 9% improvement in diversity with similar enrollment levels over the leading baselines. That improved diversity is further manifested in achieving up to a 14% increase in Hispanic enrollment, 27% increase in Black enrollment, and 60% increase in Asian enrollment compared to selecting sites with an enrollment-only model.|http://arxiv.org/abs/2305.19407v1|Brandon Theodorou,Lucas Glass,Cao Xiao,Jimeng Sun
231|Monitoring overall survival in pivotal trials in indolent cancers|Indolent cancers are characterized by long overall survival (OS) times. Therefore, powering a clinical trial to provide definitive assessment of the effects of an experimental intervention on OS in a reasonable timeframe is generally infeasible. Instead, the primary outcome in many pivotal trials is an intermediate clinical response such as progression-free survival (PFS). In several recently reported pivotal trials of interventions for indolent cancers that yielded promising results on an intermediate outcome, however, more mature data or post-approval trials showed concerning OS trends. These problematic results have prompted a keen interest in quantitative approaches for monitoring OS that can support regulatory decision-making related to the risk of an unacceptably large detrimental effect on OS. For example, the US Food and Drug Administration, the American Association for Cancer Research, and the American Statistical Association recently organized a one-day multi-stakeholder workshop entitled 'Overall Survival in Oncology Clinical Trials'. In this paper, we propose OS monitoring guidelines tailored for the setting of indolent cancers. Our pragmatic approach is modeled, in part, on the monitoring guidelines the FDA has used in cardiovascular safety trials conducted in Type 2 Diabetes Mellitus. We illustrate proposals through application to several examples informed by actual case studies.|http://arxiv.org/abs/2310.20658v3|Thomas R Fleming,Lisa V Hampson,Bharani Bharani-Dharan,Frank Bretz,Arunava Chakravartty,Thibaud Coroller,Evanthia Koukouli,Janet Wittes,Nigel Yateman,Emmanuel Zuber
232|Time-to-event estimands and loss to follow-up in oncology in light of the estimands framework|Time-to-event estimands are central to many oncology clinical trials. The estimand framework (addendum to the ICH E9 guideline) calls for precisely defining the treatment effect of interest to align with the clinical question of interest and requires predefining the handling of intercurrent events that occur after treatment initiation and either preclude the observation of an event of interest or impact the interpretation of the treatment effect. We discuss a practical problem in clinical trial design and execution, i.e. in some clinical contexts it is not feasible to systematically follow patients to an event of interest. Loss to follow-up in the presence of intercurrent events can affect the meaning and interpretation of the study results. We provide recommendations for trial design, stressing the need for close alignment of the clinical question of interest and study design, impact on data collection and other practical implications. When patients cannot be systematically followed, compromise may be necessary to select the best available estimand that can be feasibly estimated under the circumstances. We discuss the use of sensitivity and supplementary analyses to examine assumptions of interest.|http://arxiv.org/abs/2203.01781v3|Jonathan Siegel,Hans-Jochen Weber,Stefan Englert,Feng Liu
233|Single-World Intervention Graphs for Defining, Identifying, and Communicating Estimands in Clinical Trials|Confusion often arises when attempting to articulate target estimand(s) of a clinical trial in plain language. We aim to rectify this confusion by using a type of causal graph called the Single-World Intervention Graph (SWIG) to provide a visual representation of the estimand that can be effectively communicated to interdisciplinary stakeholders. These graphs not only display estimands, but also illustrate the assumptions under which a causal estimand is identifiable by presenting the graphical relationships between the treatment, intercurrent events, and clinical outcomes. To demonstrate its usefulness in pharmaceutical research, we present examples of SWIGs for various intercurrent event strategies specified in the ICH E9(R1) addendum, as well as an example from a real-world clinical trial for chronic pain. Latex code to generate all the SWIGs shown is this paper is made available. We advocate clinical trialists adopt the use of SWIGs in their estimand discussions during the planning stages of their studies.|http://arxiv.org/abs/2206.01249v1|Alex Ocampo,Jemar R. Bather
234|Sequential monitoring of response-adaptive randomized clinical trials|Clinical trials are complex and usually involve multiple objectives such as controlling type I error rate, increasing power to detect treatment difference, assigning more patients to better treatment, and more. In literature, both response-adaptive randomization (RAR) procedures (by changing randomization procedure sequentially) and sequential monitoring (by changing analysis procedure sequentially) have been proposed to achieve these objectives to some degree. In this paper, we propose to sequentially monitor response-adaptive randomized clinical trial and study it's properties. We prove that the sequential test statistics of the new procedure converge to a Brownian motion in distribution. Further, we show that the sequential test statistics asymptotically satisfy the canonical joint distribution defined in Jennison and Turnbull (\citeyearJT00). Therefore, type I error and other objectives can be achieved theoretically by selecting appropriate boundaries. These results open a door to sequentially monitor response-adaptive randomized clinical trials in practice. We can also observe from the simulation studies that, the proposed procedure brings together the advantages of both techniques, in dealing with power, total sample size and total failure numbers, while keeps the type I error. In addition, we illustrate the characteristics of the proposed procedure by redesigning a well-known clinical trial of maternal-infant HIV transmission.|http://arxiv.org/abs/1010.3901v1|Hongjian Zhu,Feifang Hu
235|Multi-armed Bandit Models for the Optimal Design of Clinical Trials: Benefits and Challenges|Multi-armed bandit problems (MABPs) are a special type of optimal control problem well suited to model resource allocation under uncertainty in a wide variety of contexts. Since the first publication of the optimal solution of the classic MABP by a dynamic index rule, the bandit literature quickly diversified and emerged as an active research topic. Across this literature, the use of bandit models to optimally design clinical trials became a typical motivating application, yet little of the resulting theory has ever been used in the actual design and analysis of clinical trials. To this end, we review two MABP decision-theoretic approaches to the optimal allocation of treatments in a clinical trial: the infinite-horizon Bayesian Bernoulli MABP and the finite-horizon variant. These models possess distinct theoretical properties and lead to separate allocation rules in a clinical trial design context. We evaluate their performance compared to other allocation rules, including fixed randomization. Our results indicate that bandit approaches offer significant advantages, in terms of assigning more patients to better treatments, and severe limitations, in terms of their resulting statistical power. We propose a novel bandit-based patient allocation rule that overcomes the issue of low power, thus removing a potential barrier for their use in practice.|http://arxiv.org/abs/1507.08025v1|Sofa S. Villar,Jack Bowden,James Wason
236|Multi-classifier prediction of knee osteoarthritis progression from incomplete imbalanced longitudinal data|Conventional inclusion criteria used in osteoarthritis clinical trials are not very effective in selecting patients who would benefit from a therapy being tested. Typically majority of selected patients show no or limited disease progression during a trial period. As a consequence, the effect of the tested treatment cannot be observed, and the efforts and resources invested in running the trial are not rewarded. This could be avoided, if selection criteria were more predictive of the future disease progression.   In this article, we formulated the patient selection problem as a multi-class classification task, with classes based on clinically relevant measures of progression (over a time scale typical for clinical trials). Using data from two long-term knee osteoarthritis studies OAI and CHECK, we tested multiple algorithms and learning process configurations (including multi-classifier approaches, cost-sensitive learning, and feature selection), to identify the best performing machine learning models. We examined the behaviour of the best models, with respect to prediction errors and the impact of used features, to confirm their clinical relevance. We found that the model-based selection outperforms the conventional inclusion criteria, reducing by 20-25% the number of patients who show no progression. This result might lead to more efficient clinical trials.|http://arxiv.org/abs/1909.13408v2|Pawe Widera,Paco M. J. Welsing,Christoph Ladel,John Loughlin,Floris P. J. G. Lafeber,Florence Petit Dop,Jonathan Larkin,Harrie Weinans,Ali Mobasheri,Jaume Bacardit
237|The Optimal Design of Clinical Trials with Potential Biomarker Effects, A Novel Computational Approach|As a future trend of healthcare, personalized medicine tailors medical treatments to individual patients. It requires to identify a subset of patients with the best response to treatment. The subset can be defined by a biomarker (e.g. expression of a gene) and its cutoff value. Topics on subset identification have received massive attention. There are over 2 million hits by keyword searches on Google Scholar. However, how to properly incorporate the identified subsets/biomarkers to design clinical trials is not trivial and rarely discussed in the literature, which leads to a gap between research results and real-world drug development.   To fill in this gap, we formulate the problem of clinical trial design into an optimization problem involving high-dimensional integration, and propose a novel computational solution based on Monte-Carlo and smoothing methods. Our method utilizes the modern techniques of General-Purpose computing on Graphics Processing Units for large-scale parallel computing. Compared to the standard method in three-dimensional problems, our approach is more accurate and 133 times faster. This advantage increases when dimensionality increases. Our method is scalable to higher-dimensional problems since the precision bound is a finite number not affected by dimensionality.   Our software will be available on GitHub and CRAN, which can be applied to guide the design of clinical trials to incorporate the biomarker better. Although our research is motivated by the design of clinical trials, the method can be used widely to solve other optimization problems involving high-dimensional integration.|http://arxiv.org/abs/2005.10494v1|Yitao Lu,Julie Zhou,Li Xing,Xuekui Zhang
238|Zero-Shot Clinical Trial Patient Matching with LLMs|Matching patients to clinical trials is a key unsolved challenge in bringing new drugs to market. Today, identifying patients who meet a trial's eligibility criteria is highly manual, taking up to 1 hour per patient. Automated screening is challenging, however, as it requires understanding unstructured clinical text. Large language models (LLMs) offer a promising solution. In this work, we explore their application to trial matching. First, we design an LLM-based system which, given a patient's medical history as unstructured clinical text, evaluates whether that patient meets a set of inclusion criteria (also specified as free text). Our zero-shot system achieves state-of-the-art scores on the n2c2 2018 cohort selection benchmark. Second, we improve the data and cost efficiency of our method by identifying a prompting strategy which matches patients an order of magnitude faster and more cheaply than the status quo, and develop a two-stage retrieval pipeline that reduces the number of tokens processed by up to a third while retaining high performance. Third, we evaluate the interpretability of our system by having clinicians evaluate the natural language justifications generated by the LLM for each eligibility decision, and show that it can output coherent explanations for 97% of its correct decisions and 75% of its incorrect ones. Our results establish the feasibility of using LLMs to accelerate clinical trial operations.|http://arxiv.org/abs/2402.05125v3|Michael Wornow,Alejandro Lozano,Dev Dash,Jenelle Jindal,Kenneth W. Mahaffey,Nigam H. Shah
239|CTBench: A Comprehensive Benchmark for Evaluating Language Model Capabilities in Clinical Trial Design|CTBench is introduced as a benchmark to assess language models (LMs) in aiding clinical study design. Given study-specific metadata, CTBench evaluates AI models' ability to determine the baseline features of a clinical trial (CT), which include demographic and relevant features collected at the trial's start from all participants. These baseline features, typically presented in CT publications (often as Table 1), are crucial for characterizing study cohorts and validating results. Baseline features, including confounders and covariates, are also necessary for accurate treatment effect estimation in studies involving observational data. CTBench consists of two datasets: "CT-Repo," containing baseline features from 1,690 clinical trials sourced from clinicaltrials.gov, and "CT-Pub," a subset of 100 trials with more comprehensive baseline features gathered from relevant publications. Two LM-based evaluation methods are developed to compare the actual baseline feature lists against LM-generated responses. "ListMatch-LM" and "ListMatch-BERT" use GPT-4o and BERT scores (at various thresholds), respectively, for evaluation. To establish baseline results, advanced prompt engineering techniques using LLaMa3-70B-Instruct and GPT-4o in zero-shot and three-shot learning settings are applied to generate potential baseline features. The performance of GPT-4o as an evaluator is validated through human-in-the-loop evaluations on the CT-Pub dataset, where clinical experts confirm matches between actual and LM-generated features. The results highlight a promising direction with significant potential for improvement, positioning CTBench as a useful tool for advancing research on AI in CT design and potentially enhancing the efficacy and robustness of CTs.|http://arxiv.org/abs/2406.17888v1|Nafis Neehal,Bowen Wang,Shayom Debopadhaya,Soham Dan,Keerthiram Murugesan,Vibha Anand,Kristin P. Bennett
240|Clarifying the Role of the Mantel-Haenszel Risk Difference Estimator in Randomized Clinical Trials|The Mantel-Haenszel (MH) risk difference estimator, commonly used in randomized clinical trials for binary outcomes, calculates a weighted average of stratum-specific risk difference estimators. Traditionally, this method requires the stringent assumption that risk differences are homogeneous across strata, also known as the common risk difference assumption. In our article, we relax this assumption and adopt a modern perspective, viewing the MH risk difference estimator as an approach for covariate adjustment in randomized clinical trials, distinguishing its use from that in meta-analysis and observational studies. We demonstrate that the MH risk difference estimator consistently estimates the average treatment effect within a standard super-population framework, which is often the primary interest in randomized clinical trials, in addition to estimating a weighted average of stratum-specific risk difference. We rigorously study its properties under both the large-stratum and sparse-stratum asymptotic regimes. Furthermore, for either estimand, we propose a unified robust variance estimator that improves over the popular variance estimators by Greenland and Robins (1985) and Sato et al. (1989) and has provable consistency across both asymptotic regimes, regardless of assuming common risk differences. Extensions of our theoretical results also provide new insights into the Cochran-Mantel-Haenszel test and the post-stratification estimator. Our findings are thoroughly validated through simulations and a clinical trial example.|http://arxiv.org/abs/2408.12541v1|Xiaoyu Qiu,Yuhan Qian,Jaehwan Yi,Jinqiu Wang,Yu Du,Yanyao Yi,Ting Ye
241|A Prototype Model of Zero-Trust Architecture Blockchain with EigenTrust-Based Practical Byzantine Fault Tolerance Protocol to Manage Decentralized Clinical Trials|The COVID-19 pandemic necessitated the emergence of decentralized Clinical Trials (DCTs) due to patient retention, accelerate trials, improve data accessibility, enable virtual care, and facilitate seamless communication through integrated systems. However, integrating systems in DCTs exposes clinical data to potential security threats, making them susceptible to theft at any stage, a high risk of protocol deviations, and monitoring issues. To mitigate these challenges, blockchain technology serves as a secure framework, acting as a decentralized ledger, creating an immutable environment by establishing a zero-trust architecture, where data are deemed untrusted until verified. In combination with Internet of Things (IoT)-enabled wearable devices, blockchain secures the transfer of clinical trial data on private blockchains during DCT automation and operations. This paper proposes a prototype model of the Zero-Trust Architecture Blockchain (z-TAB) to integrate patient-generated clinical trial data during DCT operation management. The EigenTrust-based Practical Byzantine Fault Tolerance (T-PBFT) algorithm has been incorporated as a consensus protocol, leveraging Hyperledger Fabric. Furthermore, the Internet of Things (IoT) has been integrated to streamline data processing among stakeholders within the blockchain platforms. Rigorous evaluation has been done to evaluate the quality of the system.|http://arxiv.org/abs/2408.16885v1|Ashok Kumar Peepliwall,Hari Mohan Pandey,Surya Prakash,Anand A Mahajan,Sudhinder Singh Chowhan,Vinesh Kumar,Rahul Sharma
242|Doubly robust estimation and sensitivity analysis with outcomes truncated by death in multi-arm clinical trials|In clinical trials, the observation of participant outcomes may frequently be hindered by death, leading to ambiguity in defining a scientifically meaningful final outcome for those who die. Principal stratification methods are valuable tools for addressing the average causal effect among always-survivors, i.e., the average treatment effect among a subpopulation in the principal strata of those who would survive regardless of treatment assignment. Although robust methods for the truncation-by-death problem in two-arm clinical trials have been previously studied, its expansion to multi-arm clinical trials remains unknown. In this article, we study the identification of a class of survivor average causal effect estimands with multiple treatments under monotonicity and principal ignorability, and first propose simple weighting and regression approaches. As a further improvement, we then derive the efficient influence function to motivate doubly robust estimators for the survivor average causal effects in multi-arm clinical trials. We also articulate sensitivity methods under violations of key causal assumptions. Extensive simulations are conducted to investigate the finite-sample performance of the proposed methods, and a real data example is used to illustrate how to operationalize the proposed estimators and the sensitivity methods in practice.|http://arxiv.org/abs/2410.07483v1|Jiaqi Tong,Chao Cheng,Guangyu Tong,Michael O. Harhay,Fan Li
243|Modelling Immunological Memory|Accurate immunological models offer the possibility of performing highthroughput experiments in silico that can predict, or at least suggest, in vivo phenomena. In this chapter, we compare various models of immunological memory. We first validate an experimental immunological simulator, developed by the authors, by simulating several theories of immunological memory with known results. We then use the same system to evaluate the predicted effects of a theory of immunological memory. The resulting model has not been explored before in artificial immune systems research, and we compare the simulated in silico output with in vivo measurements. Although the theory appears valid, we suggest that there are a common set of reasons why immunological memory models are a useful support tool; not conclusive in themselves.|http://arxiv.org/abs/1004.3932v1|Simon Garret,Martin Robbins,Joanne Walker,William Wilson,Uwe Aickelin
244|naplib-python: Neural Acoustic Data Processing and Analysis Tools in Python|Recently, the computational neuroscience community has pushed for more transparent and reproducible methods across the field. In the interest of unifying the domain of auditory neuroscience, naplib-python provides an intuitive and general data structure for handling all neural recordings and stimuli, as well as extensive preprocessing, feature extraction, and analysis tools which operate on that data structure. The package removes many of the complications associated with this domain, such as varying trial durations and multi-modal stimuli, and provides a general-purpose analysis framework that interfaces easily with existing toolboxes used in the field.|http://arxiv.org/abs/2304.01799v1|Gavin Mischler,Vinay Raghavan,Menoua Keshishian,Nima Mesgarani
245|An introduction to group sequential methods: planning and multi-aspect optimization|A group sequential clinical trial design can be an attractive option when planning a pivotal trial as this approach has the ability to stop the trial early for success, whilst also being well accepted from a regulatory review perspective. Compared to a single stage design there are more moving parts to consider and optimise when planning a group sequential trial. This tutorial briefly outlines the group sequential methodology before detailing some of the key operating characteristics and how these can be estimated, optimised and ultimately presented to decision makers when aligning on a final study design.|http://arxiv.org/abs/2303.01040v1|Fraser I Lewis
246|Estimating the Causal Effects of T Cell Receptors|A central question in human immunology is how a patient's repertoire of T cells impacts disease. Here, we introduce a method to infer the causal effects of T cell receptor (TCR) sequences on patient outcomes using observational TCR repertoire sequencing data and clinical outcomes data. Our approach corrects for unobserved confounders, such as a patient's environment and life history, by using the patient's immature, pre-selection TCR repertoire. The pre-selection repertoire can be estimated from nonproductive TCR data, which is widely available. It is generated by a randomized mutational process, V(D)J recombination, which provides a natural experiment. We show formally how to use the pre-selection repertoire to draw causal inferences, and develop a scalable neural-network estimator for our identification formula. Our method produces an estimate of the effect of interventions that add a specific TCR sequence to patient repertoires. As a demonstration, we use it to analyze the effects of TCRs on COVID-19 severity, uncovering potentially therapeutic TCRs that are (1) observed in patients, (2) bind SARS-CoV-2 antigens in vitro and (3) have strong positive effects on clinical outcomes.|http://arxiv.org/abs/2410.14127v1|Eli N. Weinstein,Elizabeth B. Wood,David M. Blei
247|Learning Eligibility in Cancer Clinical Trials using Deep Neural Networks|Interventional cancer clinical trials are generally too restrictive, and some patients are often excluded on the basis of comorbidity, past or concomitant treatments, or the fact that they are over a certain age. The efficacy and safety of new treatments for patients with these characteristics are, therefore, not defined. In this work, we built a model to automatically predict whether short clinical statements were considered inclusion or exclusion criteria. We used protocols from cancer clinical trials that were available in public registries from the last 18 years to train word-embeddings, and we constructed a~dataset of 6M short free-texts labeled as eligible or not eligible. A text classifier was trained using deep neural networks, with pre-trained word-embeddings as inputs, to predict whether or not short free-text statements describing clinical information were considered eligible. We additionally analyzed the semantic reasoning of the word-embedding representations obtained and were able to identify equivalent treatments for a type of tumor analogous with the drugs used to treat other tumors. We show that representation learning using {deep} neural networks can be successfully leveraged to extract the medical knowledge from clinical trial protocols for potentially assisting practitioners when prescribing treatments.|http://arxiv.org/abs/1803.08312v3|Aurelia Bustos,Antonio Pertusa
248|Blinded sample size re-estimation in three-arm trials with 'gold standard' design|The sample size of a clinical trial relies on information about nuisance parameters such as the outcome variance. When no or only limited information is available, it has been proposed to include an internal pilot study in the design of the trial. Based on the results of the internal pilot study, the initially planned sample size can be adjusted. In this paper, we study blinded sample size re-estimation in the 'gold standard' design for normally distributed outcomes. The 'gold standard' design is a three-arm clinical trial design which includes an active and a placebo control in addition to an experimental treatment. We compare several sample size re-estimation procedures in a simulation study assessing operating characteristics including power and type I error. We find that sample size re-estimation based on the popular one-sample variance estimator results in overpowered trials. Moreover, sample size re-estimation based on unbiased variance estimators such as the Xing-Ganju variance estimator results in underpowered trials, as it is expected since an overestimation of the variance and thus the sample size is in general required for the re-estimation procedure to eventually meet the target power. Moreover, we propose an inflation factor for the sample size re-estimation with the Xing-Ganju variance estimator and show that this approach results in adequately powered trials. Due to favorable features of Xing-Ganju variance estimator such as unbiasedness and a distribution independent of the group means, the inflation factor does not depend on the nuisance parameter and, therefore, can be calculated prior to a trial.|http://arxiv.org/abs/1610.09878v2|Tobias Mtze,Tim Friede
249|DeepEnroll: Patient-Trial Matching with Deep Embedding and Entailment Prediction|Clinical trials are essential for drug development but often suffer from expensive, inaccurate and insufficient patient recruitment. The core problem of patient-trial matching is to find qualified patients for a trial, where patient information is stored in electronic health records (EHR) while trial eligibility criteria (EC) are described in text documents available on the web. How to represent longitudinal patient EHR? How to extract complex logical rules from EC? Most existing works rely on manual rule-based extraction, which is time consuming and inflexible for complex inference. To address these challenges, we proposed DeepEnroll, a cross-modal inference learning model to jointly encode enrollment criteria (text) and patients records (tabular data) into a shared latent space for matching inference. DeepEnroll applies a pre-trained Bidirectional Encoder Representations from Transformers(BERT) model to encode clinical trial information into sentence embedding. And uses a hierarchical embedding model to represent patient longitudinal EHR. In addition, DeepEnroll is augmented by a numerical information embedding and entailment module to reason over numerical information in both EC and EHR. These encoders are trained jointly to optimize patient-trial matching score. We evaluated DeepEnroll on the trial-patient matching task with demonstrated on real world datasets. DeepEnroll outperformed the best baseline by up to 12.4% in average F1.|http://arxiv.org/abs/2001.08179v2|Xingyao Zhang,Cao Xiao,Lucas M. Glass,Jimeng Sun
250|From Estimands to Robust Inference of Treatment Effects in Platform Trials|A platform trial is an innovative clinical trial design that uses a master protocol (i.e., one overarching protocol) to evaluate multiple treatments in an ongoing manner and can accelerate the evaluation of new treatments. However, its flexibility introduces inferential challenges, with two fundamental ones being the precise definition of treatment effects and robust, efficient inference on these effects. Central to these challenges is defining an appropriate target population for the estimand, as the populations represented by some commonly used analysis approaches can arbitrarily depend on the randomization ratio or trial type. For the first time, this article presents a clear framework for constructing a clinically meaningful estimand with precise specificity regarding the population of interest. The proposed entire concurrently eligible (ECE) population not only preserves the integrity of randomized comparisons but also remains invariant to both the randomization ratio and trial type. This lays the groundwork for future design, analysis, and research of platform trials. Then, we develop weighting and post-stratification methods for estimation of treatment effects with minimal assumptions. To fully leverage the efficiency potential of platform trials, we also consider a model-assisted approach for baseline covariate adjustment to gain efficiency while maintaining robustness against model misspecification. We derive and compare asymptotic distributions of proposed estimators in theory and propose robust variance estimators. The proposed estimators are empirically evaluated in a simulation study and applied to the SIMPLIFY trial, using the R package RobinCID.|http://arxiv.org/abs/2411.12944v3|Yuhan Qian,Yifan Yi,Jun Shao,Yanyao Yi,Gregory Levin,Nicole Mayer-Hamblett,Patrick J. Heagerty,Ting Ye
251|New designs for Bayesian adaptive cluster-randomized trials|Adaptive approaches, allowing for more flexible trial design, have been proposed for individually randomized trials to save time or reduce sample size. However, adaptive designs for cluster-randomized trials in which groups of participants rather than individuals are randomized to treatment arms are less common. Motivated by a cluster-randomized trial designed to assess the effectiveness of a machine-learning based clinical decision support system for physicians treating patients with depression, two Bayesian adaptive designs for cluster-randomized trials are proposed to allow for early stopping for efficacy at pre-planned interim analyses. The difference between the two designs lies in the way that participants are sequentially recruited. Given a maximum number of clusters as well as maximum cluster size allowed in the trial, one design sequentially recruits clusters with the given maximum cluster size, while the other recruits all clusters at the beginning of the trial but sequentially enrolls individual participants until the trial is stopped early for efficacy or the final analysis has been reached. The design operating characteristics are explored via simulations for a variety of scenarios and two outcome types for the two designs. The simulation results show that for different outcomes the design choice may be different. We make recommendations for designs of Bayesian adaptive cluster-randomized trial based on the simulation results.|http://arxiv.org/abs/2201.02301v1|Junwei Shen,Shirin Golchi,Erica E. M. Moodie,David Benrimoh
252|Prognostic Adjustment with Efficient Estimators to Unbiasedly Leverage Historical Data in Randomized Trials|Although randomized controlled trials (RCTs) are a cornerstone of comparative effectiveness, they typically have much smaller sample size than observational studies because of financial and ethical considerations. Therefore there is interest in using plentiful historical data (either observational data or prior trials) to reduce trial sizes. Previous estimators developed for this purpose rely on unrealistic assumptions, without which the added data can bias the treatment effect estimate. Recent work proposed an alternative method (prognostic covariate adjustment) that imposes no additional assumptions and increases efficiency in trial analyses. The idea is to use historical data to learn a prognostic model: a regression of the outcome onto the covariates. The predictions from this model, generated from the RCT subjects' baseline variables, are then used as a covariate in a linear regression analysis of the trial data. In this work, we extend prognostic adjustment to trial analyses with nonparametric efficient estimators, which are more powerful than linear regression. We provide theory that explains why prognostic adjustment improves small-sample point estimation and inference without any possibility of bias. Simulations corroborate the theory: efficient estimators using prognostic adjustment compared to without provides greater power (i.e., smaller standard errors) when the trial is small. Population shifts between historical and trial data attenuate benefits but do not introduce bias. We showcase our estimator using clinical trial data provided by Novo Nordisk A/S that evaluates insulin therapy for individuals with type II diabetes.|http://arxiv.org/abs/2305.19180v4|Lauren D. Liao,Emilie Hjbjerre-Frandsen,Alan E. Hubbard,Alejandro Schuler
253|Increased adaptive immune responses and proper feedback regulation protect against clinical dengue|Dengue is the most prevalent arthropod-borne viral disease. Clinical symptoms of dengue virus (DENV) infection range from classical mild dengue fever to severe, life-threatening dengue shock syndrome. However, most DENV infections cause few or no symptoms. Asymptomatic DENV-infected patients provide a unique opportunity to decipher the host immune responses leading to virus elimination without negative impact on t v 'health. We used an integrated approach of transcriptional profiling and immunological analysis comparing a Cambodian population of strictly asymptomatic viremic individuals with clinical dengue patients. Whereas inflammatory pathways and innate immune responses were similar between asymptomatic individuals and clinical dengue patients, expression of proteins related to antigen presentation and subsequent T and B cell activation pathways were differentially regulated, independent of viral load or previous DENV infection. Feedback mechanisms controlled the immune response in asymptomatic viremic individuals as demonstrated by increased activation of T cell apoptosis-related pathways and Fc$\gamma$RIIB signaling associated with decreased anti-DENV specific antibody concentrations. Taken together, our data illustrate that symptom-free DENV infection in children is determined by increased activation of the adaptive immune compartment and proper control mechanisms leading to elimination of viral infection without excessive immune activation, having implications for novel vaccine development strategies.|http://arxiv.org/abs/1712.05692v1|Etienne Simon-Loriere,Veasna Duong,Ahmed Tawfik,Sivlin Ung,Sowath Ly,Isabelle Casademont,Matthieu Prot,Nomie Courtejoie,Kevin Bleakley,Philippe Buchy,Arnaud Tarantola,Philippe Dussart,Tineke Cantaert,Anavaj Sakuntabhai
254|Potential of proteasome inhibitors to inhibit cytokine storm in critical stage COVID-19 patients|Patients infected with SARS-CoV-2 show a wide spectrum of clinical manifestations ranging from mild febrile illness and cough up to acute respiratory distress syndrome, multiple organ failure and death. Data from patients with severe clinical manifestations compared to patients with mild symptoms indicate that highly dysregulated exuberant inflammatory responses correlate with severity of disease and lethality. Significantly elevated cytokine levels, i.e. cytokine storm, seem to play a central role in severity and lethality in COVID-19. We have previously shown that excessive cytokine release induced by highly pathogenic avian H5N1 influenza A virus was reduced by application of proteasome inhibitors. In the present study we present experimental data of a central cellular pro-inflammatory signal pathways, NF-kappaB, in the context of published clinical data from COVID-19 patients and develop a hypothesis for a therapeutic approach aiming at the simultaneous inhibition of whole cascades of pro-inflammatory cytokines and chemokines via blocking the nuclear translocation of NF-kappaB by proteasome inhibitors. The simultaneous inhibition of multiple cytokines/chemokines using clinically approved proteasome inhibitors is expected to have a higher therapeutic potential compared to single target approaches to prevent cascade (i.e. triggering, synergistic, and redundant) effects of multiple induced cytokines and may provide an additional therapeutic option to be explored for treatment of critical stage COVID-19 patients.|http://arxiv.org/abs/2008.10404v1|Ralf Kircheis,Emanuel Haasbach,Daniel Lueftenegger,Willm T. Heyken,Matthias Ocker,Oliver Planz
255|TrialGraph: Machine Intelligence Enabled Insight from Graph Modelling of Clinical Trials|A major impediment to successful drug development is the complexity, cost, and scale of clinical trials. The detailed internal structure of clinical trial data can make conventional optimization difficult to achieve. Recent advances in machine learning, specifically graph-structured data analysis, have the potential to enable significant progress in improving the clinical trial design. TrialGraph seeks to apply these methodologies to produce a proof-of-concept framework for developing models which can aid drug development and benefit patients. In this work, we first introduce a curated clinical trial data set compiled from the CT.gov, AACT and TrialTrove databases (n=1191 trials; representing one million patients) and describe the conversion of this data to graph-structured formats. We then detail the mathematical basis and implementation of a selection of graph machine learning algorithms, which typically use standard machine classifiers on graph data embedded in a low-dimensional feature space. We trained these models to predict side effect information for a clinical trial given information on the disease, existing medical conditions, and treatment. The MetaPath2Vec algorithm performed exceptionally well, with standard Logistic Regression, Decision Tree, Random Forest, Support Vector, and Neural Network classifiers exhibiting typical ROC-AUC scores of 0.85, 0.68, 0.86, 0.80, and 0.77, respectively. Remarkably, the best performing classifiers could only produce typical ROC-AUC scores of 0.70 when trained on equivalent array-structured data. Our work demonstrates that graph modelling can significantly improve prediction accuracy on appropriate datasets. Successive versions of the project that refine modelling assumptions and incorporate more data types can produce excellent predictors with real-world applications in drug development.|http://arxiv.org/abs/2112.08211v1|Christopher Yacoumatos,Stefano Bragaglia,Anshul Kanakia,Nils Svangrd,Jonathan Mangion,Claire Donoghue,Jim Weatherall,Faisal M. Khan,Khader Shameer
256|Dose Finding with Escalation with Overdose Control (EWOC) in Cancer Clinical Trials|Traditionally, the major objective in phase I trials is to identify a working-dose for subsequent studies, whereas the major endpoint in phase II and III trials is treatment efficacy. The dose sought is typically referred to as the maximum tolerated dose (MTD). Several statistical methodologies have been proposed to select the MTD in cancer phase I trials. In this manuscript, we focus on a Bayesian adaptive design, known as escalation with overdose control (EWOC). Several aspects of this design are discussed, including large sample properties of the sequence of doses selected in the trial, choice of prior distributions, and use of covariates. The methodology is exemplified with real-life examples of cancer phase I trials. In particular, we show in the recently completed ABR-217620 (naptumomab estafenatox) trial that omitting an important predictor of toxicity when dose assignments to cancer patients are determined results in a high percent of patients experiencing severe side effects and a significant proportion treated at sub-optimal doses.|http://arxiv.org/abs/1011.6479v1|Mourad Tighiouart,Andr Rogatko
257|Likelihood reweighting methods to reduce potential bias in noninferiority trials which rely on historical data to make inference|It is generally believed that bias is minimized in well-controlled randomized clinical trials. However, bias can arise in active controlled noninferiority trials because the inference relies on a previously estimated effect size obtained from a historical trial that may have been conducted for a different population. By implementing a likelihood reweighting method through propensity scoring, a study designed to estimate a treatment effect in one trial population can be used to estimate the treatment effect size in a different target population. We illustrate this method in active controlled noninferiority trials, although it can also be used in other types of studies, such as historically controlled trials, meta-analyses, and comparative effectiveness analyses.|http://arxiv.org/abs/1311.7485v1|Lei Nie,Zhiwei Zhang,Daniel Rubin,Jianxiong Chu
258|Designing an exploratory phase 2b platform trial in NASH with correlated, co-primary binary endpoints|Non-alcoholic steatohepatitis (NASH) is the progressive form of nonalcoholic fatty liver disease (NAFLD) and a disease with high unmet medical need. Platform trials provide great benefits for sponsors and trial participants in terms of accelerating drug development programs. In this article, we describe some of the activities of the EU-PEARL consortium (EU Patient-cEntric clinicAl tRial pLatforms) regarding the use of platform trials in NASH, in particular the proposed trial design, decision rules and simulation results. For a set of assumptions, we present the results of a simulation study recently discussed with two health authorities and the learnings from these meetings from a trial design perspective. Since the proposed design uses co-primary binary endpoints, we furthermore discuss the different options and practical considerations for simulating correlated binary endpoints.|http://arxiv.org/abs/2210.06228v2|Elias Laurin Meyer,Peter Mesenbrink,Nicholas A. Di Prospero,Juan M. Perics,Ekkehard Glimm,Vlad Ratziu,Elena Sena,Franz Knig
259|An Empirical Likelihood Approach to Nonparametric Covariate Adjustment in Randomized Clinical Trials|Covariate adjustment is an important tool in the analysis of randomized clinical trials and observational studies. It can be used to increase efficiency and thus power, and to reduce possible bias. While most statistical tests in randomized clinical trials are nonparametric in nature, approaches for covariate adjustment typically rely on specific regression models, such as the linear model for a continuous outcome, the logistic regression model for a dichotomous outcome and the Cox model for survival time. Several recent efforts have focused on model-free covariate adjustment. This paper makes use of the empirical likelihood method and proposes a nonparametric approach to covariate adjustment. A major advantage of the new approach is that it automatically utilizes covariate information in an optimal way without fitting nonparametric regression. The usual asymptotic properties, including the Wilks-type result of convergence to a chi-square distribution for the empirical likelihood ratio based test, and asymptotic normality for the corresponding maximum empirical likelihood estimator, are established. It is also shown that the resulting test is asymptotically most powerful and that the estimator for the treatment effect achieves the semiparametric efficiency bound. The new method is applied to the Global Use of Strategies to Open Occluded Coronary Arteries (GUSTO)-I trial. Extensive simulations are conducted, validating the theoretical findings.|http://arxiv.org/abs/1108.0484v1|Xiaoru Wu,Zhiliang Ying
260|C-PASS-PC: A Cloud-driven Prototype of Multi-Center Proactive Surveillance System for Prostate Cancer|Currently there are many clinical trials using paper case report forms as the primary data collection tool. Cloud Computing platforms provide big potential for increasing efficiency through a web-based data collection interface, especially for large-scale multi-center trials. Traditionally, clinical and biological data for multi-center trials are stored in one dedicated, centralized database system running at a data coordinating center (DCC). This paper presents C-PASS-PC, a cloud-driven prototype of multi-center proactive surveillance system for prostate cancer. The prototype is developed in PHP, JQuery and CSS with an Oracle backend in a local Web server and database server and deployed on Google App Engine (GAE) and Google Cloud SQL-MySQL. The deploying process is fast and easy to follow. The C-PASS-PC prototype can be accessed through an SSL-enabled web browser. Our approach proves the concept that cloud computing platforms such as GAE is a suitable and flexible solution in the near future for multi-center clinical trials.|http://arxiv.org/abs/1209.2641v1|Haibin Wang
261|Use of Historical Individual Patient Data in Analysis of Clinical Trials|Historical data from previous clinical trials, observational studies and health records may be utilized in analysis of clinical trials data to strengthen inference. Under the Bayesian framework incorporation of information obtained from any source other than the current data is facilitated through construction of an informative prior. The existing methodology for defining an informative prior based on historical data relies on measuring similarity to the current data at the study level and does not take advantage of individual patient data (IPD). This paper proposes a family of priors that utilize IPD to strengthen statistical inference. It is demonstrated that the proposed prior construction approach outperforms the existing methods where the historical data are partially exchangeable with the present data. The proposed method is applied to IPD from a set of trials in non-small cell lung cancer.|http://arxiv.org/abs/2002.09910v2|Shirin Golchi
262|Towards reliable and transparent vaccine phase III trials with smart contracts|Transforming a vaccine concept into a real vaccine product is a complicated process and includes finding suitable antigens and regulatory, technical, and manufacturing obstacles. A relevant issue within this scope is the clinical trial process. Monitoring and ensuring the integrity of trial data using the traditional system is not always feasible. The search for a vaccine against the coronavirus SARS-CoV-2 illustrates this situation. The scientific credibility of findings from several vaccines' clinical trials contributed to distorted perceptions concerning the benefits and risks of the drug. This scenario is ideal for applying technologies such as Blockchain and Smart Contracts in healthcare issues. This paper proposes a protocol based on Smart Contracts, named VaccSC, to enable transparency, accounting, and confidentiality to Phase III of vaccine experiments. The protocol was implemented in Solidity language, and results show that the VaccSC enables double-blindness, randomization, and the auditability of clinical data, even in the presence of dishonest participants.|http://arxiv.org/abs/2102.07022v1|Ivan da Silva Sendin,Rodrigo Sanches Miani
263|Understanding Clinical Trial Reports: Extracting Medical Entities and Their Relations|The best evidence concerning comparative treatment effectiveness comes from clinical trials, the results of which are reported in unstructured articles. Medical experts must manually extract information from articles to inform decision-making, which is time-consuming and expensive. Here we consider the end-to-end task of both (a) extracting treatments and outcomes from full-text articles describing clinical trials (entity identification) and, (b) inferring the reported results for the former with respect to the latter (relation extraction). We introduce new data for this task, and evaluate models that have recently achieved state-of-the-art results on similar tasks in Natural Language Processing. We then propose a new method motivated by how trial results are typically presented that outperforms these purely data-driven baselines. Finally, we run a fielded evaluation of the model with a non-profit seeking to identify existing drugs that might be re-purposed for cancer, showing the potential utility of end-to-end evidence extraction systems.|http://arxiv.org/abs/2010.03550v3|Benjamin E. Nye,Jay DeYoung,Eric Lehman,Ani Nenkova,Iain J. Marshall,Byron C. Wallace
264|Customizing Knowledge Graph Embedding to Improve Clinical Study Recommendation|Inferring knowledge from clinical trials using knowledge graph embedding is an emerging area. However, customizing graph embeddings for different use cases remains a significant challenge. We propose custom2vec, an algorithmic framework to customize graph embeddings by incorporating user preferences in training the embeddings. It captures user preferences by adding custom nodes and links derived from manually vetted results of a separate information retrieval method. We propose a joint learning objective to preserve the original network structure while incorporating the user's custom annotations. We hypothesize that the custom training improves user-expected predictions, for example, in link prediction tasks. We demonstrate the effectiveness of custom2vec for clinical trials related to non-small cell lung cancer (NSCLC) with two customization scenarios: recommending immuno-oncology trials evaluating PD-1 inhibitors and exploring similar trials that compare new therapies with a standard of care. The results show that custom2vec training achieves better performance than the conventional training methods. Our approach is a novel way to customize knowledge graph embeddings and enable more accurate recommendations and predictions.|http://arxiv.org/abs/2212.14102v1|Xiong Liu,Iya Khalil,Murthy Devarakonda
265|A Connection Between Covariate Adjustment and Stratified Randomization in Randomized Clinical Trials|The statistical efficiency of randomized clinical trials can be improved by incorporating information from baseline covariates (i.e., pre-treatment patient characteristics). This can be done in the design stage using stratified (permutated block) randomization or in the analysis stage through covariate adjustment. This article makes a connection between covariate adjustment and stratified randomization in a general framework where all regular, asymptotically linear estimators are identified as augmented estimators. From a geometric perspective, covariate adjustment can be viewed as an attempt to approximate the optimal augmentation function, and stratified randomization improves a given approximation by moving it closer to the optimal augmentation function. The efficiency benefit of stratified randomization is asymptotically equivalent to attaching an optimal augmentation term based on the stratification factor. In designing a trial with stratified randomization, it is not essential to include all important covariates in the stratification, because their prognostic information can be incorporated through covariate adjustment. Under stratified randomization, adjusting for the stratification factor only in data analysis is not expected to improve efficiency, and the key to efficient estimation is incorporating prognostic information from all important covariates. These observations are confirmed in a simulation study and illustrated using real clinical trial data.|http://arxiv.org/abs/2401.11352v3|Zhiwei Zhang
266|A Deployed Online Reinforcement Learning Algorithm In An Oral Health Clinical Trial|Dental disease is a prevalent chronic condition associated with substantial financial burden, personal suffering, and increased risk of systemic diseases. Despite widespread recommendations for twice-daily tooth brushing, adherence to recommended oral self-care behaviors remains sub-optimal due to factors such as forgetfulness and disengagement. To address this, we developed Oralytics, a mHealth intervention system designed to complement clinician-delivered preventative care for marginalized individuals at risk for dental disease. Oralytics incorporates an online reinforcement learning algorithm to determine optimal times to deliver intervention prompts that encourage oral self-care behaviors. We have deployed Oralytics in a registered clinical trial. The deployment required careful design to manage challenges specific to the clinical trials setting in the U.S. In this paper, we (1) highlight key design decisions of the RL algorithm that address these challenges and (2) conduct a re-sampling analysis to evaluate algorithm design decisions. A second phase (randomized control trial) of Oralytics is planned to start in spring 2025.|http://arxiv.org/abs/2409.02069v2|Anna L. Trella,Kelly W. Zhang,Hinal Jajal,Inbal Nahum-Shani,Vivek Shetty,Finale Doshi-Velez,Susan A. Murphy
267|Controlled LLM-based Reasoning for Clinical Trial Retrieval|Matching patients to clinical trials demands a systematic and reasoned interpretation of documents which require significant expert-level background knowledge, over a complex set of well-defined eligibility criteria. Moreover, this interpretation process needs to operate at scale, over vast knowledge bases of trials. In this paper, we propose a scalable method that extends the capabilities of LLMs in the direction of systematizing the reasoning over sets of medical eligibility criteria, evaluating it in the context of real-world cases. The proposed method overlays a Set-guided reasoning method for LLMs. The proposed framework is evaluated on TREC 2022 Clinical Trials, achieving results superior to the state-of-the-art: NDCG@10 of 0.693 and Precision@10 of 0.73.|http://arxiv.org/abs/2409.18998v1|Mael Jullien,Alex Bogatu,Harriet Unsworth,Andre Freitas
268|Digital Twinning of the Human Ventricular Activation Sequence to Clinical 12-lead ECGs and Magnetic Resonance Imaging Using Realistic Purkinje Networks for in Silico Clinical Trials|Cardiac in silico clinical trials can virtually assess the safety and efficacy of therapies using human-based modelling and simulation. These technologies can provide mechanistic explanations for clinically observed pathological behaviour. Designing virtual cohorts for in silico trials requires exploiting clinical data to capture the physiological variability in the human population. The clinical characterisation of ventricular activation and the Purkinje network is challenging, especially non-invasively. Our study aims to present a novel digital twinning pipeline that can efficiently generate and integrate Purkinje networks into human multiscale biventricular models based on subject-specific clinical 12-lead electrocardiogram and magnetic resonance recordings. Essential novel features of the pipeline are the human-based Purkinje network generation method, personalisation considering ECG R wave progression as well as QRS morphology, and translation from reduced-order Eikonal models to equivalent biophysically-detailed monodomain ones. We demonstrate ECG simulations in line with clinical data with clinical image-based multiscale models with Purkinje in four control subjects and two hypertrophic cardiomyopathy patients (simulated and clinical QRS complexes with Pearson's correlation coefficients > 0.7). Our methods also considered possible differences in the density of Purkinje myocardial junctions in the Eikonal-based inference as regional conduction velocities. These differences translated into regional coupling effects between Purkinje and myocardial models in the monodomain formulation. In summary, we demonstrate a digital twin pipeline enabling simulations yielding clinically-consistent ECGs with clinical CMR image-based biventricular multiscale models, including personalised Purkinje in healthy and cardiac disease conditions.|http://arxiv.org/abs/2306.13740v1|Julia Camps,Lucas Arantes Berg,Zhinuo Jenny Wang,Rafael Sebastian,Leto Luana Riebel,Ruben Doste,Xin Zhou,Rafael Sachetto,James Coleman,Brodie Lawson,Vicente Grau,Kevin Burrage,Alfonso Bueno-Orovio,Rodrigo Weber,Blanca Rodriguez
269|On Minimum Clinically Important Difference|In clinical trials, minimum clinically important difference (MCID) has attracted increasing interest as an important supportive clinical and statistical inference tool. Many estimation methods have been developed based on various intuitions, while little theoretical justification has been established. This paper proposes a new estimation framework of MCID using both diagnostic measurements and patient-reported outcomes (PROs). It first provides a precise definition of population-based MCID so that estimating such a MCID can be formulated as a large margin classification problem. The framework is then extended to personalized MCID to allow individualized thresholding value for patients whose clinical profiles may affect their PRO responses. More importantly, we show that the proposed estimation framework is asymptotically consistent, and a finite-sample upper bound is established for its prediction accuracy compared against the ideal MCID. The advantage of our proposed method is also demonstrated in a variety of simulated experiments as well as applications to two benchmark datasets and two phase-3 clinical trials.|http://arxiv.org/abs/1307.3646v2|A. S. Hedayat,Junhui Wang,Tu Xu
270|A Statistical Inference Framework for the Minimal Clinically Important Difference|In clinical research, the effect of a treatment or intervention is widely assessed through clinical importance, instead of statistical significance. In this paper, we propose a principled statistical inference framework to learning the minimal clinically important difference (MCID), a vital concept in assessing clinical importance. We formulate the scientific question into a novel statistical learning problem, develop an efficient algorithm for parameter estimation, and establish the asymptotic theory for the proposed estimator. We conduct comprehensive simulation studies to examine the finite sample performance of the proposed method. We also re-analyze the ChAMP (Chondral Lesions And Meniscus Procedures) trial, where the primary outcome is the patient-reported pain score and the ultimate goal is to determine whether there exists a significant difference in post-operative knee pain between patients undergoing debridement versus observation of chondral lesions during the surgery. Some previous analysis of this trial exhibited that the effect of debriding the chondral lesions does not reach a statistical significance. Our analysis reinforces this conclusion that the effect of debriding the chondral lesions is not only statistically non-significant, but also clinically un-important.|http://arxiv.org/abs/2108.11589v3|Zehua Zhou,Leslie J. Bisson,Jiwei Zhao
271|The use of external controls: To what extent can it currently be recommended?|With more and better clinical data being captured outside of clinical studies and greater data sharing of clinical studies, external controls may become a more attractive alternative to randomized clinical trials. Both industry and regulators recognize that in situations where a randomized study cannot be performed, external controls can provide the needed contextualization to allow a better interpretation of studies without a randomized control. It is also agreed that external controls will not fully replace randomized clinical trials as the gold standard for formal proof of efficacy in drug development and the yardstick of clinical research. However, it remains unclear in which situations conclusions about efficacy and a positive benefit/risk can reliably be based on the use of an external control. This paper will provide an overview on types of external control, their applications and the different sources of bias their use may incur, and discuss potential mitigation steps. It will also give recommendations on how the use of external controls can be justified.|http://arxiv.org/abs/2209.07776v1|Hans Ulrich Burger,Christoph Gerlinger,Chris Harbron,Armin Koch,Martin Posch,Justine Rochon,Anja Schiel
272|eSource for clinical trials: Implementation and evaluation of a standards-based approach in a real world trial|Objective: The Learning Health System (LHS) requires integration of research into routine practice. eSource or embedding clinical trial functionalities into routine electronic health record (EHR) systems has long been put forward as a solution to the rising costs of research. We aimed to create and validate an eSource solution that would be readily extensible as part of a LHS.   Materials and Methods: The EU FP7 TRANSFoRm project's approach is based on dual modelling, using the Clinical Research Information Model (CRIM) and the Clinical Data Integration Model of meaning (CDIM) to bridge the gap between clinical and research data structures, using the CDISC Operational Data Model (ODM) standard. Validation against GCP requirements was conducted in a clinical site, and a cluster randomised evaluation by site nested into a live clinical trial.   Results: Using the form definition element of ODM, we linked precisely modelled data queries to data elements, constrained against CDIM concepts, to enable automated patient identification for specific protocols and prepopulation of electronic case report forms (e-CRF). Both control and eSource sites recruited better than expected with no significant difference. Completeness of clinical forms was significantly improved by eSource, but Patient Related Outcome Measures (PROMs) were less well completed on smartphones than paper in this population.   Discussion: The TRANSFoRm approach provides an ontologically-based approach to eSource in a low-resource, heterogeneous, highly distributed environment, that allows precise prospective mapping of data elements in the EHR.   Conclusion: Further studies using this approach to CDISC should optimise the delivery of PROMS, whilst building a sustainable infrastructure for eSource with research networks, trials units and EHR vendors.|http://arxiv.org/abs/1707.07994v1|Jean-Francois Ethier,Vasa Curcin,Mark M. McGilchrist,Sarah N. Lim Choi Keung,Lei Zhao,Anna Andreasson,Piotr Brdka,Radoslaw Michalski,Theodoros N. Arvanitis,Nikolaos Mastellos,Anita Burgun,Brendan C. Delaney
273|Some t-tests for N-of-1 trials with serial correlation|N-of-1 trials allow inference between two treatments given to a single individual. Most often, clinical investigators analyze an individual's N-of-1 trial data with usual t-tests or simple nonparametric methods. These simple methods do not account for serial correlation in repeated observations coming from the individual. Existing methods accounting for serial correlation require simulation, multiple N-of-1 trials, or both. Here, we develop t-tests that account for serial correlation in a single individual. The development includes effect size and precision calculations, both of which are useful for study planning. We then evaluate and compare their Type I and II errors and interval estimators to those of usual t-tests analogues via Monte Carlo simulation. The serial t-tests clearly outperform the usual t-tests commonly used in reporting N-of-1 results. Examples from N-of-1 clinical trials in fibromyalgia patients and from a behavioral health setting exhibit how accounting for serial correlation can change inferences. These t-tests are easily implemented and more appropriate than simple methods commonly used; however, caution is needed when analyzing only a few observations. Keywords: Autocorrelation; Cross-over studies; Repeated measures analysis; Single-case experimental design; Time-series|http://arxiv.org/abs/1904.01622v2|Jillian Tang,Reid D. Landes
274|A Model of a Randomized Experiment with an Application to the PROWESS Clinical Trial|I develop a model of a randomized experiment with a binary intervention and a binary outcome. Potential outcomes in the intervention and control groups give rise to four types of participants. Fixing ideas such that the outcome is mortality, some participants would live regardless, others would be saved, others would be killed, and others would die regardless. These potential outcome types are not observable. However, I use the model to develop estimators of the number of participants of each type. The model relies on the randomization within the experiment and on deductive reasoning. I apply the model to an important clinical trial, the PROWESS trial, and I perform a Monte Carlo simulation calibrated to estimates from the trial. The reduced form from the trial shows a reduction in mortality, which provided a rationale for FDA approval. However, I find that the intervention killed two participants for every three it saved.|http://arxiv.org/abs/1908.05810v2|Amanda Kowalski
275|Can the potential benefit of individualizing treatment be assessed using trial summary statistics alone?|Individualizing treatment assignment can improve outcomes for diseases with patient-to-patient variability in comparative treatment effects. When a clinical trial demonstrates that some patients improve on treatment while others do not, it is tempting to assume that treatment effect heterogeneity exists. However, if variability in response is mainly driven by factors other than treatment, investigating the extent to which covariate data can predict differential treatment response is a potential waste of resources. Motivated by recent meta-analyses assessing the potential of individualizing treatment for major depressive disorder using only summary statistics, we provide a method that uses summary statistics widely available in published clinical trial results to bound the benefit of optimally assigning treatment to each patient. We also offer alternate bounds for settings in which trial results are stratified by another covariate. We demonstrate our approach using summary statistics from a depression treatment trial. Our methods are implemented in the rct2otrbounds R package, which is available at https://github.com/ngalanter/rct2otrbounds .|http://arxiv.org/abs/2211.00163v1|Nina Galanter,Marco Carone,Ronald C. Kessler,Alex Luedtke
276|Anscombe's Model for Sequential Clinical Trials Revisited|In Anscombe's classical model, the objective is to find the optimal sequential rule for learning about the difference between two alternative treatments and subsequently selecting the superior one. The population for which the procedure is optimised has size $N$ and includes both the patients in the trial and those which are treated with the chosen alternative after the trial. We review earlier work on this problem and give a detailed treatment of the problem itself. In particular, while previous work has mainly focused on the case of conjugate normal priors for the incremental effect, we demonstrate how to handle the problem for priors of a general form. We also discuss methods for numerical solutions and the practical implications of the results for the regulation of clinical trials.   Two extensions of the model are proposed and analysed. The first breaks the symmetry of the treatments, giving one the role of the current standard being administered in parallel with the trial. We show how certain asymptotic results due to Chernoff can be adapted to this asymmetric case. The other extension assumes that $N$ is a random variable instead of a known constant.|http://arxiv.org/abs/1712.05547v1|Sebastian Jobjrnsson,Sren Christensen
277|Guidelines for estimating causal effects in pragmatic randomized trials|Pragmatic randomized trials are designed to provide evidence for clinical decision-making rather than regulatory approval. Common features of these trials include the inclusion of heterogeneous or diverse patient populations in a wide range of care settings, the use of active treatment strategies as comparators, unblinded treatment assignment, and the study of long-term, clinically relevant outcomes. These features can greatly increase the usefulness of the trial results for patients, clinicians, and other stakeholders. However, these features also introduce an increased risk of non-adherence, which reduces the value of the intention-to-treat effect as a patient-centered measure of causal effect. In these settings, the per-protocol effect provides useful complementary information for decision making. Unfortunately, there is little guidance for valid estimation of the per-protocol effect. Here, we present our full guidelines for analyses of pragmatic trials that will result in more informative causal inferences for both the intention-to-treat effect and the per-protocol effect.|http://arxiv.org/abs/1911.06030v2|Eleanor J. Murray,Sonja A. Swanson,Miguel A. Hernn
278|Prediction of cognitive decline for enrichment of Alzheimer's disease clinical trials|A key issue to Alzheimer's disease clinical trial failures is poor participant selection. Participants have heterogeneous cognitive trajectories and many do not decline during trials, which reduces a study's power to detect treatment effects. Trials need enrichment strategies to enroll individuals who will decline. We developed machine learning models to predict cognitive trajectories in participants with early Alzheimer's disease (n=1342) and presymptomatic individuals (n=756) over 24 and 48 months respectively. Baseline magnetic resonance imaging, cognitive tests, demographics, and APOE genotype were used to classify decliners, measured by an increase in CDR-Sum of Boxes, and non-decliners with up to 79% area under the curve (cross-validated and out-of-sample). Using these prognostic models to recruit enriched cohorts of decliners can reduce required sample sizes by as much as 51%, while maintaining the same detection power, and thus may improve trial quality, derisk endpoint failures, and accelerate therapeutic development in Alzheimer's disease.|http://arxiv.org/abs/2111.04174v4|Angela Tam,Csar Laurent,Serge Gauthier,Christian Dansereau
279|Generalizing Clinical Trials with Convex Hulls|Randomized clinical trials eliminate confounding but impose strict exclusion criteria that limit recruitment to a subset of the population. Observational datasets are more inclusive but suffer from confounding -- often providing overly optimistic estimates of treatment response over time due to partially optimized physician prescribing patterns. We therefore assume that the unconfounded treatment response lies somewhere in-between the observational estimate before and the observational estimate after treatment assignment. This assumption allows us to extrapolate results from exclusive trials to the broader population by analyzing observational and trial data simultaneously using an algorithm called Optimum in Convex Hulls (OCH). OCH represents the treatment effect either in terms of convex hulls of conditional expectations or convex hulls (also known as mixtures) of conditional densities. The algorithm first learns the component expectations or densities using the observational data and then learns the linear mixing coefficients using trial data in order to approximate the true treatment effect; theory importantly explains why this linear combination should hold. OCH estimates the treatment effect in terms both expectations and densities with state of the art accuracy.|http://arxiv.org/abs/2111.13229v2|Eric V. Strobl,Thomas A. Lasko
280|Enhancing Clinical Trial Patient Matching through Knowledge Augmentation with Multi-Agents|Matching patients effectively and efficiently for clinical trials is a significant challenge due to the complexity and variability of patient profiles and trial criteria. This paper presents a novel framework, Multi-Agents for Knowledge Augmentation (MAKA), designed to enhance patient-trial matching by dynamically supplementing matching prompts with external, domain-specific knowledge. The MAKA architecture consists of five key components: a knowledge probing agent that detects gaps in domain knowledge, a navigation agent that manages interactions among multiple specialized knowledge augmentation agents, a knowledge augmentation agent that incorporates relevant information into patient-trial matching prompts, a supervision agent aligning the outputs from other agents with the instructions and a matching agent making the final selection decision. This approach enhances the accuracy and contextual richness of patient matching, addresses inherent knowledge gaps in both trail criteria and large language models (LLMs), and improves the alignment between patient characteristics and the criteria.|http://arxiv.org/abs/2411.14637v1|Hanwen Shi,Jin Zhang,Kunpeng Zhang
281|Group sequential methods for interim monitoring of randomized clinical trials with time-lagged outcome|The primary analysis in two-arm clinical trials usually involves inference on a scalar treatment effect parameter; e.g., depending on the outcome, the difference of treatment-specific means, risk difference, risk ratio, or odds ratio. Most clinical trials are monitored for the possibility of early stopping. Because ordinarily the outcome on any given subject can be ascertained only after some time lag, at the time of an interim analysis, among the subjects already enrolled, the outcome is known for only a subset and is effectively censored for those who have not been enrolled sufficiently long for it to be observed. Typically, the interim analysis is based only on the data from subjects for whom the outcome has been ascertained. A goal of an interim analysis is to stop the trial as soon as the evidence is strong enough to do so, suggesting that the analysis ideally should make the most efficient use of all available data, thus including information on censoring as well as other baseline and time-dependent covariates in a principled way. A general group sequential framework is proposed for clinical trials with a time-lagged outcome. Treatment effect estimators that take account of censoring and incorporate covariate information at an interim analysis are derived using semiparametric theory and are demonstrated to lead to stronger evidence for early stopping than standard approaches. The associated test statistics are shown to have the independent increments structure, so that standard software can be used to obtain stopping boundaries.|http://arxiv.org/abs/2204.10739v1|Anastasios A. Tsiatis,Marie Davidian
282|Exploration and Incentivizing Participation in Clinical Trials|Participation incentives is a well-known issue inhibiting randomized controlled trials (RCTs) in medicine, as well as a potential cause of user dissatisfaction for RCTs in online platforms. We frame this issue as a non-standard exploration-exploitation tradeoff: an RCT would like to explore as uniformly as possible, whereas each "agent" (a patient or a user) prefers "exploitation", i.e., treatments that seem best. We incentivize participation by leveraging information asymmetry between the trial and the agents. We measure statistical performance via worst-case estimation error under adversarially generated outcomes, a standard objective for RCTs. We obtain a near-optimal solution in terms of this objective: an incentive-compatible mechanism with a particular guarantee, and a nearly matching impossibility result for any incentive-compatible mechanism. We consider three model variants: homogeneous agents (of the same "type" comprising beliefs and preferences), heterogeneous agents, and an extension with estimated type frequencies.|http://arxiv.org/abs/2202.06191v9|Yingkai Li,Aleksandrs Slivkins
283|Automated, efficient and model-free inference for randomized clinical trials via data-driven covariate adjustment|In May 2023, the U.S. Food and Drug Administration (FDA) released guidance for industry on "Adjustment for Covariates in Randomized Clinical Trials for Drugs and Biological Products". Covariate adjustment is a statistical analysis method for improving precision and power in clinical trials by adjusting for pre-specified, prognostic baseline variables. Though recommended by the FDA and the European Medicines Agency (EMA), many trials do not exploit the available information in baseline variables or make use only of the baseline measurement of the outcome. This is likely (partly) due to the regulatory mandate to pre-specify baseline covariates for adjustment, leading to challenges in determining appropriate covariates and their functional forms. We will explore the potential of automated data-adaptive methods, such as machine learning algorithms, for covariate adjustment, addressing the challenge of pre-specification. Specifically, our approach allows the use of complex models or machine learning algorithms without compromising the interpretation or validity of the treatment effect estimate and its corresponding standard error, even in the presence of misspecified outcome working models. This contrasts the majority of competing works which assume correct model specification for the validity of standard errors. Our proposed estimators either necessitate ultra-sparsity in the outcome model (which can be relaxed by limiting the number of predictors in the model) or necessitate integration with sample splitting to enhance their performance. As such, we will arrive at simple estimators and standard errors for the marginal treatment effect in randomized clinical trials, which exploit data-adaptive outcome predictions based on prognostic baseline covariates, and have low (or no) bias in finite samples even when those predictions are themselves biased.|http://arxiv.org/abs/2404.11150v1|Kelly Van Lancker,Ivn Daz,Stijn Vansteelandt
284|Bayesian adaptive bandit-based designs using the Gittins index for multi-armed trials with normally distributed endpoints|Adaptive designs for multi-armed clinical trials have become increasingly popular recently in many areas of medical research because of their potential to shorten development times and to increase patient response. However, developing response-adaptive trial designs that offer patient benefit while ensuring the resulting trial avoids bias and provides a statistically rigorous comparison of the different treatments included is highly challenging. In this paper, the theory of Multi-Armed Bandit Problems is used to define a family of near optimal adaptive designs in the context of a clinical trial with a normally distributed endpoint with known variance. Through simulation studies based on an ongoing trial as a motivation we report the operating characteristics (type I error, power, bias) and patient benefit of these approaches and compare them to traditional and existing alternative designs. These results are then compared to those recently published in the context of Bernoulli endpoints. Many limitations and advantages are similar in both cases but there are also important differences, specially with respect to type I error control. This paper proposes a simulation-based testing procedure to correct for the observed type I error inflation that bandit-based and adaptive rules can induce. Results presented extend recent work by considering a normally distributed endpoint, a very common case in clinical practice yet mostly ignored in the response-adaptive theoretical literature, and illustrate the potential advantages of using these methods in a rare disease context. We also recommend a suitable modified implementation of the bandit-based adaptive designs for the case of common diseases.|http://arxiv.org/abs/1703.05172v1|Adam Smith,Sofia S. Villar
285|Decline of COPD exacerbations in clinical trials over two decades -- a systematic review and meta-regression|BACKGROUND: An important goal of chronic obstructive pulmonary disease (COPD) treatment is to reduce the frequency of exacerbations. Some observations suggest a decline in exacerbation rates in clinical trials over time. A more systematic understanding would help to improve the design and interpretation of COPD trials.   METHODS: We performed a systematic review and meta-regression of the placebo groups in published randomized controlled trials reporting exacerbations as an outcome. A Bayesian negative binomial model was developed to accommodate results that are reported in different formats; results are reported with credible intervals (CI) and posterior tail probabilities ($p_B$).   RESULTS: Of 1114 studies identified by our search, 55 were ultimately included. Exacerbation rates decreased by 6.7% (95% CI (4.4, 9.0); $p_B$ < 0.001) per year, or 50% (95% CI (36, 61)) per decade. Adjusting for available study and baseline characteristics such as forced expiratory volume in 1 s (FEV1) did not alter the observed trend considerably. Two subsets of studies, one using a true placebo group and the other allowing inhaled corticosteroids in the "placebo" group, also yielded consistent results.   CONCLUSIONS: In conclusion, this meta-regression indicates that the rate of COPD exacerbations decreased over the past two decades to a clinically relevant extent independent of important prognostic factors. This suggests that care is needed in the design of new trials or when comparing results from older trials with more recent ones. Also a considerable effect of adjunct therapy on COPD exacerbations can be assumed.|http://arxiv.org/abs/1908.06340v1|Stefan Andreas,Christian Rver,Judith Heinz,Sebastian Straube,Henrik Watz,Tim Friede
286|Information Extraction of Clinical Trial Eligibility Criteria|Clinical trials predicate subject eligibility on a diversity of criteria ranging from patient demographics to food allergies. Trials post their requirements as semantically complex, unstructured free-text. Formalizing trial criteria to a computer-interpretable syntax would facilitate eligibility determination. In this paper, we investigate an information extraction (IE) approach for grounding criteria from trials in ClinicalTrials(dot)gov to a shared knowledge base. We frame the problem as a novel knowledge base population task, and implement a solution combining machine learning and context free grammar. To our knowledge, this work is the first criteria extraction system to apply attention-based conditional random field architecture for named entity recognition (NER), and word2vec embedding clustering for named entity linking (NEL). We release the resources and core components of our system on GitHub at https://github.com/facebookresearch/Clinical-Trial-Parser. Finally, we report our per module and end to end performances; we conclude that our system is competitive with Criteria2Query, which we view as the current state-of-the-art in criteria extraction.|http://arxiv.org/abs/2006.07296v6|Yitong Tseo,M. I. Salkola,Ahmed Mohamed,Anuj Kumar,Freddy Abnousi
287|Elastic Priors to Dynamically Borrow Information from Historical Data in Clinical Trials|Use of historical data and real-world evidence holds great potential to improve the efficiency of clinical trials. One major challenge is how to effectively borrow information from historical data while maintaining a reasonable type I error. We propose the elastic prior approach to address this challenge and achieve dynamic information borrowing. Unlike existing approaches, this method proactively controls the behavior of dynamic information borrowing and type I errors by incorporating a well-known concept of clinically meaningful difference through an elastic function, defined as a monotonic function of a congruence measure between historical data and trial data. The elastic function is constructed to satisfy a set of information-borrowing constraints prespecified by researchers or regulatory agencies, such that the prior will borrow information when historical and trial data are congruent, but refrain from information borrowing when historical and trial data are incongruent. In doing so, the elastic prior improves power and reduces the risk of data dredging and bias. The elastic prior is information borrowing consistent, i.e. asymptotically controls type I and II errors at the nominal values when historical data and trial data are not congruent, a unique characteristics of the elastic prior approach. Our simulation study that evaluates the finite sample characteristic confirms that, compared to existing methods, the elastic prior has better type I error control and yields competitive or higher power.|http://arxiv.org/abs/2009.06083v2|Liyun Jiang,Lei Nie,Ying Yuan
288|A Markov Decision Process for Response-Adaptive Randomization in Clinical Trials|In clinical trials, response-adaptive randomization (RAR) has the appealing ability to assign more subjects to better-performing treatments based on interim results. The traditional RAR strategy alters the randomization ratio on a patient-by-patient basis; this has been heavily criticized for bias due to time-trends. An alternate approach is blocked RAR, which groups patients together in blocks and recomputes the randomization ratio in a block-wise fashion; the final analysis is then stratified by block. However, the typical blocked RAR design divides patients into equal-sized blocks, which is not generally optimal.   This paper presents TrialMDP, an algorithm that designs two-armed blocked RAR clinical trials. Our method differs from past approaches in that it optimizes the size and number of blocks as well as their treatment allocations. That is, the algorithm yields a policy that adaptively chooses the size and composition of the next block, based on results seen up to that point in the trial. TrialMDP is related to past works that compute optimal trial designs via dynamic programming.   The algorithm maximizes a utility function balancing (i) statistical power, (ii) patient outcomes, and (iii) the number of blocks. We show that it attains significant improvements in utility over a suite of baseline designs, and gives useful control over the tradeoff between statistical power and patient outcomes. It is well suited for small trials that assign high cost to failures.   We provide TrialMDP as an R package on GitHub: https://github.com/dpmerrell/TrialMDP|http://arxiv.org/abs/2109.14642v1|David Merrell,Thevaa Chandereng,Yeonhee Park
289|Multi-Task Adversarial Learning for Treatment Effect Estimation in Basket Trials|Estimating treatment effects from observational data provides insights about causality guiding many real-world applications such as different clinical study designs, which are the formulations of trials, experiments, and observational studies in medical, clinical, and other types of research. In this paper, we describe causal inference for application in a novel clinical design called basket trial that tests how well a new drug works in patients who have different types of cancer that all have the same mutation. We propose a multi-task adversarial learning (MTAL) method, which incorporates feature selection multi-task representation learning and adversarial learning to estimate potential outcomes across different tumor types for patients sharing the same genetic mutation but having different tumor types. In our paper, the basket trial is employed as an intuitive example to present this new causal inference setting. This new causal inference setting includes, but is not limited to basket trials. This setting has the same challenges as the traditional causal inference problem, i.e., missing counterfactual outcomes under different subgroups and treatment selection bias due to confounders. We present the practical advantages of our MTAL method for the analysis of synthetic basket trial data and evaluate the proposed estimator on two benchmarks, IHDP and News. The results demonstrate the superiority of our MTAL method over the competing state-of-the-art methods.|http://arxiv.org/abs/2203.05123v1|Zhixuan Chu,Stephen L. Rathbun,Sheng Li
290|Modeling restricted enrollment and optimal cost-efficient design in multicenter clinical trials|Design and forecasting of patient enrollment is among the greatest challenges that the clinical research enterprize faces today, as inefficient enrollment can be a major cause of drug development delays. Therefore, the development of the innovative statistical and artificial intelligence technologies for improving the efficiency of clinical trials operation are of the imperative need. This paper is describing further developments in the innovative statistical methodology for modeling and forecasting patient enrollment. The underlying technique uses a Poisson-gamma enrollment model developed by Anisimov & Fedorov in the previous publications and is extended here to analytic modeling of the enrollment on country/region level. A new analytic technique based on the approximation of the enrollment process in country/region by a Poisson-gamma process with aggregated parameters is developed. Another innovative direction is the development of the analytic technique for modeling the enrollment under some restrictions (enrollment caps in countries). Some discussion on using historic trials for better prediction of the enrollment in the new trials is provided. These results are used for solving the problem of optimal trial cost-efficient enrollment design: find an optimal allocation of sites/countries that minimizes the global trial cost given that the probability to reach an enrollment target in time is no less than some prescribed probability. Different techniques to find an optimal solution for high dimensional optimization problem for the cases of unrestricted and restricted enrollment and for a small and large number of countries are discussed.|http://arxiv.org/abs/2212.12930v1|Vladimir Anisimov,Matthew Austin
291|Estimating the Sampling Distribution of Posterior Decision Summaries in Bayesian Clinical Trials|Bayesian inference and the use of posterior or posterior predictive probabilities for decision making have become increasingly popular in clinical trials. The current practice in Bayesian clinical trials relies on a hybrid Bayesian-frequentist approach where the design and decision criteria are assessed with respect to frequentist operating characteristics such as power and type I error rate conditioning on a given set of parameters. These operating characteristics are commonly obtained via simulation studies. The utility of Bayesian measures, such as ``assurance", that incorporate uncertainty about model parameters in estimating the probabilities of various decisions in trials has been demonstrated recently. However, the computational burden remains an obstacle toward wider use of such criteria. In this article, we propose methodology which utilizes large sample theory of the posterior distribution to define parametric models for the sampling distribution of the posterior summaries used for decision making. The parameters of these models are estimated using a small number of simulation scenarios, thereby refining these models to capture the sampling distribution for small to moderate sample size. The proposed approach toward the assessment of conditional and marginal operating characteristics and sample size determination can be considered as simulation-assisted rather than simulation-based. It enables formal incorporation of uncertainty about the trial assumptions via a design prior and significantly reduces the computational burden for the design of Bayesian trials in general.|http://arxiv.org/abs/2306.09151v2|Shirin Golchi,James Willard
292|Assurance Methods for designing a clinical trial with a delayed treatment effect|An assurance calculation is a Bayesian alternative to a power calculation. One may be performed to aid the planning of a clinical trial, specifically setting the sample size or to support decisions about whether or not to perform a study. Immuno-oncology is a rapidly evolving area in the development of anticancer drugs. A common phenomenon that arises in trials of such drugs is one of delayed treatment effects, that is, there is a delay in the separation of the survival curves. To calculate assurance for a trial in which a delayed treatment effect is likely to be present, uncertainty about key parameters needs to be considered. If uncertainty is not considered, the number of patients recruited may not be enough to ensure we have adequate statistical power to detect a clinically relevant treatment effect and the risk of an unsuccessful trial is increased. We present a new elicitation technique for when a delayed treatment effect is likely and show how to compute assurance using these elicited prior distributions. We provide an example to illustrate how this can be used in practice and develop open-source software to implement our methods. Our methodology has the potential to improve the success rate and efficiency of Phase III trials in immuno-oncology and for other treatments where a delayed treatment effect is expected to occur.|http://arxiv.org/abs/2310.06673v2|James Salsbury,Jeremy Oakley,Steven Julious,Lisa Hampson
293|Distilling Large Language Models for Matching Patients to Clinical Trials|The recent success of large language models (LLMs) has paved the way for their adoption in the high-stakes domain of healthcare. Specifically, the application of LLMs in patient-trial matching, which involves assessing patient eligibility against clinical trial's nuanced inclusion and exclusion criteria, has shown promise. Recent research has shown that GPT-3.5, a widely recognized LLM developed by OpenAI, can outperform existing methods with minimal 'variable engineering' by simply comparing clinical trial information against patient summaries. However, there are significant challenges associated with using closed-source proprietary LLMs like GPT-3.5 in practical healthcare applications, such as cost, privacy and reproducibility concerns. To address these issues, this study presents the first systematic examination of the efficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA 7B,13B, and 70B) for the task of patient-trial matching. Employing a multifaceted evaluation framework, we conducted extensive automated and human-centric assessments coupled with a detailed error analysis for each model. To enhance the adaptability of open-source LLMs, we have created a specialized synthetic dataset utilizing GPT-4, enabling effective fine-tuning under constrained data conditions. Our findings reveal that open-source LLMs, when fine-tuned on this limited and synthetic dataset, demonstrate performance parity with their proprietary counterparts. This presents a massive opportunity for their deployment in real-world healthcare applications. To foster further research and applications in this field, we release both the annotated evaluation dataset along with the fine-tuned LLM -- Trial-LLAMA -- for public use.|http://arxiv.org/abs/2312.09958v1|Mauro Nievas,Aditya Basu,Yanshan Wang,Hrituraj Singh
294|Adaptive seamless design for establishing pharmacokinetics and efficacy equivalence in developing biosimilars|Recently, numerous pharmaceutical sponsors have expressed a great deal of interest in the development of biosimilars, which requires clinical trials to demonstrate the equivalence of pharmacokinetics (PK) and clinical efficacy. Pharmacodynamics (PD) may be used in evaluating efficacy if there are relevant PD markers available. However, in their absence, it is necessary to design the associated clinical trials to include efficacy measures as the primary endpoint. In this study, we propose an adaptive seamless PK and efficacy design with the frameworks to remedy the risk of misspecification of both PK and efficacy parameters. Here, we consider the clinical development of biosimilars including their evaluation in patients rather than healthy volunteers under a situation where both PK and efficacy parameters are required to demonstrate the equivalence. To avoid the risk associated with the failure to confirm equivalence, incorporating the new PK trial for PK equivalence within the PK portion, which is the early stage for the efficacy part, and sample size re-calculation for the efficacy equivalence are considered in the proposed method. This proposal provides appealing advantages such as a shorter period, additional cost saving, and smaller number of patients required.|http://arxiv.org/abs/1607.02283v2|Ryuji Uozumi,Chikuma Hamada
295|A Bayesian Joint model for Longitudinal DAS28 Scores and Competing Risk Informative Drop Out in a Rheumatoid Arthritis Clinical Trial|Rheumatoid arthritis clinical trials are strategically designed to collect the disease activity score of each patient over multiple clinical visits, meanwhile a patient may drop out before their intended completion due to various reasons. The dropout terminates the longitudinal data collection on the patients activity score. In the presence of informative dropout, that is, the dropout depends on latent variables from the longitudinal process, simply applying a model to analyze the longitudinal outcomes may lead to biased results because the assumption of random dropout is violated. In this paper we develop a data driven Bayesian joint model for modeling DAS28 scores and competing risk informative drop out. The motivating example is a clinical trial of Etanercept and Methotrexate with radiographic Patient Outcomes (TEMPO, Keystone et.al).|http://arxiv.org/abs/1801.08628v1|Violeta G. Hennessey,Luis G. Leon-Novelo,Juan Li,Li Zhu,Eric Chi,Joseph G. Ibrahim
296|Aim for clinical utility, not just predictive accuracy|The predictions from an accurate prognostic model can be of great interest to patients and clinicians. When predictions are reported to individuals, they may decide to take action to improve their health or they may simply be comforted by the knowledge. However, if there is a clearly defined space of actions in the clinical context, a formal decision rule based on the prediction has the potential to have a much broader impact. Even if it is not the intended use of a developed prediction model, informal decision rules can often be found in practice. The use of a prediction-based decision rule should be formalized and compared to the standard of care in a randomized trial to assess its clinical utility, however, evidence is needed to motivate such a trial. We outline how observational data can be used to propose a decision rule based on a prognostic prediction model. We then propose a framework for emulating a prediction driven trial to evaluate the utility of a prediction-based decision rule in observational data. A split-sample structure can and should be used to develop the prognostic model, define the decision rule, and evaluate its clinical utility.|http://arxiv.org/abs/1909.03801v1|Michael C Sachs,Arvid Sjlander,Erin E Gabriel
297|Emerging classes of antioxidant to cancer therapy: a review of clinical and experimental studies|This review mainly focuses on the relation between antioxidants with cancer therapy. Antioxidants have been reported to play an essential role to reduce free radical species. Free radicals commonly cause oxidative damage which is a common factor in the aging process, and also the vital factor of formation, and development of major disease specially cancer. Although, since last many decades several antioxidants belong to natural and synthetic origin have been tested in clinical trials against oxidative stress, however these clinical trials end up with undesirable effects. This review also complied with the most recent findings of oxidative stress, highlighting of free racial production, and its related oxidative damage at cellular and molecular level, with the new and existing natural and synthetic classes of free radical scavenger and their related clinical trials.|http://arxiv.org/abs/2003.04538v1|Qurat-ul-Ain,M. Iqbal Choudhary
298|Machine Learning for Health: Personalized Models for Forecasting of Alzheimer Disease Progression|In this thesis the aim is to work on optimizing the modern machine learning models for personalized forecasting of Alzheimer Disease (AD) Progression from clinical trial data. The data comes from the TADPOLE challenge, which is one of the largest publicly available datasets for AD research (ADNI dataset). The goal of the project is to develop machine learning models that can be used to perform personalized forecasts of the participants cognitive changes (e.g., ADAS-Cog13 scores) over the time period of 6,12, 18 and 24 months in the future and the change in Clinical Status (CS) i.e., whether a person will convert to AD within 2 years or not. This is important for informing current clinical trials and better design of future clinical trials for AD. We will work with personalized Gaussian processes as machine learning models to predict ADAS-Cog13 score and Cox model along with a classifier to predict the conversion in a patient within 2 years.This project is done with the collaboration with researchers from the MIT MediaLab.|http://arxiv.org/abs/2008.02667v1|Aritra Banerjee
299|CT-ADE: An Evaluation Benchmark for Adverse Drug Event Prediction from Clinical Trial Results|Adverse drug events (ADEs) significantly impact clinical research, causing many clinical trial failures. ADE prediction is key for developing safer medications and enhancing patient outcomes. To support this effort, we introduce CT-ADE, a dataset for multilabel predictive modeling of ADEs in monopharmacy treatments. CT-ADE integrates data from 2,497 unique drugs, encompassing 168,984 drug-ADE pairs extracted from clinical trials, annotated with patient and contextual information, and comprehensive ADE concepts standardized across multiple levels of the MedDRA ontology. Preliminary analyses with large language models (LLMs) achieved F1-scores up to 55.90%. Models using patient and contextual information showed F1-score improvements of 21%-38% over models using only chemical structure data. Our results highlight the importance of target population and treatment regimens in the predictive modeling of ADEs, offering greater performance gains than LLM domain specialization and scaling. CT-ADE provides an essential tool for researchers aiming to leverage artificial intelligence and machine learning to enhance patient safety and minimize the impact of ADEs on pharmaceutical research and development. The dataset is publicly accessible at https://github.com/ds4dh/CT-ADE.|http://arxiv.org/abs/2404.12827v2|Anthony Yazdani,Alban Bornet,Philipp Khlebnikov,Boya Zhang,Hossein Rouhizadeh,Poorya Amini,Douglas Teodoro
300|WATCH: A Workflow to Assess Treatment Effect Heterogeneity in Drug Development for Clinical Trial Sponsors|This paper proposes a Workflow for Assessing Treatment effeCt Heterogeneity (WATCH) in clinical drug development targeted at clinical trial sponsors. WATCH is designed to address the challenges of investigating treatment effect heterogeneity (TEH) in randomized clinical trials, where sample size and multiplicity limit the reliability of findings. The proposed workflow includes four steps: Analysis Planning, Initial Data Analysis and Analysis Dataset Creation, TEH Exploration, and Multidisciplinary Assessment. The workflow offers a general overview of how treatment effects vary by baseline covariates in the observed data, and guides interpretation of the observed findings based on external evidence and best scientific understanding. The workflow is exploratory and not inferential/confirmatory in nature, but should be pre-planned before data-base lock and analysis start. It is focused on providing a general overview rather than a single specific finding or subgroup with differential effect.|http://arxiv.org/abs/2405.00859v2|Konstantinos Sechidis,Sophie Sun,Yao Chen,Jiarui Lu,Cong Zhang,Mark Baillie,David Ohlssen,Marc Vandemeulebroecke,Rob Hemmings,Stephen Ruberg,Bjrn Bornkamp
301|How Cox models react to a study-specific confounder in a patient-level pooled dataset: Random-effects better cope with an imbalanced covariate across trials unless baseline hazards differ|Combining patient-level data from clinical trials can connect rare phenomena with clinical endpoints, but statistical techniques applied to a single trial may become problematical when trials are pooled. Estimating the hazard of a binary variable unevenly distributed across trials showcases a common pooled database issue.   We studied how an unevenly distributed binary variable can compromise the integrity of fixed and random effects Cox proportional hazards models.   We compared fixed effect and random effects Cox proportional hazards models on a set of simulated datasets inspired by a 17-trial pooled database of patients presenting with ST-segment elevation myocardial infarction (STEMI) and non-STEMI undergoing percutaneous coronary intervention.   An unevenly distributed covariate can bias hazard ratio estimates, inflate standard errors, raise type I error, and reduce power. While uneveness causes problems for all Cox proportional hazards models, random effects suffer least. Compared to fixed effect models, random effects suffer lower bias and trade inflated type I errors for improved power. Contrasting hazard rates between trials prevent accurate estimates from both fixed and random effects models.   When modeling a covariate unevenly distributed across pooled trials with similar baseline hazard rates, Cox proportional hazards models with a random trial effect more accurately estimate hazard ratios than fixed effects. Differing between-trial baseline hazard rates bias both random and fixed effect models. With an unevenly-distributed covariate and similar baseline hazard rates across trials, a random effects Cox proportional hazards model outperforms a fixed effect model, but cannot overcome contrasting baseline hazard rates.|http://arxiv.org/abs/1805.02821v1|Thomas McAndrew,Bjorn Redfors,Aaron Crowley,Yiran Zhang,Shmuel Chen,Mordechai Golomb,Maria Alu,Dominic Francese,Ori Ben-Yehuda,Akiko Maehara,Gary Mintz,Gregg Stone,Paul Jenkins
302|Performing clinical drug trials in children with a rare disease|Over the past 50 years, the advancements in medical and health research have radically changed the epidemiology of health conditions in neonates, children, and adolescents; and clinical research has on the whole, moved forward. However, large sections of the pediatric community remain vulnerable and underserved, by clinical research. One reason for this is the fact that most pediatric diseases are also rare diseases (i.e., they fit the EU definition of a rare condition, by affecting no more than 5 in 10,000 individuals), and indeed the majority of conditions under this umbrella heading are in fact much rarer, affecting fewer than 1 in 100,000. Rare pediatric diseases incur particular challenges, both in terms of actually conducting clinical trials but also planning trials (and indeed, stimulating the preclinical research and knowledge generation necessary to embark on clinical trials in the first place). The pediatric regulation and orphan regulation (covering rare diseases) were introduced to address the complexities in research and development of medicines specifically for children and for people living with a rare disease, respectively. The regulations have been reasonably effective, particularly in areas where adult and pediatric diseases overlap, driving the development of more pediatric medicines; however, challenges still remain, often exacerbated by the rarity of the diseases. These include issues around trial planning, the need for more innovative methodologies in smaller populations, significant delays in trial start up and recruitment, recruitment issues (due to small populations and the nature of the conditions), lack of endpoints, and scarce data. This chapter will discuss some of the major challenges in delivering trials in pediatric rare diseases while also assessing current and future solutions to address these.|http://arxiv.org/abs/2408.07142v1|Victoria Hedley,Rebecca Leary,Anando Sen,Anna Irvin,Emma Heslop,Volker Straub
303|A novel Phase I clinical trial design with unequal cohort sizes|This paper introduces a new Phase I design aimed at enhancing the performance of existing methods, including algorithm-based, model-based, and model-assisted designs. The design, developed by integrating the concept of Fisher information, is easily operationalized. The new design addresses the issue of the classical designs'slow dosage escalation. Simulation demonstrate that the proposed design markedly enhances performance in terms of efficiency, accuracy, and reliability. Moreover, the trial duration has been notably reduced with a large sample size.|http://arxiv.org/abs/2412.07635v1|Xiaojun Zhu
304|Optimal allocation strategies in platform trials|Platform trials are randomized clinical trials that allow simultaneous comparison of multiple interventions, usually against a common control. Arms to test experimental interventions may enter and leave the platform over time. This implies that the number of experimental intervention arms in the trial may change over time. Determining optimal allocation rates to allocate patients to the treatment and control arms in platform trials is challenging because the change in treatment arms implies that also the optimal allocation rates will change when treatments enter or leave the platform. In addition, the optimal allocation depends on the analysis strategy used. In this paper, we derive optimal treatment allocation rates for platform trials with shared controls, assuming that a stratified estimation and testing procedure based on a regression model, is used to adjust for time trends. We consider both, analysis using concurrent controls only as well as analysis methods based on also non-concurrent controls and assume that the total sample size is fixed. The objective function to be minimized is the maximum of the variances of the effect estimators. We show that the optimal solution depends on the entry time of the arms in the trial and, in general, does not correspond to the square root of $k$ allocation rule used in the classical multi-arm trials. We illustrate the optimal allocation and evaluate the power and type 1 error rate compared to trials using one-to-one and square root of $k$ allocations by means of a case study.|http://arxiv.org/abs/2304.03035v1|Marta Bofill Roig,Ekkehard Glimm,Tobias Mielke,Martin Posch
305|Metrics to find a surrogate endpoint of OS in metastatic oncology trials: a simulation study|Surrogate endpoint (SE) for overall survival in cancer patients is essential to improving the efficiency of oncology drug development. In practice, we may discover a new patient level association with survival, based on one or more clinical or biological features, in a discovery cohort; and then measure the trial level association across studies in a meta-analysis to validate the SE. To understand how well various patient level metrics would indicate the eventual trial level association, we considered causal biological trajectories based on bi-exponential functions, modeled the strength of their impact on survival hazards via a parameter {\alpha}, and simulated the trajectories and survival times in randomized trials simultaneously. We set an early time point in the trials when the trajectory measurement became the SE value. From simulated discovery cohorts, we compared patient level metrics including C index, integrated brier score, and log hazard ratio between SE values and survival times. We assembled multiple simulated studies to enable meta-analyses to estimate the trial level association. Across all the simulation scenarios considered here, we found tight correlations among the three patient level metrics and similar correlations between any of them and the trial level metric. Despite the continual increase in {\alpha}, both patient and trial level metrics often plateaued together; their association always decreased quickly as {\alpha} increased. This suggests that incorporating additional biological factors into a composite SE is likely to have diminishing returns on improving both patient level and trial level association.|http://arxiv.org/abs/2109.03421v2|Wei Zou
306|Analysis of an Incomplete Binary Outcome Dichotomized From an Underlying Continuous Variable in Clinical Trials|In many clinical trials, outcomes of interest include binary-valued endpoints. It is not uncommon that a binary-valued outcome is dichotomized from a continuous outcome at a threshold of clinical interest. To reach the objective, common approaches include (a) fitting the generalized linear mixed model (GLMM) to the dichotomized longitudinal binary outcome and (b) imputation method (MI): imputing the missing values in the continuous outcome, dichotomizing it into a binary outcome, and then fitting the generalized linear model for the "complete" data. We conducted comprehensive simulation studies to compare the performance of GLMM with MI for estimating risk difference and logarithm of odds ratio between two treatment arms at the end of study. In those simulation studies, we considered a range of multivariate distribution options for the continuous outcome (including a multivariate normal distribution, a multivariate t-distribution, a multivariate log-normal distribution, and the empirical distribution from a real clinical trial data) to evaluate the robustness of the estimators to various data-generating models. Simulation results demonstrate that both methods work well under those considered distribution options, but MI is more efficient with smaller mean squared errors compared to GLMM. We further applied both the GLMM and MI to 29 phase 3 diabetes clinical trials, and found that the MI method generally led to smaller variance estimates compared to GLMM.|http://arxiv.org/abs/2108.02064v1|Chenchen Ma,Xin Shen,Yongming Qu,Yu Du
307|Modeling Disease Progression in Mild Cognitive Impairment and Alzheimer's Disease with Digital Twins|Alzheimer's Disease (AD) is a neurodegenerative disease that affects subjects in a broad range of severity and is assessed in clinical trials with multiple cognitive and functional instruments. As clinical trials in AD increasingly focus on earlier stages of the disease, especially Mild Cognitive Impairment (MCI), the ability to model subject outcomes across the disease spectrum is extremely important. We use unsupervised machine learning models called Conditional Restricted Boltzmann Machines (CRBMs) to create Digital Twins of AD subjects. Digital Twins are simulated clinical records that share baseline data with actual subjects and comprehensively model their outcomes under standard-of-care. The CRBMs are trained on a large set of records from subjects in observational studies and the placebo arms of clinical trials across the AD spectrum. These data exhibit a challenging, but common, patchwork of measured and missing observations across subjects in the dataset, and we present a novel model architecture designed to learn effectively from it. We evaluate performance against a held-out test dataset and show how Digital Twins simultaneously capture the progression of a number of key endpoints in clinical trials across a broad spectrum of disease severity, including MCI and mild-to-moderate AD.|http://arxiv.org/abs/2012.13455v1|Daniele Bertolini,Anton D. Loukianov,Aaron M. Smith,David Li-Bland,Yannick Pouliot,Jonathan R. Walsh,Charles K. Fisher
308|Factors Affecting the Performance of Automated Speaker Verification in Alzheimer's Disease Clinical Trials|Detecting duplicate patient participation in clinical trials is a major challenge because repeated patients can undermine the credibility and accuracy of the trial's findings and result in significant health and financial risks. Developing accurate automated speaker verification (ASV) models is crucial to verify the identity of enrolled individuals and remove duplicates, but the size and quality of data influence ASV performance. However, there has been limited investigation into the factors that can affect ASV capabilities in clinical environments. In this paper, we bridge the gap by conducting analysis of how participant demographic characteristics, audio quality criteria, and severity level of Alzheimer's disease (AD) impact the performance of ASV utilizing a dataset of speech recordings from 659 participants with varying levels of AD, obtained through multiple speech tasks. Our results indicate that ASV performance: 1) is slightly better on male speakers than on female speakers; 2) degrades for individuals who are above 70 years old; 3) is comparatively better for non-native English speakers than for native English speakers; 4) is negatively affected by clinician interference, noisy background, and unclear participant speech; 5) tends to decrease with an increase in the severity level of AD. Our study finds that voice biometrics raise fairness concerns as certain subgroups exhibit different ASV performances owing to their inherent voice characteristics. Moreover, the performance of ASV is influenced by the quality of speech recordings, which underscores the importance of improving the data collection settings in clinical trials.|http://arxiv.org/abs/2306.12444v1|Malikeh Ehghaghi,Marija Stanojevic,Ali Akram,Jekaterina Novikova
309|TRIALSCOPE: A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models|The rapid digitization of real-world data offers an unprecedented opportunity for optimizing healthcare delivery and accelerating biomedical discovery. In practice, however, such data is most abundantly available in unstructured forms, such as clinical notes in electronic medical records (EMRs), and it is generally plagued by confounders. In this paper, we present TRIALSCOPE, a unifying framework for distilling real-world evidence from population-level observational data. TRIALSCOPE leverages biomedical language models to structure clinical text at scale, employs advanced probabilistic modeling for denoising and imputation, and incorporates state-of-the-art causal inference techniques to combat common confounders. Using clinical trial specification as generic representation, TRIALSCOPE provides a turn-key solution to generate and reason with clinical hypotheses using observational data. In extensive experiments and analyses on a large-scale real-world dataset with over one million cancer patients from a large US healthcare network, we show that TRIALSCOPE can produce high-quality structuring of real-world data and generates comparable results to marquee cancer trials. In addition to facilitating in-silicon clinical trial design and optimization, TRIALSCOPE may be used to empower synthetic controls, pragmatic trials, post-market surveillance, as well as support fine-grained patient-like-me reasoning in precision diagnosis and treatment.|http://arxiv.org/abs/2311.01301v2|Javier Gonzlez,Cliff Wong,Zelalem Gero,Jass Bagga,Risa Ueno,Isabel Chien,Eduard Oravkin,Emre Kiciman,Aditya Nori,Roshanthi Weerasinghe,Rom S. Leidner,Brian Piening,Tristan Naumann,Carlo Bifulco,Hoifung Poon
310|GATher: Graph Attention Based Predictions of Gene-Disease Links|Target selection is crucial in pharmaceutical drug discovery, directly influencing clinical trial success. Despite its importance, drug development remains resource-intensive, often taking over a decade with significant financial costs. High failure rates highlight the need for better early-stage target selection. We present GATher, a graph attention network designed to predict therapeutic gene-disease links by integrating data from diverse biomedical sources into a graph with over 4.4 million edges. GATher incorporates GATv3, a novel graph attention convolution layer, and GATv3HeteroConv, which aggregates transformations for each edge type, enhancing its ability to manage complex interactions within this extensive dataset. Utilizing hard negative sampling and multi-task pre-training, GATher addresses topological imbalances and improves specificity. Trained on data up to 2018 and evaluated through 2024, our results show GATher predicts clinical trial outcomes with a ROC AUC of 0.69 for unmet efficacy failures and 0.79 for positive efficacy. Feature attribution methods, using Captum, highlight key nodes and relationships, enhancing model interpretability. By 2024, GATher improved precision in prioritizing the top 200 clinical trial targets to 14.1%, an absolute increase of over 3.5% compared to other methods. GATher outperforms existing models like GAT, GATv2, and HGT in predicting clinical trial outcomes, demonstrating its potential in enhancing target validation and predicting clinical efficacy and safety.|http://arxiv.org/abs/2409.16327v1|David Narganes-Carlon,Anniek Myatt,Mani Mudaliar,Daniel J. Crowther
311|Sensitivity analysis using bias functions for studies extending inferences from a randomized trial to a target population|Extending (generalizing or transporting) causal inferences from a randomized trial to a target population requires ``generalizability'' or ``transportability'' assumptions, which state that randomized and non-randomized individuals are exchangeable conditional on baseline covariates. These assumptions are made on the basis of background knowledge, which is often uncertain or controversial, and need to be subjected to sensitivity analysis. We present simple methods for sensitivity analyses that do not require detailed background knowledge about specific unknown or unmeasured determinants of the outcome or modifiers of the treatment effect. Instead, our methods directly parameterize violations of the assumptions using bias functions. We show how the methods can be applied to non-nested trial designs, where the trial data are combined with a separately obtained sample of non-randomized individuals, as well as to nested trial designs, where a clinical trial is embedded within a cohort sampled from the target population. We illustrate the methods using data from a clinical trial comparing treatments for chronic hepatitis C infection.|http://arxiv.org/abs/1905.10684v1|Issa J. Dahabreh,James M. Robins,Sebastien J-P. A. Haneuse,Iman Saeed,Sarah E. Robertson,Elisabeth A. Stuart,Miguel A. Hernn
312|Analyzing Basket Trials under Multisource Exchangeability Assumptions|Basket designs are prospective clinical trials that are devised with the hypothesis that the presence of selected molecular features determine a patient's subsequent response to a particular "targeted" treatment strategy. Basket trials are designed to enroll multiple clinical subpopulations to which it is assumed that the therapy in question offers beneficial efficacy in the presence of the targeted molecular profile. The treatment, however, may not offer acceptable efficacy to all subpopulations enrolled. Moreover, for rare disease settings, such as oncology wherein these trials have become popular, marginal measures of statistical evidence are difficult to interpret for sparsely enrolled subpopulations. Consequently, basket trials pose challenges to the traditional paradigm for trial design, which assumes inter-patient exchangeability. The R-package \pkg{basket} facilitates the analysis of basket trials by implementing multi-source exchangeability models. By evaluating all possible pairwise exchangeability relationships, this hierarchical modeling framework facilitates Bayesian posterior shrinkage among a collection of discrete and pre-specified subpopulations. Analysis functions are provided to implement posterior inference of the response rates and all possible exchangeability relationships between subpopulations. In addition, the package can identify "poolable" subsets of and report their response characteristics. The functionality of the package is demonstrated using data from an oncology study with subpopulations defined by tumor histology.|http://arxiv.org/abs/1908.00618v1|Michael J. Kane,Nan Chen,Alexander M. Kaizer,Xun Jiang,H. Amy Xia,Brian P. Hobbs
313|How to design a pre-specified statistical analysis approach to limit p-hacking in clinical trials: the Pre-SPEC framework|Results from clinical trials can be susceptible to bias if investigators choose their analysis approach after seeing trial data, as this can allow them to perform multiple analyses and then choose the method that provides the most favourable result (commonly referred to as 'p-hacking'). Pre-specification of the planned analysis approach is essential to help reduce such bias, as it ensures analytical methods are chosen in advance of seeing the trial data. However, pre-specification is only effective if done in a way that does not allow p-hacking. For example, investigators may pre-specify a certain statistical method such as multiple imputation, but give little detail on how it will be implemented. Because there are many different ways to perform multiple imputation, this approach to pre-specification is ineffective, as it still allows investigators to analyse the data in different ways before deciding on a final approach. In this article we describe a five-point framework (the Pre-SPEC framework) for designing a pre-specified analysis approach that does not allow p-hacking. This framework is intended to be used in conjunction with the SPIRIT (Standard Protocol Items: Recommendations for Interventional Trials) statement and other similar guidelines to help investigators design the statistical analysis strategy for the trial's primary outcome in the trial protocol.|http://arxiv.org/abs/1907.04078v3|Brennan C Kahan,Gordon Forbes,Suzie Cro
314|Incorporating patient-reported outcomes in dose-finding clinical trials with continuous patient enrollment|Dose-finding clinical trials in oncology aim to estimate the maximum tolerated dose (MTD), based on safety traditionally obtained from the clinician's perspective. While the collection of patient-reported outcomes (PROs) has been advocated to better inform treatment tolerability, there is a lack of guidance and methods on how to use PROs for dose assignments and recommendations. The PRO continual reassessment method (PRO-CRM) has been proposed to formally incorporate PROs to estimate the MTD, requiring complete follow-up of both clinician and patient toxicity information per dose cohort to assign the next cohort of patients. In this paper, we propose two extensions of the PRO-CRM, allowing continuous enrollment of patients and handling longer toxicity observation windows to capture late-onset or cumulative toxicities. The first method, the TITE-PRO-CRM, uses a weighted likelihood to include the partial follow-up information from PRO in estimating the MTD during and at the end of the trial. The second method, the TITE-CRM+PRO, uses clinician's information solely to inform dose assignments during the trial and incorporates PRO at the end of the trial for dose recommendation. Simulation studies show that the TITE-PRO-CRM performs similarly to the PRO-CRM in terms of dose recommendation and assignments during the trial while reducing trial duration. The TITE-CRM + PRO slightly underperforms compared to the TITE-PRO-CRM, but similar performance can be attained by requiring larger sample sizes. We also show that the proposed methods have similar performance under higher accrual rates, different toxicity hazards, and correlated time-to-clinician toxicity and time-to-patient toxicity data.|http://arxiv.org/abs/2303.17705v1|Anas Andrillon,Lucie Biard,Shing M. Lee
315|Estimands for single arm dose optimization trials in oncology|Phase I dose escalation trials in oncology generally aim to find the maximum tolerated dose (MTD). However, with the advent of molecular targeted therapies and antibody drug conjugates, dose limiting toxicities are less frequently observed, giving rise to the concept of optimal biological dose (OBD), which considers both efficacy and toxicity. The Estimand framework presented in the addendum of the ICH E9(R1) guidelines strengthens the dialogue between different stakeholders by bringing in greater clarity in the clinical trial objectives and by providing alignment between the targeted estimand under consideration and the statistical analysis methods. However, there lacks clarity in implementing this framework in early phase dose optimization studies. This manuscript aims at discussing the Estimand framework for dose optimization trials in oncology considering efficacy and toxicity through utility functions. Such trials should include Pharmacokinetics (PK) data, toxicity data, and efficacy data. Based on these data, the analysis methods used to identify the optimized dose/s are also described. Focusing on optimizing the utility function to estimate the OBD, the population-level summary measure should reflect only the properties used for the estimating this utility function. A detailed strategy recommendation for intercurrent events has been provided using a real-life oncology case study. Key recommendations regarding the estimand attributes include that in a seamless Phase I/II dose optimization trial, the treatment attribute should start when the subject receives the first dose. We argue that such a framework brings in additional clarity to dose optimization trial objectives and strengthens the understanding of the drug under consideration that would enable the correct dose to move to Phase II of clinical development.|http://arxiv.org/abs/2501.18930v1|Ayon Mukherjee,Jonathan L. Moscovici,Zheng Liu
316|Stem Cell Transplantation As A Dynamical System: Are Clinical Outcomes Deterministic?|Outcomes in stem cell transplantation (SCT) are modeled using probability theory. However the clinical course following SCT appears to demonstrate many characteristics of dynamical systems, especially when outcomes are considered in the context of immune reconstitution. Dynamical systems tend to evolve over time according to mathematically determined rules. Characteristically, the future states of the system are predicated on the states preceding them, and there is sensitivity to initial conditions. In SCT, the interaction between donor T cells and the recipient may be considered as such a system in which, graft source, conditioning and early immunosuppression profoundly influence immune reconstitution over time. This eventually determines clinical outcomes, either the emergence of tolerance or the development of graft versus host disease. In this paper parallels between SCT and dynamical systems are explored and a conceptual framework for developing mathematical models to understand disparate transplant outcomes is proposed.|http://arxiv.org/abs/1403.6365v3|Amir A Toor,Jared D Kobulnicky,Salman Salman,Catherine H Roberts,Max Jameson-Lee,Jeremy Meier,Allison Scalora,Nihar Sheth,Vishal Koparde,Myrna Serrano,Gregory A Buck,William Clark,John McCarty,Harold Chung,Masoud H Manjili,Roy T Sabo,Michael C Neale
317|Establishing Rigorous and Cost-effective Clinical Trials for Artificial Intelligence Models|A profound gap persists between artificial intelligence (AI) and clinical practice in medicine, primarily due to the lack of rigorous and cost-effective evaluation methodologies. State-of-the-art and state-of-the-practice AI model evaluations are limited to laboratory studies on medical datasets or direct clinical trials with no or solely patient-centered controls. Moreover, the crucial role of clinicians in collaborating with AI, pivotal for determining its impact on clinical practice, is often overlooked. For the first time, we emphasize the critical necessity for rigorous and cost-effective evaluation methodologies for AI models in clinical practice, featuring patient/clinician-centered (dual-centered) AI randomized controlled trials (DC-AI RCTs) and virtual clinician-based in-silico trials (VC-MedAI) as an effective proxy for DC-AI RCTs. Leveraging 7500 diagnosis records from two-step inaugural DC-AI RCTs across 14 medical centers with 125 clinicians, our results demonstrate the necessity of DC-AI RCTs and the effectiveness of VC-MedAI. Notably, VC-MedAI performs comparably to human clinicians, replicating insights and conclusions from prospective DC-AI RCTs. We envision DC-AI RCTs and VC-MedAI as pivotal advancements, presenting innovative and transformative evaluation methodologies for AI models in clinical practice, offering a preclinical-like setting mirroring conventional medicine, and reshaping development paradigms in a cost-effective and fast-iterative manner. Chinese Clinical Trial Registration: ChiCTR2400086816.|http://arxiv.org/abs/2407.08554v2|Wanling Gao,Yunyou Huang,Dandan Cui,Zhuoming Yu,Wenjing Liu,Xiaoshuang Liang,Jiahui Zhao,Jiyue Xie,Hao Li,Li Ma,Ning Ye,Yumiao Kang,Dingfeng Luo,Peng Pan,Wei Huang,Zhongmou Liu,Jizhong Hu,Gangyuan Zhao,Chongrong Jiang,Fan Huang,Tianyi Wei,Suqin Tang,Bingjie Xia,Zhifei Zhang,Jianfeng Zhan
318|A Hassle-Free Machine Learning Method for Cohort Selection of Clinical Trials|Traditional text classification techniques in clinical domain have heavily relied on the manually extracted textual cues. This paper proposes a generally supervised machine learning method that is equally hassle-free and does not use clinical knowledge. The employed methods were simple to implement, fast to run and yet effective. This paper proposes a novel named entity recognition (NER) based an ensemble system capable of learning the keyword features in the document. Instead of merely considering the whole sentence/paragraph for analysis, the NER based keyword features can stress the important clinic relevant phases more. In addition, to capture the semantic information in the documents, the FastText features originating from the document level FastText classification results are exploited.|http://arxiv.org/abs/1808.04694v1|Liu Man
319|Heterogeneous Clinical Trial Outcomes via Multi-Output Gaussian Processes|We make use of Kronecker structure for scaling Gaussian Process models to large-scale, heterogeneous, clinical data sets. Repeated measures, commonly performed in clinical research, facilitate computational acceleration for nonlinear Bayesian nonparametric models and enable exact sampling for non-conjugate inference, when combinations of continuous and discrete endpoints are observed. Model inference is performed in Stan, and comparisons are made with brms on simulated data and two real clinical data sets, following a radiological image quality theme. Scalable Gaussian Process models compare favourably with parametric models on real data sets with 17,460 observations. Different GP model specifications are explored, with components analogous to random effects, and their theoretical properties are described.|http://arxiv.org/abs/2407.13283v2|Owen Thomas,Leiv Rnneberg
320|Beyond Low Earth Orbit: Biological Research, Artificial Intelligence, and Self-Driving Labs|Space biology research aims to understand fundamental effects of spaceflight on organisms, develop foundational knowledge to support deep space exploration, and ultimately bioengineer spacecraft and habitats to stabilize the ecosystem of plants, crops, microbes, animals, and humans for sustained multi-planetary life. To advance these aims, the field leverages experiments, platforms, data, and model organisms from both spaceborne and ground-analog studies. As research is extended beyond low Earth orbit, experiments and platforms must be maximally autonomous, light, agile, and intelligent to expedite knowledge discovery. Here we present a summary of recommendations from a workshop organized by the National Aeronautics and Space Administration on artificial intelligence, machine learning, and modeling applications which offer key solutions toward these space biology challenges. In the next decade, the synthesis of artificial intelligence into the field of space biology will deepen the biological understanding of spaceflight effects, facilitate predictive modeling and analytics, support maximally autonomous and reproducible experiments, and efficiently manage spaceborne data and metadata, all with the goal to enable life to thrive in deep space.|http://arxiv.org/abs/2112.12582v1|Lauren M. Sanders,Jason H. Yang,Ryan T. Scott,Amina Ann Qutub,Hector Garcia Martin,Daniel C. Berrios,Jaden J. A. Hastings,Jon Rask,Graham Mackintosh,Adrienne L. Hoarfrost,Stuart Chalk,John Kalantari,Kia Khezeli,Erik L. Antonsen,Joel Babdor,Richard Barker,Sergio E. Baranzini,Afshin Beheshti,Guillermo M. Delgado-Aparicio,Benjamin S. Glicksberg,Casey S. Greene,Melissa Haendel,Arif A. Hamid,Philip Heller,Daniel Jamieson,Katelyn J. Jarvis,Svetlana V. Komarova,Matthieu Komorowski,Prachi Kothiyal,Ashish Mahabal,Uri Manor,Christopher E. Mason,Mona Matar,George I. Mias,Jack Miller,Jerry G. Myers Jr.,Charlotte Nelson,Jonathan Oribello,Seung-min Park,Patricia Parsons-Wingerter,R. K. Prabhu,Robert J. Reynolds,Amanda Saravia-Butler,Suchi Saria,Aenor Sawyer,Nitin Kumar Singh,Frank Soboczenski,Michael Snyder,Karthik Soman,Corey A. Theriot,David Van Valen,Kasthuri Venkateswaran,Liz Warren,Liz Worthey,Marinka Zitnik,Sylvain V. Costes
321|Beyond Low Earth Orbit: Biomonitoring, Artificial Intelligence, and Precision Space Health|Human space exploration beyond low Earth orbit will involve missions of significant distance and duration. To effectively mitigate myriad space health hazards, paradigm shifts in data and space health systems are necessary to enable Earth-independence, rather than Earth-reliance. Promising developments in the fields of artificial intelligence and machine learning for biology and health can address these needs. We propose an appropriately autonomous and intelligent Precision Space Health system that will monitor, aggregate, and assess biomedical statuses; analyze and predict personalized adverse health outcomes; adapt and respond to newly accumulated data; and provide preventive, actionable, and timely insights to individual deep space crew members and iterative decision support to their crew medical officer. Here we present a summary of recommendations from a workshop organized by the National Aeronautics and Space Administration, on future applications of artificial intelligence in space biology and health. In the next decade, biomonitoring technology, biomarker science, spacecraft hardware, intelligent software, and streamlined data management must mature and be woven together into a Precision Space Health system to enable humanity to thrive in deep space.|http://arxiv.org/abs/2112.12554v1|Ryan T. Scott,Erik L. Antonsen,Lauren M. Sanders,Jaden J. A. Hastings,Seung-min Park,Graham Mackintosh,Robert J. Reynolds,Adrienne L. Hoarfrost,Aenor Sawyer,Casey S. Greene,Benjamin S. Glicksberg,Corey A. Theriot,Daniel C. Berrios,Jack Miller,Joel Babdor,Richard Barker,Sergio E. Baranzini,Afshin Beheshti,Stuart Chalk,Guillermo M. Delgado-Aparicio,Melissa Haendel,Arif A. Hamid,Philip Heller,Daniel Jamieson,Katelyn J. Jarvis,John Kalantari,Kia Khezeli,Svetlana V. Komarova,Matthieu Komorowski,Prachi Kothiyal,Ashish Mahabal,Uri Manor,Hector Garcia Martin,Christopher E. Mason,Mona Matar,George I. Mias,Jerry G. Myers, Jr.,Charlotte Nelson,Jonathan Oribello,Patricia Parsons-Wingerter,R. K. Prabhu,Amina Ann Qutub,Jon Rask,Amanda Saravia-Butler,Suchi Saria,Nitin Kumar Singh,Frank Soboczenski,Michael Snyder,Karthik Soman,David Van Valen,Kasthuri Venkateswaran,Liz Warren,Liz Worthey,Jason H. Yang,Marinka Zitnik,Sylvain V. Costes
322|interAdapt -- An Interactive Tool for Designing and Evaluating Randomized Trials with Adaptive Enrollment Criteria|The interAdapt R package is designed to be used by statisticians and clinical investigators to plan randomized trials. It can be used to determine if certain adaptive designs offer tangible benefits compared to standard designs, in the context of investigators' specific trial goals and constraints. Specifically, interAdapt compares the performance of trial designs with adaptive enrollment criteria versus standard (non-adaptive) group sequential trial designs. Performance is compared in terms of power, expected trial duration, and expected sample size. Users can either work directly in the R console, or with a user-friendly shiny application that requires no programming experience. Several added features are available when using the shiny application. For example, the application allows users to immediately download the results of the performance comparison as a csv-table, or as a printable, html-based report.|http://arxiv.org/abs/1404.0734v2|Aaron Fisher,Harris Jaffee,Michael Rosenblum
323|Optimal designs for active controlled dose finding trials with efficacy-toxicity outcomes|Nonlinear regression models addressing both efficacy and toxicity outcomes are increasingly used in dose-finding trials, such as in pharmaceutical drug development. However, research on related experimental design problems for corresponding active controlled trials is still scarce. In this paper we derive optimal designs to estimate efficacy and toxicity in an active controlled clinical dose finding trial when the bivariate continuous outcomes are modeled either by polynomials up to degree 2, the Michaelis- Menten model, the Emax model, or a combination thereof. We determine upper bounds on the number of different doses levels required for the optimal design and provide conditions under which the boundary points of the design space are included in the optimal design. We also provide an analytical description of the minimally supported $D$-optimal designs and show that they do not depend on the correlation between the bivariate outcomes. We illustrate the proposed methods with numerical examples and demonstrate the advantages of the $D$-optimal design for a trial, which has recently been considered in the literature.|http://arxiv.org/abs/1601.00797v1|Holger Dette,Katrin Kettelhake,Kirsten Schorning,Weng Kee Wong,Frank Bretz
324|Design of Trials with Composite Endpoints with the R Package CompAREdesign|Composite endpoints are widely used as primary endpoints in clinical trials. Designing trials with time-to-event endpoints can be particularly challenging because the proportional hazard assumption usually does not hold when using a composite endpoint, even when the premise remains true for their components. Consequently, the conventional formulae for sample size calculation do not longer apply. We present the R package CompAREdesign by means of which the key elements of trial designs, such as the sample size and effect sizes, can be computed based on the information on the composite endpoint components. CompAREdesign provides the functions to assess the sensitivity and robustness of design calculations to variations in initial values and assumptions. Furthermore, we describe other features of the package, such as functions for the design of trials with binary composite endpoints, and functions to simulate trials with composite endpoints under a wide range of scenarios.|http://arxiv.org/abs/2211.02535v1|Jordi Corts Martinez,Marta Bofill Roig,Guadalupe Gmez Melis
325|Estimating Information-Theoretic Quantities|Information theory is a practical and theoretical framework developed for the study of communication over noisy channels. Its probabilistic basis and capacity to relate statistical structure to function make it ideally suited for studying information flow in the nervous system. It has a number of useful properties: it is a general measure sensitive to any relationship, not only linear effects; it has meaningful units which in many cases allow direct comparison between different experiments; and it can be used to study how much information can be gained by observing neural responses in single trials, rather than in averages over multiple trials. A variety of information theoretic quantities are in common use in neuroscience - (see entry "Summary of Information-Theoretic Quantities"). Estimating these quantities in an accurate and unbiased way from real neurophysiological data frequently presents challenges, which are explained in this entry.|http://arxiv.org/abs/1501.01863v1|Robin A. A. Ince,Simon R. Schultz,Stefano Panzeri
326|The IBEX Imaging Knowledge-Base: A Community Resource Enabling Adoption and Development of Immunofluoresence Imaging Methods|The iterative bleaching extends multiplexity (IBEX) Knowledge-Base is a central portal for researchers adopting IBEX and related 2D and 3D immunofluorescence imaging methods. The design of the Knowledge-Base is modeled after efforts in the open-source software community and includes three facets: a development platform (GitHub), static website, and service for data archiving. The Knowledge-Base facilitates the practice of open science throughout the research life cycle by providing validation data for recommended and non-recommended reagents, e.g., primary and secondary antibodies. In addition to reporting negative data, the Knowledge-Base empowers method adoption and evolution by providing a venue for sharing protocols, videos, datasets, software, and publications. A dedicated discussion forum fosters a sense of community among researchers while addressing questions not covered in published manuscripts. Together, scientists from around the world are advancing scientific discovery at a faster pace, reducing wasted time and effort, and instilling greater confidence in the resulting data.|http://arxiv.org/abs/2412.12965v1|Ziv Yaniv,Ifeanyichukwu U. Anidi,Leanne Arakkal,Armando J. Arroyo-Mejas,Rebecca T. Beuschel,Katy Brner,Colin J. Chu,Beatrice Clark,Menna R. Clatworthy,Jake Colautti,Fabian Coscia,Joshua Croteau,Saven Denha,Rose Dever,Walderez O. Dutra,Sonja Fritzsche,Spencer Fullam,Michael Y. Gerner,Anita Gola,Kenneth J. Gollob,Jonathan M. Hernandez,Jyh Liang Hor,Hiroshi Ichise,Zhixin Jing,Danny Jonigk,Evelyn Kandov,Wolfgang Kastenmller,Joshua F. E. Koenig,Aanandita Kothurkar,Rosa K. Kortekaas,Alexandra Y. Kreins,Ian T. Lamborn,Yuri Lin,Katia Luciano Pereira Morais,Aleksandra Lunich,Jean C. S. Luz,Ryan B. MacDonald,Chen Makranz,Vivien I. Maltez,John E. McDonough,Ryan V. Moriarty,Juan M. Ocampo-Godinez,Vitoria M. Olyntho,Annette Oxenius,Kartika Padhan,Kirsten Remmert,Nathan Richoz,Edward C. Schrom,Wanjing Shang,Lihong Shi,Rochelle M. Shih,Emily Speranza,Salome Stierli,Sarah A. Teichmann,Tibor Z. Veres,Megan Vierhout,Brianna T. Wachter,Margaret Williams,Nathan Zangger,Ronald N. Germain,Andrea J. Radtke
327|Using routinely collected patient data to support clinical trials research in accountable care organizations|Background: More than half (57%) of pharma clinical research spend is in support of clinical trials. One reason is that Electronic Health Record (EHR) systems and HIPAA privacy rules often limit how broadly patient information can be shared, resulting in laborious human efforts to manually collect, de-identify, and summarize patient information for use in clinical studies.   Purpose: Conduct feasibility study for a Rheumatoid Arthritis (RA) clinical trial in an Accountable Care Organization. Measure prevalence of RA and related conditions matching study criteria. Evaluate automation of patient de-identification and summarization to support patient cohort development for clinical studies.   Methods: Collect original clinical documentation directly from the provider EHR system and extract clinical concepts necessary for matching study criteria. Automatically de-identify Protected Health Information (PHI) protect patient privacy and promote sharing. Leverage existing physician expert knowledge sources to enable analysis of patient populations.   Results: Prevalence of RA was four percent (4%) in the study population (mean age 53 years, 52% female, 48% male). Clinical documentation for 3500 patient were extracted from three (3) EHR systems. Grouped diagnosis codes revealed high prevalence of diabetes and diseases of the circulatory system, as expected. De-identification accurately removed 99% of PHI identifiers with 99% sensitivity and 99% specificity.   Conclusions: Results suggest the approach can improve automation and accelerate planning and construction of new clinical studies in the ACO setting. De-identification accuracy was better than previously approved requirements defined by four (4) hospital Institutional Review Boards.|http://arxiv.org/abs/1807.00668v1|Andrew J McMurry,Richen Zhang,Alex Foxman,Lawrence Reiter,Ronny Schnel,DeLeys Brandman
328|Technological Competence is a Precondition for Effective Implementation of Virtual Reality Head Mounted Displays in Human Neuroscience: A Technological Review and Meta-analysis|Immersive virtual reality (VR) emerges as a promising research and clinical tool. However, several studies suggest that VR induced adverse symptoms and effects (VRISE) may undermine the health and safety standards, and the reliability of the scientific results. In the current literature review, the technical reasons for the adverse symptomatology are investigated to provide suggestions and technological knowledge for the implementation of VR head-mounted display (HMD) systems in cognitive neuroscience. The technological systematic literature indicated features pertinent to display, sound, motion tracking, navigation, ergonomic interactions, user experience, and computer hardware that should be considered by the researchers. Subsequently, a meta-analysis of 44 neuroscientific or neuropsychological studies involving VR HMD systems was performed. The meta-analysis of the VR studies demonstrated that new generation HMDs induced significantly less VRISE and marginally fewer dropouts.Importantly, the commercial versions of the new generation HMDs with ergonomic interactions had zero incidents of adverse symptomatology and dropouts. HMDs equivalent to or greater than the commercial versions of contemporary HMDs accompanied with ergonomic interactions are suitable for implementation in cognitive neuroscience. In conclusion, researchers technological competency, along with meticulous methods and reports pertinent to software, hardware, and VRISE, are paramount to ensure the health and safety standards and the reliability of neuroscientific results.|http://arxiv.org/abs/2101.08123v1|Panagiotis Kourtesis,Simona Collina,Leonidas A. A. Doumas,Sarah E. MacPherson
329|Adaptive Allocation Theory in Clinical Trials|Various adaptive randomization procedures (adaptive designs) have been proposed to clinical trials. This paper discusses several broad families of procedures, such as the play-the-winner rule and Markov chain model, randomized play-the-winner rule and urn models, drop-the-loser rule, doubly biased coin adaptive design. Asymptotic theories are presented with several pivotal proofs. The effect of delayed responses, the power and variability comparison of these designs are also discussed.|http://arxiv.org/abs/math/0612811v1|Li-Xin Zhang
330|Application of the Signature Method to Pattern Recognition in the CEQUEL Clinical Trial|The classification procedure of streaming data usually requires various ad hoc methods or particular heuristic models. We explore a novel non-parametric and systematic approach to analysis of heterogeneous sequential data. We demonstrate an application of this method to classification of the delays in responding to the prompts, from subjects with bipolar disorder collected during a clinical trial, using both synthetic and real examples. We show how this method can provide a natural and systematic way to extract characteristic features from sequential data.|http://arxiv.org/abs/1606.02074v1|A. B. Kormilitzin,K. E. A. Saunders,P. J. Harrison,J. R. Geddes,T. J. Lyons
331|Group Sequential Clinical Trial Designs for Normally Distributed Outcome Variables|In a group sequential clinical trial, accumulated data are analysed at numerous time-points in order to allow early decisions about a hypothesis of interest. These designs have historically been recommended for their ethical, administrative and economic benefits. In this work, we discuss a collection of new Stata commands for computing the stopping boundaries and required group size of various classical group sequential designs, assuming a normally distributed outcome variable. Following this, we demonstrate how the performance of several designs can be compared graphically.|http://arxiv.org/abs/1710.03127v2|Michael Grayling,James Wason,Adrian Mander
332|Statistical Methods for the meta-analysis paper by Itzhaky et al|This document describes the statistical methods used in Itzhaky et al ("Systematic Review and Meta-analysis: Twenty-six Years of Randomized Clinical Trials of Psychosocial Interventions to Reduce Suicide Risk in Adolescents"). That paper is a meta-analysis of randomized controlled clinical trials testing methods for preventing suicidal behavior and/or ideation in youth. Particularly on the behavior side the meta-data are challenging to analyze.   This paper has two parts. The first is an informal discussion of the statistical methods used. The second gives detailed mathematical derivations of some formulas and methods.|http://arxiv.org/abs/2106.13874v2|Steven P. Ellis
333|A bayesian reanalysis of the phase III aducanumab (ADU) trial|In this article we have conducted a reanalysis of the phase III aducanumab (ADU) summary statistics announced by Biogen, in particular the result of the Clinical Dementia Rating-Sum of Boxes (CDR-SB). The results showed that the evidence on the efficacy of the drug is very low and a more clearer view of the results of clinical trials are presented in the Bayesian framework that can be useful for future development and research in the field.|http://arxiv.org/abs/2107.03686v2|Tommaso Costa,Franco Cauda
334|Adaptive Experiments and a Rigorous Framework for Type I Error Verification and Computational Experiment Design|This PhD thesis covers breakthroughs in several areas of adaptive experiment design: (i) (Chapter 2) Novel clinical trial designs and statistical methods in the era of precision medicine. (ii) (Chapter 3) Multi-armed bandit theory, with applications to learning healthcare systems and clinical trials. (iii) (Chapter 4) Bandit and covariate processes, with finite and non-denumerable set of arms. (iv) (Chapter 5) A rigorous framework for simulation-based verification of adaptive design properties.|http://arxiv.org/abs/2205.09369v1|Michael Sklar
335|Robust Variance Estimation for Covariate-Adjusted Unconditional Treatment Effect in Randomized Clinical Trials with Binary Outcomes|To improve precision of estimation and power of testing hypothesis for an unconditional treatment effect in randomized clinical trials with binary outcomes, researchers and regulatory agencies recommend using g-computation as a reliable method of covariate adjustment. However, the practical application of g-computation is hindered by the lack of an explicit robust variance formula that can be used for different unconditional treatment effects of interest. To fill this gap, we provide explicit and robust variance estimators for g-computation estimators and demonstrate through simulations that the variance estimators can be reliably applied in practice.|http://arxiv.org/abs/2302.10404v2|Ting Ye,Marlena Bannick,Yanyao Yi,Jun Shao
336|Drug Supply Chain Optimization for Adaptive Clinical Trials|With increasing interest in adaptive clinical trial designs, challenges are present to drug supply chain management which may offset the benefit of adaptive designs. Thus, it is necessary to develop an optimization tool to facilitate the decision making and analysis of drug supply chain planning. The challenges include the uncertainty of maximum drug supply needed, the shifting of supply requirement, and rapid availability of new supply at decision points. In this paper, statistical simulations are designed to optimize the pre-study medication supply strategy and monitor ongoing drug supply using real-time data collected with the progress of study. Particle swarm algorithm is applied when performing optimization, where feature extraction is implemented to reduce dimensionality and save computational cost.|http://arxiv.org/abs/2310.08721v1|Jincheng Pang,Hong Yan,Zoe Hua
337|The nph2ph-transform: applications to the statistical analysis of completed clinical trials|We present several illustrations from completed clinical trials on a statistical approach that allows us to gain useful insights regarding the time dependency of treatment effects. Our approach leans on a simple proposition: all non-proportional hazards (NPH) models are equivalent to a proportional hazards model. The nph2ph transform brings an NPH model into a PH form. We often find very simple approximations for this transform, enabling us to analyze complex NPH observations as though they had arisen under proportional hazards. Many techniques become available to us, and we use these to understand treatment effects better.|http://arxiv.org/abs/2407.18905v1|Sean M. Devlin,John O'Quigley
338|Uncertainty Quantification on Clinical Trial Outcome Prediction|The importance of uncertainty quantification is increasingly recognized in the diverse field of machine learning. Accurately assessing model prediction uncertainty can help provide deeper understanding and confidence for researchers and practitioners. This is especially critical in medical diagnosis and drug discovery areas, where reliable predictions directly impact research quality and patient health.   In this paper, we proposed incorporating uncertainty quantification into clinical trial outcome predictions. Our main goal is to enhance the model's ability to discern nuanced differences, thereby significantly improving its overall performance.   We have adopted a selective classification approach to fulfill our objective, integrating it seamlessly with the Hierarchical Interaction Network (HINT), which is at the forefront of clinical trial prediction modeling. Selective classification, encompassing a spectrum of methods for uncertainty quantification, empowers the model to withhold decision-making in the face of samples marked by ambiguity or low confidence, thereby amplifying the accuracy of predictions for the instances it chooses to classify. A series of comprehensive experiments demonstrate that incorporating selective classification into clinical trial predictions markedly enhances the model's performance, as evidenced by significant upticks in pivotal metrics such as PR-AUC, F1, ROC-AUC, and overall accuracy.   Specifically, the proposed method achieved 32.37\%, 21.43\%, and 13.27\% relative improvement on PR-AUC over the base model (HINT) in phase I, II, and III trial outcome prediction, respectively. When predicting phase III, our method reaches 0.9022 PR-AUC scores.   These findings illustrate the robustness and prospective utility of this strategy within the area of clinical trial predictions, potentially setting a new benchmark in the field.|http://arxiv.org/abs/2401.03482v3|Tianyi Chen,Yingzhou Lu,Nan Hao,Yuanyuan Zhang,Capucine Van Rechem,Jintai Chen,Tianfan Fu
339|A hypothesis test of feasibility for external pilot trials assessing recruitment, follow-up and adherence rates|The power of a large clinical trial can be adversely affected by low recruitment, follow-up and adherence rates. External pilot trials estimate these rates and use them, via pre-specified decision rules, to determine if the definitive trial is feasible and should go ahead. There is little methodological research underpinning how these decision rules, or the sample size of the pilot, should be chosen. In this paper we propose a hypothesis test of the feasibility of a definitive trial, to be applied to the external pilot data and used to make progression decisions. We quantify feasibility by the power of the planned trial, as a function of recruitment, follow-up and adherence rates. We use this measure to define hypotheses to test in the pilot, propose a test statistic, and show how the error rates of this test can be calculated for the common scenario of a two-arm parallel group definitive trial with a single normally distributed primary endpoint. We use our method to re-design TIGA-CUB, an external pilot trial comparing a psychotherapy with treatment as usual for children with conduct disorders. We then extend our formulation to include using the pilot data to estimate the standard deviation of the primary endpoint. and incorporate this into the progression decision.|http://arxiv.org/abs/1908.05562v1|Duncan T. Wilson,Rebecca E. A. Walwyn,Julia Brown,Amanda J. Farrin
340|Generalizing the intention-to-treat effect of an active control against placebo from historical placebo-controlled trials to an active-controlled trial: A case study of the efficacy of daily oral TDF/FTC in the HPTN 084 study|In many clinical settings, an active-controlled trial design (e.g., a non-inferiority or superiority design) is often used to compare an experimental medicine to an active control (e.g., an FDA-approved, standard therapy). One prominent example is a recent phase 3 efficacy trial, HIV Prevention Trials Network Study 084 (HPTN 084), comparing long-acting cabotegravir, a new HIV pre-exposure prophylaxis (PrEP) agent, to the FDA-approved daily oral tenofovir disoproxil fumarate plus emtricitabine (TDF/FTC) in a population of heterosexual women in 7 African countries. One key complication of interpreting study results in an active-controlled trial like HPTN 084 is that the placebo arm is not present and the efficacy of the active control (and hence the experimental drug) compared to the placebo can only be inferred by leveraging other data sources. \bz{In this article, we study statistical inference for the intention-to-treat (ITT) effect of the active control using relevant historical placebo-controlled trials data under the potential outcomes (PO) framework}. We highlight the role of adherence and unmeasured confounding, discuss in detail identification assumptions and two modes of inference (point versus partial identification), propose estimators under identification assumptions permitting point identification, and lay out sensitivity analyses needed to relax identification assumptions. We applied our framework to estimating the intention-to-treat effect of daily oral TDF/FTC versus placebo in HPTN 084 using data from an earlier Phase 3, placebo-controlled trial of daily oral TDF/FTC (Partners PrEP).|http://arxiv.org/abs/2304.03476v2|Qijia He,Fei Gao,Oliver Dukes,Sinead Delany-Moretlwe,Bo Zhang
341|A gated group sequential design for seamless Phase II/III trial with subpopulation selection|Due to the high cost and high failure rate of Phase III trials, seamless Phase II/III designs are more and more popular to trial efficiency. A potential attraction of Phase II/III design is to allow a randomized proof-of-concept stage prior to committing to the full cost of the Phase III trial. Population selection during the trial allows a trial to adapt and focus investment where it is most likely to provide patient benefit. Motivated by a clinical trial to find the population that potential benefits with dual-primary endpoints progression free survival (PFS) and overall survival (OS), we propose a gated group sequential design for a seamless Phase II/III trial design with population selection. The investigated design controls the familywise error rate and allows multiple interim analyses to enable early stopping for efficacy or futility. Simulations and an illustrative example suggest that the proposed gated group sequential design can have more power than the commonly used classical group sequential design, and reduces the patient's exposure to less effective treatment if the complementary sub-group has less significant treatment effect. The proposed design has the potential to save drug development cost and more quickly fulfill unmet medical needs.|http://arxiv.org/abs/2206.12536v1|Guanhong Miao,Jason J. Z. Liao,Jing Yang,Keaven Anderson
342|Simultaneous confidence intervals for an extended Koch-Rhmel design in three-arm non-inferiority trials|Three-arm `gold-standard' non-inferiority trials are recommended for indications where only unstable reference treatments are available and the use of a placebo group can be justified ethically. For such trials several study designs have been suggested that use the placebo group for testing 'assay sensitivity', i.e. the ability of the trial to replicate efficacy. Should the reference fail in the given trial, then non-inferiority could also be shown with an ineffective experimental treatment and hence becomes useless. In this paper we extend the so called Koch-R\"ohmel design where a proof of efficacy for the experimental treatment is required in order to qualify the non-inferiority test. While efficacy of the experimental treatment is an indication for assay sensitivity, it does not guarantee that the reference is sufficient efficient to let the non-inferiority claim be meaningful. It has therefore been suggested to adaptively test non-inferiority only if the reference demonstrates superiority to placebo and otherwise to test $\delta$-superiority of the experimental treatment over placebo, where $\delta$ is chosen in such a way that it provides proof of non-inferiority with regard to the reference's historical effect. In this paper we extend the previous work by complementing its adaptive test with compatible simultaneous confidence intervals. Confidence intervals are commonly used and suggested by regulatory guidelines for non-inferiority trials. We show how to adopt different approaches to simultaneous confidence intervals from the literature to the setting of three-arm non-inferiority trials and compare these methods in a simulation study. Finally we apply these methods to a real clinical trial example.|http://arxiv.org/abs/2210.08931v1|Martin Scharpenberg,Werner Brannath
343|Considerations for Master Protocols Using External Controls|There has been an increasing use of master protocols in oncology clinical trials because of its efficiency and flexibility to accelerate cancer drug development. Depending on the study objective and design, a master protocol trial can be a basket trial, an umbrella trial, a platform trial, or any other form of trials in which multiple investigational products and/or subpopulations are studied under a single protocol. Master protocols can use external data and evidence (e.g., external controls) for treatment effect estimation, which can further improve efficiency of master protocol trials. This paper provides an overview of different types of external controls and their unique features when used in master protocols. Some key considerations in master protocols with external controls are discussed including construction of estimands, assessment of fit-for-use real-world data, and considerations for different types of master protocols. Similarities and differences between regular randomized controlled trials and master protocols when using external controls are discussed. A targeted learning-based causal roadmap is presented which constitutes three key steps: (1) define a target statistical estimand that aligns with the causal estimand for the study objective, (2) use an efficient estimator to estimate the target statistical estimand and its uncertainty, and (3) evaluate the impact of causal assumptions on the study conclusion by performing sensitivity analyses. Two illustrative examples for master protocols using external controls are discussed for their merits and possible improvement in causal effect estimation.|http://arxiv.org/abs/2307.05050v2|Jie Chen,Xiaoyun,Li,Chengxing,Lu,Sammy Yuan,Godwin Yung,Jingjing Ye,Hong Tian,Jianchang Lin
344|Applying the estimands framework to non-inferiority trials: guidance on choice of hypothetical estimands for non-adherence and comparison of estimation methods|A common concern in non-inferiority (NI) trials is that non adherence due, for example, to poor study conduct can make treatment arms artificially similar. Because intention to treat analyses can be anti-conservative in this situation, per protocol analyses are sometimes recommended. However, such advice does not consider the estimands framework, nor the risk of bias from per protocol analyses. We therefore sought to update the above guidance using the estimands framework, and compare estimators to improve on the performance of per protocol analyses. We argue the main threat to validity of NI trials is the occurrence of trial specific intercurrent events (IEs), that is, IEs which occur in a trial setting, but would not occur in practice. To guard against erroneous conclusions of non inferiority, we suggest an estimand using a hypothetical strategy for trial specific IEs should be employed, with handling of other non trial specific IEs chosen based on clinical considerations. We provide an overview of estimators that could be used to estimate a hypothetical estimand, including inverse probability weighting (IPW), and two instrumental variable approaches (one using an informative Bayesian prior on the effect of standard treatment, and one using a treatment by covariate interaction as an instrument). We compare them, using simulation in the setting of all or nothing compliance in two active treatment arms, and conclude both IPW and the instrumental variable method using a Bayesian prior are potentially useful approaches, with the choice between them depending on which assumptions are most plausible for a given trial.|http://arxiv.org/abs/2312.00494v1|Katy E Morgan,Ian R White,Clmence Leyrat,Simon Stanworth,Brennan C Kahan
345|Is control of type I error rate needed in Bayesian clinical trial designs?|Practical employment of Bayesian trial designs is still rare. Even if accepted in principle, the regulators have commonly required that such designs be calibrated according to an upper bound for the frequentist type I error rate. This represents an internally inconsistent hybrid methodology, where important advantages from following the Bayesian principles are lost. In particular, all preplanned interim looks have an inflating multiplicity effect on type I error rate. To present an alternative approach, we consider the prototype case of a 2-arm superiority trial with dichotomous outcomes. The design is adaptive, using error control based on sequentially updated posterior probabilities, to conclude efficacy of the experimental treatment or futility of the trial. As gatekeepers for a proposed design, the regulators have the main responsibility in determining the parameters of the control of false positives, whereas the trial sponsors and investigators will have a natural role in specifying the criteria for stopping the trial due to futility. It is suggested that the traditional frequentist operating characteristics in the design, type I and type II error rates, be replaced, respectively, by Bayesian criteria called False Discovery Probability (FDP) and False Futility Probability (FFP), both terms corresponding directly to their probability interpretations. Importantly, the sequential error control during the data analysis based on posterior probabilities will satisfy these numerical criteria automatically, without need of preliminary computations before the trial is started. The method contains the option of applying a decision rule for terminating the trial early if the predicted costs from continuing would exceed the corresponding gains.|http://arxiv.org/abs/2312.15222v3|Elja Arjas,Dario Gasbarra
346|Algorithms for multi-armed bandit problems|Although many algorithms for the multi-armed bandit problem are well-understood theoretically, empirical confirmation of their effectiveness is generally scarce. This paper presents a thorough empirical study of the most popular multi-armed bandit algorithms. Three important observations can be made from our results. Firstly, simple heuristics such as epsilon-greedy and Boltzmann exploration outperform theoretically sound algorithms on most settings by a significant margin. Secondly, the performance of most algorithms varies dramatically with the parameters of the bandit problem. Our study identifies for each algorithm the settings where it performs well, and the settings where it performs poorly. Thirdly, the algorithms' performance relative each to other is affected only by the number of bandit arms and the variance of the rewards. This finding may guide the design of subsequent empirical evaluations. In the second part of the paper, we turn our attention to an important area of application of bandit algorithms: clinical trials. Although the design of clinical trials has been one of the principal practical problems motivating research on multi-armed bandits, bandit algorithms have never been evaluated as potential treatment allocation strategies. Using data from a real study, we simulate the outcome that a 2001-2002 clinical trial would have had if bandit algorithms had been used to allocate patients to treatments. We find that an adaptive trial would have successfully treated at least 50% more patients, while significantly reducing the number of adverse effects and increasing patient retention. At the end of the trial, the best treatment could have still been identified with a high level of statistical confidence. Our findings demonstrate that bandit algorithms are attractive alternatives to current adaptive treatment allocation strategies.|http://arxiv.org/abs/1402.6028v1|Volodymyr Kuleshov,Doina Precup
347|Sample size re-estimation incorporating prior information on a nuisance parameter|Prior information is often incorporated informally when planning a clinical trial. Here, we present an approach on how to incorporate prior information, such as data from historical clinical trials, into the nuisance parameter based sample size re-estimation in a design with an internal pilot study. We focus on trials with continuous endpoints in which the outcome variance is the nuisance parameter. For planning and analyzing the trial frequentist methods are considered. Moreover, the external information on the variance is summarized by the Bayesian meta-analytic-predictive (MAP) approach. To incorporate external information into the sample size re-estimation, we propose to update the MAP prior based on the results of the internal pilot study and to re-estimate the sample size using an estimator from the posterior. By means of a simulation study, we compare the operating characteristics such as power and sample size distribution of the proposed procedure with the traditional sample size re-estimation approach which uses the pooled variance estimator. The simulation study shows that, if no prior-data conflict is present, incorporating external information into the sample size re-estimation improves the operating characteristics compared to the traditional approach. In the case of a prior-data conflict, that is when the variance of the ongoing clinical trial is unequal to the prior location, the performance of the traditional sample size re-estimation procedure is in general superior, even when the prior information is robustified. When considering to include prior information in sample size re-estimation, the potential gains should be balanced against the risks.|http://arxiv.org/abs/1703.06957v2|Tobias Mtze,Heinz Schmidli,Tim Friede
348|Graphical approaches for the control of generalised error rates|When simultaneously testing multiple hypotheses, the usual approach in the context of confirmatory clinical trials is to control the familywise error rate (FWER), which bounds the probability of making at least one false rejection. In many trial settings, these hypotheses will additionally have a hierarchical structure that reflects the relative importance and links between different clinical objectives. The graphical approach of Bretz et al. (2009) is a flexible and easily communicable way of controlling the FWER while respecting complex trial objectives and multiple structured hypotheses. However, the FWER can be a very stringent criterion that leads to procedures with low power, and may not be appropriate in exploratory trial settings. This motivates controlling generalised error rates, particularly when the number of hypotheses tested is no longer small. We consider the generalised familywise error rate (k-FWER), which is the probability of making k or more false rejections, as well as the tail probability of the false discovery proportion (FDP), which is the probability that the proportion of false rejections is greater than some threshold. We also consider asymptotic control of the false discovery rate (FDR), which is the expectation of the FDP. In this paper, we show how to control these generalised error rates when using the graphical approach and its extensions. We demonstrate the utility of the resulting graphical procedures on three clinical trial case studies.|http://arxiv.org/abs/2004.01759v2|David S. Robertson,James M. S. Wason,Frank Bretz
349|P-hacking in clinical trials and how incentives shape the distribution of results across phases|Clinical research should conform to high standards of ethical and scientific integrity, given that human lives are at stake. However, economic incentives can generate conflicts of interest for investigators, who may be inclined to withhold unfavorable results or even tamper with data in order to achieve desired outcomes. To shed light on the integrity of clinical trial results, this paper systematically analyzes the distribution of p-values of primary outcomes for phase II and phase III drug trials reported to the ClinicalTrials.gov registry. First, we detect no bunching of results just above the classical 5% threshold for statistical significance. Second, a density discontinuity test reveals an upward jump at the 5% threshold for phase III results by small industry sponsors. Third, we document a larger fraction of significant results in phase III compared to phase II. Linking trials across phases, we find that early favorable results increase the likelihood of continuing into the next phase. Once we take into account this selective continuation, we can explain almost completely the excess of significant results in phase III for trials conducted by large industry sponsors. For small industry sponsors, instead, part of the excess remains unexplained.|http://arxiv.org/abs/1907.00185v3|Jrme Adda,Christian Decker,Marco Ottaviani
350|Increasing the efficiency of randomized trial estimates via linear adjustment for a prognostic score|Estimating causal effects from randomized experiments is central to clinical research. Reducing the statistical uncertainty in these analyses is an important objective for statisticians. Registries, prior trials, and health records constitute a growing compendium of historical data on patients under standard-of-care that may be exploitable to this end. However, most methods for historical borrowing achieve reductions in variance by sacrificing strict type-I error rate control. Here, we propose a use of historical data that exploits linear covariate adjustment to improve the efficiency of trial analyses without incurring bias. Specifically, we train a prognostic model on the historical data, then estimate the treatment effect using a linear regression while adjusting for the trial subjects' predicted outcomes (their prognostic scores). We prove that, under certain conditions, this prognostic covariate adjustment procedure attains the minimum variance possible among a large class of estimators. When those conditions are not met, prognostic covariate adjustment is still more efficient than raw covariate adjustment and the gain in efficiency is proportional to a measure of the predictive accuracy of the prognostic model above and beyond the linear relationship with the raw covariates. We demonstrate the approach using simulations and a reanalysis of an Alzheimer's Disease clinical trial and observe meaningful reductions in mean-squared error and the estimated variance. Lastly, we provide a simplified formula for asymptotic variance that enables power calculations that account for these gains. Sample size reductions between 10% and 30% are attainable when using prognostic models that explain a clinically realistic percentage of the outcome variance.|http://arxiv.org/abs/2012.09935v3|Alejandro Schuler,David Walsh,Diana Hall,Jon Walsh,Charles Fisher
351|Sent2Span: Span Detection for PICO Extraction in the Biomedical Text without Span Annotations|The rapid growth in published clinical trials makes it difficult to maintain up-to-date systematic reviews, which requires finding all relevant trials. This leads to policy and practice decisions based on out-of-date, incomplete, and biased subsets of available clinical evidence. Extracting and then normalising Population, Intervention, Comparator, and Outcome (PICO) information from clinical trial articles may be an effective way to automatically assign trials to systematic reviews and avoid searching and screening - the two most time-consuming systematic review processes. We propose and test a novel approach to PICO span detection. The major difference between our proposed method and previous approaches comes from detecting spans without needing annotated span data and using only crowdsourced sentence-level annotations. Experiments on two datasets show that PICO span detection results achieve much higher results for recall when compared to fully supervised methods with PICO sentence detection at least as good as human annotations. By removing the reliance on expert annotations for span detection, this work could be used in human-machine pipeline for turning low-quality crowdsourced, and sentence-level PICO annotations into structured information that can be used to quickly assign trials to relevant systematic reviews.|http://arxiv.org/abs/2109.02254v1|Shifeng Liu,Yifang Sun,Bing Li,Wei Wang,Florence T. Bourgeois,Adam G. Dunn
352|A Bayesian Precision Response-adaptive Phase II Clinical Trial Design for Radiotherapies with Competing Risk Survival Outcomes|Many phase II clinical trials have used survival outcomes as the primary endpoints in recent decades. Suppose the radiotherapy is evaluated in a phase II trial using survival outcomes. In that case, the competing risk issue often arises because the time to disease progression can be censored by the time to normal tissue complications, and vice versa. Besides, much literature has examined that patients receiving the same radiotherapy dose may yield distinct responses due to their heterogeneous radiation susceptibility statuses. Therefore, the "one-dose-fit-all" strategy often fails, and it is more relevant to evaluate the subgroup-specific treatment effect with the subgroup defined by the radiation susceptibility status. In this paper, we propose a Bayesian precision phase II trial design evaluating the subgroup-specific treatment effects of radiotherapy. We use the cause-specific hazard approach to model the competing risk survival outcomes. We propose restricting the candidate radiation doses based on each patient's radiation susceptibility status. Only the clinically feasible personalized dose will be considered, which enhances the benefit for the patients in the trial. In addition, we propose a stratified Bayesian adaptive randomization scheme such that more patients will be randomized to the dose reporting more favorable survival outcomes. Numerical studies have shown that the proposed design performed well and outperformed the conventional design ignoring the competing risk issue.|http://arxiv.org/abs/2203.06830v1|Jina Park,Wenjing Hu,Ick Hoon Jin,Hao Liu,Yong Zang
353|Generating the right evidence at the right time: Principles of a new class of flexible augmented clinical trial designs|The past few years have seen an increasing number of initiatives aimed at integrating information generated outside of confirmatory randomised clinical trials (RCTs) into drug development. However, data generated non-concurrently and through observational studies can provide results that are difficult to compare with randomised trial data. Moreover, the scientific questions these data can serve to answer often remain vague. Our starting point is to use clearly defined objectives for evidence generation, which are formulated towards early discussion with health technology assessment (HTA) bodies and are additional to regulatory requirements for authorisation of a new treatment. We propose FACTIVE (Flexible Augmented Clinical Trial for Improved eVidencE generation), a new class of study designs enabling flexible augmentation of confirmatory randomised controlled trials with concurrent and close-to-real-world elements. These enabling designs facilitate estimation of certain treatment effects in the confirmatory part and other, complementary treatment effects in a concurrent real-world part. Each stakeholder should use the evidence that is relevant within their own decision-making framework. High quality data are generated under one single protocol and the use of randomisation ensures rigorous statistical inference and interpretation within and between the different parts of the experiment. Evidence for the decision-making of HTA bodies could be available earlier than is currently the case.|http://arxiv.org/abs/2210.15264v3|Cornelia Dunger-Baldauf,Rob Hemmings,Frank Bretz,Byron Jones,Anja Schiel,Chris Holmes
354|Precision Dose-finding Cancer Clinical Trials in the Setting of Broadened Eligibility|Broadening eligibility criteria in cancer trials has been advocated to represent the true patient population more accurately. While the advantages are clear in terms of generalizability and recruitment, novel dose-finding designs are needed to ensure patient safety. These designs should be able to recommend precise doses for subpopulations if such subpopulations with different toxicity profiles exist. While dose-finding designs accounting for patient heterogeneity have been proposed, all existing methods assume the source of heterogeneity is known and thus pre-specify the subpopulations or only allow inclusion of a few patient characteristics. We propose a precision dose-finding design to address the setting of unknown patient heterogeneity in phase I cancer clinical trials where eligibility is expanded, and multiple eligibility criteria could potentially lead to different optimal doses for patient subgroups. The design offers a two-in-one approach to dose-finding by simultaneously selecting patient criteria that differentiate the maximum tolerated dose (MTD) and recommending the subpopulation-specific MTD if needed, using marginal models to sequentially incorporate patient covariates. Our simulation study compares the proposed design to the naive approach of assuming patient homogeneity and our design recommends multiple doses when heterogeneity exists and a single dose when no heterogeneity exists. The proposed dose-finding design addresses the challenges of broadening eligibility criteria in cancer trials and the desire for a more precise dose in the context of early phase clinical trials.|http://arxiv.org/abs/2301.04578v1|Rebecca B. Silva,Bin Cheng,Richard D. Carvajal,Shing M. Lee
355|Patient stratification in multi-arm trials: a two-stage procedure with Bayesian profile regression|Precision medicine is an emerging field that takes into account individual heterogeneity to inform better clinical practice. In clinical trials, the evaluation of treatment effect heterogeneity is an important component, and recently, many statistical methods have been proposed for stratifying patients into different subgroups based on such heterogeneity. However, the majority of existing methods developed for this purpose focus on the case with a dichotomous treatment and are not directly applicable to multi-arm trials. In this paper, we consider the problem of patient stratification in multi-arm trial settings and propose a two-stage procedure within the Bayesian nonparametric framework. Specifically, we first use Bayesian additive regression trees (BART) to predict potential outcomes (treatment responses) under different treatment options for each patient, and then we leverage Bayesian profile regression to cluster patients into subgroups according to their baseline characteristics and predicted potential outcomes. We further embed a variable selection procedure into our proposed framework to identify the patient characteristics that actively "drive" the clustering structure. We conduct simulation studies to examine the performance of our proposed method and demonstrate the method by applying it to a UK-based multi-arm blood donation trial, wherein our method uncovers five clinically meaningful donor subgroups.|http://arxiv.org/abs/2302.11647v1|Yuejia Xu,Angela M. Wood,Brian D. M. Tom
356|SECRETS: Subject-Efficient Clinical Randomized Controlled Trials using Synthetic Intervention|The randomized controlled trial (RCT) is the gold standard for estimating the average treatment effect (ATE) of a medical intervention but requires 100s-1000s of subjects, making it expensive and difficult to implement. While a cross-over trial can reduce sample size requirements by measuring the treatment effect per individual, it is only applicable to chronic conditions and interventions whose effects dissipate rapidly. Another approach is to replace or augment data collected from an RCT with external data from prospective studies or prior RCTs, but it is vulnerable to confounders in the external or augmented data. We propose to simulate the cross-over trial to overcome its practical limitations while exploiting its strengths. We propose a novel framework, SECRETS, which, for the first time, estimates the individual treatment effect (ITE) per patient in the RCT study without using any external data by leveraging a state-of-the-art counterfactual estimation algorithm, called synthetic intervention. It also uses a new hypothesis testing strategy to determine whether the treatment has a clinically significant ATE based on the estimated ITEs. We show that SECRETS can improve the power of an RCT while maintaining comparable significance levels; in particular, on three real-world clinical RCTs (Phase-3 trials), SECRETS increases power over the baseline method by $\boldsymbol{6}$-$\boldsymbol{54\%}$ (average: 21.5%, standard deviation: 15.8%).|http://arxiv.org/abs/2305.05078v1|Sayeri Lala,Niraj K. Jha
357|Study Duration Prediction for Clinical Trials with Time-to-Event Endpoints Using Mixture Distributions Accounting for Heterogeneous Population|In the era of precision medicine, more and more clinical trials are now driven or guided by biomarkers, which are patient characteristics objectively measured and evaluated as indicators of normal biological processes, pathogenic processes, or pharmacologic responses to therapeutic interventions. With the overarching objective to optimize and personalize disease management, biomarker-guided clinical trials increase the efficiency by appropriately utilizing prognostic or predictive biomarkers in the design. However, the efficiency gain is often not quantitatively compared to the traditional all-comers design, in which a faster enrollment rate is expected (e.g. due to no restriction to biomarker positive patients) potentially leading to a shorter duration. To accurately predict biomarker-guided trial duration, we propose a general framework using mixture distributions accounting for heterogeneous population. Extensive simulations are performed to evaluate the impact of heterogeneous population and the dynamics of biomarker characteristics and disease on the study duration. Several influential parameters including median survival time, enrollment rate, biomarker prevalence and effect size are identitied. Re-assessments of two publicly available trials are conducted to empirically validate the prediction accuracy and to demonstrate the practical utility. The R package \emph{detest} is developed to implement the proposed method and is publicly available on CRAN.|http://arxiv.org/abs/2401.00540v1|Hong Zhang,Jie Pu,Shibing Deng,Satrajit Roychoudhury,Haitao Chu,Douglas Robinson
358|Chauhan Weighted Trajectory Analysis reduces sample size requirements and expedites time-to-efficacy signals in advanced cancer clinical trials|As Kaplan-Meier (KM) analysis is limited to single unidirectional endpoints, most advanced cancer randomized clinical trials (RCTs) are powered for either progression free survival (PFS) or overall survival (OS). This discards efficacy information carried by partial responses, complete responses, and stable disease that frequently precede progressive disease and death. Chauhan Weighted Trajectory Analysis (CWTA) is a generalization of KM that simultaneously assesses multiple rank-ordered endpoints. We hypothesized that CWTA could use this efficacy information to reduce sample size requirements and expedite efficacy signals in advanced cancer trials. We performed 100-fold and 1000-fold simulations of solid tumour systemic therapy RCTs with health statuses rank ordered from complete response (Stage 0) to death (Stage 4). At increments of sample size and hazard ratio, we compared KM PFS and OS with CWTA for (i) sample size requirements to achieve a power of 0.8 and (ii) time-to-first significant efficacy signal. CWTA consistently demonstrated greater power, and reduced sample size requirements by 18% to 35% compared to KM PFS and 14% to 20% compared to KM OS. CWTA also expedited time-to-efficacy signals 2- to 6-fold. CWTA, by incorporating all efficacy signals in the cancer treatment trajectory, provides clinically relevant reduction in required sample size and meaningfully expedites the efficacy signals of cancer treatments compared to KM PFS and KM OS. Using CWTA rather than KM as the primary trial outcome has the potential to meaningfully reduce the numbers of patients, trial duration, and costs to evaluate therapies in advanced cancer.|http://arxiv.org/abs/2405.02529v4|Utkarsh Chauhan,Daylen Mackey,John R. Mackey
359|Optimal Bayesian predictive probability for delayed response in single-arm clinical trials with binary efficacy outcome|In oncology, phase II or multiple expansion cohort trials are crucial for clinical development plans. This is because they aid in identifying potent agents with sufficient activity to continue development and confirm the proof of concept. Typically, these clinical trials are single-arm trials, with the primary endpoint being short-term treatment efficacy. Despite the development of several well-designed methodologies, there may be a practical impediment in that the endpoints may be observed within a sufficient time such that adaptive go/no-go decisions can be made in a timely manner at each interim monitoring. Specifically, Response Evaluation Criteria in Solid Tumors guideline defines a confirmed response and necessitates it in non-randomized trials, where the response is the primary endpoint. However, obtaining the confirmed outcome from all participants entered at interim monitoring may be time-consuming as non-responders should be followed up until the disease progresses. Thus, this study proposed an approach to accelerate the decision-making process that incorporated the outcome without confirmation by discounting its contribution to the decision-making framework using the generalized Bayes' theorem. Further, the behavior of the proposed approach was evaluated through a simple simulation study. The results demonstrated that the proposed approach made appropriate interim go/no-go decisions.|http://arxiv.org/abs/2405.14166v1|Takuya Yoshimoto,Satoru Shinoda,Kouji Yamamoto,Kouji Tahata
360|MEXA-CTP: Mode Experts Cross-Attention for Clinical Trial Outcome Prediction|Clinical trials are the gold standard for assessing the effectiveness and safety of drugs for treating diseases. Given the vast design space of drug molecules, elevated financial cost, and multi-year timeline of these trials, research on clinical trial outcome prediction has gained immense traction. Accurate predictions must leverage data of diverse modes such as drug molecules, target diseases, and eligibility criteria to infer successes and failures. Previous Deep Learning approaches for this task, such as HINT, often require wet lab data from synthesized molecules and/or rely on prior knowledge to encode interactions as part of the model architecture. To address these limitations, we propose a light-weight attention-based model, MEXA-CTP, to integrate readily-available multi-modal data and generate effective representations via specialized modules dubbed "mode experts", while avoiding human biases in model design. We optimize MEXA-CTP with the Cauchy loss to capture relevant interactions across modes. Our experiments on the Trial Outcome Prediction (TOP) benchmark demonstrate that MEXA-CTP improves upon existing approaches by, respectively, up to 11.3% in F1 score, 12.2% in PR-AUC, and 2.5% in ROC-AUC, compared to HINT. Ablation studies are provided to quantify the effectiveness of each component in our proposed method.|http://arxiv.org/abs/2501.06823v1|Yiqing Zhang,Xiaozhong Liu,Fabricio Murai
361|Ultrasonic Actuation of a Fine-Needle Improves Biopsy Yield|Despite the ubiquitous use over the past 150 years, the functions of the current medical needle are facilitated only by mechanical shear and cutting by the needle tip,i.e.the lancet. In this study, we demonstrate how nonlinear ultrasonics (NLU) extends the functionality of the medical needle far beyond its present capability. The NLU actions were found to be localized to the proximity of the needle tip, the SonoLancet, but the effects extend several millimeters from the physical needle boundary. The observed nonlinear phenomena, transient cavitation, fluid streams, translation of micro- and nanoparticles and atomization, were quantitatively characterized. In the fine-needle biopsy application, the SonoLancet contributed to obtaining tissue cores with increase in tissue yield by 3-6x in different tissue types compared to conventional needle biopsy technique using the same 21G needle. In conclusion, the SonoLancet could be of interest to several other medical applications, including drug or gene delivery, cell modulation, and minimally invasive surgical procedures.|http://arxiv.org/abs/2006.16604v3|Emanuele Perra,Eetu Lampsijrvi,Gonalo Barreto,Muhammad Arif,Tuomas Puranen,Edward Hggstrm,Kenneth P. H. Pritzker,Heikki J. Nieminen
362|BaySize: Bayesian Sample Size Planning for Phase I Dose-Finding Trials|We propose BaySize, a sample size calculator for phase I clinical trials using Bayesian models. BaySize applies the concept of effect size in dose finding, assuming the MTD is defined based on an equivalence interval. Leveraging a decision framework that involves composite hypotheses, BaySize utilizes two prior distributions, the fitting prior (for model fitting) and sampling prior (for data generation), to conduct sample size calculation under desirable statistical power. Look-up tables are generated to facilitate practical applications. To our knowledge, BaySize is the first sample size tool that can be applied to a broad range of phase I trial designs.|http://arxiv.org/abs/2103.06421v1|Xiaolei Lin,Jiaying Lyu,Shijie Yuan,Sue-Jane Wang,Yuan Ji
363|Treatments for pregestational chronic conditions during pregnancy: emulating a target trial with a treatment decision design|As a solution to methodologic challenges inherent to estimating causal effects of exposures in early pregnancy, we suggest emulating a target trial using a treatment decision design, wherein time zero is centered around clinical landmarks where treatment decisions may occur, such as the date of preconception counseling or prenatal care initiation. These ideas are illustrated via protocols for two target trials in large administrative databases, antidepressant use for pre-existing depressive disorder and antihypertensive medication use for mild-to-moderate chronic hypertension. Careful consideration of these issues is critical to the identification of the causal effects of early-pregnancy pharmacotherapies on pregnancy outcomes.|http://arxiv.org/abs/2305.13540v1|Mollie E. Wood,Chase D. Latour,Lucia C. Petito
364|Immersive virtual reality methods in cognitive neuroscience and neuropsychology: Meeting the criteria of the National Academy of Neuropsychology and American Academy of Clinical Neuropsychology|Clinical tools involving immersive virtual reality (VR) may bring several advantages to cognitive neuroscience and neuropsychology. However, there are some technical and methodological pitfalls. The American Academy of Clinical Neuropsychology (AACN) and the National Academy of Neuropsychology (NAN) raised 8 key issues pertaining to Computerized Neuropsychological Assessment Devices. These issues pertain to: (1) the safety and effectivity; (2) the identity of the end-user; (3) the technical hardware and software features; (4) privacy and data security; (5) the psychometric properties; (6) examinee issues; (7) the use of reporting services; and (8) the reliability of the responses and results. The VR Everyday Assessment Lab (VR-EAL) is the first immersive VR neuropsychological battery with enhanced ecological validity for the assessment of everyday cognitive functions by offering a pleasant testing experience without inducing cybersickness. The VR-EAL meets the criteria of the NAN and AACN, addresses the methodological pitfalls, and brings advantages for neuropsychological testing. However, there are still shortcomings of the VR-EAL, which should be addressed. Future iterations should strive to improve the embodiment illusion in VR-EAL and the creation of an open access VR software library should be attempted. The discussed studies demonstrate the utility of VR methods in cognitive neuroscience and neuropsychology.|http://arxiv.org/abs/2105.11909v2|Panagiotis Kourtesis,Sarah E. MacPherson
365|Item response models for the longitudinal analysis of health-related quality of life in cancer clinical trials|Statistical research regarding health-related quality of life (HRQoL) is a major challenge to better evaluate the impact of the treatments on their everyday life and to improve patients' care. Among the models that are used for the longitudinal analysis of HRQoL, we focused on the mixed models from the item response theory to analyze directly the raw data from questionnaires. Using a recent classification of generalized linear models for categorical data, we discussed about a conceptual selection of these models for the longitudinal analysis of HRQoL in cancer clinical trials. Through methodological and practical arguments, the adjacent and cumulative models seem particularly suitable for this {context}. Specially in cancer clinical trials and for the comparison between two groups, the cumulative models has the advantage of providing intuitive illustrations of results. To complete the comparison studies already performed in literature, a simulation study based on random part of the mixed models is then carried out to compare the linear mixed model classically used to the discussed item response models. As expected, the sensitivity of item response models to detect random effect with lower variance is better than the linear mixed model sensitivity. Finally, a longitudinal analysis of HRQoL data from cancer clinical trial is carried out using an item response cumulative model.|http://arxiv.org/abs/1611.06851v1|Antoine Barbieri,Jean Peyhardi,Thierry Conroy,Sophie Gourgou,Christian Lavergne,Caroline Mollevi
366|Group sequential designs for negative binomial outcomes|Count data and recurrent events in clinical trials, such as the number of lesions in magnetic resonance imaging in multiple sclerosis, the number of relapses in multiple sclerosis, the number of hospitalizations in heart failure, and the number of exacerbations in asthma or in chronic obstructive pulmonary disease (COPD) are often modeled by negative binomial distributions. In this manuscript we study planning and analyzing clinical trials with group sequential designs for negative binomial outcomes. We propose a group sequential testing procedure for negative binomial outcomes based on Wald statistics using maximum likelihood estimators. The asymptotic distribution of the proposed group sequential tests statistics are derived. The finite sample size properties of the proposed group sequential test for negative binomial outcomes and the methods for planning the respective clinical trials are assessed in a simulation study. The simulation scenarios are motivated by clinical trials in chronic heart failure and relapsing multiple sclerosis, which cover a wide range of practically relevant settings. Our research assures that the asymptotic normal theory of group sequential designs can be applied to negative binomial outcomes when the hypotheses are tested using Wald statistics and maximum likelihood estimators. We also propose two methods, one based on Student's t-distribution and one based on resampling, to improve type I error rate control in small samples. The statistical methods studied in this manuscript are implemented in the R package \textit{gscounts}, which is available for download on the Comprehensive R Archive Network (CRAN).|http://arxiv.org/abs/1707.04612v2|Tobias Mtze,Ekkehard Glimm,Heinz Schmidli,Tim Friede
367|Using clinical trial registries to inform Copas selection model for publication bias in meta-analysis|Prospective registration of study protocols in clinical trial registries is a useful way to minimize the risk of publication bias in meta-analysis, and several clinical trial registries are available nowadays. However, they are mainly used as a tool for searching studies and information submitted to the registries has not been utilized as efficiently as it could. In addressing publication bias in meta-analyses, sensitivity analysis with the Copas selection model is a more objective alternative to widely-used graphical methods such as the funnel-plot and the trim-and-fill method. Despite its ability to quantify the potential impact of publication bias, a drawback of the model is that some parameters not to be specified. This may result in some difficulty in interpreting the results of the sensitivity analysis. In this paper, we propose an alternative inference procedure for the Copas selection model by utilizing information from clinical trial registries. Our method provides a simple and accurate way to estimate all unknown parameters in the Copas selection model. A simulation study revealed that our proposed method resulted in smaller biases and more accurate confidence intervals than existing methods. Furthermore, two published meta-analyses had been re-analysed to demonstrate how to implement the proposed method in practice.|http://arxiv.org/abs/2005.14396v1|Ao Huang,Sho Komukai,Tim Friede,Satoshi Hattori
368|Assessing the Impact of COVID-19 on the Objective and Analysis of Oncology Clinical Trials -- Application of the Estimand Framework|COVID-19 outbreak has rapidly evolved into a global pandemic. The impact of COVID-19 on patient journeys in oncology represents a new risk to interpretation of trial results and its broad applicability for future clinical practice. We identify key intercurrent events that may occur due to COVID-19 in oncology clinical trials with a focus on time-to-event endpoints and discuss considerations pertaining to the other estimand attributes introduced in the ICH E9 addendum. We propose strategies to handle COVID-19 related intercurrent events, depending on their relationship with malignancy and treatment and the interpretability of data after them. We argue that the clinical trial objective from a world without COVID-19 pandemic remains valid. The estimand framework provides a common language to discuss the impact of COVID-19 in a structured and transparent manner. This demonstrates that the applicability of the framework may even go beyond what it was initially intended for.|http://arxiv.org/abs/2006.04480v2|Evgeny Degtyarev,Kaspar Rufibach,Yue Shentu,Godwin Yung,Michelle Casey,Stefan Englert,Feng Liu,Yi Liu,Oliver Sailer,Jonathan Siegel,Steven Sun,Rui Tang,Jiangxiu Zhou
369|Long-term effect estimation when combining clinical trial and observational follow-up datasets|Combining experimental and observational follow-up datasets has received a lot of attention lately. In a time-to-event setting, recent work has used medicare claims to extend the follow-up period for participants in a prostate cancer clinical trial. This allows the estimation of the long-term effect that cannot be estimated by clinical trial data alone. In this paper, we study the estimation of long-term effect when participants in a clinical trial are linked to an observational follow-up dataset with incomplete data. Such data linkages are often incomplete for various reasons. We formulate incomplete linkages as a missing data problem with careful considerations of the relationship between the linkage status and the missing data mechanism. We use the popular Cox proportional hazard model as a working model to define the long-term effect. We propose a conditional linking at random (CLAR) assumption and an inverse probability of linkage weighting (IPLW) partial likelihood estimator. We show that our IPLW partial likelihood estimator is consistent and asymptotically normal. We further extend our approach to incorporate time-dependent covariates. Simulations results confirm the validity of our method, and we further apply our methods to the SWOG study.|http://arxiv.org/abs/2204.04309v1|Gang Cheng,Yen-Chi Chen,Joseph M. Unger,Cathee Till,Ying-Qi Zhao
370|Drug and Disease Interpretation Learning with Biomedical Entity Representation Transformer|Concept normalization in free-form texts is a crucial step in every text-mining pipeline. Neural architectures based on Bidirectional Encoder Representations from Transformers (BERT) have achieved state-of-the-art results in the biomedical domain. In the context of drug discovery and development, clinical trials are necessary to establish the efficacy and safety of drugs. We investigate the effectiveness of transferring concept normalization from the general biomedical domain to the clinical trials domain in a zero-shot setting with an absence of labeled data. We propose a simple and effective two-stage neural approach based on fine-tuned BERT architectures. In the first stage, we train a metric learning model that optimizes relative similarity of mentions and concepts via triplet loss. The model is trained on available labeled corpora of scientific abstracts to obtain vector embeddings of concept names and entity mentions from texts. In the second stage, we find the closest concept name representation in an embedding space to a given clinical mention. We evaluated several models, including state-of-the-art architectures, on a dataset of abstracts and a real-world dataset of trial records with interventions and conditions mapped to drug and disease terminologies. Extensive experiments validate the effectiveness of our approach in knowledge transfer from the scientific literature to clinical trials.|http://arxiv.org/abs/2101.09311v1|Zulfat Miftahutdinov,Artur Kadurin,Roman Kudrin,Elena Tutubalina
371|Using Targeted Maximum Likelihood Estimation to Estimate Treatment Effect with Longitudinal Continuous or Binary Data: A Systematic Evaluation of 28 Diabetes Clinical Trials|The primary analysis of clinical trials in diabetes therapeutic area often involves a mixed-model repeated measure (MMRM) approach to estimate the average treatment effect for longitudinal continuous outcome, and a generalized linear mixed model (GLMM) approach for longitudinal binary outcome. In this paper, we considered another estimator of the average treatment effect, called targeted maximum likelihood estimator (TMLE). This estimator can be a one-step alternative to model either continuous or binary outcome. We compared those estimators by simulation studies and by analyzing real data from 28 diabetes clinical trials. The simulations involved different missing data scenarios, and the real data sets covered a wide range of possible distributions of the outcome and covariates in real-life clinical trials for diabetes drugs with different mechanisms of action. For all the settings, adjusted estimators tended to be more efficient than the unadjusted one. In the setting of longitudinal continuous outcome, the MMRM approach with visits and baseline variables interaction appeared to dominate the performance of the MMRM considering the main effects only for the baseline variables while showing better or comparable efficiency to the TMLE estimator in both simulations and data applications. For modeling longitudinal binary outcome, TMLE generally outperformed GLMM in terms of relative efficiency, and its avoidance of the cumbersome covariance fitting procedure from GLMM makes TMLE a more advantageous estimator.|http://arxiv.org/abs/2208.01168v1|Lingjing Jiang,Michael Rosenblum,Yu Du
372|Covariate-adjusted Group Sequential Comparisons of Survival Probabilities|In confirmatory clinical trials, survival outcomes are frequently studied and interim analyses for efficacy and/or futility are often desirable. Methods such as the log rank test and Cox regression model are commonly used to compare treatments in this setting. They rely on a proportional hazards (PH) assumption and are subject to type I error rate inflation and loss of power when PH are violated. Such violations may be expected a priori, particularly when the mechanisms of treatments differ such as immunotherapy vs. chemotherapy for treating cancer. We develop group sequential tests for comparing survival curves with covariate adjustment that allow for interim analyses in the presence of non-PH and offer easily interpreted, clinically meaningful summary measures of the treatment effect. The joint distribution of repeatedly computed test statistics converges to the canonical joint distribution with a Markov structure. The asymptotic distribution of the test statistics allows marginal comparisons of survival probabilities at multiple fixed time points and facilitates both critical value specification to maintain type I error control and sample size/power determination. Simulations demonstrate that the achieved type I error rate and power of the proposed tests meet targeted levels and are robust to the PH assumption and covariate influence. The proposed tests are illustrated using a clinical trial dataset from the Blood and Marrow Transplant Clinical Trials Network 1101 trial.|http://arxiv.org/abs/2403.17117v1|Peter Zhang,Brent Logan,Michael Martens
373|AlpaPICO: Extraction of PICO Frames from Clinical Trial Documents Using LLMs|In recent years, there has been a surge in the publication of clinical trial reports, making it challenging to conduct systematic reviews. Automatically extracting Population, Intervention, Comparator, and Outcome (PICO) from clinical trial studies can alleviate the traditionally time-consuming process of manually scrutinizing systematic reviews. Existing approaches of PICO frame extraction involves supervised approach that relies on the existence of manually annotated data points in the form of BIO label tagging. Recent approaches, such as In-Context Learning (ICL), which has been shown to be effective for a number of downstream NLP tasks, require the use of labeled examples. In this work, we adopt ICL strategy by employing the pretrained knowledge of Large Language Models (LLMs), gathered during the pretraining phase of an LLM, to automatically extract the PICO-related terminologies from clinical trial documents in unsupervised set up to bypass the availability of large number of annotated data instances. Additionally, to showcase the highest effectiveness of LLM in oracle scenario where large number of annotated samples are available, we adopt the instruction tuning strategy by employing Low Rank Adaptation (LORA) to conduct the training of gigantic model in low resource environment for the PICO frame extraction task. Our empirical results show that our proposed ICL-based framework produces comparable results on all the version of EBM-NLP datasets and the proposed instruction tuned version of our framework produces state-of-the-art results on all the different EBM-NLP datasets. Our project is available at \url{https://github.com/shrimonmuke0202/AlpaPICO.git}.|http://arxiv.org/abs/2409.09704v1|Madhusudan Ghosh,Shrimon Mukherjee,Asmit Ganguly,Partha Basuchowdhuri,Sudip Kumar Naskar,Debasis Ganguly
374|Fragility Index for Time-to-Event Endpoints in Single-Arm Clinical Trials|The reliability of clinical trial outcomes is crucial, especially in guiding medical decisions. In this paper, we introduce the Fragility Index (FI) for time-to-event endpoints in single-arm clinical trials - a novel metric designed to quantify the robustness of study conclusions. The FI represents the smallest number of censored observations that, when reclassified as uncensored events, causes the posterior probability of the median survival time exceeding a specified threshold to fall below a predefined confidence level. While drug effectiveness is typically assessed by determining whether the posterior probability exceeds a specified confidence level, the FI offers a complementary measure, indicating how robust these conclusions are to potential shifts in the data. Using a Bayesian approach, we develop a practical framework for computing the FI based on the exponential survival model. To facilitate the application of our method, we developed an R package fi, which provides a tool to compute the Fragility Index. Through real world case studies involving time to event data from single arms clinical trials, we demonstrate the utility of this index. Our findings highlight how the FI can be a valuable tool for assessing the robustness of survival analyses in single-arm studies, aiding researchers and clinicians in making more informed decisions.|http://arxiv.org/abs/2411.16938v1|Arnab Kumar Maity,Jhanvi Garg,Cynthia Basu
375|A Simple Cellular Automaton Model for Influenza A Viral Infections|Viral kinetics have been extensively studied in the past through the use of spatially homogeneous ordinary differential equations describing the time evolution of the diseased state. However, spatial characteristics such as localized populations of dead cells might adversely affect the spread of infection, similar to the manner in which a counter-fire can stop a forest fire from spreading. In order to investigate the influence of spatial heterogeneities on viral spread, a simple 2-D cellular automaton (CA) model of a viral infection has been developed. In this initial phase of the investigation, the CA model is validated against clinical immunological data for uncomplicated influenza A infections. Our results will be shown and discussed.|http://arxiv.org/abs/q-bio/0402012v1|Catherine Beauchemin,John Samuel,Jack Tuszynski
376|Depressed serum IgM levels in SLE are restricted to defined subgroups|Natural IgM autoantibodies have been proposed to convey protection from autoimmune pathogenesis. Herein, we investigated the IgM responses in 396 systemic lupus erythematosus (SLE) patients, divided into subgroups based on distinct autoantibody profiles. Depressed IgM levels were more common in SLE than in matched population controls. Strikingly, an autoreactivity profile defined by IgG anti-Ro/La was associated with reduced levels of specific natural IgM anti-phosphorylcholine (PC) antigens and anti-malondialdehyde (MDA) modified-protein, as well total IgM, while no differences were detected in SLE patients with an autoreactivity profile defined by anti-cardiolipin/Beta2glycoprotein-I. We also observed an association of reduced IgM levels with the HLA-DRB1*03 allelic variant amongst SLE patients and controls. Associations of low IgM anti-PC with cardiovascular disease were primarily found in patients without antiphospholipid antibodies. These studies further highlight the clinical relevance of depressed IgM. Our results suggest that low IgM levels in SLE patients reflect immunological and genetic differences between SLE subgroups.|http://arxiv.org/abs/1710.10872v1|Caroline Gronwall,Uta Hardt,Johanna T. Gustafsson,Kerstin Elvin,Kerstin Jensen-Urstad,Marika Kvarnstrom,Giorgia Grosso,Johan Ronnelid,Leonid Padyukov,Iva Gunnarsson,Gregg J. Silverman,Elisabet Svenungsson
377|Rank the triplets: A ranking-based multiple instance learning framework for detecting HPV infection in head and neck cancers using routine H&E images|The aetiology of head and neck squamous cell carcinoma (HNSCC) involves multiple carcinogens such as alcohol, tobacco and infection with human papillomavirus (HPV). As the HPV infection influences the prognosis, treatment and survival of patients with HNSCC, it is important to determine the HPV status of these tumours. In this paper, we propose a novel triplet-ranking loss function and a multiple instance learning pipeline for HPV status prediction. This achieves a new state-of-the-art performance in HPV detection using only the routine H&E stained WSIs on two HNSCC cohorts. Furthermore, a comprehensive tumour microenvironment profiling was performed, which characterised the unique patterns between HPV+/- HNSCC from genomic, immunology and cellular perspectives. Positive correlations of the proposed score with different subtypes of T cells (e.g. T cells follicular helper, CD8+ T cells), and negative correlations with macrophages and connective cells (e.g. fibroblast) were identified, which is in line with clinical findings. Unique gene expression profiles were also identified with respect to HPV infection status, and is in line with existing findings.|http://arxiv.org/abs/2206.08275v1|Ruoyu Wang,Syed Ali Khurram,Amina Asif,Lawrence Young,Nasir Rajpoot
378|Subgroup identification in individual patient data meta-analysis using model-based recursive partitioning|Model-based recursive partitioning (MOB) can be used to identify subgroups with differing treatment effects. The detection rate of treatment-by-covariate interactions and the accuracy of identified subgroups using MOB depend strongly on the sample size. Using data from multiple randomized controlled clinical trials can overcome the problem of too small samples. However, naively pooling data from multiple trials may result in the identification of spurious subgroups as differences in study design, subject selection and other sources of between-trial heterogeneity are ignored. In order to account for between-trial heterogeneity in individual participant data (IPD) meta-analysis random-effect models are frequently used. Commonly, heterogeneity in the treatment effect is modelled using random effects whereas heterogeneity in the baseline risks is modelled by either fixed effects or random effects. In this article, we propose metaMOB, a procedure using the generalized mixed-effects model tree (GLMM tree) algorithm for subgroup identification in IPD meta-analysis. Although the application of metaMOB is potentially wider, e.g. randomized experiments with participants in social sciences or preclinical experiments in life sciences, we focus on randomized controlled clinical trials. In a simulation study, metaMOB outperformed GLMM trees assuming a random intercept only and model-based recursive partitioning (MOB), whose algorithm is the basis for GLMM trees, with respect to the false discovery rates, accuracy of identified subgroups and accuracy of estimated treatment effect. The most robust and therefore most promising method is metaMOB with fixed effects for modelling the between-trial heterogeneity in the baseline risks.|http://arxiv.org/abs/2009.10518v1|Cynthia Huber,Norbert Benda,Tim Friede
379|Vaccine efficacy for binary post-infection outcomes under misclassification without monotonicity|In order to meet regulatory approval, pharmaceutical companies often must demonstrate that new vaccines reduce the total risk of a post-infection outcome like transmission, symptomatic disease, severe illness, or death in randomized, placebo-controlled trials. Given that infection is a necessary precondition for a post-infection outcome, one can use principal stratification to partition the total causal effect of vaccination into two causal effects: vaccine efficacy against infection, and the principal effect of vaccine efficacy against a post-infection outcome in the patients that would be infected under both placebo and vaccination. Despite the importance of such principal effects to policymakers, these estimands are generally unidentifiable, even under strong assumptions that are rarely satisfied in real-world trials. We develop a novel method to nonparametrically point identify these principal effects while eliminating the monotonicity assumption and allowing for measurement error. Furthermore, our results allow for multiple treatments, and are general enough to be applicable outside of vaccine efficacy. Our method relies on the fact that many vaccine trials are run at geographically disparate health centers, and measure biologically-relevant categorical pretreatment covariates. We show that our method can be applied to a variety of clinical trial settings where vaccine efficacy against infection and a post-infection outcome can be jointly inferred. This can yield new insights from existing vaccine efficacy trial data and will aid researchers in designing new multi-arm clinical trials.|http://arxiv.org/abs/2211.16502v5|Rob Trangucci,Yang Chen,Jon Zelner
380|Oncology Dose Finding Using Approximate Bayesian Computation Design|In the development of new cancer treatment, an essential step is to determine the maximum tolerated dose (MTD) via phase I clinical trials. Generally speaking, phase I trial designs can be classified as either model-based or algorithm-based approaches. Model-based phase I designs are typically more efficient by using all observed data, while there is a potential risk of model misspecification that may lead to unreliable dose assignment and incorrect MTD identification. In contrast, most of the algorithm-based designs are less efficient in using cumulative information, because they tend to focus on the observed data in the neighborhood of the current dose level for dose movement. To use the data more efficiently yet without any model assumption, we propose a novel approximate Bayesian computation (ABC) approach for phase I trial design. Not only is the ABC design free of any dose--toxicity curve assumption, but it can also aggregate all the available information accrued in the trial for dose assignment. Extensive simulation studies demonstrate its robustness and efficiency compared with other phase I designs. We apply the ABC design to the MEK inhibitor selumetinib trial to demonstrate its satisfactory performance. The proposed design can be a useful addition to the family of phase I clinical trial designs due to its simplicity, efficiency and robustness.|http://arxiv.org/abs/2203.00173v1|Huaqing Jin,Wenbin Du,Guosheng Yin
381|Information-adaptive clinical trials: a selective recruitment design|We propose a novel adaptive design for clinical trials with time-to-event outcomes and covariates (which may consist of or include biomarkers). Our method is based on the expected entropy of the posterior distribution of a proportional hazards model. The expected entropy is evaluated as a function of a patient's covariates, and the information gained due to a patient is defined as the decrease in the corresponding entropy. Candidate patients are only recruited onto the trial if they are likely to provide sufficient information. Patients with covariates that are deemed uninformative are filtered out. A special case is where all patients are recruited, and we determine the optimal treatment arm allocation. This adaptive design has the advantage of potentially elucidating the relationship between covariates, treatments, and survival probabilities using fewer patients, albeit at the cost of rejecting some candidates. We assess the performance of our adaptive design using data from the German Breast Cancer Study group and numerical simulations of a biomarker validation trial.|http://arxiv.org/abs/1502.03813v3|James E. Barrett
382|Combining Survival Trials Using Aggregate Data Based on Misspecified Models|The treatment effects of the same therapy observed from multiple clinical trials can often be very different. Yet the patient characteristics accounting for these differences may not be identifiable in real world practice. There needs to be an unbiased way to combine the results from multiple trials and report the overall treatment effect for the general population during the development and validation of a new therapy. The non-linear structure of the maximum partial likelihood estimates for the (log) hazard ratio defined with a Cox proportional hazard model leads to challenges in the statistical analyses for combining such clinical trials. In this paper, we formulated the expected overall treatment effects using various modeling assumptions. Thus we are proposing efficient estimates and a version of Wald test for the combined hazard ratio using only aggregate data. Interpretation of the methods are provided in the framework of robust data analyses involving misspecified models.|http://arxiv.org/abs/1503.05852v1|Tinghui Yu,Yabing Mai,Sherry Liu,Xiaofei Hu
383|A Stopped Negative Binomial Distribution|This paper introduces a new discrete distribution suggested by curtailed sampling rules common in early-stage clinical trials. We derive the distribution of the smallest number of independent Bernoulli(p) trials needed in order to observe either s successes or t failures. The closed form expression for the distribution as well as the compound distribution are derived. Properties of the distribution are shown and discussed. A case study is presented showing how the distribution can be used to monitor sequential enrollment of clinical trials with binary outcomes as well as providing post-hoc analysis of completed trials.|http://arxiv.org/abs/1508.01264v8|Michelle DeVeaux,Michael J. Kane,Daniel Zelterman
384|Information-adaptive clinical trials with selective recruitment and binary outcomes|Selective recruitment designs preferentially recruit individuals that are estimated to be statistically informative onto a clinical trial. Individuals that are expected to contribute less information have a lower probability of recruitment. Furthermore, in an information-adaptive design recruits are allocated to treatment arms in a manner that maximises information gain. The informativeness of an individual depends on their covariate (or biomarker) values and how information is defined is a critical element of information-adaptive designs. In this paper we define and evaluate four different methods for quantifying statistical information. Using both experimental data and numerical simulations we show that selective recruitment designs can offer a substantial increase in statistical power compared to randomised designs. In trials without selective recruitment we find that allocating individuals to treatment arms according to information-adaptive protocols also leads to an increase in statistical power. Consequently, selective recruitment designs can potentially achieve successful trials using fewer recruits thereby offering economic and ethical advantages.|http://arxiv.org/abs/1509.01058v3|James E. Barrett
385|AAA: Triple-adaptive Bayesian designs for the identification of optimal dose combinations in dual-agent dose-finding trials|We propose a flexible design for the identification of optimal dose combinations in dual-agent dose-finding clinical trials. The design is called AAA, standing for three adaptations: adaptive model selection, adaptive dose insertion, and adaptive cohort divi- sion. The adaptations highlight the need and opportunity for innovation for dual-agent dose finding, and are supported by the numerical results presented in the proposed simulation studies. To our knowledge, this is the first design that allows for all three adaptations at the same time. We find that AAA improves the statistical inference, enhances the chance of finding the optimal dose combinations, and shortens the trial duration. A clinical trial is being planned to apply the AAA design.|http://arxiv.org/abs/1706.03278v1|Jiaying Lyu,Yuan Ji,Naiqing Zhao,Daniel V. T. Catenacci
386|Power Priors Based on Multiple Historical Studies for Binary Outcomes|Incorporating historical information into the design and analysis of a new clinical trial has been the subject of much recent discussion. For example, in the context of clinical trials of antibiotics for drug resistant infections, where patients with specific infections can be difficult to recruit, there is often only limited and heterogeneous information available from the historical trials. To make the best use of the combined information at hand, we consider an approach based on the multiple power prior which allows the prior weight of each historical study to be chosen adaptively by empirical Bayes. This choice of weight has advantages in that it varies commensurably with differences in the historical and current data and can choose weights near 1 if the data from the corresponding historical study are similar enough to the data from the current study. Fully Bayesian approaches are also considered. The methods are applied to data from antibiotics trials. An analysis of the operating characteristics in a binomial setting shows that the proposed empirical Bayes adaptive method works well, compared to several alternative approaches, including the meta-analytic prior.|http://arxiv.org/abs/1708.08239v3|Isaac Gravestock,Leonhard Held
387|Optimizing Interim Analysis Timing for Bayesian Adaptive Commensurate Designs|In developing products for rare diseases, statistical challenges arise due to the limited number of patients available for participation in drug trials and other clinical research. Bayesian adaptive clinical trial designs offer the possibility of increased statistical efficiency, reduced development cost and ethical hazard prevention via their incorporation of evidence from external sources (historical data, expert opinions, and real-world evidence), and flexibility in the specification of interim looks. In this paper, we propose a novel Bayesian adaptive commensurate design that borrows adaptively from historical information and also uses a particular payoff function to optimize the timing of the study's interim analysis. The trial payoff is a function of how many samples can be saved via early stopping and the probability of making correct early decisions for either futility or efficacy. We calibrate our Bayesian algorithm to have acceptable long-run frequentist properties (Type I error and power) via simulation at the design stage. We illustrate our approach using a pediatric trial design setting testing the effect of a new drug for a rare genetic disease. The optimIA R package available at https://github.com/wxwx1993/Bayesian_IA_Timing provides an easy-to-use implementation of our approach.|http://arxiv.org/abs/1905.07456v2|Xiao Wu,Yi Xu,Bradley P. Carlin
388|Interim recruitment prediction for multi-centre clinical trials|We introduce a general framework for monitoring, modelling, and predicting the recruitment to multi-centre clinical trials. The work is motivated by overly optimistic and narrow prediction intervals produced by existing time-homogeneous recruitment models for multi-centre recruitment. We first present two tests for detection of decay in recruitment rates, together with a power study. We then introduce a model based on the inhomogeneous Poisson process with monotonically decaying intensity, motivated by recruitment trends observed in oncology trials. The general form of the model permits adaptation to any parametric curve-shape. A general method for constructing sensible parameter priors is provided and Bayesian model averaging is used for making predictions which account for the uncertainty in both the parameters and the model. The validity of the method and its robustness to misspecification are tested using simulated datasets. The new methodology is then applied to oncology trial data, where we make interim accrual predictions, comparing them to those obtained by existing methods, and indicate where unexpected changes in the accrual pattern occur.|http://arxiv.org/abs/1910.07965v4|Szymon Urbas,Chris Sherlock,Paul Metcalfe
389|Statistical Issues and Recommendations for Clinical Trials Conducted During the COVID-19 Pandemic|The COVID-19 pandemic has had and continues to have major impacts on planned and ongoing clinical trials. Its effects on trial data create multiple potential statistical issues. The scale of impact is unprecedented, but when viewed individually, many of the issues are well defined and feasible to address. A number of strategies and recommendations are put forward to assess and address issues related to estimands, missing data, validity and modifications of statistical analysis methods, need for additional analyses, ability to meet objectives and overall trial interpretability.|http://arxiv.org/abs/2005.10248v1|R. Daniel Meyer,Bohdana Ratitch,Marcel Wolbers,Olga Marchenko,Hui Quan,Daniel Li,Chrissie Fletcher,Xin Li,David Wright,Yue Shentu,Stefan Englert,Wei Shen,Jyotirmoy Dey,Thomas Liu,Ming Zhou,Norman Bohidar,Peng-Liang Zhao,Michael Hale
390|Survival analysis under non-proportional hazards: investigating non-inferiority or equivalence in time-to-event data|The classical approach to analyze time-to-event data, e.g. in clinical trials, is to fit Kaplan-Meier curves yielding the treatment effect as the hazard ratio between treatment groups. Afterwards commonly a log-rank test is performed in order to investigate whether there is a difference in survival, or, depending on additional covariates, a Cox proportional hazard model is used. However, in numerous trials these approaches fail due to the presence of non-proportional hazards, resulting in difficulties of interpreting the hazard ratio and a loss of power. When considering equivalence or non-inferiority trials, the commonly performed log-rank based tests are similarly affected by a violation of this assumption. Here we propose a parametric framework to assess equivalence or non-inferiority for survival data. We derive pointwise confidence bands for both, the hazard ratio and the difference of the survival curves. Further we propose a test procedure addressing non-inferiority and equivalence by directly comparing the survival functions at certain time points or over an entire range of time. We demonstrate the validity of the methods by a clinical trial example and by numerous simulation results.|http://arxiv.org/abs/2009.06699v1|Kathrin Mllenhoff,Achim Tresch
391|Finding and assessing treatment effect sweet spots in clinical trial data|Identifying heterogeneous treatment effects (HTEs) in randomized controlled trials is an important step toward understanding and acting on trial results. However, HTEs are often small and difficult to identify, and HTE modeling methods which are very general can suffer from low power. We present a method that exploits any existing relationship between illness severity and treatment effect, and identifies the "sweet spot", the contiguous range of illness severity where the estimated treatment benefit is maximized. We further compute a bias-corrected estimate of the conditional average treatment effect (CATE) in the sweet spot, and a $p$-value. Because we identify a single sweet spot and $p$-value, we believe our method to be straightforward to interpret and actionable: results from our method can inform future clinical trials and help clinicians make personalized treatment recommendations.|http://arxiv.org/abs/2011.10157v2|Erin Craig,Donald A Redelmeier,Robert J Tibshirani
392|Bayesian prognostic covariate adjustment|Historical data about disease outcomes can be integrated into the analysis of clinical trials in many ways. We build on existing literature that uses prognostic scores from a predictive model to increase the efficiency of treatment effect estimates via covariate adjustment. Here we go further, utilizing a Bayesian framework that combines prognostic covariate adjustment with an empirical prior distribution learned from the predictive performances of the prognostic model on past trials. The Bayesian approach interpolates between prognostic covariate adjustment with strict type I error control when the prior is diffuse, and a single-arm trial when the prior is sharply peaked. This method is shown theoretically to offer a substantial increase in statistical power, while limiting the type I error rate under reasonable conditions. We demonstrate the utility of our method in simulations and with an analysis of a past Alzheimer's disease clinical trial.|http://arxiv.org/abs/2012.13112v1|David Walsh,Alejandro Schuler,Diana Hall,Jon Walsh,Charles Fisher
393|Lessons Learned from the Bayesian Design and Analysis for the BNT162b2 COVID-19 Vaccine Phase 3 Trial|The phase III BNT162b2 mRNA COVID-19 vaccine trial is based on a Bayesian design and analysis, and the main evidence of vaccine efficacy is presented in Bayesian statistics. Confusion and mistakes are produced in the presentation of the Bayesian results. Some key statistics, such as Bayesian credible intervals, are mislabeled and stated as confidence intervals. Posterior probabilities of the vaccine efficacy are not reported as the main results. We illustrate the main differences in the reporting of Bayesian analysis results for a clinical trial and provide four recommendations. We argue that statistical evidence from a Bayesian trial, when presented properly, is easier to interpret and directly addresses the main clinical questions, thereby better supporting regulatory decision making. We also recommend using abbreviation "BI" to represent Bayesian credible intervals as a differentiation to "CI" which stands for confidence interval.|http://arxiv.org/abs/2103.05499v1|Yuan Ji,Shijie Yuan
394|Application of Multi-Armed Bandits to Model-assisted designs for Dose-Finding Clinical Trials|We consider applying multi-armed bandits to model-assisted designs for dose-finding clinical trials. Multi-armed bandits are very simple and powerful methods to determine actions to maximize a reward in a limited number of trials. Among the multi-armed bandits, we first consider the use of Thompson sampling which determines actions based on random samples from a posterior distribution. In the small sample size, as shown in dose-finding trials, because the tails of posterior distribution are heavier and random samples are too much variability, we also consider an application of regularized Thompson sampling and greedy algorithm. The greedy algorithm determines a dose based on a posterior mean. In addition, we also propose a method to determine a dose based on a posterior median. We evaluate the performance of our proposed designs for six scenarios via simulation studies.|http://arxiv.org/abs/2201.05268v1|Masahiro Kojima
395|Priors from Envisioned Posterior Judgments: A Novel Elicitation Approach With Application to Bayesian Clinical Trials|The uptake of formalized prior elicitation from experts in Bayesian clinical trials has been limited, largely due to the challenges associated with complex statistical modeling, the lack of practical tools, and the cognitive burden on experts required to quantify their uncertainty using probabilistic language. Additionally, existing methods do not address prior-posterior coherence, i.e., does the posterior distribution, obtained mathematically from combining the estimated prior with the trial data, reflect the expert's actual posterior beliefs? We propose a new elicitation approach that seeks to ensure prior-posterior coherence and reduce the expert's cognitive burden. This is achieved by eliciting responses about the expert's envisioned posterior judgments under various potential data outcomes and inferring the prior distribution by minimizing the discrepancies between these responses and the expected responses obtained from the posterior distribution. The feasibility and potential value of the new approach are illustrated through an application to a real trial currently underway.|http://arxiv.org/abs/2409.05271v2|Yongdong Ouyang,Janice J Eng,Denghuang Zhan,Hubert Wong
396|Assessing the Role of Volumetric Brain Information in Multiple Sclerosis Progression|Multiple sclerosis is a chronic autoimmune disease that affects the central nervous system. Understanding multiple sclerosis progression and identifying the implicated brain structures is crucial for personalized treatment decisions. Deformation-based morphometry utilizes anatomical magnetic resonance imaging to quantitatively assess volumetric brain changes at the voxel level, providing insight into how each brain region contributes to clinical progression with regards to neurodegeneration. Utilizing such voxel-level data from a relapsing multiple sclerosis clinical trial, we extend a model-agnostic feature importance metric to identify a robust and predictive feature set that corresponds to clinical progression. These features correspond to brain regions that are clinically meaningful in MS disease research, demonstrating their scientific relevance. When used to predict progression using classical survival models and 3D convolutional neural networks, the identified regions led to the best-performing models, demonstrating their prognostic strength. We also find that these features generalize well to other definitions of clinical progression and can compensate for the omission of highly prognostic clinical features, underscoring the predictive power and clinical relevance of deformation-based morphometry as a regional identification tool.|http://arxiv.org/abs/2412.09497v1|Andy A. Shen,Aidan McLoughlin,Zoe Vernon,Jonathan Lin,Richard A. D. Carano,Peter J. Bickel,Zhuang Song,Haiyan Huang
397|Handling Covariates in the Design of Clinical Trials|There has been a split in the statistics community about the need for taking covariates into account in the design phase of a clinical trial. There are many advocates of using stratification and covariate-adaptive randomization to promote balance on certain known covariates. However, balance does not always promote efficiency or ensure more patients are assigned to the better treatment. We describe these procedures, including model-based procedures, for incorporating covariates into the design of clinical trials, and give examples where balance, efficiency and ethical considerations may be in conflict. We advocate a new class of procedures, covariate-adjusted response-adaptive (CARA) randomization procedures that attempt to optimize both efficiency and ethical considerations, while maintaining randomization. We review all these procedures, present a few new simulation studies, and conclude with our philosophy.|http://arxiv.org/abs/1102.3773v1|William F. Rosenberger,Oleksandr Sverdlov
398|An Empirical Comparison of Parametric and Permutation Tests for Regression Analysis of Randomized Experiments|Hypothesis tests based on linear models are widely accepted by organizations that regulate clinical trials. These tests are derived using strong assumptions about the data-generating process so that the resulting inference can be based on parametric distributions. Because these methods are well understood and robust, they are sometimes applied to data that depart from assumptions, such as ordinal integer scores. Permutation tests are a nonparametric alternative that require minimal assumptions which are often guaranteed by the randomization that was conducted. We compare analysis of covariance (ANCOVA), a special case of linear regression that incorporates stratification, to several permutation tests based on linear models that control for pretreatment covariates. In simulations of randomized experiments using models which violate some of the parametric regression assumptions, the permutation tests maintain power comparable to ANCOVA. We illustrate the use of these permutation tests alongside ANCOVA using data from a clinical trial comparing the effectiveness of two treatments for gastroesophageal reflux disease. Given the considerable costs and scientific importance of clinical trials, an additional nonparametric method, such as a linear model permutation test, may serve as a robustness check on the statistical inference for the main study endpoints.|http://arxiv.org/abs/1702.04851v2|Kellie Ottoboni,Fraser Lewis,Luigi Salmaso
399|Quantifying and Detecting Individual Level `Always Survivor' Causal Effects Under `Truncation by Death' and Censoring Through Time|The analysis of causal effects when the outcome of interest is possibly truncated by death has a long history in statistics and causal inference. The survivor average causal effect is commonly identified with more assumptions than those guaranteed by the design of a randomized clinical trial or using sensitivity analysis. This paper demonstrates that individual level causal effects in the `always survivor' principal stratum can be identified with no stronger identification assumptions than randomization. We illustrate the practical utility of our methods using data from a clinical trial on patients with prostate cancer. Our methodology is the first and, as of yet, only proposed procedure that enables detecting causal effects in the presence of truncation by death using only the assumptions that are guaranteed by design of the clinical trial. This methodology is applicable to all types of outcomes.|http://arxiv.org/abs/1905.11300v3|Jaffer M. Zaidi,Eric J. Tchetgen Tchetgen,Tyler J. VanderWeele
400|Knowledge-guided Text Structuring in Clinical Trials|Clinical trial records are variable resources or the analysis of patients and diseases. Information extraction from free text such as eligibility criteria and summary of results and conclusions in clinical trials would better support computer-based eligibility query formulation and electronic patient screening. Previous research has focused on extracting information from eligibility criteria, with usually a single pair of medical entity and attribute, but seldom considering other kinds of free text with multiple entities, attributes and relations that are more complex for parsing. In this paper, we propose a knowledge-guided text structuring framework with an automatically generated knowledge base as training corpus and word dependency relations as context information to transfer free text into formal, computer-interpretable representations. Experimental results show that our method can achieve overall high precision and recall, demonstrating the effectiveness and efficiency of the proposed method.|http://arxiv.org/abs/1912.12380v1|Yingcheng Sun,Kenneth Loparo
401|A practical Response Adaptive Block Randomization (RABR) design with analytic type I error protection|Response adaptive randomization (RAR) is appealing from methodological, ethical, and pragmatic perspectives in the sense that subjects are more likely to be randomized to better performing treatment groups based on accumulating data. However, applications of RAR in confirmatory drug clinical trials with multiple active arms are limited largely due to its complexity, and lack of control of randomization ratios to different treatment groups. To address the aforementioned issues, we propose a Response Adaptive Block Randomization (RABR) design allowing arbitrarily pre-specified randomization ratios for the control and high-performing groups to meet clinical trial objectives. We show the validity of the conventional unweighted test in RABR with a controlled type I error rate based on the weighted combination test for sample size adaptive design invoking no large sample approximation. The advantages of the proposed RABR in terms of robustly reaching target final sample size to meet regulatory requirements and increasing statistical power as compared with the popular Doubly Adaptive Biased Coin Design (DBCD) are demonstrated by statistical simulations and a practical clinical trial design example.|http://arxiv.org/abs/2004.07356v2|Tianyu Zhan,Lu Cui,Ziqian Geng,Lanju Zhang,Yihua Gu,Ivan S. F. Chan
402|Response-adaptive randomization in clinical trials: from myths to practical considerations|Response-Adaptive Randomization (RAR) is part of a wider class of data-dependent sampling algorithms, for which clinical trials are typically used as a motivating application. In that context, patient allocation to treatments is determined by randomization probabilities that change based on the accrued response data in order to achieve experimental goals. RAR has received abundant theoretical attention from the biostatistical literature since the 1930's and has been the subject of numerous debates. In the last decade, it has received renewed consideration from the applied and methodological communities, driven by well-known practical examples and its widespread use in machine learning. Papers on the subject present different views on its usefulness, and these are not easy to reconcile. This work aims to address this gap by providing a unified, broad and fresh review of methodological and practical issues to consider when debating the use of RAR in clinical trials.|http://arxiv.org/abs/2005.00564v4|David S. Robertson,Kim May Lee,Boryana C. Lopez-Kolkovska,Sofia S. Villar
403|A novel estimand to adjust for rescue treatment in clinical trials|The interpretation of randomised clinical trial results is often complicated by intercurrent events. For instance, rescue medication is sometimes given to patients in response to worsening of their disease, either in addition to the randomised treatment or in its place. The use of such medication complicates the interpretation of the intention-to-treat analysis. In view of this, we propose a novel estimand defined as the intention-to-treat effect that would have been observed, had patients on the active arm been switched to rescue medication if and only if they would have been switched when randomised to control. This enables us to disentangle the treatment effect from the effect of rescue medication on a patient's outcome, while avoiding the strong extrapolations that are typically needed when inferring what the intention-to-treat effect would have been in the absence of rescue medication. We develop an inverse probability weighting method to estimate this estimand under specific untestable assumptions, in view of which we propose a sensitivity analysis. We use the method for the analysis of a clinical trial conducted by Janssen Pharmaceuticals, in which chronically ill patients can switch to rescue medication for ethical reasons. Monte Carlo simulations confirm that the proposed estimator is unbiased in moderate sample sizes.|http://arxiv.org/abs/2009.12052v1|Hege Michiels,Cristina Sotto,An Vandebosch,Stijn Vansteelandt
404|Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports|With the increasing number of clinical trial reports generated every day, it is becoming hard to keep up with novel discoveries that inform evidence-based healthcare recommendations. To help automate this process and assist medical experts, NLP solutions are being developed. This motivated the SemEval-2023 Task 7, where the goal was to develop an NLP system for two tasks: evidence retrieval and natural language inference from clinical trial data. In this paper, we describe our two developed systems. The first one is a pipeline system that models the two tasks separately, while the second one is a joint system that learns the two tasks simultaneously with a shared representation and a multi-task learning approach. The final system combines their outputs in an ensemble system. We formalize the models, present their characteristics and challenges, and provide an analysis of achieved results. Our system ranked 3rd out of 40 participants with a final submission.|http://arxiv.org/abs/2304.13180v2|Juraj Vladika,Florian Matthes
405|A note on stratification errors in the analysis of clinical trials|Stratification in both the design and analysis of randomized clinical trials is common. Despite features in automated randomization systems to re-confirm the stratifying variables, incorrect values of these variables may be entered. These errors are often detected during subsequent data collection and verification. Questions remain about whether to use the mis-reported initial stratification or the corrected values in subsequent analyses. It is shown that the likelihood function resulting from the design of randomized clinical trials supports the use of the corrected values. New definitions are proposed that characterize misclassification errors as `ignorable' and `non-ignorable'. Ignorable errors may depend on the correct strata and any other modeled baseline covariates, but they are otherwise unrelated to potential treatment outcomes. Data management review suggests most misclassification errors are arbitrarily produced by distracted investigators, so they are ignorable or at most weakly dependent on measured and unmeasured baseline covariates. Ignorable misclassification errors may produce a small increase in standard errors, but other properties of the planned analyses are unchanged (e.g., unbiasedness, confidence interval coverage). It is shown that unbiased linear estimation in the absence of misclassification errors remains unbiased when there are non-ignorable misclassification errors, and the corresponding confidence intervals based on the corrected strata values are conservative.|http://arxiv.org/abs/2304.14538v2|Neal Thomas
406|Using Recursive Partitioning to Find and Estimate Heterogenous Treatment Effects In Randomized Clinical Trials|Heterogeneous treatment effects can be very important in the analysis of randomized clinical trials. Heightened risks or enhanced benefits may exist for particular subsets of study subjects. When the heterogeneous treatment effects are specified as the research is being designed, there are proper and readily available analysis techniques. When the heterogeneous treatment effects are inductively obtained as an experiment's data are analyzed, significant complications are introduced. There can be a need for special loss functions designed to find local average treatment effects and for techniques that properly address post selection statistical inference. In this paper, we tackle both while undertaking a recursive partitioning analysis of a randomized clinical trial testing whether individuals on probation, who are low risk, can be minimally supervised with no increase in recidivism.|http://arxiv.org/abs/1807.04164v1|Richard Berk,Matthew Olson,Andreas Buja,Aurelie Ouss
407|A Simulation Study Evaluating Phase I Clinical Trial Designs for Combinational Agents|Nowadays, more and more clinical trials choose combinational agents as the intervention to achieve better therapeutic responses. However, dose-finding for combinational agents is much more complicated than single agent as the full order of combination dose toxicity is unknown. Therefore, regular phase I designs are not able to identify the maximum tolerated dose (MTD) of combinational agents. Motivated by such needs, plenty of novel phase I clinical trial designs for combinational agents were proposed. With so many available designs, research that compare their performances, explore parameters' impacts, and provide recommendations is very limited. Therefore, we conducted a simulation study to evaluate multiple phase I designs that proposed to identify single MTD for combinational agents under various scenarios. We also explored influences of different design parameters. In the end, we summarized the pros and cons of each design, and provided a general guideline in design selection.|http://arxiv.org/abs/2103.07746v2|Shu Wang,Ji-Hyun Lee
408|Improving Adaptive Seamless Designs through Bayesian optimization|We propose to use Bayesian optimization (BO) to improve the efficiency of the design selection process in clinical trials. BO is a method to optimize expensive black-box functions, by using a regression as a surrogate to guide the search. In clinical trials, planning test procedures and sample sizes is a crucial task. A common goal is to maximize the test power, given a set of treatments, corresponding effect sizes, and a total number of samples. From a wide range of possible designs we aim to select the best one in a short time to allow quick decisions. The standard approach to simulate the power for each single design can become too time-consuming. When the number of possible designs becomes very large, either large computational resources are required or an exhaustive exploration of all possible designs takes too long. Here, we propose to use BO to quickly find a clinical trial design with high power from a large number of candidate designs. We demonstrate the effectiveness of our approach by optimizing the power of adaptive seamless designs for different sets of treatment effect sizes. Comparing BO with an exhaustive evaluation of all candidate designs shows that BO finds competitive designs in a fraction of the time.|http://arxiv.org/abs/2105.09223v1|Jakob Richter,Tim Friede,Jrg Rahnenfhrer
409|The Predictive Individual Effect for Survival Data|The call for patient-focused drug development is loud and clear, as expressed in the 21st Century Cures Act and in recent guidelines and initiatives of regulatory agencies. Among the factors contributing to modernized drug development and improved health-care activities are easily interpretable measures of clinical benefit. In addition, special care is needed for cancer trials with time-to-event endpoints if the treatment effect is not constant over time. We propose the predictive individual effect which is a patient-centric and tangible measure of clinical benefit under a wide variety of scenarios. It can be obtained by standard predictive calculations under a rank preservation assumption that has been used previously in trials with treatment switching. We discuss four recent Oncology trials that cover situations with proportional as well as non-proportional hazards (delayed treatment effect or crossing of survival curves). It is shown that the predictive individual effect offers valuable insights beyond p-values, estimates of hazard ratios or differences in median survival. Compared to standard statistical measures, the predictive individual effect is a direct, easily interpretable measure of clinical benefit. It facilitates communication among clinicians, patients, and other parties and should therefore be considered in addition to standard statistical results.|http://arxiv.org/abs/2112.10404v1|Beat Neuenschwander,Satrajit Roychoudhury,Simon Wandel,Kannan Natarajan,Emmanuel Zuber
410|Modelling and forecasting patient recruitment in clinical trials with patients' dropout|This paper focuses on statistical modelling and prediction of patient recruitment in clinical trials accounting for patients dropout. The recruitment model is based on a Poisson-gamma model introduced by Anisimov and Fedorov (2007), where the patients arrive at different centres according to Poisson processes with rates viewed as gamma-distributed random variables. Each patient can drop the study during some screening period. Managing the dropout process is of a major importance but data related to dropout are rarely correctly collected. In this paper, a few models of dropout are proposed. The technique for estimating parameters and predicting the number of recruited patients over time and the recruitment time is developed. Simulation results confirm the applicability of the technique and thus, the necessity to account for patients dropout at the stage of forecasting recruitment in clinical trials.|http://arxiv.org/abs/2202.06779v1|Vladimir Anisimov,Guillaume Mijoule,Armando Turchetta,Nicolas Savy
411|Missing data imputation for a multivariate outcome of mixed variable types|Data collected in clinical trials are often composed of multiple types of variables. For example, laboratory measurements and vital signs are longitudinal data of continuous or categorical variables, adverse events may be recurrent events, and death is a time-to-event variable. Missing data due to patients' discontinuation from the study or as a result of handling intercurrent events using a hypothetical strategy almost always occur during any clinical trial. Imputing these data with mixed types of variables simultaneously is a challenge that has not been studied extensively. In this article, we propose using an approximate fully conditional specification to impute the missing data. Simulation shows the proposed method provides satisfactory results under the assumption of missing at random. Finally, real data from a clinical trial evaluating treatments for diabetes are analyzed to illustrate the potential benefit of the proposed method.|http://arxiv.org/abs/2206.01873v2|Tuo Wang,Rachel Zilinskas,Ying Li,Yongming Qu
412|UNIMIB at TREC 2021 Clinical Trials Track|This contribution summarizes the participation of the UNIMIB team to the TREC 2021 Clinical Trials Track. We have investigated the effect of different query representations combined with several retrieval models on the retrieval performance. First, we have implemented a neural re-ranking approach to study the effectiveness of dense text representations. Additionally, we have investigated the effectiveness of a novel decision-theoretic model for relevance estimation. Finally, both of the above relevance models have been compared with standard retrieval approaches. In particular, we combined a keyword extraction method with a standard retrieval process based on the BM25 model and a decision-theoretic relevance model that exploits the characteristics of this particular search task. The obtained results show that the proposed keyword extraction method improves 84% of the queries over the TREC's median NDCG@10 measure when combined with either traditional or decision-theoretic relevance models. Moreover, regarding RPEC@10, the employed decision-theoretic model improves 85% of the queries over the reported TREC's median value.|http://arxiv.org/abs/2207.13514v1|Georgios Peikos,Oscar Espitia,Gabriella Pasi
413|Natural cubic splines for the analysis of Alzheimer's clinical trials|Mixed model repeated measures (MMRM) is the most common analysis approach used in clinical trials for Alzheimer's disease and other progressive diseases measured with continuous outcomes measured over time. The model treats time as a categorical variable, which allows an unconstrained estimate of the mean for each study visit in each randomized group. Categorizing time in this way can be problematic when assessments occur off-schedule, as including off-schedule visits can induce bias, and excluding them ignores valuable information and violates the intention to treat principle. This problem has been exacerbated by clinical trial visits which have been delayed due to the COVID19 pandemic. As an alternative to MMRM, we propose a constrained longitudinal data analysis with natural cubic splines that treats time as continuous and uses test version effects to model the mean over time. The spline model is shown to be superior, in practice and simulation studies, to categorical-time models like MMRM and models that assume a proportional treatment effect.|http://arxiv.org/abs/2208.08077v1|M. C. Donohue,O. Langford,P. Insel,C. H. van Dyck,R. C. Petersen,S. Craft,G. Sethuraman,R. Raman,P. S. Aisen
414|Modern approaches for evaluating treatment effect heterogeneity from clinical trials and observational data|In this paper we review recent advances in statistical methods for the evaluation of the heterogeneity of treatment effects (HTE), including subgroup identification and estimation of individualized treatment regimens, from randomized clinical trials and observational studies. We identify several types of approaches using the features introduced in Lipkovich, Dmitrienko and D'Agostino (2017) that distinguish the recommended principled methods from basic methods for HTE evaluation that typically rely on rules of thumb and general guidelines (the methods are often referred to as common practices). We discuss the advantages and disadvantages of various principled methods as well as common measures for evaluating their performance. We use simulated data and a case study based on a historical clinical trial to illustrate several new approaches to HTE evaluation.|http://arxiv.org/abs/2311.14889v3|Ilya Lipkovich,David Svensson,Bohdana Ratitch,Alex Dmitrienko
415|Group Sequential Design Under Non-proportional Hazards|Non-proportional hazards (NPH) are often observed in clinical trials with time-to-event endpoints. A common example is a long-term clinical trial with a delayed treatment effect in immunotherapy for cancer. When designing clinical trials with time-to-event endpoints, it is crucial to consider NPH scenarios to gain a complete understanding of design operating characteristics. In this paper, we focus on group sequential design for three NPH methods: the average hazard ratio, the weighted logrank test, and the MaxCombo combination test. For each of these approaches, we provide analytic forms of design characteristics that facilitate sample size calculation and bound derivation for group sequential designs. Examples are provided to illustrate the proposed methods. To facilitate statisticians in designing and comparing group sequential designs under NPH, we have implemented the group sequential design methodology in the gsDesign2 R package at https://cran.r-project.org/web/packages/gsDesign2/.|http://arxiv.org/abs/2312.01723v1|Yujie Zhao,Yilong Zhang,Larry Leon,Keaven M. Anderson
416|Enhancing Robustness in Biomedical NLI Models: A Probing Approach for Clinical Trials|Large Language Models have revolutionized various fields and industries, such as Conversational AI, Content Generation, Information Retrieval, Business Intelligence, and Medical, to name a few. One major application in the field of medical is to analyze and investigate clinical trials for entailment tasks.However, It has been observed that Large Language Models are susceptible to shortcut learning, factual inconsistency, and performance degradation with little variation in context. Adversarial and robust testing is performed to ensure the integrity of models output. But, ambiguity still persists. In order to ensure the integrity of the reasoning performed and investigate the model has correct syntactic and semantic understanding probing is used. Here, I used mnestic probing to investigate the Sci-five model, trained on clinical trial. I investigated the model for feature learnt with respect to natural logic. To achieve the target, I trained task specific probes. Used these probes to investigate the final layers of trained model. Then, fine tuned the trained model using iterative null projection. The results shows that model accuracy improved. During experimentation, I observed that size of the probe has affect on the fine tuning process.|http://arxiv.org/abs/2402.02558v1|Ata Mustafa
417|IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe Biomedical Natural Language Inference for Clinical Trials|Large Language models (LLMs) have demonstrated state-of-the-art performance in various natural language processing (NLP) tasks across multiple domains, yet they are prone to shortcut learning and factual inconsistencies. This research investigates LLMs' robustness, consistency, and faithful reasoning when performing Natural Language Inference (NLI) on breast cancer Clinical Trial Reports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials. We examine the reasoning capabilities of LLMs and their adeptness at logical problem-solving. A comparative analysis is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro under zero-shot settings using Retrieval-Augmented Generation (RAG) framework, integrating various reasoning chains. The evaluation yields an F1 score of 0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test dataset.|http://arxiv.org/abs/2404.04510v1|Shreyasi Mandal,Ashutosh Modi
418|Considerations for Single-Arm Trials to Support Accelerated Approval of Oncology Drugs|In the last two decades, single-arm trials (SATs) have been effectively used to study anticancer therapies in well-defined patient populations using durable response rates as an objective and interpretable clinical endpoints. With a growing trend of regulatory accelerated approval (AA) requiring randomized controlled trials (RCTs), some confusions have arisen about the roles of SATs in AA. This paper is intended to elucidate conditions under which an SAT may be considered reasonable for AA. Specifically, the paper describes (1) two necessary conditions for designing an SAT, (2) three sufficient conditions that help either optimize the study design or interpret the study results, (3) four conditions that demonstrate substantial evidence of clinical benefits of the drug, and (4) a plan of a confirmatory RCT to verify the clinical benefits. Some further considerations are discussed to help design a scientifically sound SAT and communicate with regulatory agencies. Conditions presented in this paper may serve as a set of references for sponsors using SATs for regulatory decision.|http://arxiv.org/abs/2405.12437v2|Feinan Lu,Tao Wang,Ying Lu,Jie Chen
419|Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials|The integration of Large Language Models (LLMs) into the drug discovery and development field marks a significant paradigm shift, offering novel methodologies for understanding disease mechanisms, facilitating drug discovery, and optimizing clinical trial processes. This review highlights the expanding role of LLMs in revolutionizing various stages of the drug development pipeline. We investigate how these advanced computational models can uncover target-disease linkage, interpret complex biomedical data, enhance drug molecule design, predict drug efficacy and safety profiles, and facilitate clinical trial processes. Our paper aims to provide a comprehensive overview for researchers and practitioners in computational biology, pharmacology, and AI4Science by offering insights into the potential transformative impact of LLMs on drug discovery and development.|http://arxiv.org/abs/2409.04481v1|Yizhen Zheng,Huan Yee Koh,Maddie Yang,Li Li,Lauren T. May,Geoffrey I. Webb,Shirui Pan,George Church
420|Improve Sensitivity Analysis Synthesizing Randomized Clinical Trials With Limited Overlap|Randomized clinical trials are the gold standard when estimating the average treatment effect. However, they are usually not a random sample from the real-world population because of the inclusion/exclusion rules. Meanwhile, observational studies typically consist of representative samples from the real-world population. However, due to unmeasured confounding, sensitivity analysis is often used to estimate bounds for the average treatment effect without relying on stringent assumptions of other existing methods. This article introduces a synthesis estimator that improves sensitivity analysis in observational studies by incorporating randomized clinical trial data, even when overlap in covariate distribution is limited due to inclusion/exclusion criteria. We show that the proposed estimator will give a tighter bound when a "separability" condition holds for the sensitivity parameter. Theoretical proofs and simulations show that this method provides a tighter bound than the sensitivity analysis using only observational study. We apply this method to combine an observational study on drug effectiveness with a partially overlapping RCT dataset, yielding improved average treatment effect bounds.|http://arxiv.org/abs/2409.07391v3|Kuan Jiang,Wenjie Hu,Shu Yang,Xinxing Lai,Xiaohua Zhou
421|Comparison of g-estimation approaches for handling symptomatic medication at multiple timepoints in Alzheimer's Disease with a hypothetical strategy|For handling intercurrent events in clinical trials, one of the strategies outlined in the ICH E9(R1) addendum targets the hypothetical scenario of non-occurrence of the intercurrent event. While this strategy is often implemented by setting data after the intercurrent event to missing even if they have been collected, g-estimation allows for a more efficient estimation by using the information contained in post-IE data. As the g-estimation methods have largely developed outside of randomised clinical trials, optimisations for the application in clinical trials are possible. In this work, we describe and investigate the performance of modifications to the established g-estimation methods, leveraging the assumption that some intercurrent events are expected to have the same impact on the outcome regardless of the timing of their occurrence. In a simulation study in Alzheimer disease, the modifications show a substantial efficiency advantage for the estimation of an estimand that applies the hypothetical strategy to the use of symptomatic treatment while retaining unbiasedness and adequate type I error control.|http://arxiv.org/abs/2409.10943v1|Florian Lasch,Lorenzo Guizzaro,Wen Wei Loh
422|Direct Estimation for Commonly Used Pattern-Mixture Models in Clinical Trials|Pattern-mixture models have received increasing attention as they are commonly used to assess treatment effects in primary or sensitivity analyses for clinical trials with nonignorable missing data. Pattern-mixture models have traditionally been implemented using multiple imputation, where the variance estimation may be a challenge because the Rubin's approach of combining between- and within-imputation variance may not provide consistent variance estimation while bootstrap methods may be time-consuming. Direct likelihood-based approaches have been proposed in the literature and implemented for some pattern-mixture models, but the assumptions are sometimes restrictive, and the theoretical framework is fragile. In this article, we propose an analytical framework for an efficient direct likelihood estimation method for commonly used pattern-mixture models corresponding to return-to-baseline, jump-to-reference, placebo washout, and retrieved dropout imputations. A parsimonious tipping point analysis is also discussed and implemented. Results from simulation studies demonstrate that the proposed methods provide consistent estimators. We further illustrate the utility of the proposed methods using data from a clinical trial evaluating a treatment for type 2 diabetes.|http://arxiv.org/abs/2410.06939v1|Jitong Lou,Mallikarjuna Rettiganti,Yongming Qu
423|PubMed knowledge graph 2.0: Connecting papers, patents, and clinical trials in biomedical science|Papers, patents, and clinical trials are indispensable types of scientific literature in biomedicine, crucial for knowledge sharing and dissemination. However, these documents are often stored in disparate databases with varying management standards and data formats, making it challenging to form systematic, fine-grained connections among them. To address this issue, we introduce PKG2.0, a comprehensive knowledge graph dataset encompassing over 36 million papers, 1.3 million patents, and 0.48 million clinical trials in the biomedical field. PKG2.0 integrates these previously dispersed resources through various links, including biomedical entities, author networks, citation relationships, and research projects. Fine-grained biomedical entity extraction, high-performance author name disambiguation, and multi-source citation integration have played a crucial role in the construction of the PKG dataset. Additionally, project data from the NIH Exporter enriches the dataset with metadata of NIH-funded projects and their scholarly outputs. Data validation demonstrates that PKG2.0 excels in key tasks such as author disambiguation and biomedical entity recognition. This dataset provides valuable resources for biomedical researchers, bibliometric scholars, and those engaged in literature mining.|http://arxiv.org/abs/2410.07969v1|Jian Xu,Chao Yu,Jiawei Xu,Ying Ding,Vetle I. Torvik,Jaewoo Kang,Mujeen Sung,Min Song
424|A novel longitudinal rank-sum test for multiple primary endpoints in clinical trials: Applications to neurodegenerative disorders|Neurodegenerative disorders such as Alzheimer's disease (AD) present a significant global health challenge, characterized by cognitive decline, functional impairment, and other debilitating effects. Current AD clinical trials often assess multiple longitudinal primary endpoints to comprehensively evaluate treatment efficacy. Traditional methods, however, may fail to capture global treatment effects, require larger sample sizes due to multiplicity adjustments, and may not fully exploit multivariate longitudinal data. To address these limitations, we introduce the Longitudinal Rank Sum Test (LRST), a novel nonparametric rank-based omnibus test statistic. The LRST enables a comprehensive assessment of treatment efficacy across multiple endpoints and time points without multiplicity adjustments, effectively controlling Type I error while enhancing statistical power. It offers flexibility against various data distributions encountered in AD research and maximizes the utilization of longitudinal data. Extensive simulations and real-data applications demonstrate the LRST's performance, underscoring its potential as a valuable tool in AD clinical trials.|http://arxiv.org/abs/2410.19190v1|Xiaoming Xu,Dhrubajyoti Ghosh,Sheng Luo
425|G-computation for increasing performances of clinical trials with individual randomization and binary response|In a clinical trial, the random allocation aims to balance prognostic factors between arms, preventing true confounders. However, residual differences due to chance may introduce near-confounders. Adjusting on prognostic factors is therefore recommended, especially because the related increase of the power. In this paper, we hypothesized that G-computation associated with machine learning could be a suitable method for randomized clinical trials even with small sample sizes. It allows for flexible estimation of the outcome model, even when the covariates' relationships with outcomes are complex. Through simulations, penalized regressions (Lasso, Elasticnet) and algorithm-based methods (neural network, support vector machine, super learner) were compared. Penalized regressions reduced variance but may introduce a slight increase in bias. The associated reductions in sample size ranged from 17\% to 54\%. In contrast, algorithm-based methods, while effective for larger and more complex data structures, underestimated the standard deviation, especially with small sample sizes. In conclusion, G-computation with penalized models, particularly Elasticnet with splines when appropriate, represents a relevant approach for increasing the power of RCTs and accounting for potential near-confounders.|http://arxiv.org/abs/2411.10089v1|Joe de Keizer,Rmi Lenain,Raphal Porcher,Sarah Zoha,Arthur Chatton,Yohann Foucher
426|Fast Learning-based Registration of Sparse 3D Clinical Images|We introduce SparseVM, a method that registers clinical-quality 3D MR scans both faster and more accurately than previously possible. Deformable alignment, or registration, of clinical scans is a fundamental task for many clinical neuroscience studies. However, most registration algorithms are designed for high-resolution research-quality scans. In contrast to research-quality scans, clinical scans are often sparse, missing up to 86% of the slices available in research-quality scans. Existing methods for registering these sparse images are either inaccurate or extremely slow. We present a learning-based registration method, SparseVM, that is more accurate and orders of magnitude faster than the most accurate clinical registration methods. To our knowledge, it is the first method to use deep learning specifically tailored to registering clinical images. We demonstrate our method on a clinically-acquired MRI dataset of stroke patients and on a simulated sparse MRI dataset. Our code is available as part of the VoxelMorph package at http://voxelmorph.mit.edu/.|http://arxiv.org/abs/1812.06932v3|Kathleen M. Lewis,Natalia S. Rost,John Guttag,Adrian V. Dalca
427|Optimal Patient Allocation in Multi-Arm Clinical Trials|A multi-arm multi-stage trial is a multi-arm trial which includes interim analyses - analysing the data at certain specified points, generally discontinuing treatments which are concluded to not work and proceeding with the remainder.   It is possible that the advantages of multi-arm trials over single-arm trials may be enhanced further by considering the allocation ratio, R. For an R:1 allocation ratio, Rn patients are allocated to the control arm and n patients allocated to each active treatment arm. In this study, the optimal allocation ratio will be defined as the allocation ratio which results in the smallest total sample size satisfying some required power and probability of type I error. This is an intuitive definition in the context of clinical trials, as a smaller trial will in general be more ethical and less expensive than a larger one satisfying the same error rates. The purpose of this paper is to investigate the optimal allocation ratio in the case of multiple active treatment arms.   The setup for a single stage trial with K active treatment arms is described in Section 2, along with a brief exposition of Dunnett's statement regarding the optimal allocation ratio in such circumstances. Equations for type I error and power are derived, and the methodology used to investigate how total sample size may be minimised using allocation ratio is described. A two-stage trial is then considered, using the same methodology. Figures and tables showing how total sample size changes with allocation ratio, for a range of type I error and power values, are given in Section 3. The possible ethical and financial benefits of changing allocation ratio, including a simple example, is also included in Section 3. The results, and what they could mean in practical terms, are discussed in Section 4.|http://arxiv.org/abs/2211.06342v1|Martin Law
428|LeafAI: query generator for clinical cohort discovery rivaling a human programmer|Objective: Identifying study-eligible patients within clinical databases is a critical step in clinical research. However, accurate query design typically requires extensive technical and biomedical expertise. We sought to create a system capable of generating data model-agnostic queries while also providing novel logical reasoning capabilities for complex clinical trial eligibility criteria.   Materials and Methods: The task of query creation from eligibility criteria requires solving several text-processing problems, including named entity recognition and relation extraction, sequence-to-sequence transformation, normalization, and reasoning. We incorporated hybrid deep learning and rule-based modules for these, as well as a knowledge base of the Unified Medical Language System (UMLS) and linked ontologies. To enable data-model agnostic query creation, we introduce a novel method for tagging database schema elements using UMLS concepts. To evaluate our system, called LeafAI, we compared the capability of LeafAI to a human database programmer to identify patients who had been enrolled in 8 clinical trials conducted at our institution. We measured performance by the number of actual enrolled patients matched by generated queries.   Results: LeafAI matched a mean 43% of enrolled patients with 27,225 eligible across 8 clinical trials, compared to 27% matched and 14,587 eligible in queries by a human database programmer. The human programmer spent 26 total hours crafting queries compared to several minutes by LeafAI.   Conclusions: Our work contributes a state-of-the-art data model-agnostic query generation system capable of conditional reasoning using a knowledge base. We demonstrate that LeafAI can rival an experienced human programmer in finding patients eligible for clinical trials.|http://arxiv.org/abs/2304.06203v2|Nicholas J Dobbins,Bin Han,Weipeng Zhou,Kristine Lan,H. Nina Kim,Robert Harrington,Ozlem Uzuner,Meliha Yetisgen
429|Sex Differences in Severity and Mortality Among Patients With COVID-19: Evidence from Pooled Literature Analysis and Insights from Integrated Bioinformatic Analysis|Objective: To conduct a meta-analysis of current studies that examined sex differences in severity and mortality in patients with COVID-19, and identify potential mechanisms underpinning these differences. Methods: We performed a systematic review to collate data from observational studies examining associations of sex differences with clinical outcomes of COVID-19. PubMed, Web of Science and four preprint servers were searched for relevant studies. Data were extracted and analyzed using meta-analysis where possible, with summary data presented otherwise. Publicly available bulk RNA sequencing (RNA-seq), single-cell RNA sequencing (scRNA-seq), and chromatin immunoprecipitation sequencing (ChIP-seq) data were analyzed to explore the potential mechanisms underlying the observed association. Results: 39 studies met inclusion criteria, representing 77932 patients, of which 41510 (53.3%) were males. Men were at a markedly increased risk of developing severe cases compared with women. Furthermore, the pooled odds ratio (OR) of mortality for male group compared with the female group indicated significant higher mortality rate for male. Data from scRNA-seq suggest that men have a higher amount of ACE2-expressing pulmonary alveolar type II cells than women. Sex-based immunological differences exist. The expression of androgen receptor (AR) is positively correlated with ACE2, and there is evidence that AR may directly regulate the expression of ACE2. Conclusions: This meta-analysis detected an increased severity and mortality rate in the male populations with COVID-19, which might be attributable to the sex-based differences in cellular compositions and immunological microenvironments of the lung. The host cell receptor ACE2 is likely regulated by AR signaling pathway, which is identified as a potential target for prevention and treatment of SARS-Cov-2 infections in men.|http://arxiv.org/abs/2003.13547v1|Xiyi Wei,Yu-Tian Xiao,Jian Wang,Rui Chen,Wei Zhang,Yue Yang,Daojun Lv,Chao Qin,Di Gu,Bo Zhang,Weidong Chen,Jianquan Hou,Ninghong Song,Guohua Zeng,Shancheng Ren
430|A Semi-parametric Bayesian Approach to Population Finding with Time-to-Event and Toxicity Data in a Randomized Clinical Trial|A utility-based Bayesian population finding (BaPoFi) method was proposed by Morita and M\"uller (2017, Biometrics, 1355-1365) to analyze data from a randomized clinical trial with the aim of identifying good predictive baseline covariates for optimizing the target population for a future study. The approach casts the population finding process as a formal decision problem together with a flexible probability model using a random forest to define a regression mean function. BaPoFi is constructed to handle a single continuous or binary outcome variable. In this paper, we develop BaPoFi-TTE as an extension of the earlier approach for clinically important cases of time-to-event (TTE) data with censoring, and also accounting for a toxicity outcome. We model the association of TTE data with baseline covariates using a semi-parametric failure time model with a P\'olya tree prior for an unknown error term and a random forest for a flexible regression mean function. We define a utility function that addresses a trade-off between efficacy and toxicity as one of the important clinical considerations for population finding. We examine the operating characteristics of the proposed method in extensive simulation studies. For illustration, we apply the proposed method to data from a randomized oncology clinical trial. Concerns in a preliminary analysis of the same data based on a parametric model motivated the proposed more general approach.|http://arxiv.org/abs/1910.12174v1|Satoshi Morita,Peter Mller,Hiroyasu Abe
431|Three dimensional simulations of embolic stroke: clinical comparisons and an equation for sizing emboli from imaging|There is a need to develop Monte Carlo simulations of stroke to run in-silico trials to replace animal models, explore clinical scenarios to develop hypotheses for clinical studies and for interpreting clinical monitoring. We perform three-dimensional (3D) stroke simulations, carrying out in-silico trials to relate lesion volume to embolus diameter and calculate probabilistic lesion overlap maps, building on our previous Monte Carlo method. Simulated emboli were released into a 3D in silico vasculature, supplying gray and white matter brain volumes, to generate individual lesion estimates and probabilistic lesion overlap maps. Computer generated lesions were assessed by clinicians and compared with real world radiological images. Simulations of large single emboli reproduced similar middle cerebral artery (MCA), posterior cerebral artery (PCA) and anterior cerebral artery (ACA) lesions to those observed clinically. A proof-of-concept in-silico trial led to a conjecture relating estimated infarct volume as a percentage of total brain volume to relative embolus diameter: $\mathrm{relative diameter} = [\% \mathrm{infarct volume} / a]^{1/b}$, where $a= 104.2 \pm 0.98$, $b=3.380 \pm 0.030$. Probabilistic lesion overlap maps were created, confirming the MCA territory as the most probable resting place of emboli in the computational vasculature, followed by the PCA then ACA. The article shows proof of concept for developing a 3D stroke model from an automatically constructed vasculature.|http://arxiv.org/abs/2110.15141v2|James P. Hague,Jonathan Keelan,Lucy Beishon,David Swienton,Thompson G. Robinson,Emma M. L. Chung
432|Misclassification of Vaccination Status in Electronic Health Records: A Bayesian Approach in Cluster Randomized Trials|Misclassification in binary outcomes is not uncommon and statistical methods to investigate its impact on policy-driving study results are lacking. While misclassifying binary outcomes is a statistically ubiquitous phenomena, we focus on misclassification in a public health application: vaccinations. One such study design in public health that addresses policy is the cluster controlled randomized trial (CCRT). A CCRT that measures the impact of a novel behavioral intervention on increasing vaccine uptake can be severely biased when the supporting data are incomplete vaccination records. In particular, these vaccine records more often may be prone to negative misclassification, that is, a clinic's record of an individual patient's vaccination status may be unvaccinated when, in reality, this patient was vaccinated outside of the clinic. With large nation-wide endeavors to encourage vaccinations without a gold-standard vaccine record system, sensitivity analyses that incorporate misclassification rates are promising for robust inference. In this work we introduce a novel extension of Bayesian logistic regression where we perturb the clinic size and vaccination count with random draws from expert-elicited prior distributions. These prior distributions represent the misclassification rates for each clinic that stochastically add unvaccinated counts to the observed vaccinated counts. These prior distributions are assigned for each clinic (the first level in a group-level randomized trial). We demonstrate this method with a data application from a CCRT evaluating the influence of a behavioral intervention on vaccination uptake among U.S. veterans. A simulation study is carried out demonstrating its estimation properties.|http://arxiv.org/abs/2411.05215v1|Adam Kaplan,Collin Calvert,Bridget C. Griffith,Daniel Bertenthal,Natalie Purcell,Karen Seal,Jeffrey M. Pyne,Karen Anderson Oliver,Denise Esserman,David Nelson
433|Patient recruitment forecasting in clinical trials using time-dependent Poisson-gamma model and homogeneity testing criteria|Clinical trials in the modern era are characterized by their complexity and high costs and usually involve hundreds/thousands of patients to be recruited across multiple clinical centres in many countries, as typically a rather large sample size is required in order to prove the efficiency of a particular drug. As the imperative to recruit vast numbers of patients across multiple clinical centres has become a major challenge, an accurate forecasting of patient recruitment is one of key factors for the operational success of clinical trials. A classic Poisson-gamma (PG) recruitment model assumes time-homogeneous recruitment rates. However, there can be potential time-trends in the recruitment driven by various factors, e.g. seasonal changes, exhaustion of patients on particular treatments in some centres, etc. Recently a few authors considered some extensions of the PG model to time-dependent rates under some particular assumptions. In this paper, a natural generalization of the original PG model to a PG model with non-homogeneous time-dependent rates is introduced. It is also proposed a new analytic methodology for modelling/forecasting patient recruitment using a Poisson-gamma approximation of recruitment processes in different countries and globally. The properties of some tests on homogeneity of the rates (non-parametric one using a Poisson model and two parametric tests using Poisson and PG model) are investigated. The techniques for modeling and simulation of the recruitment using time-dependent model are discussed. For re-projection of the remaining recruitment it is proposed to use a moving window and re-estimating parameters at every interim time. The results are supported by simulation of some artificial data sets.|http://arxiv.org/abs/2411.17393v1|Volodymyr Anisimov,Lucas Oliver
434|Integrating Phase 2 into Phase 3 based on an Intermediate Endpoint While Accounting for a Cure Proportion -- with an Application to the Design of a Clinical Trial in Acute Myeloid Leukemia|For a trial with primary endpoint overall survival for a molecule with curative potential, statistical methods that rely on the proportional hazards assumption may underestimate the power and the time to final analysis. We show how a cure proportion model can be used to get the necessary number of events and appropriate timing via simulation. If Phase 1 results for the new drug are exceptional and/or the medical need in the target population is high, a Phase 3 trial might be initiated after Phase 1. Building in a futility interim analysis into such a pivotal trial may mitigate the uncertainty of moving directly to Phase 3. However, if cure is possible, overall survival might not be mature enough at the interim to support a futility decision. We propose to base this decision on an intermediate endpoint that is sufficiently associated with survival. Planning for such an interim can be interpreted as making a randomized Phase 2 trial a part of the pivotal trial: if stopped at the interim, the trial data would be analyzed and a decision on a subsequent Phase 3 trial would be made. If the trial continues at the interim then the Phase 3 trial is already underway. To select a futility boundary, a mechanistic simulation model that connects the intermediate endpoint and survival is proposed. We illustrate how this approach was used to design a pivotal randomized trial in acute myeloid leukemia, discuss historical data that informed the simulation model, and operational challenges when implementing it.|http://arxiv.org/abs/1901.01308v2|Kaspar Rufibach,Dominik Heinzmann,Annabelle Monnet
435|Power contours: optimising sample size and precision in experimental psychology and human neuroscience|When designing experimental studies with human participants, experimenters must decide how many trials each participant will complete, as well as how many participants to test. Most discussion of statistical power (the ability of a study design to detect an effect) has focussed on sample size, and assumed sufficient trials. Here we explore the influence of both factors on statistical power, represented as a two-dimensional plot on which iso-power contours can be visualised. We demonstrate the conditions under which the number of trials is particularly important, i.e. when the within-participant variance is large relative to the between-participants variance. We then derive power contour plots using existing data sets for eight experimental paradigms and methodologies (including reaction times, sensory thresholds, fMRI, MEG, and EEG), and provide example code to calculate estimates of the within- and between-participant variance for each method. In all cases, the within-participant variance was larger than the between-participants variance, meaning that the number of trials has a meaningful influence on statistical power in commonly used paradigms. An online tool is provided (https://shiny.york.ac.uk/powercontours/) for generating power contours, from which the optimal combination of trials and participants can be calculated when designing future studies.|http://arxiv.org/abs/1902.06122v5|Daniel H. Baker,Greta Vilidaite,Freya A. Lygo,Anika K. Smith,Tessa R. Flack,Andre D. Gouws,Timothy J. Andrews
436|Time-frequency analysis of event-related brain recordings: Connecting power of evoked potential and inter-trial coherence|Objective. In neuroscience, time-frequency analysis has been used to get insight into brain rhythms from brain recordings. In event-related protocols, one applies it to investigate how the brain responds to a stimulation repeated over many trials. In this framework, three measures have been considered: the amplitude of the transform for each single trial averaged across trials, avgAMP; inter-trial phase coherence, ITC; and the power of the evoked potential transform, POWavg. These three measures are sensitive to different aspects of event-related responses, ITC and POWavg sharing a common sensitivity to phase resetting phenomena. Methods. In the present manuscript, we further investigated the connection between ITC and POWavg using theoretical calculations, a simulation study and analysis of experimental data. Results. We derived exact expressions for the relationship between POWavg and ITC in the particular case of the S-transform of an oscillatory signal. In the more general case, we showed that POWavg and ITC are connected through a relationship that roughly reads $\mathrm{POWavg} \approx \mathrm{avgAMP}^2 \times \mathrm{ITC}^2$. This result was confirmed on simulations. We finally compared the theoretical prediction with results from real data. Conclusion. We showed that POWavg and ITC are related through an approximate, simple relationship that also involves avgAMP. Significance. The presented relationship between POWavg, ITC, and avgAMP confirms previous empirical evidence and provides a novel perspective to investigate evoked brain rhythms. It may provide a significant refinement to the neuroscientific toolbox for studying evoked oscillations.|http://arxiv.org/abs/2211.08811v1|Jonas Benhamou,Michel Le Van Quyen,Guillaume Marrelec
437|Unraveling Post-COVID-19 Immune Dysregulation Using Machine Learning-based Immunophenotyping|The COVID-19 pandemic has left a significant mark on global healthcare, with many individuals experiencing lingering symptoms long after recovering from the acute phase of the disease, a condition often referred to as "long COVID." This study delves into the intricate realm of immune dysregulation that ensues in 509 post-COVID-19 patients across multiple Iraqi regions during the years 2022 and 2023. Utilizing advanced machine learning techniques for immunophenotyping, this research aims to shed light on the diverse immune dysregulation patterns present in long COVID patients. By analyzing a comprehensive dataset encompassing clinical, immunological, and demographic information, the study provides valuable insights into the complex interplay of immune responses following COVID-19 infection. The findings reveal that long COVID is associated with a spectrum of immune dysregulation phenomena, including persistent inflammation, altered cytokine profiles, and abnormal immune cell subsets. These insights highlight the need for personalized interventions and tailored treatment strategies for individuals suffering from long COVID-19. This research represents a significant step forward in our understanding of the post-COVID-19 immune landscape and opens new avenues for targeted therapies and clinical management of long COVID patients. As the world grapples with the long-term implications of the pandemic, these findings offer hope for improving the quality of life for those affected by this enigmatic condition.|http://arxiv.org/abs/2310.01428v1|Maitham G. Yousif,Ghizal Fatima,Hector J. Castro,Fadhil G. Al-Amran,Salman Rawaf
438|CardioLab: Laboratory Values Estimation and Monitoring from Electrocardiogram Signals -- A Multimodal Deep Learning Approach|Background: Laboratory values are fundamental to medical diagnosis and management, but acquiring these values can be costly, invasive, and time-consuming. While electrocardiogram (ECG) patterns have been linked to certain laboratory abnormalities, the comprehensive modeling of these relationships remains underexplored.   Methods: We utilize MIMIC-IV dataset to develop multimodal deep-learning models to demonstrate the feasibility of estimating (real-time) and monitoring (predict at future intervals) laboratory value abnormalities from ECG waveforms, demographics, biometrics, and vital signs.   Results: The models exhibit a strong predictive performance with AUROC scores above 0.70 in a statistically significant manner for 23 laboratory values in the estimation setting and up to 26 values in the monitoring setting. Most notably, the accurately predictable values encompassing abnormalities across diverse physiological categories such as cardiac, renal, hematological, metabolic, immunological and coagulation. To name examples, for estimation NTproBNP (>353 pg/mL) with 0.882, whereas for monitoring at 30 minutes Urea nitrogen (<6 mg/dL) with 0.851, at 60 minutes creatinine (<0.5 mg/dL) with 0.85, and at 120 minutes hemoglobin (>17.5 g/dL) with 0.821.   Conclusions: This study provides first evidence for the feasibility of using ECG data alongside clinical routine data for the real-time estimation and monitoring of laboratory value abnormalities, which could provide a non-invasive, cost-effective supplement to traditional laboratory testing, with strong implications for enhanced patient monitoring and early intervention. Further validation could facilitate their integration into routine clinical practice.|http://arxiv.org/abs/2411.14886v1|Juan Miguel Lopez Alcaraz,Nils Strodthoff
439|Endpoints for randomized controlled clinical trials for COVID-19 treatments|Introduction: Endpoint choice for randomized controlled trials of treatments for COVID-19 is complex. A new disease brings many uncertainties, but trials must start rapidly. COVID-19 is heterogeneous, ranging from mild disease that improves within days to critical disease that can last weeks and can end in death. While improvement in mortality would provide unquestionable evidence about clinical significance of a treatment, sample sizes for a study evaluating mortality are large and may be impractical. Furthermore, patient states in between "cure" and "death" represent meaningful distinctions. Clinical severity scores have been proposed as an alternative. However, the appropriate summary measure for severity scores has been the subject of debate, particularly in relating to the uncertainty about the time-course of COVID-19. Outcomes measured at fixed time-points may risk missing the time of clinical benefit. An endpoint such as time-to-improvement (or recovery), avoids the timing problem. However, some have argued that power losses will result from reducing the ordinal scale to a binary state of "recovered" vs "not recovered."   Methods: We evaluate statistical power for possible trial endpoints for COVID-19 treatment trials using simulation models and data from two recent COVID-19 treatment trials.   Results: Power for fixed-time point methods depends heavily on the time selected for evaluation. Time-to-improvement (or recovery) analyses do not specify a time-point. Time-to-event approaches have reasonable statistical power, even when compared to a fixed time-point method evaluated at the optimal time.   Discussion: Time-to-event analyses methods have advantages in the COVID-19 setting, unless the optimal time for evaluating treatment effect is known in advance. Even when the optimal time is known, a time-to-event approach may increase power for interim analyses.|http://arxiv.org/abs/2006.10533v1|Lori E Dodd,Dean Follmann,Jing Wang,Franz Koenig,Lisa L Korn,Christian Schoergenhofer,Michael Proschan,Sally Hunsberger,Tyler Bonnett,Mat Makowski,Drifa Belhadi,Yeming Wang,Bin Cao,France Mentre,Thomas Jaki
440|Incidence of the Bertillon and Gompertz effects on the outcome of clinical trials|The accounts of medical trials provide very detailed information about the patients' health conditions. In contrast, only minimal data are usually given about demographic factors. Yet, some of these factors can have a notable impact on the overall death rate, thereby changing the outcome and conclusions of the trial. This paper focuses on two of these variables. The first is marital status; this effect, which will be referred to as the Bertillon effect, may change death rates by over 100%. The second is the age of the oldest patients; because of the exponential nature of Gompertz's law, changes in the distribution of ages in the oldest age group can have dramatic consequences on the overall number of deaths. It will be seen that randomization alone can hardly take care of these problems. Appropriate remedies are easy to formulate however. First, the marital status of patients as well as the age distribution of those over 65 should be documented for both study groups. Then, thanks to these data and based on the Bertillon and Gompertz laws, it will become possible to perform appropriate corrections. Such corrections will notably improve the reliability and accuracy of the trial's conclusions, especially for trials which include a large proportion of elderly subjects.|http://arxiv.org/abs/1306.5186v1|Bertrand M. Roehner
441|Retrospective analysis of a fatal dose-finding trial|The commonplace description of phase 1 clinical trials in oncology as "primarily concerned with safety" is belied by their near universal adoption of dose-escalation practices which are inherently unsafe. In contrast with dose titration, cohort-wise dose escalation regards patients as exchangeable, an indefensible assumption in the face of widely appreciated inter-individual heterogeneity in pharmacokinetics and pharmacodynamics (PKPD). I have previously advanced this argument in terms of a precautionary coherence principle that brings the well-known coherence notion of Cheung (2005) into contact with modern imperatives of patient-centeredness and precision dosing. Here, however, I explore these matters in some mechanistic detail by analyzing a trial of the bispecific T cell engager AFM11, in which a fatal toxicity occurred. To this end, I develop a Bayesian dose-response model for a single ordinal toxicity. By constructing this model's priors to align with the AFM11 trial as designed and conducted, I demonstrate the incompatibility of that design with any reasonable expectation of safety. Indeed, the model readily yields prospective estimates of toxic response probabilities that suggest the fatality in this trial could have been foreseen as likely.|http://arxiv.org/abs/2004.12755v1|David C. Norris
442|What Were They Thinking? Pharmacologic priors implicit in a choice of 3+3 dose-escalation design|If explicit, formal consideration of clinical pharmacology at all informs the design and conduct of modern oncology dose-finding trials, the designs themselves hardly attest to this. Yet in conducting a trial, investigators affirm that they hold reasonable expectations of participant safety - expectations that necessarily depend on beliefs about how certain pharmacologic parameters are distributed in the study population. Thus, these beliefs are implicit in a trial's presumed conformance to a community standard of safety, and may therefore to some extent be reverse-engineered from trial designs. For one popular form of dose-escalation trial design, I demonstrate here how this may be done.|http://arxiv.org/abs/2012.05301v2|David C. Norris
443|Approximating the Operating Characteristics of Bayesian Uncertainty Directed Trial Designs|Bayesian response adaptive clinical trials are currently evaluating experimental therapies for several diseases. Adaptive decisions, such as pre-planned variations of the randomization probabilities, attempt to accelerate the development of new treatments. The design of response adaptive trials, in most cases, requires time consuming simulation studies to describe operating characteristics, such as type I/II error rates, across plausible scenarios. We investigate large sample approximations of pivotal operating characteristics in Bayesian Uncertainty directed trial Designs (BUDs). A BUD trial utilizes an explicit metric u to quantify the information accrued during the study on parameters of interest, for example the treatment effects. The randomization probabilities vary during time to minimize the uncertainty summary u at completion of the study. We provide an asymptotic analysis (i) of the allocation of patients to treatment arms and (ii) of the randomization probabilities. For BUDs with outcome distributions belonging to the natural exponential family with quadratic variance function, we illustrate the asymptotic normality of the number of patients assigned to each arm and of the randomization probabilities. We use these results to approximate relevant operating characteristics such as the power of the BUD. We evaluate the accuracy of the approximations through simulations under several scenarios for binary, time-to-event and continuous outcome models.|http://arxiv.org/abs/2105.11177v1|Marta Bonsaglio,Sandra Fortini,Steffen Ventz,Lorenzo Trippa
444|Modeling synergism in early phase cancer trials with drug combination with continuous dose levels: is there an added value?|In parametric Bayesian designs of early phase cancer clinical trials with drug combinations exploring a discrete set of partially ordered doses, several authors claimed that there is no added value in including an interaction term to model synergism between the two drugs. In this paper, we investigate these claims in the setting of continuous dose levels of the two agents. Parametric models will be used to describe the relationship between the doses of the two agents and the probability of dose limiting toxicity and efficacy. Trial design proceeds by treating cohorts of two patients simultaneously receiving different dose combinations and response adaptive randomization. We compare trial safety and efficiency of the estimated maximum tolerated dose (MTD) curve between models that include an interaction term with models without the synergism parameter with extensive simulations. Under a selected class of dose-toxicity models and dose escalation algorithm, we found that not including an interaction term in the model can compromise the safety of the trial and reduce the pointwise reliability of the estimated MTD curve.|http://arxiv.org/abs/2208.05726v1|Mourad Tighiouart,Jos L. Jimnez,Marcio A. Diniz,Andr Rogatko
445|The stochastic digital human is now enrolling for in silico imaging trials -- Methods and tools for generating digital cohorts|Randomized clinical trials, while often viewed as the highest evidentiary bar by which to judge the quality of a medical intervention, are far from perfect. In silico imaging trials are computational studies that seek to ascertain the performance of a medical device by collecting this information entirely via computer simulations. The benefits of in silico trials for evaluating new technology include significant resource and time savings, minimization of subject risk, the ability to study devices that are not achievable in the physical world, allow for the rapid and effective investigation of new technologies and ensure representation from all relevant subgroups. To conduct in silico trials, digital representations of humans are needed. We review the latest developments in methods and tools for obtaining digital humans for in silico imaging studies. First, we introduce terminology and a classification of digital human models. Second, we survey available methodologies for generating digital humans with healthy and diseased status and examine briefly the role of augmentation methods. Finally, we discuss the trade-offs of four approaches for sampling digital cohorts and the associated potential for study bias with selecting specific patient distributions.|http://arxiv.org/abs/2301.08719v1|A Badano,M Lago,E Sizikova,JG Delfino,S Guan,MA Anastasio,B Sahiner
446|Frequentist analysis of basket trials with one-sample Mantel-Haenszel procedures|Recent substantial advances of molecular targeted oncology drug development is requiring new paradigms for early-phase clinical trial methodologies to enable us to evaluate efficacy of several subtypes simultaneously and efficiently. The concept of the basket trial is getting of much attention to realize this requirement borrowing information across subtypes, which are called baskets. Bayesian approach is a natural approach to this end and indeed the majority of the existing proposals relies on it. On the other hand, it required complicated modeling and may not necessarily control the type 1 error probabilities at the nominal level. In this paper, we develop a purely frequentist approach for basket trials based on one-sample Mantel-Haenszel procedure relying on a very simple idea for borrowing information under the common treatment effect assumption over baskets. We show that the proposed estimator is consistent under two limiting models of the large strata and sparse data limiting models (dually consistent) and propose dually consistent variance estimators. The proposed Mantel-Haenszel estimators are interpretable even if the common treatment assumptions are violated. Then, we can design basket trials in a confirmatory matter. We also propose an information criterion approach to identify effective subclass of baskets.|http://arxiv.org/abs/2302.08308v1|Satoshi Hattori,Satoshi Morita
447|Implementing Response-Adaptive Randomisation in Stratified Rare-disease Trials: Design Challenges and Practical Solutions|Although response-adaptive randomisation (RAR) has gained substantial attention in the literature, it still has limited use in clinical trials. Amongst other reasons, the implementation of RAR in the real world raises important practical questions, often neglected. Motivated by an innovative phase-II stratified RAR trial, this paper addresses two challenges: (1) How to ensure that RAR allocations are both desirable and faithful to target probabilities, even in small samples? and (2) What adaptations to trigger after interim analyses in the presence of missing data? We propose a Mapping strategy that discretises the randomisation probabilities into a vector of allocation ratios, resulting in improved frequentist errors. Under the implementation of Mapping, we analyse the impact of missing data on operating characteristics by examining selected scenarios. Finally, we discuss additional concerns including: pooling data across trial strata, analysing the level of blinding in the trial, and reporting safety results.|http://arxiv.org/abs/2410.03346v1|Rajenki Das,Nina Deliu,Mark Toshner,Sofa S Villar
448|A Comparative Evaluation of Bayesian Model-Assisted Two-Stage Designs for Phase I/II Clinical Trials|The primary goal of a two-stage Phase I/II trial is to identify the optimal dose for the following large-scale Phase III trial. Recently, Phase I dose-finding designs have shifted from identifying the maximum tolerated dose (MTD) to the optimal biological dose (OBD). Typically, several doses are selected as recommended Phase II doses (RP2D) for further evaluation. In Phase II dose optimization trials, each RP2D is evaluated independently to determine its "go/no-go" decision. The optimal RP2D is then chosen from the remaining RP2Ds as the recommended Phase III dose (RP3D). The effectiveness of both dose-finding and dose optimization designs at two stages impacts RP3D selection. This paper reviews and compares fifteen Bayesian model-assisted two-stage designs, combining five Phase I dose-finding designs (BOIN, TITE-BOIN, BF-BOIN, BOIN12, and TITE-BOIN12) with three Phase II dose optimization designs (TS, BOP2, and TOP). We conduct extensive simulation studies to evaluate their performance under different dose-response scenarios, with and without the existence of the OBD. Based on our results, we recommend the TITE-BOIN12 + TOP combination as the optimal two-stage design for Phase I/II trials.|http://arxiv.org/abs/2501.08410v1|Hao Sun,Jerry Li
449|Stochastic Approximation and Modern Model-Based Designs for Dose-Finding Clinical Trials|In 1951 Robbins and Monro published the seminal article on stochastic approximation and made a specific reference to its application to the "estimation of a quantal using response, nonresponse data." Since the 1990s, statistical methodology for dose-finding studies has grown into an active area of research. The dose-finding problem is at its core a percentile estimation problem and is in line with what the Robbins--Monro method sets out to solve. In this light, it is quite surprising that the dose-finding literature has developed rather independently of the older stochastic approximation literature. The fact that stochastic approximation has seldom been used in actual clinical studies stands in stark contrast with its constant application in engineering and finance. In this article, I explore similarities and differences between the dose-finding and the stochastic approximation literatures. This review also sheds light on the present and future relevance of stochastic approximation to dose-finding clinical trials. Such connections will in turn steer dose-finding methodology on a rigorous course and extend its ability to handle increasingly complex clinical situations.|http://arxiv.org/abs/1011.6240v1|Ying Kuen Cheung
450|Extracting Factual Min/Max Age Information from Clinical Trial Studies|Population age information is an essential characteristic of clinical trials. In this paper, we focus on extracting minimum and maximum (min/max) age values for the study samples from clinical research articles. Specifically, we investigate the use of a neural network model for question answering to address this information extraction task. The min/max age QA model is trained on the massive structured clinical study records from ClinicalTrials.gov. For each article, based on multiple min and max age values extracted from the QA model, we predict both actual min/max age values for the study samples and filter out non-factual age expressions. Our system improves the results over (i) a passage retrieval based IE system and (ii) a CRF-based system by a large margin when evaluated on an annotated dataset consisting of 50 research papers on smoking cessation.|http://arxiv.org/abs/1904.03262v1|Yufang Hou,Debasis Ganguly,Lea A. Deleris,Francesca Bonin
451|Implementation of Tripartite Estimands Using Adherence Causal Estimators Under the Causal Inference Framework|Intercurrent events (ICEs) and missing values are inevitable in clinical trials of any size and duration, making it difficult to assess the treatment effect for all patients in randomized clinical trials. Defining the appropriate estimand that is relevant to the clinical research question is the first step in analyzing data. The tripartite estimands, which evaluate the treatment differences in the proportion of patients with ICEs due to adverse events, the proportion of patients with ICEs due to lack of efficacy, and the primary efficacy outcome for those who can adhere to study treatment under the causal inference framework, are of interest to many stakeholders in understanding the totality of treatment effects. In this manuscript, we discuss the details of how to estimate tripartite estimands based on a causal inference framework and how to interpret tripartite estimates through a phase 3 clinical study evaluating a basal insulin treatment for patients with type 1 diabetes.|http://arxiv.org/abs/2005.14624v1|Yongming Qu,Junxiang Luo,Stephen J. Ruberg
452|A Nonparametric Method for Value Function Guided Subgroup Identification via Gradient Tree Boosting for Censored Survival Data|In randomized clinical trials with survival outcome, there has been an increasing interest in subgroup identification based on baseline genomic, proteomic markers or clinical characteristics. Some of the existing methods identify subgroups that benefit substantially from the experimental treatment by directly modeling outcomes or treatment effect. When the goal is to find an optimal treatment for a given patient rather than finding the right patient for a given treatment, methods under the individualized treatment regime framework estimate an individualized treatment rule that would lead to the best expected clinical outcome as measured by a value function. Connecting the concept of value function to subgroup identification, we propose a nonparametric method that searches for subgroup membership scores by maximizing a value function that directly reflects the subgroup-treatment interaction effect based on restricted mean survival time. A gradient tree boosting algorithm is proposed to search for the individual subgroup membership scores. We conduct simulation studies to evaluate the performance of the proposed method and an application to an AIDS clinical trial is performed for illustration.|http://arxiv.org/abs/2006.08807v1|Pingye Zhang,Junshui Ma,Xinqun Chen,Yue Shentu
453|Utilizing Win Ratio Approaches and Two-Stage Enrichment Designs for Small-Sized Clinical Trials|Conventional methods for analyzing composite endpoints in clinical trials often only focus on the time to the first occurrence of all events in the composite. Therefore, they have inherent limitations because the individual patients' first event can be the outcome of lesser clinical importance. To overcome this limitation, the concept of the win ratio (WR), which accounts for the relative priorities of the components and gives appropriate priority to the more clinically important event, was examined. For example, because mortality has a higher priority than hospitalization, it is reasonable to give a higher priority when obtaining the WR. In this paper, we evaluate three innovative WR methods (stratified matched, stratified unmatched, and unstratified unmatched) for two and multiple components under binary and survival composite endpoints. We compare these methods to traditional ones, including the Cox regression, O'Brien's rank-sum-type test, and the contingency table for controlling study Type I error rate. We also incorporate these approaches into two-stage enrichment designs with the possibility of sample size adaptations to gain efficiency for rare disease studies.|http://arxiv.org/abs/2211.14996v1|Jialu Wang,Yeh-Fong Chen,Thomas Gwise
454|Sequential knockoffs for continuous and categorical predictors: with application to a large Psoriatic Arthritis clinical trial pool|Knockoffs provide a general framework for controlling the false discovery rate when performing variable selection. Much of the Knockoffs literature focuses on theoretical challenges and we recognize a need for bringing some of the current ideas into practice. In this paper we propose a sequential algorithm for generating knockoffs when underlying data consists of both continuous and categorical (factor) variables. Further, we present a heuristic multiple knockoffs approach that offers a practical assessment of how robust the knockoff selection process is for a given data set. We conduct extensive simulations to validate performance of the proposed methodology. Finally, we demonstrate the utility of the methods on a large clinical data pool of more than $2,000$ patients with psoriatic arthritis evaluated in 4 clinical trials with an IL-17A inhibitor, secukinumab (Cosentyx), where we determine prognostic factors of a well established clinical outcome. The analyses presented in this paper could provide a wide range of applications to commonly encountered data sets in medical practice and other fields where variable selection is of particular interest.|http://arxiv.org/abs/2010.14026v1|Matthias Kormaksson,Luke J. Kelly,Xuan Zhu,Sibylle Haemmerle,Luminita Pricop,David Ohlssen
455|Optimal personalised treatment computation through in silico clinical trials on patient digital twins|In Silico Clinical Trials (ISTC), i.e., clinical experimental campaigns carried out by means of computer simulations, hold the promise to decrease time and cost for the safety and efficacy assessment of pharmacological treatments, reduce the need for animal and human testing, and enable precision medicine. In this paper we present methods and an algorithm that, by means of extensive computer simulation--based experimental campaigns (ISTC) guided by intelligent search, optimise a pharmacological treatment for an individual patient (precision medicine). e show the effectiveness of our approach on a case study involving a real pharmacological treatment, namely the downregulation phase of a complex clinical protocol for assisted reproduction in humans.|http://arxiv.org/abs/2106.10684v1|Stefano Sinisi,Vadim Alimguzhin,Toni Mancini,Enrico Tronci,Federico Mari,Brigitte Leeners
456|An automated approach to extracting positive and negative clinical research results|Failure is common in clinical trials since the successful failures presented in negative results always indicate the ways that should not be taken. In this paper, we proposed an automated approach to extracting positive and negative clinical research results by introducing a PICOE (Population, Intervention, Comparation, Outcome, and Effect) framework to represent randomized controlled trials (RCT) reports, where E indicates the effect between a specific I and O. We developed a pipeline to extract and assign the corresponding statistical effect to a specific I-O pair from natural language RCT reports. The extraction models achieved a high degree of accuracy for ICO and E descriptive words extraction through two rounds of training. By defining a threshold of p-value, we find in all Covid-19 related intervention-outcomes pairs with statistical tests, negative results account for nearly 40%. We believe that this observation is noteworthy since they are extracted from the published literature, in which there is an inherent risk of reporting bias, preferring to report positive results rather than negative results. We provided a tool to systematically understand the current level of clinical evidence by distinguishing negative results from the positive results.|http://arxiv.org/abs/2212.03464v1|Xuanyu Shi,Shiyao Xie,Wenjia Wang,Ting Chen,Jian Du
457|Estimands in Real-World Evidence Studies|A Real-World Evidence (RWE) Scientific Working Group (SWG) of the American Statistical Association Biopharmaceutical Section (ASA BIOP) has been reviewing statistical considerations for the generation of RWE to support regulatory decision-making. As part of the effort, the working group is addressing estimands in RWE studies. Constructing the right estimand -- the target of estimation -- which reflects the research question and the study objective, is one of the key components in formulating a clinical study. ICH E9(R1) describes statistical principles for constructing estimands in clinical trials with a focus on five attributes -- population, treatment, endpoints, intercurrent events, and population-level summary. However, defining estimands for clinical studies using real-world data (RWD), i.e., RWE studies, requires additional considerations due to, for example, heterogeneity of study population, complexity of treatment regimes, different types and patterns of intercurrent events, and complexities in choosing study endpoints. This paper reviews the essential components of estimands and causal inference framework, discusses considerations in constructing estimands for RWE studies, highlights similarities and differences in traditional clinical trial and RWE study estimands, and provides a roadmap for choosing appropriate estimands for RWE studies.|http://arxiv.org/abs/2307.00190v1|Jie Chen,Daniel Scharfstein,Hongwei Wang,Binbing Yu,Yang Song,Weili He,John Scott,Xiwu Lin,Hana Lee
458|An Integrated e-science Analysis Base for Computation Neuroscience Experiments and Analysis|Recent developments in data management and imaging technologies have significantly affected diagnostic and extrapolative research in the understanding of neurodegenerative diseases. However, the impact of these new technologies is largely dependent on the speed and reliability with which the medical data can be visualised, analysed and interpreted. The EUs neuGRID for Users (N4U) is a follow-on project to neuGRID, which aims to provide an integrated environment to carry out computational neuroscience experiments. This paper reports on the design and development of the N4U Analysis Base and related Information Services, which addresses existing research and practical challenges by offering an integrated medical data analysis environment with the necessary building blocks for neuroscientists to optimally exploit neuroscience workflows, large image datasets and algorithms in order to conduct analyses. The N4U Analysis Base enables such analyses by indexing and interlinking the neuroimaging and clinical study datasets stored on the N4U Grid infrastructure, algorithms and scientific workflow definitions along with their associated provenance information.|http://arxiv.org/abs/1402.5757v1|Kamran Munir,Saad Liaquat Kiani,Khawar Hasham,Richard McClatchey,Andrew Branson,Jetendr Shamdasani,the N4U Consortium
459|Quantifying efficiency gains of innovative designs of two-arm vaccine trials for COVID-19 using an epidemic simulation model|Clinical trials of a vaccine during an epidemic face particular challenges, such as the pressure to identify an effective vaccine quickly to control the epidemic, and the effect that time-space-varying infection incidence has on the power of a trial. We illustrate how the operating characteristics of different trial design elements may be evaluated using a network epidemic and trial simulation model, based on COVID-19 and individually randomised two-arm trials with a binary outcome. We show that "ring" recruitment strategies, prioritising participants at high risk of infection, can result in substantial improvement in terms of power, if sufficiently many contacts of observed cases are at high risk. In addition, we introduce a novel method to make more efficient use of the data from the earliest cases of infection observed in the trial, whose infection may have been too early to be vaccine-preventable. Finally, we compare several methods of response-adaptive randomisation, discussing their advantages and disadvantages in this two-arm context and identifying particular adaptation strategies that preserve power and estimation properties, while slightly reducing the number of infections, given an effective vaccine.|http://arxiv.org/abs/2104.00546v2|Rob Johnson,Chris Jackson,Anne Presanis,Sofia S. Villar,Daniela De Angelis
460|Designing efficient randomized trials: power and sample size calculation when using semiparametric efficient estimators|Trials enroll a large number of subjects in order to attain power, making them expensive and time-consuming. Sample size calculations are often performed with the assumption of an unadjusted analysis, even if the trial analysis plan specifies a more efficient estimator (e.g. ANCOVA). This leads to conservative estimates of required sample sizes and an opportunity for savings. Here we show that a relatively simple formula can be used to estimate the power of any two-arm, single-timepoint trial analyzed with a semiparametric efficient estimator, regardless of the domain of the outcome or kind of treatment effect (e.g. odds ratio, mean difference). Since an efficient estimator attains the minimum possible asymptotic variance, this allows for the design of trials that are as small as possible while still attaining design power and control of type I error. The required sample size calculation is parsimonious and requires the analyst to provide only a small number of population parameters. We verify in simulation that the large-sample properties of trials designed this way attain their nominal values. Lastly, we demonstrate how to use this formula in the "design" (and subsequent reanalysis) of a real clinical trial and show that fewer subjects are required to attain the same design power when a semiparametric efficient estimator is accounted for at the design stage.|http://arxiv.org/abs/2104.10784v2|Alejandro Schuler
461|Combining Covariate Adjustment with Group Sequential, Information Adaptive Designs to Improve Randomized Trial Efficiency|In clinical trials, there is potential to improve precision and reduce the required sample size by appropriately adjusting for baseline variables in the statistical analysis. This is called covariate adjustment. Despite recommendations by regulatory agencies in favor of covariate adjustment, it remains underutilized leading to inefficient trials. We address two obstacles that make it challenging to use covariate adjustment. A first obstacle is the incompatibility of many covariate adjusted estimators with commonly used boundaries in group sequential designs (GSDs). A second obstacle is the uncertainty at the design stage about how much precision gain will result from covariate adjustment. We propose a method that modifies the original estimator so that it becomes compatible with GSDs, while increasing or leaving unchanged the estimator's precision. Our approach allows the use of any asymptotically linear estimator, which covers many estimators used in randomized trials. Building on this, we propose using an information adaptive design, that is, continuing the trial until the required information level is achieved. Such a design adapts to the amount of precision gain and can lead to faster, more efficient trials, without sacrificing validity or power. We evaluate estimator performance in simulations that mimic features of a completed stroke trial.|http://arxiv.org/abs/2201.12921v3|Kelly Van Lancker,Joshua Betz,Michael Rosenblum
462|Bayesian sample size determination in basket trials borrowing information between subsets|Basket trials are increasingly used for the simultaneous evaluation of a new treatment in various patient subgroups under one overarching protocol. We propose a Bayesian approach to sample size determination in basket trials that permit borrowing of information between commensurate subsets. Specifically, we consider a randomised basket trial design where patients are randomly assigned to the new treatment or a control within each trial subset (`subtrial' for short). Closed-form sample size formulae are derived to ensure each subtrial has a specified chance of correctly deciding whether the new treatment is superior to or not better than the control by some clinically relevant difference. Given pre-specified levels of pairwise (in)commensurability, the subtrial sample sizes are solved simultaneously. The proposed Bayesian approach resembles the frequentist formulation of the problem in yielding comparable sample sizes for circumstances of no borrowing. When borrowing is enabled between commensurate subtrials, a considerably smaller trial sample size is required compared to the widely implemented approach of no borrowing. We illustrate the use of our sample size formulae with two examples based on real basket trials. A comprehensive simulation study further shows that the proposed methodology can maintain the true positive and false positive rates at desired levels.|http://arxiv.org/abs/2205.12227v1|Haiyan Zheng,Michael J. Grayling,Pavel Mozgunov,Thomas Jaki,James M. S. Wason
463|Transporting survival of an HIV clinical trial to the external target populations|Due to the heterogeneity of the randomized controlled trial (RCT) and external target populations, the estimated treatment effect from the RCT is not directly applicable to the target population. For example, the patient characteristics of the ACTG 175 HIV trial are significantly different from that of the three external target populations of interest: US early-stage HIV patients, Thailand HIV patients, and southern Ethiopia HIV patients. This paper considers several methods to transport the treatment effect from the ACTG 175 HIV trial to the target populations beyond the trial population. Most transport methods focus on continuous and binary outcomes; on the contrary, we derive and discuss several transport methods for survival outcomes: an outcome regression method based on a Cox proportional hazard (PH) model, an inverse probability weighting method based on the models for treatment assignment, sampling score, and censoring, and a doubly robust method that combines both methods, called the augmented calibration weighting (ACW) method. However, as the PH assumption was found to be incorrect for the ACTG 175 trial, the methods that depend on the PH assumption may lead to the biased quantification of the treatment effect. To account for the violation of the PH assumption, we extend the ACW method with the linear spline-based hazard regression model that does not require the PH assumption. Applying the aforementioned methods for transportability, we explore the effect of PH assumption, or the violation thereof, on transporting the survival results from the ACTG 175 trial to various external populations.|http://arxiv.org/abs/2210.02571v1|Dasom Lee,Sujit Ghosh,Shu Yang
464|Using Limited Trial Evidence to Credibly Choose Treatment Dosage when Efficacy and Adverse Effects Weakly Increase with Dose|In medical treatment and elsewhere, it has become standard to base treatment intensity (dosage) on evidence in randomized trials. Yet it has been rare to study how outcomes vary with dosage. In trials to obtain drug approval, the norm has been to specify some dose of a new drug and compare it with an established therapy or placebo. Design-based trial analysis views each trial arm as qualitatively different, but it may be highly credible to assume that efficacy and adverse effects (AEs) weakly increase with dosage. Optimization of patient care requires joint attention to both, as well as to treatment cost. This paper develops methodology to credibly use limited trial evidence to choose dosage when efficacy and AEs weakly increase with dose. I suppose that dosage is an integer choice t in (0, 1, . . . , T), T being a specified maximum dose. I study dosage choice when trial evidence on outcomes is available for only K dose levels, where K < T + 1. Then the population distribution of dose response is partially rather than point identified. The identification region is a convex polygon determined by linear equalities and inequalities. I characterize clinical and public-health decision making using the minimax-regret criterion. A simple analytical solution exists when T = 2 and computation is tractable when T is larger.|http://arxiv.org/abs/2305.17206v1|Charles F. Manski
465|Design Considerations for a Phase II platform trial in Major Depressive Disorder|Major Depressive Disorder (MDD) is one of the most common causes of disability worldwide. Unfortunately, about one-third of patients do not benefit sufficiently from available treatments and not many new drugs have been developed in this area in recent years. We thus need better and faster ways to evaluate many different treatment options quickly. Platform trials are a possible remedy - they facilitate the evaluation of more investigational treatments in a shorter period of time by sharing controls, as well as reducing clinical trial activation and recruitment times. We discuss design considerations for a platform trial in MDD, taking into account the unique disease characteristics, and present the results of extensive simulations to investigate the operating characteristics under various realistic scenarios. To allow the testing of more treatments, interim futility analyses should be performed to eliminate treatments that have either no or negligible treatment effect. Furthermore, we investigate different randomisation and allocation strategies as well as the impact of the per-treatment arm sample size. We compare the operating characteristics of such platform trials to those of traditional randomised controlled trials and highlight the potential advantages of platform trials.|http://arxiv.org/abs/2310.02080v1|Michaela Maria Freitag,Dario Zocholl,Elias Laurin Meyer,Stefan M. Gold,Marta Bofill Roig,Heidi De Smedt,Martin Posch,Franz Knig
466|A Prospective Randomized Clinical Trial for Measuring Radiology Study Reporting Time on Artificial Intelligence-Based Detection of Intracranial Hemorrhage in Emergent Care Head CT|We propose Artificial Intelligence Prospective Randomized Observer Blinding Evaluation (AI-PROBE) for quantitative clinical performance evaluation of radiology AI systems within prospective randomized clinical trials. AI-PROBE encompasses a study design and a matching radiology IT infrastructure that randomly blinds radiologists for results provided by AI-based image analysis. To demonstrate the applicability of our evaluation framework, we present a first prospective randomized clinical trial on the effect of Intra-Cranial Hemorrhage (ICH) detection in emergent care head CT on radiology study Turn-Around Time (TAT). Here, we acquired 620 non-contrast head CT scans from inpatient and emergency room patients at a large academic hospital. Following acquisition, scans were automatically analyzed for the presence of ICH using commercially available software (Aidoc, Tel Aviv, Israel). Cases identified positive for ICH by AI (ICH-AI+) were flagged in radiologists' reading worklists, where flagging was randomly switched off with probability 50%. TAT was measured as time difference between study completion and first clinically communicated reporting, with time stamps automatically retrieved from various IT systems. TATs for flagged cases (73+/-143 min) were significantly lower than TATs for non-flagged (132+/-193 min) cases (p<0.05, one-sided t-test), where 105 of 122 ICH-AI+ cases were true positive. Total sensitivity, specificity, and accuracy over all analyzed cases were 95.0%, 96.7%, and 96.4%, respectively. We conclude that automatic identification of ICH reduces TAT for ICH in emergent care head CT, which carries the potential for improving timely clinical management of ICH. Our results suggest that AI-PROBE can contribute to systematic quantitative evaluation of AI systems in clinical practice using clinically meaningful quantities, such as TAT or diagnostic accuracy.|http://arxiv.org/abs/2002.12515v1|Axel Wismller,Larry Stockmaster
467|Multiscale modelling of replicated nonstationary time series|Within the neurosciences, to observe variability across time in the dynamics of an underlying brain process is neither new nor unexpected. Wavelets are essential in analyzing brain signals because, even within a single trial, brain signals exhibit nonstationary behaviour. However, neurological signals generated within an experiment may also potentially exhibit evolution across trials (replicates). As neurologists consider localised spectra of brain signals to be most informative, here we develop a novel wavelet-based tool capable to formally represent process nonstationarities across both time and replicate dimensions. Specifically, we propose the Replicate Locally Stationary Wavelet (RLSW) process, that captures the potential nonstationary behaviour within and across trials. Estimation using wavelets gives a natural desired time- and replicate-localisation of the process dynamics. We develop the associated spectral estimation framework and establish its asymptotic properties. By means of thorough simulation studies, we demonstrate the theoretical estimator properties hold in practice. A real data investigation into the evolutionary dynamics of the hippocampus and nucleus accumbens during an associative learning experiment, demonstrate the applicability of our proposed methodology, as well as the new insights it provides.|http://arxiv.org/abs/2005.09440v1|Jonathan Embleton,Marina I. Knight,Hernando Ombao
468|A symbolic information approach to characterize response-related differences in cortical activity during a Go/No-Go task|How the brain processes information from external stimuli in order to perceive the world and act on it is one of the greatest questions in neuroscience. To address this question different time series analyzes techniques have been employed to characterize the statistical properties of brain signals during cognitive tasks. Typically response-specific processes are addressed by comparing the time course of average event-related potentials in different trials type. Here we analyze monkey Local Field Potentials data during visual pattern discrimination called Go/No-Go task in the light of information theory quantifiers. We show that the Bandt-Pompe symbolization methodology to calculate entropy and complexity of data is a useful tool to distinguish response-related differences between Go and No-Go trials. We propose to use an asymmetry index to statistically validate trial type differences. Moreover, by using the multi-scale approach and embedding time delays to downsample the data we can estimate the important time scales in which the relevant information is been processed.|http://arxiv.org/abs/2101.08905v1|Helena B. Lucas,Steven L. Bressler,Fernanda S. Matias,Osvaldo A. Rosso
469|Clinical connectivity map for drug repurposing: using laboratory tests to bridge drugs and diseases|Drug repurposing has attracted increasing attention from both the pharmaceutical industry and the research community. Many existing computational drug repurposing methods rely on preclinical data (e.g., chemical structures, drug targets), resulting in translational problems for clinical trials. In this study, we propose a clinical connectivity map framework for drug repurposing by leveraging laboratory tests to analyze complementarity between drugs and diseases. We establish clinical drug effect vectors (i.e., drug-laboratory test associations) by applying a continuous self-controlled case series model on a longitudinal electronic health record data. We establish clinical disease sign vectors (i.e., disease-laboratory test associations) by applying a Wilcoxon rank sum test on a large-scale national survey data. Finally, we compute a repurposing possibility score for each drug-disease pair by applying a dot product-based scoring function on clinical disease sign vectors and clinical drug effect vectors. We comprehensively evaluate 392 drugs for 6 important chronic diseases (e.g., asthma, coronary heart disease, type 2 diabetes, etc.). We discover not only known associations between diseases and drugs but also many hidden drug-disease associations. Moreover, we are able to explain the predicted drug-disease associations via the corresponding complementarity between laboratory tests of drug effect vectors and disease sign vectors. The proposed clinical connectivity map framework uses laboratory tests from electronic clinical information to bridge drugs and diseases, which is explainable and has better translational power than existing computational methods. Experimental results demonstrate the effectiveness of the proposed framework and suggest that our method could help identify drug repurposing opportunities, which will benefit patients by offering more effective and safer treatments.|http://arxiv.org/abs/2007.07886v2|Qianlong Wen,Ruoqi Liu,Ping Zhang
470|Predicting the clinical citation count of biomedical papers using multilayer perceptron neural network|The number of clinical citations received from clinical guidelines or clinical trials has been considered as one of the most appropriate indicators for quantifying the clinical impact of biomedical papers. Therefore, the early prediction of the clinical citation count of biomedical papers is critical to scientific activities in biomedicine, such as research evaluation, resource allocation, and clinical translation. In this study, we designed a four-layer multilayer perceptron neural network (MPNN) model to predict the clinical citation count of biomedical papers in the future by using 9,822,620 biomedical papers published from 1985 to 2005. We extracted ninety-one paper features from three dimensions as the input of the model, including twenty-one features in the paper dimension, thirty-five in the reference dimension, and thirty-five in the citing paper dimension. In each dimension, the features can be classified into three categories, i.e., the citation-related features, the clinical translation-related features, and the topic-related features. Besides, in the paper dimension, we also considered the features that have previously been demonstrated to be related to the citation counts of research papers. The results showed that the proposed MPNN model outperformed the other five baseline models, and the features in the reference dimension were the most important.|http://arxiv.org/abs/2210.06346v3|Xin Li,Xuli Tang,Qikai Cheng
471|CROCS: Clustering and Retrieval of Cardiac Signals Based on Patient Disease Class, Sex, and Age|The process of manually searching for relevant instances in, and extracting information from, clinical databases underpin a multitude of clinical tasks. Such tasks include disease diagnosis, clinical trial recruitment, and continuing medical education. This manual search-and-extract process, however, has been hampered by the growth of large-scale clinical databases and the increased prevalence of unlabelled instances. To address this challenge, we propose a supervised contrastive learning framework, CROCS, where representations of cardiac signals associated with a set of patient-specific attributes (e.g., disease class, sex, age) are attracted to learnable embeddings entitled clinical prototypes. We exploit such prototypes for both the clustering and retrieval of unlabelled cardiac signals based on multiple patient attributes. We show that CROCS outperforms the state-of-the-art method, DTC, when clustering and also retrieves relevant cardiac signals from a large database. We also show that clinical prototypes adopt a semantically meaningful arrangement based on patient attributes and thus confer a high degree of interpretability.|http://arxiv.org/abs/2011.14230v2|Dani Kiyasseh,Tingting Zhu,David A. Clifton
472|Leveraging contact network structure in the design of cluster randomized trials|Background: In settings where proof-of-principle trials have succeeded but the effectiveness of different forms of implementation remains uncertain, trials that not only generate information about intervention effects but also provide public health benefit would be useful. Cluster randomized trials (CRT) capture both direct and indirect intervention effects; the latter depends heavily on contact networks within and across clusters. We propose a novel class of connectivity-informed trial designs that leverages information about such networks in order to improve public health impact and preserve ability to detect intervention effects.   Methods: We consider CRTs in which the order of enrollment is based on the total number of ties between individuals across clusters (based either on the total number of inter-cluster connections or on connections only to untreated clusters). We include options analogous both to traditional Parallel and Stepped Wedge designs. We also allow for control clusters to be "held-back" from re-randomization for some period. We investigate the performance epidemic control and power to detect vaccine effect performance of these designs by simulating vaccination trials during an SEIR-type epidemic using a network-structured agent-based model.   Results: In our simulations, connectivity-informed designs have lower peak infectiousness than comparable traditional designs and reduce cumulative incidence by 20%, but with little impact on time to end of epidemic and reduced power to detect differences in incidence across clusters. However even a brief "holdback" period restores most of the power lost compared to traditional approaches.   Conclusion: Incorporating information about cluster connectivity in design of CRTs can increase their public health impact, especially in acute outbreak settings, with modest cost in power to detect an effective intervention.|http://arxiv.org/abs/1610.09926v1|Guy Harling,Rui Wang,Jukka-Pekka Onnela,Victor De Gruttola
473|Fusing Trial Data for Treatment Comparisons: Single versus Multi-Span Bridging|While randomized controlled trials (RCTs) are critical for establishing the efficacy of new therapies, there are limitations regarding what comparisons can be made directly from trial data. RCTs are limited to a small number of comparator arms and often compare a new therapeutic to a standard of care which has already proven efficacious. It is sometimes of interest to estimate the efficacy of the new therapy relative to a treatment that was not evaluated in the same trial, such as a placebo or an alternative therapy that was evaluated in a different trial. Such multi-study comparisons are challenging because of potential differences between trial populations that can affect the outcome. In this paper, two bridging estimators are considered that allow for comparisons of treatments evaluated in different trials using data fusion methods to account for measured differences in trial populations. A "multi-span'' estimator leverages a shared arm between two trials, while a "single-span'' estimator does not require a shared arm. A diagnostic statistic that compares the outcome in the standardized shared arms is provided. The two estimators are compared in simulations, where both estimators demonstrate minimal empirical bias and nominal confidence interval coverage when the identification assumptions are met. The estimators are applied to data from the AIDS Clinical Trials Group 320 and 388 to compare the efficacy of two-drug versus four-drug antiretroviral therapy on CD4 cell counts among persons with advanced HIV. The single-span approach requires fewer identification assumptions and was more efficient in simulations and the application.|http://arxiv.org/abs/2305.00845v1|Bonnie E. Shook-Sa,Paul N. Zivich,Samuel P. Rosin,Jessie K. Edwards,Adaora A. Adimora,Michael G. Hudgens,Stephen R. Cole
474|The R.O.A.D. to clinical trial emulation|Observational studies provide the only evidence on the effectiveness of interventions when randomized controlled trials (RCTs) are impractical due to cost, ethical concerns, or time constraints. While many methodologies aim to draw causal inferences from observational data, there is a growing trend to model observational study designs after RCTs, a strategy known as "target trial emulation." Despite its potential, causal inference through target trial emulation cannot fully address the confounding bias in real-world data due to the lack of randomization. In this work, we present a novel framework for target trial emulation that aims to overcome several key limitations, including confounding bias. The framework proceeds as follows: First, we apply the eligibility criteria of a specific trial to an observational cohort. We then "correct" this cohort by extracting a subset that matches both the distribution of covariates and the baseline prognosis of the control group in the target RCT. Next, we address unmeasured confounding by adjusting the prognosis estimates of the treated group to align with those observed in the trial. Following trial emulation, we go a step further by leveraging the emulated cohort to train optimal decision trees, to identify subgroups of patients with heterogeneity in treatment effects (HTE). The absence of confounding is verified using two external models, and the validity of the treatment recommendations is independently confirmed by the team responsible for the original trial we emulate. To our knowledge, this is the first framework to successfully address both observed and unobserved confounding, a challenge that has historically limited the use of randomized trial emulation and causal inference. Additionally, our framework holds promise in advancing precision medicine by identifying patient subgroups that benefit most from specific treatments.|http://arxiv.org/abs/2412.03528v1|Dimitris Bertsimas,Angelos G. Koulouras,Hiroshi Nagata,Carol Gao,Junki Mizusawa,Yukihide Kanemitsu,Georgios Antonios Margonis
475|Hidden Markov models for alcoholism treatment trial data|In a clinical trial of a treatment for alcoholism, a common response variable of interest is the number of alcoholic drinks consumed by each subject each day, or an ordinal version of this response, with levels corresponding to abstinence, light drinking and heavy drinking. In these trials, within-subject drinking patterns are often characterized by alternating periods of heavy drinking and abstinence. For this reason, many statistical models for time series that assume steady behavior over time and white noise errors do not fit alcohol data well. In this paper we propose to describe subjects' drinking behavior using Markov models and hidden Markov models (HMMs), which are better suited to describe processes that make sudden, rather than gradual, changes over time. We incorporate random effects into these models using a hierarchical Bayes structure to account for correlated responses within subjects over time, and we estimate the effects of covariates, including a randomized treatment, on the outcome in a novel way. We illustrate the models by fitting them to a large data set from a clinical trial of the drug Naltrexone. The HMM, in particular, fits this data well and also contains unique features that allow for useful clinical interpretations of alcohol consumption behavior.|http://arxiv.org/abs/1010.1410v1|Kenneth E. Shirley,Dylan S. Small,Kevin G. Lynch,Stephen A. Maisto,David W. Oslin
476|Small-Sample Behavior of Novel Phase I Cancer Trial Designs|Novel dose-finding designs, using estimation to assign the best estimated maximum- tolerated-dose (MTD) at each point in the experiment, most commonly via Bayesian techniques, have recently entered large-scale implementation in Phase I cancer clinical trials. We examine the small-sample behavior of these "Bayesian Phase I" (BP1) designs, and also of non-Bayesian designs sharing the same main "long-memory" traits (hereafter: LMP1s).   For all LMP1s examined, the number of cohorts treated at the true MTD (denoted here as n*) was highly variable between numerical runs drawn from the same toxicity-threshold distribution, especially when compared with "up-and-down" (U&D) short-memory designs. Further investigation using the same set of thresholds in permuted order, produced a nearly-identical magnitude of variability in n*. Therefore, this LMP1 behavior is driven by a strong sensitivity to the order in which toxicity thresholds appear in the experiment. We suggest that the sensitivity is related to LMP1's tendency to "settle" early on a specific dose level - a tendency caused by the repeated likelihood-based "winner-takes-all" dose assignment rule, which grants the early cohorts a disproportionately large influence upon experimental trajectories.   Presently, U&D designs offer a simpler and more stable alternative, with roughly equivalent MTD estimation performance. A promising direction for combining the two approaches is briefly discussed (note: the '3+3' protocol is not a U&D design).|http://arxiv.org/abs/1202.4962v2|Assaf P. Oron,Peter D. Hoff
477|Mathematical Model of Colorectal Cancer with Monoclonal Antibody Treatments|We present a new mathematical model of colorectal cancer growth and its response to monoclonal-antibody (mAb) therapy. Although promising, most mAb drugs are still in trial phases, and the possible variations in the dosing schedules of those currently approved for use have not yet been thoroughly explored. To investigate the effectiveness of current mAb treatment schedules, and to test hypothetical treatment strategies, we have created a system of nonlinear ordinary differential equations (ODE) to model colorectal cancer growth and treatment. The model includes tumor cells, elements of the host's immune response, and treatments. Model treatments include the chemotherapy agent irinotecan and one of two monoclonal antibodies - cetuximab, which is FDA-approved for colorectal cancer, and panitumumab, which is still being evaluated in clinical trials. The model incorporates patient-specific parameters to account for individual variations in immune system strength and in medication efficacy against the tumor. We have simulated outcomes for groups of virtual patients on treatment protocols for which clinical trial data are available, using a range of biologically reasonable patient-specific parameter values. Our results closely match clinical trial results for these protocols. We also simulated experimental dosing schedules, and have found new schedules which, in our simulations, reduce tumor size more effectively than current treatment schedules. Additionally, we examined the system's equilibria and sensitivity to parameter values. In the absence of treatment, tumor evolution is most affected by the intrinsic tumor growth rate and carrying capacity. When treatment is introduced, tumor growth is most affected by drug-specific PK/PD parameters.|http://arxiv.org/abs/1312.3023v1|L. G. dePillis,H. Savage,A. E. Radunskaya
478|Response adaptive designs for binary responses: how to offer patient benefit while being robust to time trends?|Response-adaptive randomisation (RAR) can considerably improve the chances of a successful treatment outcome for patients in a clinical trial by skewing the allocation probability towards better performing treatments as data accumulates. There is considerable interest in using RAR designs in drug development for rare diseases, where traditional designs are not feasible or ethically objectionable. In this paper we discuss and address a major criticism of RAR: the undesirable type I error inflation due to unknown time trends in the trial. Time trends can appear because of changes in the characteristics of recruited patients - so-called "patient drift". Patient drift is a realistic concern for clinical trials in rare diseases because these typically recruit patients over a very long period of time. We compute by simulations how large the type I error inflation is as a function of the time trend magnitude in order to determine in which contexts a potentially costly correction is actually necessary. We then assess the ability of different correction methods to preserve type I error in this context and their performance in terms of other operating characteristics, including patient benefit and power. We make recommendations of which correction methods are most suitable in the rare disease context for several RAR rules, differentiating between the two-armed and the multi-armed case. We further propose a RAR design for multi-armed clinical trials, which is computationally cheap and robust to several time trends considered.|http://arxiv.org/abs/1703.04341v1|Sofia S. Villar,Jack Bowden,James Wason
479|An information-theoretic Phase I/II design for molecularly targeted agents that does not require an assumption of monotonicity|For many years Phase I and Phase II clinical trials were conducted separately, but there was a recent shift to combine these Phases. While a variety of Phase~I/II model-based designs for cytotoxic agents were proposed in the literature, methods for molecularly targeted agents (TA) are just starting to develop. The main challenge of the TA setting is the unknown dose-efficacy relation that can have either an increasing, plateau or umbrella shape. To capture these, approaches with more parameters are needed to model the dose-efficacy relationship or, alternatively, more orderings of the dose-efficacy relationship are required to account for the uncertainty in the curve shape. As a result, designs for more complex clinical trials, for example, trials looking at schedules of a combination treatment involving TA, have not been extensively studied yet. We propose a novel regimen-finding design which is based on a derived efficacy-toxicity trade-off function. Due to its special properties, an accurate regimen selection can be achieved without any parametric or monotonicity assumptions. We illustrate how this design can be applied in the context of a complex combination-schedule clinical trial. We discuss practical and ethical issues such as coherence, delayed and missing efficacy responses, safety and futility constraints.|http://arxiv.org/abs/1803.04397v2|Pavel Mozgunov,Thomas Jaki
480|Generation of digital patients for the simulation of tuberculosis with UISS-TB|EC funded STriTuVaD project aims to test, through a phase IIb clinical trial, two of the most advanced therapeutic vaccines against tuberculosis. In parallel, we have extended the Universal Immune System Simulator to include all relevant determinants of such clinical trial, to establish its predictive accuracy against the individual patients recruited in the trial, to use it to generate digital patients and predict their response to the HRT being tested, and to combine them to the observations made on physical patients using a new in silico-augmented clinical trial approach that uses a Bayesian adaptive design. This approach, where found effective could drastically reduce the cost of innovation in this critical sector of public healthcare. One of the most challenging task is to develop a methodology to reproduce biological diversity of the subjects that have to be simulated, i.e., provide an appropriate strategy for the generation of libraries of digital patients. This has been achieved through the the creation of the initial immune system repertoire in a stochastic way, and though the identification of a "vector of features" that combines both biological and pathophysiological parameters that personalize the digital patient to reproduce the physiology and the pathophysiology of the subject.|http://arxiv.org/abs/1910.12293v1|Marzio Pennisi,Miguel A. Juarez,Giulia Russo,Marco Viceconti,Francesco Pappalardo
481|Advanced models for predicting event occurrence in event-driven clinical trials accounting for patient dropout, cure and ongoing recruitment|We consider event-driven clinical trials, where the analysis is performed once a pre-determined number of clinical events has been reached. For example, these events could be progression in oncology or a stroke in cardiovascular trials. At the interim stage, one of the main tasks is predicting the number of events over time and the time to reach specific milestones, where we need to account for events that may occur not only in patients already recruited and are followed-up but also in patients yet to be recruited. Therefore, in such trials we need to model patient recruitment and event counts together. In the paper we develop a new analytic approach which accounts for the opportunity of patients to be cured, as well as for them to dropout and be lost to follow-up. Recruitment is modelled using a Poisson-gamma model developed in previous publications. When considering the occurrence of events, we assume that the time to the main event and the time to dropout are independent random variables, and we have developed a few advanced models with cure using exponential, Weibull and log-normal distributions. This technique is supported by well developed, tested and documented software. The results are illustrated using simulation and a real dataset with reference to the developed software.|http://arxiv.org/abs/2108.09196v1|Vladimir Anisimov,Stephen Gormley,Rosalind Baverstock,Cynthia Kineza
482|A Bayesian hierarchical model for bridging across patient subgroups in phase I clinical trials with animal data|Incorporating preclinical animal data, which can be regarded as a special kind of historical data, into phase I clinical trials can improve decision making when very little about human toxicity is known. In this paper, we develop a robust hierarchical modelling approach to leverage animal data into new phase I clinical trials, where we bridge across non-overlapping, potentially heterogeneous patient subgroups. Translation parameters are used to bring both historical and contemporary data onto a common dosing scale. This leads to feasible exchangeability assumptions that the parameter vectors, which underpin the dose-toxicity relationship per study, are assumed to be drawn from a common distribution. Moreover, human dose-toxicity parameter vectors are assumed to be exchangeable either with the standardised, animal study-specific parameter vectors, or between themselves. Possibility of non-exchangeability for each parameter vector is considered to avoid inferences for extreme subgroups being overly influenced by the other. We illustrate the proposed approach with several trial data examples, and evaluate the operating characteristics of our model compared with several alternatives in a simulation study. Numerical results show that our approach yields robust inferences in circumstances, where data from multiple sources are inconsistent and/or the bridging assumptions are incorrect.|http://arxiv.org/abs/1911.05592v1|Haiyan Zheng,Lisa V. Hampson,Thomas Jaki
483|Covariate Adjustment in Randomized Clinical Trials with Missing Covariate and Outcome Data|When analyzing data from randomized clinical trials, covariate adjustment can be used to account for chance imbalance in baseline covariates and to increase precision of the treatment effect estimate. A practical barrier to covariate adjustment is the presence of missing data. In this paper, in the light of recent theoretical advancement, we first review several covariate adjustment methods with incomplete covariate data. We investigate the implications of the missing data mechanism on estimating the average treatment effect in randomized clinical trials with continuous or binary outcomes. In parallel, we consider settings where the outcome data are fully observed or are missing at random; in the latter setting, we propose a full weighting approach that combines inverse probability weighting for adjusting missing outcomes and overlap weighting for covariate adjustment. We highlight the importance of including the interaction terms between the missingness indicators and covariates as predictors in the models. We conduct comprehensive simulation studies to examine the finite-sample performance of the proposed methods and compare with a range of common alternatives. We find that conducting the proposed adjustment methods generally improves the precision of treatment effect estimates regardless of the imputation methods when the adjusted covariate is associated with the outcome. We apply the methods to the Childhood Adenotonsillectomy Trial to assess the effect of adenotonsillectomy on neurocognitive functioning scores.|http://arxiv.org/abs/2207.07890v2|Chia-Rui Chang,Yue Song,Fan Li,Rui Wang
484|Sensitivity Analyses of Clinical Trial Designs: Selecting Scenarios and Summarizing Operating Characteristics|The use of simulation-based sensitivity analyses is fundamental to evaluate and compare candidate designs for future clinical trials. In this context, sensitivity analyses are especially useful to assess the dependence of important design operating characteristics (OCs) with respect to various unknown parameters (UPs). Typical examples of OCs include the likelihood of detecting treatment effects and the average study duration, which depend on UPs that are not known until after the onset of the clinical study, such as the distributions of the primary outcomes and patient profiles. Two crucial components of sensitivity analyses are (i) the choice of a set of plausible simulation scenarios $\{\boldsymbol{\theta}_1,...,\boldsymbol{\theta}_K\}$ and (ii) the list of OCs of interest. We propose a new approach to choose the set of scenarios for inclusion in design sensitivity analyses. Our approach balances the need for simplicity and interpretability of OCs computed across several scenarios with the need to faithfully summarize -- through simulations -- how the OCs vary across all plausible values of the UPs. Our proposal also supports the selection of the number of simulation scenarios to be included in the final sensitivity analysis report. To achieve these goals, we minimize a loss function $\mathcal{L}(\boldsymbol{\theta}_1,...,\boldsymbol{\theta}_K)$ that formalizes whether a specific set of $K$ sensitivity scenarios $\{\boldsymbol{\theta}_1,...,\boldsymbol{\theta}_K\}$ is adequate to summarize how the OCs of the trial design vary across all plausible values of the UPs. Then, we use optimization techniques to select the best set of simulation scenarios to exemplify the OCs of the trial design.|http://arxiv.org/abs/2208.03887v1|Larry Han,Andrea Arfe,Lorenzo Trippa
485|Adaptive Identification of Populations with Treatment Benefit in Clinical Trials: Machine Learning Challenges and Solutions|We study the problem of adaptively identifying patient subpopulations that benefit from a given treatment during a confirmatory clinical trial. This type of adaptive clinical trial has been thoroughly studied in biostatistics, but has been allowed only limited adaptivity so far. Here, we aim to relax classical restrictions on such designs and investigate how to incorporate ideas from the recent machine learning literature on adaptive and online experimentation to make trials more flexible and efficient. We find that the unique characteristics of the subpopulation selection problem -- most importantly that (i) one is usually interested in finding subpopulations with any treatment benefit (and not necessarily the single subgroup with largest effect) given a limited budget and that (ii) effectiveness only has to be demonstrated across the subpopulation on average -- give rise to interesting challenges and new desiderata when designing algorithmic solutions. Building on these findings, we propose AdaGGI and AdaGCPI, two meta-algorithms for subpopulation construction. We empirically investigate their performance across a range of simulation scenarios and derive insights into their (dis)advantages across different settings.|http://arxiv.org/abs/2208.05844v2|Alicia Curth,Alihan Hyk,Mihaela van der Schaar
486|Enrollment Forecast for Clinical Trials at the Portfolio Planning Phase Based on Site-Level Historical Data|Accurate forecast of a clinical trial enrollment timeline at the planning phase is of great importance to both corporate strategic planning and trial operational excellence. While predictions of key milestones such as last subject first dose date can inform strategic decision-making, detailed predictive insights (e.g., median number of enrolled subjects by month for a country) can facilitate the planning of clinical trial operation activities and promote execution excellence. The naive approach often calculates an average enrollment rate from historical data and generates an inaccurate prediction based on a linear trend with the average rate. The traditional statistical approach utilizes the simple Poisson-Gamma model that assumes time-invariant site activation rates and it can fail to capture the underlying nonlinear patterns (e.g., up and down site activation pattern). We present a novel statistical approach based on generalized linear mixed-effects models and the use of non-homogeneous Poisson processes through Bayesian framework to model the country initiation, site activation and subject enrollment sequentially in a systematic fashion. We validate the performance of our proposed enrollment modeling framework based on a set of preselected 25 studies from four therapeutic areas. Our modeling framework shows a substantial improvement in prediction accuracy in comparison to the traditional statistical approach. Furthermore, we show that our modeling and simulation approach calibrates the data variability appropriately and gives correct coverage rates for prediction intervals of various nominal levels. Finally, we demonstrate the use of our approach to generate the predicted enrollment curves through time with confidence bands overlaid.|http://arxiv.org/abs/2301.01351v1|Sheng Zhong,Yunzhao Xing,Mengjia Yu,Li Wang
487|The Clinical Trials Puzzle: How Network Effects Limit Drug Discovery|The depth of knowledge offered by post-genomic medicine has carried the promise of new drugs, and cures for multiple diseases. To explore the degree to which this capability has materialized, we extract meta-data from 356,403 clinical trials spanning four decades, aiming to offer mechanistic insights into the innovation practices in drug discovery. We find that convention dominates over innovation, as over 96% of the recorded trials focus on previously tested drug targets, and the tested drugs target only 12% of the human interactome. If current patterns persist, it would take 170 years to target all druggable proteins. We uncover two network-based fundamental mechanisms that currently limit target discovery: preferential attachment, leading to the repeated exploration of previously targeted proteins; and local network effects, limiting exploration to proteins interacting with highly explored proteins. We build on these insights to develop a quantitative network-based model of drug discovery. We demonstrate that the model is able to accurately recreate the exploration patterns observed in clinical trials. Most importantly, we show that a network-based search strategy can widen the scope of drug discovery by guiding exploration to novel proteins that are part of under explored regions in the human interactome.|http://arxiv.org/abs/2301.10709v1|Kishore Vasan,Deisy Gysi,Albert-Laszlo Barabasi
488|PULSAR Effect: Revealing Potential Synergies in Combined Radiation Therapy and Immunotherapy via Differential Equations|PULSAR (personalized ultrafractionated stereotactic adaptive radiotherapy) is a form of radiotherapy method where a patient is given a large dose or pulse of radiation a couple of weeks apart rather than daily small doses. The tumor response is then monitored to determine when the subsequent pulse should be given. Pre-clinical trials have shown better tumor response in mice that received immunotherapy along with pulses spaced 10 days apart. However, this was not the case when the pulses were 1 day apart. Therefore, a synergistic effect between immunotherapy and PULSAR is observed when the pulses are spaced out by a certain number of days. In our study, we aimed to develop a mathematical model that can capture the synergistic effect by considering a time-dependent weight function that takes into account the spacing between pulses. By determining feasible parameters, and applying reasonable conditions, we utilize our model to simulate murine trials with varying sequencing of pulses. We successfully demonstrate that our model is simple to implement and can generate tumor volume data that is consistent with the pre-clinical trial data. Our model has the potential to aid in the development of clinical trials of PULSAR therapy.|http://arxiv.org/abs/2402.06101v1|Samiha Rouf,Casey Moore,Debabrata Saha,Dan Nguyen,MaryLena Bleile,Robert Timmerman,Hao Peng,Steve Jiang
489|GenTwoArmsTrialSize: An R Statistical Software Package to estimate Generalized Two Arms Randomized Clinical Trial Sample Size|The precise calculation of sample sizes is a crucial aspect in the design of clinical trials particularly for pharmaceutical statisticians. While various R statistical software packages have been developed by researchers to estimate required sample sizes under different assumptions, there has been a notable absence of a standalone R statistical software package that allows researchers to comprehensively estimate sample sizes under generalized scenarios. This paper introduces the R statistical software package "GenTwoArmsTrialSize" available on the Comprehensive R Archive Network (CRAN), designed for estimating the required sample size in two-arm clinical trials. The package incorporates four endpoint types, two trial treatment designs, four types of hypothesis tests, as well as considerations for noncompliance and loss of follow-up, providing researchers with the capability to estimate sample sizes across 24 scenarios. To facilitate understanding of the estimation process and illuminate the impact of noncompliance and loss of follow-up on the size and variability of estimations, the paper includes four hypothetical examples and one applied example. The discussion encompasses the package's limitations and outlines directions for future extensions and improvements.|http://arxiv.org/abs/2407.11342v1|Mohsen Soltanifar,Chel Hee Lee,Amin Shirazi,Martha Behnke,Ilfra Raymond-Loher,Getachew A. Dagne
490|Top-Down Multilevel Simulation of Tumor Response to Treatment in the Context of In Silico Oncology|The aim of this chapter is to provide a brief introduction into the basics of a top-down multilevel tumor dynamics modeling method primarily based on discrete entity consideration and manipulation. The method is clinically oriented, one of its major goals being to support patient individualized treatment optimization through experimentation in silico (=on the computer). Therefore, modeling of the treatment response of clinical tumors lies at the epicenter of the approach. Macroscopic data, including i.a. anatomic and metabolic tomographic images of the tumor, provide the framework for the integration of data and mechanisms pertaining to lower and lower biocomplexity levels such as clinically approved cellular and molecular biomarkers. The method also provides a powerful framework for the investigation of multilevel (multiscale) tumor biology in the generic investigational context. The Oncosimulator, a multiscale physics and biomedical engineering concept and construct tightly associated with the method and currently undergoing clinical adaptation, optimization and validation, is also sketched. A brief outline of the approach is provided in natural language. Two specific models of tumor response to chemotherapeutic and radiotherapeutic schemes are briefly outlined and indicative results are presented in order to exemplify the application potential of the method. The chapter concludes with a discussion of several important aspects of the method including i.a. numerical analysis aspects, technological issues, model extensions and validation within the framework of actual running clinico-genomic trials. Future perspectives and challenges are also addressed.|http://arxiv.org/abs/1009.2186v1|Georgios Stamatakos
491|Identifying Treatment Effects using Trimmed Means when Data are Missing Not at Random|Patients often discontinue treatment in a clinical trial because their health condition is not improving. Consequently, the patients still in the study at the end of the trial have better health outcomes on average than the initial patient population would have had if every patient had completed the trial. If we only analyze the patients who complete the trial, then this missing data problem biases the estimator of a medication's efficacy because study outcomes are missing not at random (MNAR). One way to overcome this problem - the trimmed means approach for missing data - sets missing values as slightly worse than the worst observed outcome and then trims away a fraction of the distribution from each treatment arm before calculating differences in treatment efficacy (Permutt 2017, Pharmaceutical statistics 16.1:20-28). In this paper we derive sufficient and necessary conditions for when this approach can identify the average population treatment effect in the presence of MNAR data. Numerical studies show the trimmed means approach's ability to effectively estimate treatment efficacy when data are MNAR and missingness is strongly associated with an unfavorable outcome, but trimmed means fail when data are missing at random (MAR) when the better approach would be to multiply impute the missing values. If the reasons for discontinuation in a clinical trial are known analysts can improve estimates with a combination of multiple imputation (MI) and the trimmed means approach when the assumptions of each missing data mechanism hold. When the assumptions are justifiable, using trimmed means can help identify treatment effects notwithstanding MNAR data.|http://arxiv.org/abs/1908.01044v2|Alex Ocampo,Heinz Schmidli,Peter Quarg,Francesca Callegari,Marcello Pagano
492|Model-Robust Inference for Clinical Trials that Improve Precision by Stratified Randomization and Covariate Adjustment|Two commonly used methods for improving precision and power in clinical trials are stratified randomization and covariate adjustment. However, many trials do not fully capitalize on the combined precision gains from these two methods, which can lead to wasted resources in terms of sample size and trial duration. We derive consistency and asymptotic normality of model-robust estimators that combine these two methods, and show that these estimators can lead to substantial gains in precision and power. Our theorems cover a class of estimators that handle continuous, binary, and time-to-event outcomes; missing outcomes under the missing at random assumption are handled as well. For each estimator, we give a formula for a consistent variance estimator that is model-robust and that fully captures variance reductions from stratified randomization and covariate adjustment. Also, we give the first proof (to the best of our knowledge) of consistency and asymptotic normality of the Kaplan-Meier estimator under stratified randomization, and we derive its asymptotic variance. The above results also hold for the biased-coin covariate-adaptive design. We demonstrate our results using three completed, phase 3, randomized trial data sets of treatments for substance use disorder, where the variance reduction due to stratified randomization and covariate adjustment ranges from 1% to 36%.|http://arxiv.org/abs/1910.13954v3|Bingkai Wang,Ryoko Susukida,Ramin Mojtabai,Masoumeh Amin-Esmaeili,Michael Rosenblum
493|Adaptive enrichment trial designs using joint modeling of longitudinal and time-to-event data|Adaptive enrichment allows for pre-defined patient subgroups of interest to be investigated throughout the course of a clinical trial. Many trials which measure a long-term time-to-event endpoint often also routinely collect repeated measures on biomarkers which may be predictive of the primary endpoint. Although these data may not be leveraged directly to support subgroup selection decisions and early stopping decisions, we aim to make greater use of these data to increase efficiency and improve interim decision making. In this work, we present a joint model for longitudinal and time-to-event data and two methods for creating standardised statistics based on this joint model. We can use the estimates to define enrichment rules and efficacy and futility early stopping rules for a flexible efficient clinical trial with possible enrichment. Under this framework, we show asymptotically that the familywise error rate is protected in the strong sense. To assess the results, we consider a trial for the treatment of metastatic breast cancer where repeated ctDNA measurements are available and the subgroup criteria is defined by patients' ER and HER2 status. Using simulation, we show that incorporating biomarker information leads to accurate subgroup identification and increases in power.|http://arxiv.org/abs/2301.10640v2|Abigail J. Burdon,Richard D. Baird,Thomas Jaki
494|Exact statistical analysis for response-adaptive clinical trials: a general and computationally tractable approach|Response-adaptive (RA) designs of clinical trials allow targeting a given objective by skewing the allocation of participants to treatments based on observed outcomes. RA designs face greater regulatory scrutiny due to potential type I error inflation, which limits their uptake in practice. Existing approaches to type I error control either only work for specific designs, have a risk of Monte Carlo/approximation error, are conservative, or computationally intractable. We develop a general and computationally tractable approach for exact analysis in two-arm RA designs with binary outcomes. We use the approach to construct exact tests applicable to designs that use either randomized or deterministic RA procedures, allowing for complexities such as delayed outcomes, early stopping or allocation of participants in blocks. Our efficient forward recursion implementation allows for testing of two-arm trials with 1,000 participants on a standard computer. Through an illustrative computational study of trials using randomized dynamic programming we show that, contrary to what is known for equal allocation, a conditional exact test has, almost uniformly, higher power than the unconditional test. Two real-world trials with the above-mentioned complexities are re-analyzed to demonstrate the value of our approach in controlling type I error and/or improving the statistical power.|http://arxiv.org/abs/2407.01055v1|Stef Baas,Peter Jacko,Sofa S. Villar
495|Decoding the circuitry of consciousness: from local microcircuits to brain-scale networks|Identifying the physiological processes underlying the emergence and maintenance of consciousness is one of the most fundamental problems of neuroscience, with implications ranging from fundamental neuroscience to the treatment of patients with disorders of consciousness (DOC). One major challenge is to understand how cortical circuits at drastically different spatial scales, from local networks to brain-scale networks, operate in concert to enable consciousness, and how those processes are impaired in DOC patients. In this review, we attempt to relate available neurophysiological and clinical data with existing theoretical models of consciousness, while linking the micro- and macro-circuit levels. First, we address the relationships between awareness and wakefulness on the one hand, and cortico-cortical, and thalamo-cortical connectivity on the other hand. Second, we discuss the role of three main types of GABAergic interneurons in specific circuits responsible for the dynamical re-organization of functional networks. Third, we explore advances in the functional role of nested oscillations for neural synchronization and communication, emphasizing the importance of the balance between local (high-frequency) and distant (low-frequency) activity for efficient information processing. The clinical implications of these theoretical considerations are presented. We propose that such cellular-scale mechanisms could extend current theories of consciousness.|http://arxiv.org/abs/1907.11570v1|Julien Modolo,Mahmoud Hassan,Fabrice Wendling,Pascal Benquet
496|FoME: A Foundation Model for EEG using Adaptive Temporal-Lateral Attention Scaling|Electroencephalography (EEG) is a vital tool to measure and record brain activity in neuroscience and clinical applications, yet its potential is constrained by signal heterogeneity, low signal-to-noise ratios, and limited labeled datasets. In this paper, we propose FoME (Foundation Model for EEG), a novel approach using adaptive temporal-lateral attention scaling to address above-mentioned challenges. FoME is pre-trained on a diverse 1.7TB dataset of scalp and intracranial EEG recordings, comprising 745M parameters trained for 1,096k steps. Our model introduces two key innovations: a time-frequency fusion embedding technique and an adaptive time-lateral attention scaling (ATLAS) mechanism. These components synergistically capture complex temporal and spectral EEG dynamics, enabling FoME to adapt to varying patterns across diverse data streams and facilitate robust multi-channel modeling. Evaluations across four downstream tasks demonstrate FoME's superior performance in classification and forecasting applications, consistently achieving state-of-the-art results. To conclude, FoME establishes a new paradigm for EEG analysis, offering a versatile foundation that advances brain-computer interfaces, clinical diagnostics, and cognitive research across neuroscience and related fields. Our code will be available at https://github.com/1061413241/FoME.|http://arxiv.org/abs/2409.12454v1|Enze Shi,Kui Zhao,Qilong Yuan,Jiaqi Wang,Huawen Hu,Sigang Yu,Shu Zhang
497|EEG Signal Denoising Using pix2pix GAN: Enhancing Neurological Data Analysis|Electroencephalography (EEG) is essential in neuroscience and clinical practice, yet it suffers from physiological artifacts, particularly electromyography (EMG), which distort signals. We propose a deep learning model using pix2pixGAN to remove such noise and generate reliable EEG signals. Leveraging the EEGdenoiseNet dataset, we created synthetic datasets with controlled EMG noise levels for model training and testing across a signal-to-noise ratio (SNR) from -7 to 2. Our evaluation metrics included RRMSE and Pearson's CC, assessing both time and frequency domains, and compared our model with others. The pix2pixGAN model excelled, especially under high noise conditions, showing significant improvements in lower RRMSE and higher CC values. This demonstrates the model's superior accuracy and stability in purifying EEG signals, offering a robust solution for EEG analysis challenges and advancing clinical and neuroscience applications.|http://arxiv.org/abs/2411.13288v1|Haoyi Wang,Xufang Chen,Yue Yang,Kewei Zhou,Meining Lv,Dongrui Wang,Wenjie Zhang
498|Real-Time Machine Learning Strategies for a New Kind of Neuroscience Experiments|Function and dysfunctions of neural systems are tied to the temporal evolution of neural states. The current limitations in showing their causal role stem largely from the absence of tools capable of probing the brain's internal state in real-time. This gap restricts the scope of experiments vital for advancing both fundamental and clinical neuroscience. Recent advances in real-time machine learning technologies, particularly in analyzing neural time series as nonlinear stochastic dynamical systems, are beginning to bridge this gap. These technologies enable immediate interpretation of and interaction with neural systems, offering new insights into neural computation. However, several significant challenges remain. Issues such as slow convergence rates, high-dimensional data complexities, structured noise, non-identifiability, and a general lack of inductive biases tailored for neural dynamics are key hurdles. Overcoming these challenges is crucial for the full realization of real-time neural data analysis for the causal investigation of neural computation and advanced perturbation based brain machine interfaces. In this paper, we provide a comprehensive perspective on the current state of the field, focusing on these persistent issues and outlining potential paths forward. We emphasize the importance of large-scale integrative neuroscience initiatives and the role of meta-learning in overcoming these challenges. These approaches represent promising research directions that could redefine the landscape of neuroscience experiments and brain-machine interfaces, facilitating breakthroughs in understanding brain function, and treatment of neurological disorders.|http://arxiv.org/abs/2409.01280v2|Ayesha Vermani,Matthew Dowling,Hyungju Jeon,Ian Jordan,Josue Nassar,Yves Bernaerts,Yuan Zhao,Steven Van Vaerenbergh,Il Memming Park
499|Decision SincNet: Neurocognitive models of decision making that predict cognitive processes from neural signals|Human decision making behavior is observed with choice-response time data during psychological experiments. Drift-diffusion models of this data consist of a Wiener first-passage time (WFPT) distribution and are described by cognitive parameters: drift rate, boundary separation, and starting point. These estimated parameters are of interest to neuroscientists as they can be mapped to features of cognitive processes of decision making (such as speed, caution, and bias) and related to brain activity. The observed patterns of RT also reflect the variability of cognitive processes from trial to trial mediated by neural dynamics. We adapted a SincNet-based shallow neural network architecture to fit the Drift-Diffusion model using EEG signals on every experimental trial. The model consists of a SincNet layer, a depthwise spatial convolution layer, and two separate FC layers that predict drift rate and boundary for each trial in-parallel. The SincNet layer parametrized the kernels in order to directly learn the low and high cutoff frequencies of bandpass filters that are applied to the EEG data to predict drift and boundary parameters. During training, model parameters were updated by minimizing the negative log likelihood function of WFPT distribution given trial RT. We developed separate decision SincNet models for each participant performing a two-alternative forced-choice task. Our results showed that single-trial estimates of drift and boundary performed better at predicting RTs than the median estimates in both training and test data sets, suggesting that our model can successfully use EEG features to estimate meaningful single-trial Diffusion model parameters. Furthermore, the shallow SincNet architecture identified time windows of information processing related to evidence accumulation and caution and the EEG frequency bands that reflect these processes within each participant.|http://arxiv.org/abs/2208.02845v2|Qinhua Jenny Sun,Khuong Vo,Kitty Lui,Michael Nunez,Joachim Vandekerckhove,Ramesh Srinivasan
500|Biophysical effects and neuromodulatory dose of transcranial ultrasonic stimulation|Transcranial ultrasonic stimulation (TUS) has the potential to usher in a new era for human neuroscience by allowing spatially precise and high-resolution non-invasive targeting of both deep and superficial brain regions. Currently, fundamental research on the mechanisms of interaction between ultrasound and neural tissues is progressing in parallel with application-focused research. However, a major hurdle in the wider use of TUS is the selection of optimal parameters to enable safe and effective neuromodulation in humans. In this paper, we will discuss the major factors that determine both the safety and efficacy of TUS. We will discuss the thermal and mechanical biophysical effects of ultrasound, which underlie its biological effects, in the context of their relationships with tunable parameters. Based on this knowledge of biophysical effects, and drawing on concepts from radiotherapy, we propose a framework for conceptualising TUS dose.|http://arxiv.org/abs/2406.19869v3|Tulika Nandi,Benjamin R. Kop,Kasra Naftchi-Ardebili,Charlotte J. Stagg,Kim Butts Pauly,Lennart Verhagen
501|Umbilical Cord Blood Banking and its Therapeutic Uses|Umbilical cord blood (UBC) can be viewed as the most promising source of stem cells, in which collection cost is minimal and its benefits are immense. The cord blood is used to treat malignant and nonmalignant diseases; this is due to its progenitor characteristics know as stem cells.Its properties of being, immunologically immature and high plasticity has made it superior to other sources of stem cells. The stem cells collected from cord blood have neutral differentiation capabilities which allow medical professionals to produce functional neural cells from these stem cells.Cord Blood Banking (CBB) is the storing of the umbilical cord blood which is collected immediately after the delivery of the baby. Great care and concern are needed for proper storage of these progenitor cells, hence cord blood banks come into the play, they are of 3 types which are: public, private and direct donation banks.Clinical trials are still at its very early stages having abundances to still be uncovered but results were obtained have demonstrated high potential and more scope towards effective development therapies and treatments for rare disorders.|http://arxiv.org/abs/1802.07450v1|Nivethika Sivakumaran,Imesha Rashmini Rathnayaka,Rashida Shabbir,Sasini Sandareka Wimalsinghe,J. A. Sumalimina Jayakody,Mahisha Chandrasekaran,Mawatha,Sri Lanka
502|A Bayesian Hierarchical Model for the Analysis of a Longitudinal Dynamic Contrast-Enhanced MRI Cancer Study|Imaging in clinical oncology trials provides a wealth of information that contributes to the drug development process, especially in early phase studies. This paper focuses on kinetic modeling in DCE-MRI, inspired by mixed-effects models that are frequently used in the analysis of clinical trials. Instead of summarizing each scanning session as a single kinetic parameter -- such as median $\ktrans$ across all voxels in the tumor ROI -- we propose to analyze all voxel time courses from all scans and across all subjects simultaneously in a single model. The kinetic parameters from the usual non-linear regression model are decomposed into unique components associated with factors from the longitudinal study; e.g., treatment, patient and voxel effects. A Bayesian hierarchical model provides the framework in order to construct a data model, a parameter model, as well as prior distributions. The posterior distribution of the kinetic parameters is estimated using Markov chain Monte Carlo (MCMC) methods. Hypothesis testing at the study level for an overall treatment effect is straightforward and the patient- and voxel-level parameters capture random effects that provide additional information at various levels of resolution to allow a thorough evaluation of the clinical trial. The proposed method is validated with a breast cancer study, where the subjects were imaged before and after two cycles of chemotherapy, demonstrating the clinical potential of this method to longitudinal oncology studies.|http://arxiv.org/abs/0710.4788v1|Volker J. Schmid,Brandon Whitcher,Anwar R. Padhani,N. Jane Taylor,Guang-Zhong Yang
503|Evaluation of Treatment Effect Modification by Biomarkers Measured Pre- and Post-randomization in the Presence of Non-monotone Missingness|In vaccine studies, investigators are often interested in studying effect modifiers of clinical treatment efficacy by biomarker-based principal strata, which is useful for selecting biomarker study endpoints for evaluating treatments in new trials, exploring biological mechanisms of clinical treatment efficacy, and studying mediators of clinical treatment efficacy. However, in trials where participants may enter the study with prior exposure therefore with variable baseline biomarker values, clinical treatment efficacy may depend jointly on a biomarker measured at baseline and measured at a fixed time after vaccination. Therefore, it is of interest to conduct a bivariate effect modification analysis by biomarker-based principal strata and baseline biomarker values. Previous methods allow this assessment if participants who have the biomarker measured at the the fixed time point post randomization would also have the biomarker measured at baseline. However, additional complications in study design could happen in practice. For example, in the Dengue correlates study, baseline biomarker values were only available from a fraction of participants who have biomarkers measured post-randomization. How to conduct the bivariate effect modification analysis in these studies remains an open research question. In this article, we propose an estimated likelihood method to utilize the sub-sampled baseline biomarker in the effect modification analysis and illustrate our method with datasets from two dengue phase 3 vaccine efficacy trials.|http://arxiv.org/abs/1710.09923v1|Yingying Zhuang,Ying Huang,Peter B. Gilbert
504|Drug Repurposing to find Inhibitors of SARS-CoV-2 Main Protease|Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the strain of coronavirus that causes coronavirus disease 2019 (COVID-19), the respiratory illness responsible for the COVID-19 pandemic. Currently there is no known vaccine or specific antiviral treatment for COVID-19 and so, there is an urgent need for expedite discovery of new therapeutics to combat the disease until a vaccine will be available worldwide. Drug repurposing is a strategy for identifying new uses for approved drugs that has the advantage (over conventional approaches that attempt to develop a drug from scratch) that time frame of the overall process can be significantly reduced because of the few number of clinical trial required. In this work, a virtual screening of FDA-approved drugs was performed for repositioning as potential inhibitors of the main protease Mpro of SARS-CoV-2. As a result of this study, 12 drugs are proposed as candidates for inhibitors of the Mpro enzyme. Some of the selected compounds are antiviral drugs that are already being tested in COVID-19 clinical trials (i.e. ribavirin) or are used to alleviate symptoms of the disease (i.e. codeine). Surprisingly, the most promising candidate is the naturally occurring broad spectrum antibiotic oxytetracycline. This compound has largely outperformed the remaining selected candidates along all filtering steps of our virtual screening protocol. If the activity of any of these drugs is experimentally corroborated, they could be used directly in clinical trials without the need for pre-clinical testing or safety evaluation since they are already used as drugs for other diseases.|http://arxiv.org/abs/2006.14790v1|Emilio Angelina,Sebastian Andujar,Oscar Parravicini,Daniel Enriz,Nelida Peruchena
505|A Bias Correction Method in Meta-analysis of Randomized Clinical Trials with no Adjustments for Zero-inflated Outcomes|Many clinical endpoint measures, such as the number of standard drinks consumed per week or the number of days that patients stayed in the hospital, are count data with excessive zeros. However, the zero-inflated nature of such outcomes is sometimes ignored in analyses of clinical trials. This leads to biased estimates of study-level intervention effect and, consequently, a biased estimate of the overall intervention effect in a meta-analysis. The current study proposes a novel statistical approach, the Zero-inflation Bias Correction (ZIBC) method, that can account for the bias introduced when using the Poisson regression model, despite a high rate of inflated zeros in the outcome distribution of a randomized clinical trial. This correction method only requires summary information from individual studies to correct intervention effect estimates as if they were appropriately estimated using the zero-inflated Poisson regression model, thus it is attractive for meta-analysis when individual participant-level data are not available in some studies. Simulation studies and real data analyses showed that the ZIBC method performed well in correcting zero-inflation bias in most situations.|http://arxiv.org/abs/2009.10265v3|Zhengyang Zhou,Minge Xie,David Huh,Eun-Young Mun
506|Implementation of ICH E9 (R1): a few points learned during the COVID-19 pandemic|The current COVID-19 pandemic poses numerous challenges for ongoing clinical trials and provides a stress-testing environment for the existing principles and practice of estimands in clinical trials. The pandemic may increase the rate of intercurrent events (ICEs) and missing values, spurring a great deal of discussion on amending protocols and statistical analysis plans to address these issues. In this article we revisit recent research on estimands and handling of missing values, especially the ICH E9 (R1) on Estimands and Sensitivity Analysis in Clinical Trials. Based on an in-depth discussion of the strategies for handling ICEs using a causal inference framework, we suggest some improvements in applying the estimand and estimation framework in ICH E9 (R1). Specifically, we discuss a mix of strategies allowing us to handle ICEs differentially based on reasons for ICEs. We also suggest ICEs should be handled primarily by hypothetical strategies and provide examples of different hypothetical strategies for different types of ICEs as well as a road map for estimation and sensitivity analyses. We conclude that the proposed framework helps streamline translating clinical objectives into targets of statistical inference and automatically resolves many issues with defining estimands and choosing estimation procedures arising from events such as the pandemic.|http://arxiv.org/abs/2012.10796v2|Yongming Qu,Ilya Lipkovich
507|Automated causal inference in application to randomized controlled clinical trials|Randomized controlled trials (RCTs) are considered as the gold standard for testing causal hypotheses in the clinical domain. However, the investigation of prognostic variables of patient outcome in a hypothesized cause-effect route is not feasible using standard statistical methods. Here, we propose a new automated causal inference method (AutoCI) built upon the invariant causal prediction (ICP) framework for the causal re-interpretation of clinical trial data. Compared to existing methods, we show that the proposed AutoCI allows to efficiently determine the causal variables with a clear differentiation on two real-world RCTs of endometrial cancer patients with mature outcome and extensive clinicopathological and molecular data. This is achieved via suppressing the causal probability of non-causal variables by a wide margin. In ablation studies, we further demonstrate that the assignment of causal probabilities by AutoCI remain consistent in the presence of confounders. In conclusion, these results confirm the robustness and feasibility of AutoCI for future applications in real-world clinical analysis.|http://arxiv.org/abs/2201.05773v3|Jiqing Wu,Nanda Horeweg,Marco de Bruyn,Remi A. Nout,Ina M. Jrgenliemk-Schulz,Ludy C. H. W. Lutgens,Jan J. Jobsen,Elzbieta M. van der Steen-Banasik,Hans W. Nijman,Vincent T. H. B. M. Smit,Tjalling Bosse,Carien L. Creutzberg,Viktor H. Koelzer
508|SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data|This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence Natural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2 tasks, a Natural Language Inference (NLI) task, and an evidence selection task on clinical trial data. The proposed challenges require multi-hop biomedical and numerical reasoning, which are of significant importance to the development of systems capable of large-scale interpretation and retrieval of medical evidence, to provide personalized evidence-based care.   Task 1, the entailment task, received 643 submissions from 40 participants, and Task 2, the evidence selection task, received 364 submissions from 23 participants. The tasks are challenging, with the majority of submitted systems failing to significantly outperform the majority class baseline on the entailment task, and we observe significantly better performance on the evidence selection task than on the entailment task. Increasing the number of model parameters leads to a direct increase in performance, far more significant than the effect of biomedical pre-training. Future works could explore the limitations of large models for generalization and numerical inference, and investigate methods to augment clinical datasets to allow for more rigorous testing and to facilitate fine-tuning.   We envisage that the dataset, models, and results of this task will be useful to the biomedical NLI and evidence retrieval communities. The dataset, competition leaderboard, and website are publicly available.|http://arxiv.org/abs/2305.02993v2|Mal Jullien,Marco Valentino,Hannah Frost,Paul O'Regan,Donal Landers,Andr Freitas
509|NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports|How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of 2400 statements and CTRs, annotated for these tasks. Baselines on this corpus expose the limitations of existing NLI models, with 6 state-of-the-art NLI models achieving a maximum F1 score of 0.627. To the best of our knowledge, we are the first to design a task that covers the interpretation of full CTRs. To encourage further work on this challenging dataset, we make the corpus, competition leaderboard, website and code to replicate the baseline experiments available at: https://github.com/ai-systems/nli4ct|http://arxiv.org/abs/2305.03598v3|Mal Jullien,Marco Valentino,Hannah Frost,Paul O'Regan,Donal Landers,Andr Freitas
510|Nonparametric estimation of the total treatment effect with multiple outcomes in the presence of terminal events|As standards of care advance, patients are living longer and once-fatal diseases are becoming manageable. Clinical trials increasingly focus on reducing disease burden, which can be quantified by the timing and occurrence of multiple non-fatal clinical events. Most existing methods for the analysis of multiple event-time data require stringent modeling assumptions that can be difficult to verify empirically, leading to treatment efficacy estimates that forego interpretability when the underlying assumptions are not met. Moreover, most existing methods do not appropriately account for informative terminal events, such as premature treatment discontinuation or death, which prevent the occurrence of subsequent events. To address these limitations, we derive and validate estimation and inference procedures for the area under the mean cumulative function (AUMCF), an extension of the restricted mean survival time to the multiple event-time setting. The AUMCF is nonparametric, clinically interpretable, and properly accounts for terminal competing risks. To enable covariate adjustment, we also develop an augmentation estimator that provides efficiency at least equaling, and often exceeding, the unadjusted estimator. The utility and interpretability of the AUMCF are illustrated with extensive simulation studies and through an analysis of multiple heart-failure-related endpoints using data from the Beta-Blocker Evaluation of Survival Trial (BEST) clinical trial. Our open-source R package MCC makes conducting AUMCF analyses straightforward and accessible.|http://arxiv.org/abs/2412.09304v2|Jessica Gronsbell,Zachary R. McCaw,Isabelle-Emmanuella Nogues,Xiangshan Kong,Tianxi Cai,Lu Tian,LJ Wei
511|Hi Sigma, do I have the Coronavirus?: Call for a New Artificial Intelligence Approach to Support Health Care Professionals Dealing With The COVID-19 Pandemic|Just like your phone can detect what song is playing in crowded spaces, we show that Artificial Intelligence transfer learning algorithms trained on cough phone recordings results in diagnostic tests for COVID-19. To gain adoption by the health care community, we plan to validate our results in a clinical trial and three other venues in Mexico, Spain and the USA . However, if we had data from other on-going clinical trials and volunteers, we may do much more. For example, for confirmed stay-at-home COVID-19 patients, a longitudinal audio test could be developed to determine contact-with-hospital recommendations, and for the most critical COVID-19 patients a success ratio forecast test, including patient clinical data, to prioritize ICU allocation. As a challenge to the engineering community and in the context of our clinical trial, the authors suggest distributing cough recordings daily, hoping other trials and crowdsourcing users will contribute more data. Previous approaches to complex AI tasks have either used a static dataset or were private efforts led by large corporations. All existing COVID-19 trials published also follow this paradigm. Instead, we suggest a novel open collective approach to large-scale real-time health care AI. We will be posting updates at https://opensigma.mit.edu. Our personal view is that our approach is the right one for large scale pandemics, and therefore is here to stay - will you join?|http://arxiv.org/abs/2004.06510v1|Brian Subirana,Ferran Hueto,Prithvi Rajasekaran,Jordi Laguarta,Susana Puig,Josep Malvehy,Oriol Mitja,Antoni Trilla,Carlos Ivn Moreno,Jos Francisco Muoz Valle,Ana Esther Mercado Gonzlez,Barbara Vizmanos,Sanjay Sarma
512|Joint Application of the Target Trial Causal Framework and Machine Learning Modeling to Optimize Antibiotic Therapy: Use Case on Acute Bacterial Skin and Skin Structure Infections due to Methicillin-resistant Staphylococcus aureus|Bacterial infections are responsible for high mortality worldwide. Antimicrobial resistance underlying the infection, and multifaceted patient's clinical status can hamper the correct choice of antibiotic treatment. Randomized clinical trials provide average treatment effect estimates but are not ideal for risk stratification and optimization of therapeutic choice, i.e., individualized treatment effects (ITE). Here, we leverage large-scale electronic health record data, collected from Southern US academic clinics, to emulate a clinical trial, i.e., 'target trial', and develop a machine learning model of mortality prediction and ITE estimation for patients diagnosed with acute bacterial skin and skin structure infection (ABSSSI) due to methicillin-resistant Staphylococcus aureus (MRSA). ABSSSI-MRSA is a challenging condition with reduced treatment options - vancomycin is the preferred choice, but it has non-negligible side effects. First, we use propensity score matching to emulate the trial and create a treatment randomized (vancomycin vs. other antibiotics) dataset. Next, we use this data to train various machine learning methods (including boosted/LASSO logistic regression, support vector machines, and random forest) and choose the best model in terms of area under the receiver characteristic (AUC) through bootstrap validation. Lastly, we use the models to calculate ITE and identify possible averted deaths by therapy change. The out-of-bag tests indicate that SVM and RF are the most accurate, with AUC of 81% and 78%, respectively, but BLR/LASSO is not far behind (76%). By calculating the counterfactuals using the BLR/LASSO, vancomycin increases the risk of death, but it shows a large variation (odds ratio 1.2, 95% range 0.4-3.8) and the contribution to outcome probability is modest. Instead, the RF exhibits stronger changes in ITE, suggesting more complex treatment heterogeneity.|http://arxiv.org/abs/2207.07458v1|Inyoung Jun,Simone Marini,Christina A. Boucher,J. Glenn Morris,Jiang Bian,Mattia Prosperi
513|How to improve the quality of comparisons using external control cohorts in single-arm clinical trials?|PURPOSE Providing rapid answers and early acces to patients to innovative treatments without randomized clinical trial (RCT) is growing, with benefit estimated from single-arm trials. This has become common in oncology, impacting the approval pathway of health technology assessment agencies. We aimed to provide some guidance for indirect comparison to external controls to improve the level of evidence following such uncontrolled designs. METHODS We used the illustrative example of blinatumomab, a bispecific antibody for the treatment of B-cell ALL in complete remission (CR) with persistent minimal residual disease (MRD). Its approval relied on a single-arm trial conducted in 86 adults with B-cell ALL in CR, with undetectable MRD after one cycle as the main endpoint. To maximize the validity of indirect comparisons, a 3-step process for incorporating external control data to such single-arm trial data is proposed and detailed, with emphasis on the example. RESULTS The first step includes the definition of estimand, i.e. the treatment effect reflecting the clinical question. The second step relies on the adequate selection of external controls, from previous RCT or real-world data (RWD) obtained from patient cohort, registries, or electronic patient files. The third step consists in chosing the statistical approach targeting the treatment effect of interest, either in the whole population or restricted to the single-arm trial or the external controls, and depending on the available individual-level or aggregrated external data. CONCLUSION Validity of treatment effect derived from indirect comparisons heavily depends on carefull methodological considerations that are included in the proposed 3-step procedure. Because the level of evidence of a well conducted RCT cannot be guaranteed, post-market authorization evaluation is even more important than in standard settings.|http://arxiv.org/abs/2206.09669v1|Jrme Lambert,Etienne Lengline,Raphal Porcher,Rodolphe Thibaut,Sarah Zohar,Sylvie Chevret
514|The impact of motor and non-motor symptoms fluctuations on health-related quality of life in people with functional motor disorder|Objective: To assess the effect of overall, between- and within-day subjectively rated fluctuations in motor and non-motor symptoms in people with functional motor disorder (FMD) on the health-related quality of life (HRQoL).   Background: FMD is a complex condition characterized by fluctuating motor and non-motor symptoms that may negatively impact HRQoL.   Methods: Seventy-seven patients (54 females, mean age 45.4 (SD 10.4) years) with a clinically established diagnosis of FMD, including weakness, completed symptom diaries, rating the severity of motor and non-motor symptoms (i.e., pain, fatigue, mood, cognitive difficulties) on a 10-point numerical scale three times daily for seven consecutive days. HRQoL was assessed using the SF-36 questionnaire. For the analysis, fluctuation magnitude was defined in terms of the variability in self-reported symptom scores.   Results: The mental component of SF-36 was jointly predicted by the overall severity scores (P<0.001) and overall general fluctuations (P=0.004). The physical SF-36 was found to be related only to the overall symptom severity scores (P<0.001), but not to the overall fluctuations. The assessment of the impact of different components showed that the mental component of SF-36 was significantly influenced by the combined effect of average fatigue (P<0.001), between-day cognitive symptoms fluctuations (P=0.002), and within-day mood fluctuations (P=0.015).   Conclusions: This study demonstrated the impact of self-reported symptom fluctuations across multiple motor and non-motor domains on mental but not physical HRQoL in FMD and highlighted the importance of assessing and managing fluctuations in clinical practice.|http://arxiv.org/abs/2501.01966v1|Martin Jirsek,Tom Sieger,Gabriela Chaloupkov,Lucia Novkov,Petr Sojka,Mark J Edwards,Tereza Serranov
515|Named Clinical Entity Recognition Benchmark|This technical report introduces a Named Clinical Entity Recognition Benchmark for evaluating language models in healthcare, addressing the crucial natural language processing (NLP) task of extracting structured information from clinical narratives to support applications like automated coding, clinical trial cohort identification, and clinical decision support.   The leaderboard provides a standardized platform for assessing diverse language models, including encoder and decoder architectures, on their ability to identify and classify clinical entities across multiple medical domains. A curated collection of openly available clinical datasets is utilized, encompassing entities such as diseases, symptoms, medications, procedures, and laboratory measurements. Importantly, these entities are standardized according to the Observational Medical Outcomes Partnership (OMOP) Common Data Model, ensuring consistency and interoperability across different healthcare systems and datasets, and a comprehensive evaluation of model performance. Performance of models is primarily assessed using the F1-score, and it is complemented by various assessment modes to provide comprehensive insights into model performance. The report also includes a brief analysis of models evaluated to date, highlighting observed trends and limitations.   By establishing this benchmarking framework, the leaderboard aims to promote transparency, facilitate comparative analyses, and drive innovation in clinical entity recognition tasks, addressing the need for robust evaluation methods in healthcare NLP.|http://arxiv.org/abs/2410.05046v1|Wadood M Abdul,Marco AF Pimentel,Muhammad Umar Salman,Tathagata Raha,Clment Christophe,Praveen K Kanithi,Nasir Hayat,Ronnie Rajan,Shadab Khan
516|Efficient adaptive designs with mid-course sample size adjustment in clinical trials|Adaptive designs have been proposed for clinical trials in which the nuisance parameters or alternative of interest are unknown or likely to be misspecified before the trial. Whereas most previous works on adaptive designs and mid-course sample size re-estimation have focused on two-stage or group sequential designs in the normal case, we consider here a new approach that involves at most three stages and is developed in the general framework of multiparameter exponential families. Not only does this approach maintain the prescribed type I error probability, but it also provides a simple but asymptotically efficient sequential test whose finite-sample performance, measured in terms of the expected sample size and power functions, is shown to be comparable to the optimal sequential design, determined by dynamic programming, in the simplified normal mean case with known variance and prespecified alternative, and superior to the existing two-stage designs and also to adaptive group sequential designs when the alternative or nuisance parameters are unknown or misspecified.|http://arxiv.org/abs/1105.3280v1|Jay Bartroff,Tze Leung Lai
517|Bayesian Decision-optimal Interval Designs for Phase I Clinical Trials|Interval designs are a class of phase I trial designs for which the decision of dose assignment is determined by comparing the observed toxicity rate at the current dose with a prespecified (toxicity tolerance) interval. If the observed toxicity rate is located within the interval, we retain the current dose; if the observed toxicity rate is greater than the upper boundary of the interval, we deescalate the dose; and if the observed toxicity rate is smaller than the lower boundary of the interval, we escalate the dose. The most critical issue for the interval design is choosing an appropriate interval so that the design has good operating characteristics. By casting dose finding as a Bayesian decision-making problem, we propose new flexible methods to select the interval boundaries so as to minimize the probability of inappropriate dose assignment for patients. We show, both theoretically and numerically, that the resulting optimal interval designs not only have desirable finite- and large-sample properties, but also are particularly easy to implement in practice. Compared to existing designs, the proposed (local) optimal design has comparable average performance, but a lower risk of yielding a poorly performing clinical trial.|http://arxiv.org/abs/1309.5019v1|Suyu Liu,Ying Yuan
518|Adaptive Survival Trials|Mid-study design modifications are becoming increasingly accepted in confirmatory clinical trials, so long as appropriate methods are applied such that error rates are controlled. It is therefore unfortunate that the important case of time-to-event endpoints is not easily handled by the standard theory. We analyze current methods that allow design modifications to be based on the full interim data, i.e., not only the observed event times but also secondary endpoint and safety data from patients who are yet to have an event. We show that the final test statistic may ignore a substantial subset of the observed event times. Since it is the data corresponding to the earliest recruited patients that is ignored, this neglect becomes egregious when there is specific interest in learning about long-term survival. An alternative test incorporating all event times is proposed, where a conservative assumption is made in order to guarantee type I error control. We examine the properties of our proposed approach using the example of a clinical trial comparing two cancer therapies.|http://arxiv.org/abs/1405.1569v1|Dominic Magirr,Thomas Jaki,Franz Koenig,Martin Posch
519|There is Individualized Treatment. Why Not Individualized Inference?|Doctors use statistics to advance medical knowledge; we use a medical analogy to introduce statistical inference "from scratch" and to highlight an improvement. Your doctor, perhaps implicitly, predicts the effectiveness of a treatment for you based on its performance in a clinical trial; the trial patients serve as controls for you. The same logic underpins statistical inference: to identify the best statistical procedure to use for a problem, we simulate a set of control problems and evaluate candidate procedures on the controls. Now for the improvement: recent interest in personalized/individualized medicine stems from the recognition that some clinical trial patients are better controls for you than others. Therefore, treatment decisions for you should depend only on a subset of relevant patients. Individualized statistical inference implements this idea for control problems (rather than patients). Its potential for improving data analysis matches personalized medicine's for improving healthcare. The central issue--for both individualized medicine and individualized inference--is how to make the right relevance robustness trade-off: if we exercise too much judgement in determining which controls are relevant, our inferences will not be robust. How much is too much? We argue that the unknown answer is the Holy Grail of statistical inference.|http://arxiv.org/abs/1510.08539v1|Keli Liu,Xiao-Li Meng
520|Blinded continuous information monitoring of recurrent event endpoints with time trends in clinical trials|Blinded sample size re-estimation and information monitoring based on blinded data has been suggested to mitigate risks due to planning uncertainties regarding nuisance parameters. Motivated by a randomized controlled trial in pediatric multiple sclerosis (MS), a continuous monitoring procedure for overdispersed count data was proposed recently. However, this procedure assumed constant event rates, an assumption often not met in practice. Here we extend the procedure to accommodate time trends in the event rates considering two blinded approaches: (a) the mixture approach modeling the number of events by a mixture of two negative binomial distributions, and (b) the lumping approach approximating the marginal distribution of the event counts by a negative binomial distribution. Through simulations the operating characteristics of the proposed procedures are investigated under decreasing event rates. We find that the type I error rate is not inflated relevantly by either of the monitoring procedures, with the exception of strong time dependencies where the procedure assuming constant rates exhibits some inflation. Furthermore, the procedure accommodating time trends has generally favorable power properties compared to the procedure based on constant rates which stops often too late. The proposed method is illustrated by the clinical trial in pediatric MS.|http://arxiv.org/abs/1903.02538v1|Tobias Mtze,Susanna Salem,Norbert Benda,Heinz Schmidli,Tim Friede
521|On Multi-Armed Bandit Designs for Dose-Finding Clinical Trials|We study the problem of finding the optimal dosage in early stage clinical trials through the multi-armed bandit lens. We advocate the use of the Thompson Sampling principle, a flexible algorithm that can accommodate different types of monotonicity assumptions on the toxicity and efficacy of the doses. For the simplest version of Thompson Sampling, based on a uniform prior distribution for each dose, we provide finite-time upper bounds on the number of sub-optimal dose selections, which is unprecedented for dose-finding algorithms. Through a large simulation study, we then show that variants of Thompson Sampling based on more sophisticated prior distributions outperform state-of-the-art dose identification algorithms in different types of dose-finding studies that occur in phase I or phase I/II trials.|http://arxiv.org/abs/1903.07082v2|Maryam Aziz,Emilie Kaufmann,Marie-Karelle Riviere
522|Inferring Which Medical Treatments Work from Reports of Clinical Trials|How do we know if a particular medical treatment actually works? Ideally one would consult all available evidence from relevant clinical trials. Unfortunately, such results are primarily disseminated in natural language scientific articles, imposing substantial burden on those trying to make sense of them. In this paper, we present a new task and corpus for making this unstructured evidence actionable. The task entails inferring reported findings from a full-text article describing a randomized controlled trial (RCT) with respect to a given intervention, comparator, and outcome of interest, e.g., inferring if an article provides evidence supporting the use of aspirin to reduce risk of stroke, as compared to placebo.   We present a new corpus for this task comprising 10,000+ prompts coupled with full-text articles describing RCTs. Results using a suite of models --- ranging from heuristic (rule-based) approaches to attentive neural architectures --- demonstrate the difficulty of the task, which we believe largely owes to the lengthy, technical input texts. To facilitate further work on this important, challenging problem we make the corpus, documentation, a website and leaderboard, and code for baselines and evaluation available at http://evidence-inference.ebm-nlp.com/.|http://arxiv.org/abs/1904.01606v2|Eric Lehman,Jay DeYoung,Regina Barzilay,Byron C. Wallace
523|Bayesian models for survival data of clinical trials: Comparison of implementations using R software|Objective: To provide guidance for the use of the main functions available in R for performing post hoc Bayesian analysis of a randomized clinical trial with a survival endpoint using proportional hazard models. Study Design and Setting: Data derived from the ALLOZITHRO trial, conducted with 465 patients after allograft to prevent pulmonary complications and allocated between azithromycin and placebo; airflow decline-free survival at 2 years after randomization was the main endpoint. Results: Despite heterogeneity in modeling assumptions, in particular for the baseline hazard (parametric or nonparametric), and in estimation methods, Bayesian posterior mean hazard ratio (HR) estimates of azithromycin effect were close to those obtained by the maximum likelihood approach. Conclusion: Bayesian models can be implemented using various R packages, providing results in close agreement with the maximum likelihood estimates. These models provide probabilistic statements that could not be obtained otherwise.|http://arxiv.org/abs/1908.06687v3|Lucie Biard,Anne Bergeron,Sylvie Chevret
524|Identification of causal intervention effects under contagion|Defining and identifying causal intervention effects for transmissible infectious disease outcomes is challenging because a treatment -- such as a vaccine -- given to one individual may affect the infection outcomes of others. Epidemiologists have proposed causal estimands to quantify effects of interventions under contagion using a two-person partnership model. These simple conceptual models have helped researchers develop causal estimands relevant to clinical evaluation of vaccine effects. However, many of these partnership models are formulated under structural assumptions that preclude realistic infectious disease transmission dynamics, limiting their conceptual usefulness in defining and identifying causal treatment effects in empirical intervention trials. In this paper, we propose causal intervention effects in two-person partnerships under arbitrary infectious disease transmission dynamics, and give nonparametric identification results showing how effects can be estimated in empirical trials using time-to-infection or binary outcome data. The key insight is that contagion is a causal phenomenon that induces conditional independencies on infection outcomes that can be exploited for the identification of clinically meaningful causal estimands. These new estimands are compared to existing quantities, and results are illustrated using a realistic simulation of an HIV vaccine trial.|http://arxiv.org/abs/1912.04151v2|Xiaoxuan Cai,Wen Wei Loh,Forrest W. Crawford
525|Using public clinical trial reports to evaluate observational study methods|Observational studies are valuable for estimating the effects of various medical interventions, but are notoriously difficult to evaluate because the methods used in observational studies require many untestable assumptions. This lack of verifiability makes it difficult both to compare different observational study methods and to trust the results of any particular observational study. In this work, we propose TrialVerify, a new approach for evaluating observational study methods based on ground truth sourced from clinical trial reports. We process trial reports into a denoised collection of known causal relationships that can then be used to estimate the precision and recall of various observational study methods. We then use TrialVerify to evaluate multiple observational study methods in terms of their ability to identify the known causal relationships from a large national insurance claims dataset. We found that inverse propensity score weighting is an effective approach for accurately reproducing known causal relationships and outperforms other observational study methods. TrialVerify is made freely available for others to evaluate observational study methods.|http://arxiv.org/abs/2006.14102v3|Ethan Steinberg,Nikolaos Ignatiadis,Steve Yadlowsky,Yizhe Xu,Nigam H. Shah
526|Causal inference methods for small non-randomized studies: Methods and an application in COVID-19|The usual development cycles are too slow for the development of vaccines, diagnostics and treatments in pandemics such as the ongoing SARS-CoV-2 pandemic. Given the pressure in such a situation, there is a risk that findings of early clinical trials are overinterpreted despite their limitations in terms of size and design. Motivated by a non-randomized open-label study investigating the efficacy of hydroxychloroquine in patients with COVID-19, we describe in a unified fashion various alternative approaches to the analysis of non-randomized studies. A widely used tool to reduce the impact of treatment-selection bias are so-called propensity score (PS) methods. Conditioning on the propensity score allows one to replicate the design of a randomized controlled trial, conditional on observed covariates. Extensions include the g-computation approach, which is less frequently applied, in particular in clinical studies. Moreover, doubly robust estimators provide additional advantages. Here, we investigate the properties of propensity score based methods including three variations of doubly robust estimators in small sample settings, typical for early trials, in a simulation study. R code for the simulations is provided.|http://arxiv.org/abs/2007.15991v2|Sarah Friedrich,Tim Friede
527|Sample Size Calculation for Cluster Randomized Trials with Zero-inflated Count Outcomes|Cluster randomized trails (CRT) have been widely employed in medical and public health research. Many clinical count outcomes, such as the number of falls in nursing homes, exhibit excessive zero values. In the presence of zero inflation, traditional power analysis methods for count data based on Poisson or negative binomial distribution may be inadequate. In this study, we present a sample size method for CRTs with zero-inflated count outcomes. It is developed based on GEE regression directly modeling the marginal mean of a ZIP outcome, which avoids the challenge of testing two intervention effects under traditional modeling approaches. A closed-form sample size formula is derived which properly accounts for zero inflation, ICCs due to clustering, unbalanced randomization, and variability in cluster size. Robust approaches, including t-distribution-based approximation and Jackknife re-sampling variance estimator, are employed to enhance trial properties under small sample sizes. Extensive simulations are conducted to evaluate the performance of the proposed method. An application example is presented in a real clinical trial setting.|http://arxiv.org/abs/2009.10117v1|Zhengyang Zhou,Dateng Li,Song Zhang
528|A Novel Statistical Test for Treatment Differences in Clinical Trials using a Response Adaptive Forward Looking Gittins Index Rule|The most common objective for response adaptive clinical trials is to seek to ensure that patients within a trial have a high chance of receiving the best treatment available by altering the chance of allocation on the basis of accumulating data. Approaches which yield good patient benefit properties suffer from low power from a frequentist perspective when testing for a treatment difference at the end of the study due to the high imbalance in treatment allocations. In this work we develop an alternative pairwise test for treatment difference on the basis of allocation probabilities of the covariate-adjusted response-adaptive randomization with forward looking Gittins index rule (CARA-FLGI). The performance of the novel test is evaluated in simulations for two-armed studies and then its applications to multi-armed studies is illustrated. The proposed test has markedly improved power over the traditional Fisher exact test when this class of non-myopic response adaptation is used. We also find that the test's power is close to the power of a Fisher exact test under equal randomization.|http://arxiv.org/abs/2011.03270v1|Helen Yvette Barnett,Sofia S Villar,Helena Geys,Thomas Jaki
529|A liberal type I error rate for studies in precision medicine|We introduce a new multiple type I error criterion for clinical trials with multiple populations. Such trials are of interest in precision medicine where the goal is to develop treatments that are targeted to specific sub-populations defined by genetic and/or clinical biomarkers. The new criterion is based on the observation that not all type I errors are relevant to all patients in the overall population. If disjoint sub-populations are considered, no multiplicity adjustment appears necessary, since a claim in one sub-population does not affect patients in the other ones. For intersecting sub-populations we suggest to control the average multiple type error rate, i.e. the probably that a randomly selected patient will be exposed to an inefficient treatment. We call this the population-wise error rate, exemplify it by a number of examples and illustrate how to control it with an adjustment of critical boundaries or adjusted p-values. We furthermore define corresponding simultaneous confidence intervals. We finally illustrate the power gain achieved by passing from family-wise to population-wise error rate control with two simple examples and a recently suggest multiple testing approach for umbrella trials.|http://arxiv.org/abs/2011.04766v2|Werner Brannath,Charlie Hillner,Kornelius Rohmeyer
530|Unit Information Prior for Adaptive Information Borrowing from Multiple Historical Datasets|In clinical trials, there often exist multiple historical studies for the same or related treatment investigated in the current trial. Incorporating historical data in the analysis of the current study is of great importance, as it can help to gain more information, improve efficiency, and provide a more comprehensive evaluation of treatment. Enlightened by the unit information prior (UIP) concept in the reference Bayesian test, we propose a new informative prior called UIP from an information perspective that can adaptively borrow information from multiple historical datasets. We consider both binary and continuous data and also extend the new UIP methods to linear regression settings. Extensive simulation studies demonstrate that our method is comparable to other commonly used informative priors, while the interpretation of UIP is intuitive and its implementation is relatively easy. One distinctive feature of UIP is that its construction only requires summary statistics commonly reported in the literature rather than the patient-level data. By applying our UIP methods to phase III clinical trials for investigating the efficacy of memantine in Alzheimer's disease, we illustrate its ability of adaptively borrowing information from multiple historical datasets in the real application.|http://arxiv.org/abs/2102.00796v1|Huaqing Jin,Guosheng Yin
531|Optimal designs for the development of personalized treatment rules|We study the design of multi-armed parallel group clinical trials to estimate personalized treatment rules that identify the best treatment for a given patient with given covariates. Assuming that the outcomes in each treatment arm are given by a homoscedastic linear model, with possibly different variances between treatment arms, and that the trial subjects form a random sample from an unselected overall population, we optimize the (possibly randomized) treatment allocation allowing the allocation rates to depend on the covariates. We find that, for the case of two treatments, the approximately optimal allocation rule does not depend on the value of the covariates but only on the variances of the responses. In contrast, for the case of three treatments or more, the optimal treatment allocation does depend on the values of the covariates as well as the true regression coefficients. The methods are illustrated with a recently published dietary clinical trial.|http://arxiv.org/abs/2102.07093v2|David Azriel,Yosef Rinott,Martin Posch
532|Surrogacy Validation for Time-to-Event Outcomes with Illness-Death Frailty Models|A common practice in clinical trials is to evaluate a treatment effect on an intermediate endpoint when the true outcome of interest would be difficult or costly to measure. We consider how to validate intermediate endpoints in a causally-valid way when the trial outcomes are time-to-event. Using counterfactual outcomes, those that would be observed if the counterfactual treatment had been given, the causal association paradigm assesses the relationship of the treatment effect on the surrogate $S$ with the treatment effect on the true endpoint $T$. In particular, we propose illness death models to accommodate the censored and semi-competing risk structure of survival data. The proposed causal version of these models involves estimable and counterfactual frailty terms. Via these multi-state models, we characterize what a valid surrogate would look like using a causal effect predictiveness plot. We evaluate the estimation properties of a Bayesian method using Markov Chain Monte Carlo and assess the sensitivity of our model assumptions. Our motivating data source is a localized prostate cancer clinical trial where the two survival endpoints are time to distant metastasis and time to death.|http://arxiv.org/abs/2211.15826v1|Emily K. Roberts,Michael R. Elliott,Jeremy M. G. Taylor
533|Semiparametric Inference of the Complier Average Causal Effect with Nonignorable Missing Outcomes|Noncompliance and missing data often occur in randomized trials, which complicate the inference of causal effects. When both noncompliance and missing data are present, previous papers proposed moment and maximum likelihood estimators for binary and normally distributed continuous outcomes under the latent ignorable missing data mechanism. However, the latent ignorable missing data mechanism may be violated in practice, because the missing data mechanism may depend directly on the missing outcome itself. Under noncompliance and an outcome-dependent nonignorable missing data mechanism, previous studies showed the identifiability of complier average causal effect for discrete outcomes. In this paper, we study the semiparametric identifiability and estimation of complier average causal effect in randomized clinical trials with both all-or-none noncompliance and the outcome-dependent nonignorable missing continuous outcomes, and propose a two-step maximum likelihood estimator in order to eliminate the infinite dimensional nuisance parameter. Our method does not need to specify a parametric form for the missing data mechanism. We also evaluate the finite sample property of our method via extensive simulation studies and sensitivity analysis, with an application to a double-blinded psychiatric clinical trial.|http://arxiv.org/abs/1409.0895v1|Hua Chen,Peng Ding,Zhi Geng,Xiao-Hua Zhou
534|Deep Neural Networks Guided Ensemble Learning for Point Estimation|In modern statistics, interests shift from pursuing the uniformly minimum variance unbiased estimator to reducing mean squared error (MSE) or residual squared error. Shrinkage based estimation and regression methods offer better prediction accuracy and improved interpretation. However, the characterization of such optimal statistics in terms of minimizing MSE remains open and challenging in many problems, for example estimating treatment effect in adaptive clinical trials with pre-planned modifications to design aspects based on accumulated data. From an alternative perspective, we propose a deep neural network based automatic method to construct an improved estimator from existing ones. Theoretical properties are studied to provide guidance on applicability of our estimator to seek potential improvement. Simulation studies demonstrate that the proposed method has considerable finite-sample efficiency gain as compared with several common estimators. In the Adaptive COVID-19 Treatment Trial (ACTT) as an important application, our ensemble estimator essentially contributes to a more ethical and efficient adaptive clinical trial with fewer patients enrolled. The proposed framework can be generally applied to various statistical problems, and can be served as a reference measure to guide statistical research.|http://arxiv.org/abs/2105.06523v3|Tianyu Zhan,Haoda Fu,Jian Kang
535|Comparison of Time-to-First-Event and Recurrent Event Methods in Multiple Sclerosis Trials|Suppression of disability progression is an important goal in the treatment of multiple sclerosis (MS). Randomized clinical trials in MS frequently use the time to the first confirmed disability progression (CDP) on the ordinal Expanded Disability Status Scale (EDSS) as an endpoint. However, especially in progressive forms of MS, patients may experience repeated CDP events over time. We investigate first how recurrent disability progression events can be defined, and consider then recurrent event analyses that could increase power and improve clinical interpretation of results. Data on disease activity and results from two simulation studies which compare analyses of the time to the first event with recurrent event analyses (including negative binomial, Andersen-Gill and Lin-Wei-Ying-Yang models) are presented to demonstrate challenges in defining disability progression and to identify analysis methods suitable for treatment comparisons in MS trials with disability progression endpoints.   This article is based on the Master Thesis by Alexandra B\=uhler at the University of Ulm which was written under the supervision of Jan Beyersmann, Marcel Wolbers, Fabian Model and Qing Wang.|http://arxiv.org/abs/2111.01937v1|Alexandra Bhler
536|A Critical Review of Methods for Real-World Applications to Generalize or Transport Clinical Trial Findings to Target Populations of Interest|Generalizability and transportability methods have been proposed to address the external validity bias of randomized clinical trials that results from differences in the distribution of treatment effect modifiers between trial and target populations. However, such studies present many challenges. We review and summarize state-of-the-art methodological considerations. We additionally provide investigators with a step-by-step guide to address these challenges, illustrated through a published case study. When conducted with rigor, such studies may play an integral role in regulatory decisions by providing key real-world evidence.|http://arxiv.org/abs/2202.00820v1|Albee Y. Ling,Maria E. Montez-Rath,Paulo Carita,Karen Chandross,Laurence Lucats,Zhaoling Meng,Bernard Sebastien,Kris Kapphahn,Manisha Desai
537|Multistate Models as a Framework for Estimand Specification in Clinical Trials of Complex Processes|Intensity-based multistate models provide a useful framework for characterizing disease processes, the introduction of interventions, loss to follow-up, and other complications arising in the conduct of randomized trials studying complex life history processes. Within this framework we discuss the issues involved in the specification of estimands and show the limiting values of common estimators of marginal process features based on cumulative incidence function regression models. When intercurrent events arise we stress the need to carefully define the target estimand and the importance of avoiding targets of inference that are not interpretable in the real world. This has implications for analyses, but also the design of clinical trials where protocols may help in the interpretation of estimands based on marginal features.|http://arxiv.org/abs/2209.13658v1|Alexandra Bhler,Richard J. Cook,Jerald F. Lawless
538|Adjusting for time-varying treatment switches in randomized clinical trials: the danger of extrapolation and how to avoid it|When choosing estimands and estimators in randomized clinical trials, caution is warranted as intercurrent events, such as - due to patients who switch treatment after disease progression, are often extreme. Statistical analyses may then easily lure one into making large implicit extrapolations, which often go unnoticed. We will illustrate this problem of implicit extrapolations using a real oncology case study, with a right-censored time-to-event endpoint, in which patients can cross over from the control to the experimental treatment after disease progression, for ethical reasons. We resolve this by developing an estimator for the survival risk ratio contrasting the survival probabilities at each time t if all patients would take experimental treatment with the survival probabilities at those times t if all patients would take control treatment up to time t, using randomization as an instrumental variable to avoid reliance on no unmeasured confounders assumptions. This doubly robust estimator can handle time-varying treatment switches and right-censored survival times. Insight into the rationale behind the estimator is provided and the approach is demonstrated by re-analyzing the oncology trial.|http://arxiv.org/abs/2303.06099v1|Hege Michiels,An Vandebosch,Stijn Vansteelandt
539|Enrollment Forecast for Clinical Trials at the Planning Phase with Study-Level Historical Data|Given progressive developments and demands on clinical trials, accurate enrollment timeline forecasting is increasingly crucial for both strategic decision-making and trial execution excellence. Naive approach assumes flat rates on enrollment using average of historical data, while traditional statistical approach applies simple Poisson-Gamma model using timeinvariant rates for site activation and subject recruitment. Both of them are lack of nontrivial factors such as time and location. We propose a novel two-segment statistical approach based on Quasi-Poisson regression for subject accrual rate and Poisson process for subject enrollment and site activation. The input study-level data is publicly accessible and it can be integrated with historical study data from user's organization to prospectively predict enrollment timeline. The new framework is neat and accurate compared to preceding works. We validate the performance of our proposed enrollment model and compare the results with other frameworks on 7 curated studies.|http://arxiv.org/abs/2303.08065v1|Mengjia Yu,Sheng Zhong,Yunzhao Xing,Li Wang
540|Bayesian predictive probability based on a bivariate index vector for single-arm phase II study with binary efficacy and safety endpoints|In oncology, phase II studies are crucial for clinical development plans as such studies identify potent agents with sufficient activity to continue development in the subsequent phase III trials. Traditionally, phase II studies are single-arm studies, with the primary endpoint being short-term treatment efficacy. However, drug safety is also an important consideration. In the context of such multiple-outcome designs, predictive probability-based Bayesian monitoring strategies have been developed to assess whether a clinical trial will provide enough evidence to continue with a phase III study at the scheduled end of the trial. Herein, we propose a new simple index vector for summarizing results that cannot be captured by existing strategies. Specifically, for each interim monitoring time point, we calculate the Bayesian predictive probability using our new index and use it to assign a go/no-go decision. Finally, simulation studies are performed to evaluate the operating characteristics of the design. The obtained results demonstrate that the proposed method makes appropriate interim go/no-go decisions.|http://arxiv.org/abs/2305.10767v2|Takuya Yoshimoto,Satoru Shinoda,Kouji Yamamoto,Kouji Tahata
541|Estimating hypothetical estimands with causal inference and missing data estimators in a diabetes trial|The recently published ICH E9 addendum on estimands in clinical trials provides a framework for precisely defining the treatment effect that is to be estimated, but says little about estimation methods. Here we report analyses of a clinical trial in type 2 diabetes, targeting the effects of randomised treatment, handling rescue treatment and discontinuation of randomised treatment using the so-called hypothetical strategy. We show how this can be estimated using mixed models for repeated measures, multiple imputation, inverse probability of treatment weighting, G-formula and G-estimation. We describe their assumptions and practical details of their implementation using packages in R. We report the results of these analyses, broadly finding similar estimates and standard errors across the estimators. We discuss various considerations relevant when choosing an estimation approach, including computational time, how to handle missing data, whether to include post intercurrent event data in the analysis, whether and how to adjust for additional time-varying confounders, and whether and how to model different types of ICE separately.|http://arxiv.org/abs/2308.13085v2|Camila Olarte Parra,Rhian M. Daniel,David Wright,Jonathan W. Bartlett
542|PAM-HC: A Bayesian Nonparametric Construction of Hybrid Control for Randomized Clinical Trials Using External Data|It is highly desirable to borrow information from external data to augment a control arm in a randomized clinical trial, especially in settings where the sample size for the control arm is limited. However, a main challenge in borrowing information from external data is to accommodate potential heterogeneous subpopulations across the external and trial data. We apply a Bayesian nonparametric model called Plaid Atoms Model (PAM) to identify overlapping and unique subpopulations across datasets, with which we restrict the information borrowing to the common subpopulations. This forms a hybrid control (HC) that leads to more precise estimation of treatment effects Simulation studies demonstrate the robustness of the new method, and an application to an Atopic Dermatitis dataset shows improved treatment effect estimation.|http://arxiv.org/abs/2310.20087v1|Dehua Bi,Tianjian Zhou,Wei Zhong,Yuan Ji
543|Adaptive Experiments Toward Learning Treatment Effect Heterogeneity|Understanding treatment effect heterogeneity has become an increasingly popular task in various fields, as it helps design personalized advertisements in e-commerce or targeted treatment in biomedical studies. However, most of the existing work in this research area focused on either analyzing observational data based on strong causal assumptions or conducting post hoc analyses of randomized controlled trial data, and there has been limited effort dedicated to the design of randomized experiments specifically for uncovering treatment effect heterogeneity. In the manuscript, we develop a framework for designing and analyzing response adaptive experiments toward better learning treatment effect heterogeneity. Concretely, we provide response adaptive experimental design frameworks that sequentially revise the data collection mechanism according to the accrued evidence during the experiment. Such design strategies allow for the identification of subgroups with the largest treatment effects with enhanced statistical efficiency. The proposed frameworks not only unify adaptive enrichment designs and response-adaptive randomization designs but also complement A/B test designs in e-commerce and randomized trial designs in clinical settings. We demonstrate the merit of our design with theoretical justifications and in simulation studies with synthetic e-commerce and clinical trial data.|http://arxiv.org/abs/2312.06883v3|Waverly Wei,Xinwei Ma,Jingshen Wang
544|An Executable Specification of Oncology Dose-Escalation Protocols with Prolog|We present, as a pure Prolog program, the first executable specification of the 3 + 3 dose-escalation protocol commonly used in early-phase oncology drug development. In this program, the imperative operations of the protocol emerge as consequences of clinically meaningful anticipatory-regret scenarios that are declared as CLP(Z) constraints. This 'regret-constrained' (RC) specification yields a robust formulation which can be used to prove clinically meaningful safety and liveness properties of the protocol before incorporating it into a trial, and then as an on-line decision support system while the trial is underway. Our RC specification also readily accommodates certain pragmatic modifications to trial enrollment which severely strain traditionally imperative formulations. The features of modern Prolog systems let us describe the 3 + 3 protocol with a short and general program that has desirable algebraic properties and can therefore be used, tested and reasoned about in several different ways.|http://arxiv.org/abs/2402.08334v1|David C. Norris,Markus Triska
545|A Bayesian Hybrid Design with Borrowing from Historical Study|In early phase drug development of combination therapy, the primary objective is to preliminarily assess whether there is additive activity when a novel agent combined with an established monotherapy. Due to potential feasibility issues with a large randomized study, uncontrolled single-arm trials have been the mainstream approach in cancer clinical trials. However, such trials often present significant challenges in deciding whether to proceed to the next phase of development. A hybrid design, leveraging data from a completed historical clinical study of the monotherapy, offers a valuable option to enhance study efficiency and improve informed decision-making. Compared to traditional single-arm designs, the hybrid design may significantly enhance power by borrowing external information, enabling a more robust assessment of activity. The primary challenge of hybrid design lies in handling information borrowing. We introduce a Bayesian dynamic power prior (DPP) framework with three components of controlling amount of dynamic borrowing. The framework offers flexible study design options with explicit interpretation of borrowing, allowing customization according to specific needs. Furthermore, the posterior distribution in the proposed framework has a closed form, offering significant advantages in computational efficiency. The proposed framework's utility is demonstrated through simulations and a case study.|http://arxiv.org/abs/2404.13177v2|Zhaohua Lu,John Toso,Girma Ayele,Philip He
546|Personalized Predictions from Population Level Experiments: A Study on Alzheimer's Disease|The purpose of this article is to infer patient level outcomes from population level randomized control trials (RCTs). In this pursuit, we utilize the recently proposed synthetic nearest neighbors (SNN) estimator. At its core, SNN leverages information across patients to impute missing data associated with each patient of interest. We focus on two types of missing data: (i) unrecorded outcomes from discontinuing the assigned treatments and (ii) unobserved outcomes associated with unassigned treatments. Data imputation in the former powers and de-biases RCTs, while data imputation in the latter simulates "synthetic RCTs" to predict the outcomes for each patient under every treatment. The SNN estimator is interpretable, transparent, and causally justified under a broad class of missing data scenarios. Relative to several standard methods, we empirically find that SNN performs well for the above two applications using Phase 3 clinical trial data on patients with Alzheimer's Disease. Our findings directly suggest that SNN can tackle a current pain point within the clinical trial workflow on patient dropouts and serve as a new tool towards the development of precision medicine. Building on our insights, we discuss how SNN can further generalize to real-world applications.|http://arxiv.org/abs/2405.20088v1|Dennis Shen,Anish Agarwal,Vishal Misra,Bjoern Schelter,Devavrat Shah,Helen Shiells,Claude Wischik
547|Use of Real-World Data and Real-World Evidence in Rare Disease Drug Development: A Statistical Perspective|Real-world data (RWD) and real-world evidence (RWE) have been increasingly used in medical product development and regulatory decision-making, especially for rare diseases. After outlining the challenges and possible strategies to address the challenges in rare disease drug development (see the accompanying paper), the Real-World Evidence (RWE) Scientific Working Group of the American Statistical Association Biopharmaceutical Section reviews the roles of RWD and RWE in clinical trials for drugs treating rare diseases. This paper summarizes relevant guidance documents and frameworks by selected regulatory agencies and the current practice on the use of RWD and RWE in natural history studies and the design, conduct, and analysis of rare disease clinical trials. A targeted learning roadmap for rare disease trials is described, followed by case studies on the use of RWD and RWE to support a natural history study and marketing applications in various settings.|http://arxiv.org/abs/2410.06586v1|Jie Chen,Susan Gruber,Hana Lee,Haitao Chu,Shiowjen Lee,Haijun Tian,Yan Wang,Weili He,Thomas Jemielita,Yang Song,Roy Tamura,Lu Tian,Yihua Zhao,Yong Chen,Mark van der Laan,Lei Nie
548|Doubly protected estimation for survival outcomes utilizing external controls for randomized clinical trials|Censored survival data are common in clinical trials, but small control groups can pose challenges, particularly in rare diseases or where balanced randomization is impractical. Recent approaches leverage external controls from historical studies or real-world data to strengthen treatment evaluation for survival outcomes. However, using external controls directly may introduce biases due to data heterogeneity. We propose a doubly protected estimator for the treatment-specific restricted mean survival time difference that is more efficient than trial-only estimators and mitigates biases from external data. Our method adjusts for covariate shifts via doubly robust estimation and addresses outcome drift using the DR-Learner for selective borrowing. The approach incorporates machine learning to approximate survival curves and detect outcome drifts without strict parametric assumptions, borrowing only comparable external controls. Extensive simulation studies and a real-data application evaluating the efficacy of Galcanezumab in mitigating migraine headaches have been conducted to illustrate the effectiveness of our proposed framework.|http://arxiv.org/abs/2410.18409v1|Chenyin Gao,Shu Yang,Mingyang Shan,Wenyu Wendy Ye,Ilya Lipkovich,Douglas Faries
549|Leveraging Large Language Models for Medical Information Extraction and Query Generation|This paper introduces a system that integrates large language models (LLMs) into the clinical trial retrieval process, enhancing the effectiveness of matching patients with eligible trials while maintaining information privacy and allowing expert oversight. We evaluate six LLMs for query generation, focusing on open-source and relatively small models that require minimal computational resources. Our evaluation includes two closed-source and four open-source models, with one specifically trained in the medical field and five general-purpose models. We compare the retrieval effectiveness achieved by LLM-generated queries against those created by medical experts and state-of-the-art methods from the literature. Our findings indicate that the evaluated models reach retrieval effectiveness on par with or greater than expert-created queries. The LLMs consistently outperform standard baselines and other approaches in the literature. The best performing LLMs exhibit fast response times, ranging from 1.7 to 8 seconds, and generate a manageable number of query terms (15-63 on average), making them suitable for practical implementation. Our overall findings suggest that leveraging small, open-source LLMs for clinical trials retrieval can balance performance, computational efficiency, and real-world applicability in medical settings.|http://arxiv.org/abs/2410.23851v1|Georgios Peikos,Pranav Kasela,Gabriella Pasi
550|Interpretable Deep Neural Networks for Single-Trial EEG Classification|Background: In cognitive neuroscience the potential of Deep Neural Networks (DNNs) for solving complex classification tasks is yet to be fully exploited. The most limiting factor is that DNNs as notorious 'black boxes' do not provide insight into neurophysiological phenomena underlying a decision. Layer-wise Relevance Propagation (LRP) has been introduced as a novel method to explain individual network decisions. New Method: We propose the application of DNNs with LRP for the first time for EEG data analysis. Through LRP the single-trial DNN decisions are transformed into heatmaps indicating each data point's relevance for the outcome of the decision. Results: DNN achieves classification accuracies comparable to those of CSP-LDA. In subjects with low performance subject-to-subject transfer of trained DNNs can improve the results. The single-trial LRP heatmaps reveal neurophysiologically plausible patterns, resembling CSP-derived scalp maps. Critically, while CSP patterns represent class-wise aggregated information, LRP heatmaps pinpoint neural patterns to single time points in single trials. Comparison with Existing Method(s): We compare the classification performance of DNNs to that of linear CSP-LDA on two data sets related to motor-imaginery BCI. Conclusion: We have demonstrated that DNN is a powerful non-linear tool for EEG analysis. With LRP a new quality of high-resolution assessment of neural activity can be reached. LRP is a potential remedy for the lack of interpretability of DNNs that has limited their utility in neuroscientific applications. The extreme specificity of the LRP-derived heatmaps opens up new avenues for investigating neural activity underlying complex perception or decision-related processes.|http://arxiv.org/abs/1604.08201v1|Irene Sturm,Sebastian Bach,Wojciech Samek,Klaus-Robert Mller
551|Autoreject: Automated artifact rejection for MEG and EEG data|We present an automated algorithm for unified rejection and repair of bad trials in magnetoencephalography (MEG) and electroencephalography (EEG) signals. Our method capitalizes on cross-validation in conjunction with a robust evaluation metric to estimate the optimal peak-to-peak threshold -- a quantity commonly used for identifying bad trials in M/EEG. This approach is then extended to a more sophisticated algorithm which estimates this threshold for each sensor yielding trial-wise bad sensors. Depending on the number of bad sensors, the trial is then repaired by interpolation or by excluding it from subsequent analysis. All steps of the algorithm are fully automated thus lending itself to the name Autoreject.   In order to assess the practical significance of the algorithm, we conducted extensive validation and comparison with state-of-the-art methods on four public datasets containing MEG and EEG recordings from more than 200 subjects. Comparison include purely qualitative efforts as well as quantitatively benchmarking against human supervised and semi-automated preprocessing pipelines. The algorithm allowed us to automate the preprocessing of MEG data from the Human Connectome Project (HCP) going up to the computation of the evoked responses. The automated nature of our method minimizes the burden of human inspection, hence supporting scalability and reliability demanded by data analysis in modern neuroscience.|http://arxiv.org/abs/1612.08194v3|Mainak Jas,Denis A. Engemann,Yousra Bekhti,Federico Raimondo,Alexandre Gramfort
552|Estimating a Separably-Markov Random Field (SMuRF) from Binary Observations|A fundamental problem in neuroscience is to characterize the dynamics of spiking from the neurons in a circuit that is involved in learning about a stimulus or a contingency. A key limitation of current methods to analyze neural spiking data is the need to collapse neural activity over time or trials, which may cause the loss of information pertinent to understanding the function of a neuron or circuit. We introduce a new method that can determine not only the trial-to-trial dynamics that accompany the learning of a contingency by a neuron, but also the latency of this learning with respect to the onset of a conditioned stimulus. The backbone of the method is a separable two-dimensional (2D) random field (RF) model of neural spike rasters, in which the joint conditional intensity function of a neuron over time and trials depends on two latent Markovian state sequences that evolve separately but in parallel. Classical tools to estimate state-space models cannot be applied readily to our 2D separable RF model. We develop efficient statistical and computational tools to estimate the parameters of the separable 2D RF model. We apply these to data collected from neurons in the pre-frontal cortex (PFC) in an experiment designed to characterize the neural underpinnings of the associative learning of fear in mice. Overall, the separable 2D RF model provides a detailed, interpretable, characterization of the dynamics of neural spiking that accompany the learning of a contingency.|http://arxiv.org/abs/1709.09723v1|Yingzhuo Zhang,Noa Malem-Shinitski,Stephen A Allsop,Kay Tye,Demba Ba
553|Multimodal Outcomes in N-of-1 Trials: Combining Unsupervised Learning and Statistical Inference|N-of-1 trials are randomized multi-crossover trials in single participants with the purpose of investigating the possible effects of one or more treatments.   Research in the field of N-of-1 trials has primarily focused on scalar outcomes. However, with the increasing use of digital technologies, we propose to adapt this design to multimodal outcomes, such as audio, video, or image data or also sensor measurements, that can easily be collected by the trial participants on their personal mobile devices.   We present here a fully automated approach for analyzing multimodal N-of-1 trials by combining unsupervised deep learning models with statistical inference. First, we train an autoencoder on all images across all patients to create a lower-dimensional embedding. In the second step, the embeddings are reduced to a single dimension by projecting on the first principal component, again using all images. Finally, we test on an individual level whether treatment and non-treatment periods differ with respect to the component.   We apply our proposed approach to a published series of multimodal N-of-1 trials of 5 participants who tested the effect of creams on acne captured through images over 16 days. We compare several parametric and non-parametric statistical tests, and we also compare the results to an expert analysis that rates the pictures directly with respect to their acne severity and applies a t-test on these scores. The results indicate a treatment effect for one individual in the expert analysis. This effect was replicated with the proposed unsupervised pipeline.   In summary, our proposed approach enables the use of novel data types in N-of-1 trials while avoiding the need for manual labels. We anticipate that this can be the basis for further explorations of valid and interpretable approaches and their application in clinical multimodal N-of-1 trials.|http://arxiv.org/abs/2309.06455v1|Juliana Schneider,Thomas Grtner,Stefan Konigorski
554|Novel Development of LLM Driven mCODE Data Model for Improved Clinical Trial Matching to Enable Standardization and Interoperability in Oncology Research|Each year, the lack of efficient data standardization and interoperability in cancer care contributes to the severe lack of timely and effective diagnosis, while constantly adding to the burden of cost, with cancer costs nationally reaching over $208 billion in 2023 alone. Traditional methods regarding clinical trial enrollment and clinical care in oncology are often manual, time-consuming, and lack a data-driven approach. This paper presents a novel framework to streamline standardization, interoperability, and exchange of cancer domains and enhance the integration of oncology-based EHRs across disparate healthcare systems. This paper utilizes advanced LLMs and Computer Engineering to streamline cancer clinical trials and discovery. By utilizing FHIR's resource-based approach and LLM-generated mCODE profiles, we ensure timely, accurate, and efficient sharing of patient information across disparate healthcare systems. Our methodology involves transforming unstructured patient treatment data, PDFs, free-text information, and progress notes into enriched mCODE profiles, facilitating seamless integration with our novel AI and ML-based clinical trial matching engine. The results of this study show a significant improvement in data standardization, with accuracy rates of our trained LLM peaking at over 92% with datasets consisting of thousands of patient data. Additionally, our LLM demonstrated an accuracy rate of 87% for SNOMED-CT, 90% for LOINC, and 84% for RxNorm codes. This trumps the current status quo, with LLMs such as GPT-4 and Claude's 3.5 peaking at an average of 77%. This paper successfully underscores the potential of our standardization and interoperability framework, paving the way for more efficient and personalized cancer treatment.|http://arxiv.org/abs/2410.19826v1|Aarsh Shekhar,Mincheol Kim
555|Assessing surrogate endpoints in vaccine trials with case-cohort sampling and the Cox model|Assessing immune responses to study vaccines as surrogates of protection plays a central role in vaccine clinical trials. Motivated by three ongoing or pending HIV vaccine efficacy trials, we consider such surrogate endpoint assessment in a randomized placebo-controlled trial with case-cohort sampling of immune responses and a time to event endpoint. Based on the principal surrogate definition under the principal stratification framework proposed by Frangakis and Rubin [Biometrics 58 (2002) 21--29] and adapted by Gilbert and Hudgens (2006), we introduce estimands that measure the value of an immune response as a surrogate of protection in the context of the Cox proportional hazards model. The estimands are not identified because the immune response to vaccine is not measured in placebo recipients. We formulate the problem as a Cox model with missing covariates, and employ novel trial designs for predicting the missing immune responses and thereby identifying the estimands. The first design utilizes information from baseline predictors of the immune response, and bridges their relationship in the vaccine recipients to the placebo recipients. The second design provides a validation set for the unmeasured immune responses of uninfected placebo recipients by immunizing them with the study vaccine after trial closeout. A maximum estimated likelihood approach is proposed for estimation of the parameters. Simulated data examples are given to evaluate the proposed designs and study their properties.|http://arxiv.org/abs/0803.3919v1|Li Qin,Peter B. Gilbert,Dean Follmann,Dongfeng Li
556|Statistics, ethics, and probiotica|A randomized clinical trial comparing an experimental new treatment to a standard therapy for a life-threatening medical condition should be stopped early on ethical grounds, in either of the following scenarios: (1) it has become overwhelmingly clear that the new treatment is better than the standard; (2) it has become overwhelmingly clear that the trial is not going to show that the new treatment is any better than the standard. The trial is continued in the third scenario: (3) there is a reasonable chance that the new treatment will finally turn out to be better than the standard, but we aren't sure yet.   However, the (blinded) data monitoring committee in the "PROPATRIA" trial of an experimental probiotica treatment for patients with acute pancreatitis allowed the trial to continue at the half way interim analysis, in effect because there was still a good chance of proving that the probiotica treatment was very harmful to their patients. The committee did not know whether treatment A was the probiotica treatment or the placebo. In itself this should not have caused a problem, since it could easily have determined the appropriate decision under both scenarios. Were the decisions in the two scenarios different, then the data would have to be de-blinded, in order to determine the appropriate decision. The committee mis-read the output of SPSS, which reports the smaller of two one-sided p-values, without informing the user what it is doing. It seems that about 5 lives were sacrificed to the chance of getting a significant result that the probiotica treatment was bad for the patients in the trial.|http://arxiv.org/abs/0804.2522v2|Richard D. Gill
557|Point estimation for adaptive trial designs II: practical considerations and guidance|In adaptive clinical trials, the conventional end-of-trial point estimate of a treatment effect is prone to bias, that is, a systematic tendency to deviate from its true value. As stated in recent FDA guidance on adaptive designs, it is desirable to report estimates of treatment effects that reduce or remove this bias. However, it may be unclear which of the available estimators are preferable, and their use remains rare in practice. This paper is the second in a two-part series that studies the issue of bias in point estimation for adaptive trials. Part I provided a methodological review of approaches to remove or reduce the potential bias in point estimation for adaptive designs. In part II, we discuss how bias can affect standard estimators and assess the negative impact this can have. We review current practice for reporting point estimates and illustrate the computation of different estimators using a real adaptive trial example (including code), which we use as a basis for a simulation study. We show that while on average the values of these estimators can be similar, for a particular trial realisation they can give noticeably different values for the estimated treatment effect. Finally, we propose guidelines for researchers around the choice of estimators and the reporting of estimates following an adaptive design. The issue of bias should be considered throughout the whole lifecycle of an adaptive design, with the estimation strategy pre-specified in the statistical analysis plan. When available, unbiased or bias-reduced estimates are to be preferred.|http://arxiv.org/abs/2211.15620v1|David S. Robertson,Babak Choodari-Oskooei,Munya Dimairo,Laura Flight,Philip Pallmann,Thomas Jaki
558|A new approach for sizing trials with composite binary endpoints using anticipated marginal values and accounting for the correlation between components|Composite binary endpoints are increasingly used as primary endpoints in clinical trials. When designing a trial, it is crucial to determine the appropriate sample size for testing the statistical differences between treatment groups for the primary endpoint. As shown in this work, when using a composite binary endpoint to size a trial, one needs to specify the event rates and the effect sizes of the composite components as well as the correlation between them. In practice, the marginal parameters of the components can be obtained from previous studies or pilot trials, however, the correlation is often not previously reported and thus usually unknown. We first show that the sample size for composite binary endpoints is strongly dependent on the correlation and, second, that slight deviations in the prior information on the marginal parameters may result in underpowered trials for achieving the study objectives at a pre-specified significance level. We propose a general strategy for calculating the required sample size when the correlation is not specified, and accounting for uncertainty in the marginal parameter values. We present the web platform CompARE to characterize composite endpoints and to calculate the sample size just as we propose in this paper. We evaluate the performance of the proposal with a simulation study, and illustrate it by means of a real case study using CompARE.|http://arxiv.org/abs/1807.01305v2|Marta Bofill Roig,Guadalupe Gmez Melis
559|A Bayesian decision-theoretic approach to incorporate preclinical information into phase I oncology trials|Leveraging preclinical animal data for a phase I first-in-man trial is appealing yet challenging. A prior based on animal data may place large probability mass on values of the dose-toxicity model parameter(s), which appear infeasible in light of data accrued from the ongoing phase I clinical trial. In this paper, we seek to use animal data to improve decision making in a model-based dose-escalation procedure for phase I oncology trials. Specifically, animal data are incorporated via a robust mixture prior for the parameters of the dose-toxicity relationship. This prior changes dynamically as the trial progresses. After completion of treatment for each cohort, the weight allocated to the informative component, obtained based on animal data alone, is updated using a decision-theoretic approach to assess the commensurability of the animal data with the human toxicity data observed thus far. In particular, we measure commensurability as a function of the utility of optimal prior predictions for the human responses (toxicity or no toxicity) on each administered dose. The proposed methodology is illustrated through several examples and an extensive simulation study. Results show that our proposal can address difficulties in coping with prior-data conflict commencing in sequential trials with a small sample size.|http://arxiv.org/abs/1907.13620v2|Haiyan Zheng,Lisa V. Hampson
560|Hi3+3: A Model-Assisted Dose-Finding Design Borrowing Historical Data|Background -- In phase I clinical trials, historical data may be available through multi-regional programs, reformulation of the same drug, or previous trials for a drug under the same class. Statistical designs that borrow information from historical data can reduce cost, speed up drug development, and maintain safety. Purpose -- Based on a hybrid design that partly uses probability models and partly uses algorithmic rules for decision making, we aim to improve the efficiency of the dose-finding trials in the presence of historical data, maintain safety for patients, and achieve a level of simplicity for practical applications. Methods -- We propose the Hi3+3 design, in which the letter "H" represents "historical data". We apply the idea in power prior to borrow historical data and define the effective sample size (ESS) of the prior. Dose-finding decision rules follow the idea in the i3+3 design while incorporating the historical data via the power prior and ESS. The proposed Hi3+3 design pretabulates the dosing decisions before the trial starts, a desirable feature for ease of application in practice. Results -- The Hi3+3 design is superior than the i3+3 design due to information borrow from historical data. It is capable of maintaining a high level of safety for trial patients without sacrificing the ability to identify the correct MTD. Illustration of this feature are found in the simulation results. Conclusion -- With the demonstrated safety, efficiency, and simplicity, the Hi3+3 design could be a desirable choice for dose-finding trials borrowing historical data.|http://arxiv.org/abs/2010.10244v1|Yunshan Duan,Sue-Jane Wang,Yuan Ji
561|StudyU: a platform for designing and conducting innovative digital N-of-1 trials|N-of-1 trials are the gold standard study design to evaluate individual treatment effects and derive personalized treatment strategies. Digital tools have the potential to initiate a new era of N-of-1 trials in terms of scale and scope, but fully-functional platforms are not yet available. Here, we present the open source StudyU platform which includes the StudyU designer and StudyU app. With the StudyU designer, scientists are given a collaborative web application to digitally specify, publish, and conduct N-of-1 trials. The StudyU app is a smartphone application with innovative user-centric elements for participants to partake in the published trials and assess the effects of different interventions on their health. Thereby, the StudyU platform allows clinicians and researchers worldwide to easily design and conduct digital N-of-1 trials in a safe manner. We envision that StudyU can change the landscape of personalized treatments both for patients and healthy individuals, democratize and personalize evidence generation for self-optimization and medicine, and can be integrated in clinical practice.|http://arxiv.org/abs/2012.14201v2|Stefan Konigorski,Sarah Wernicke,Tamara Slosarek,Alexander M. Zenner,Nils Strelow,Ferenc D. Ruether,Florian Henschel,Manisha Manaswini,Fabian Pottbcker,Jonathan A. Edelman,Babajide Owoyele,Matteo Danieletto,Eddye Golden,Micol Zweig,Girish Nadkarni,Erwin Bttinger
562|Point estimation for adaptive trial designs I: a methodological review|Recent FDA guidance on adaptive clinical trial designs defines bias as "a systematic tendency for the estimate of treatment effect to deviate from its true value", and states that it is desirable to obtain and report estimates of treatment effects that reduce or remove this bias. The conventional end-of-trial point estimates of the treatment effects are prone to bias in many adaptive designs, because they do not take into account the potential and realised trial adaptations. While much of the methodological developments on adaptive designs have tended to focus on control of type I error rates and power considerations, in contrast the question of biased estimation has received relatively less attention. This paper is the first in a two-part series that studies the issue of potential bias in point estimation for adaptive trials. Part I provides a comprehensive review of the methods to remove or reduce the potential bias in point estimation of treatment effects for adaptive designs, while part II illustrates how to implement these in practice and proposes a set of guidelines for trial statisticians. The methods reviewed in this paper can be broadly classified into unbiased and bias-reduced estimation, and we also provide a classification of estimators by the type of adaptive design. We compare the proposed methods, highlight available software and code, and discuss potential methodological gaps in the literature.|http://arxiv.org/abs/2105.08836v4|David S. Robertson,Babak Choodari-Oskooei,Munya Dimairo,Laura Flight,Philip Pallmann,Thomas Jaki
563|Incentive-Theoretic Bayesian Inference for Collaborative Science|Contemporary scientific research is a distributed, collaborative endeavor, carried out by teams of researchers, regulatory institutions, funding agencies, commercial partners, and scientific bodies, all interacting with each other and facing different incentives. To maintain scientific rigor, statistical methods should acknowledge this state of affairs. To this end, we study hypothesis testing when there is an agent (e.g., a researcher or a pharmaceutical company) with a private prior about an unknown parameter and a principal (e.g., a policymaker or regulator) who wishes to make decisions based on the parameter value. The agent chooses whether to run a statistical trial based on their private prior and then the result of the trial is used by the principal to reach a decision. We show how the principal can conduct statistical inference that leverages the information that is revealed by an agent's strategic behavior -- their choice to run a trial or not. In particular, we show how the principal can design a policy to elucidate partial information about the agent's private prior beliefs and use this to control the posterior probability of the null. One implication is a simple guideline for the choice of significance threshold in clinical trials: the type-I error level should be set to be strictly less than the cost of the trial divided by the firm's profit if the trial is successful.|http://arxiv.org/abs/2307.03748v2|Stephen Bates,Michael I. Jordan,Michael Sklar,Jake A. Soloff
564|Automatically Extracting Numerical Results from Randomized Controlled Trials with Large Language Models|Meta-analyses statistically aggregate the findings of different randomized controlled trials (RCTs) to assess treatment effectiveness. Because this yields robust estimates of treatment effectiveness, results from meta-analyses are considered the strongest form of evidence. However, rigorous evidence syntheses are time-consuming and labor-intensive, requiring manual extraction of data from individual trials to be synthesized. Ideally, language technologies would permit fully automatic meta-analysis, on demand. This requires accurately extracting numerical results from individual trials, which has been beyond the capabilities of natural language processing (NLP) models to date. In this work, we evaluate whether modern large language models (LLMs) can reliably perform this task. We annotate (and release) a modest but granular evaluation dataset of clinical trial reports with numerical findings attached to interventions, comparators, and outcomes. Using this dataset, we evaluate the performance of seven LLMs applied zero-shot for the task of conditionally extracting numerical findings from trial reports. We find that massive LLMs that can accommodate lengthy inputs are tantalizingly close to realizing fully automatic meta-analysis, especially for dichotomous (binary) outcomes (e.g., mortality). However, LLMs -- including ones trained on biomedical texts -- perform poorly when the outcome measures are complex and tallying the results requires inference. This work charts a path toward fully automatic meta-analysis of RCTs via LLMs, while also highlighting the limitations of existing models for this aim.|http://arxiv.org/abs/2405.01686v2|Hye Sun Yun,David Pogrebitskiy,Iain J. Marshall,Byron C. Wallace
565|Virtual Lung Screening Trial (VLST): An In Silico Replica of the National Lung Screening Trial for Lung Cancer Detection|Objectives: To demonstrate that a virtual imaging trial platform can accurately emulate a major clinical trial, specifically the National Lung Screening Trial (NLST) that compared computed tomography (CT) and chest radiography (CXR) imaging for lung cancer screening.   Design, Setting, and Participants: A virtual patient population of 294 subjects was created from human models (XCAT) emulating the NLST, with two types of simulated cancerous lung nodules. Each virtual patient in the cohort was assessed using simulated CT and CXR systems to generate images reflecting the NLST imaging technologies. Deep learning models trained for lesion detection, AI CT-Reader, and AI CXR-Reader served as virtual readers.   Main Outcomes and Measures: The primary outcome was the difference in the Receiver Operating Characteristic Area Under the Curve (AUC) for CT and CXR modalities.   Results: The study analyzed paired CT and CXR simulated images from 294 virtual patients. The AI CT-Reader outperformed the AI CXR-Reader across all levels of analysis. At the patient level, CT demonstrated superior diagnostic performance with an AUC of 0.92 (95% CI: 0.90 to 0.95), compared to CXR AUC of 0.72 (0.67 to 0.77). Subgroup analyses of lesion types revealed CT had significantly better detection of homogeneous lesions (AUC 0.97, 95% CI: 0.95 to 0.98) compared to heterogeneous lesions(0.89; 0.86 to 0.93). Furthermore, when the specificity of the AI CT-Reader was adjusted to match the NLST sensitivity of 94% for CT, the VLST results closely mirrored the NLST findings, further highlighting the alignment between the two studies.   Conclusion and Relevance: The VIT results closely replicated those of the earlier NLST, underscoring its potential to replicate real clinical imaging trials. Integration of virtual trials may aid in the evaluation and improvement of imaging-based diagnosis.|http://arxiv.org/abs/2404.11221v3|Fakrul Islam Tushar,Liesbeth Vancoillie,Cindy McCabe,Amareswararao Kavuri,Lavsen Dahal,Brian Harrawood,Milo Fryling,Mojtaba Zarei,Saman Sotoudeh-Paima,Fong Chi Ho,Dhrubajyoti Ghosh,Michael R. Harowicz,Tina D. Tailor,Sheng Luo,W. Paul Segars,Ehsan Abadi,Kyle J. Lafata,Joseph Y. Lo,Ehsan Samei
566|Multiscale Bayesian State Space Model for Granger Causality Analysis of Brain Signal|Modelling time-varying and frequency-specific relationships between two brain signals is becoming an essential methodological tool to answer heoretical questions in experimental neuroscience. In this article, we propose to estimate a frequency Granger causality statistic that may vary in time in order to evaluate the functional connections between two brain regions during a task. We use for that purpose an adaptive Kalman filter type of estimator of a linear Gaussian vector autoregressive model with coefficients evolving over time. The estimation procedure is achieved through variational Bayesian approximation and is extended for multiple trials. This Bayesian State Space (BSS) model provides a dynamical Granger-causality statistic that is quite natural. We propose to extend the BSS model to include the \`{a} trous Haar decomposition. This wavelet-based forecasting method is based on a multiscale resolution decomposition of the signal using the redundant \`{a} trous wavelet transform and allows us to capture short- and long-range dependencies between signals. Equally importantly it allows us to derive the desired dynamical and frequency-specific Granger-causality statistic. The application of these models to intracranial local field potential data recorded during a psychological experimental task shows the complex frequency based cross-talk between amygdala and medial orbito-frontal cortex.   Keywords: \`{a} trous Haar wavelets; Multiple trials; Neuroscience data; Nonstationarity; Time-frequency; Variational methods   The published version of this article is   Cekic, S., Grandjean, D., Renaud, O. (2018). Multiscale Bayesian state-space model for Granger causality analysis of brain signal. Journal of Applied Statistics. https://doi.org/10.1080/02664763.2018.1455814|http://arxiv.org/abs/1704.02778v2|Sezen Cekic,Didier Grandjean,Olivier Renaud
567|Weighted trajectory analysis and application to clinical outcome assessment|The Kaplan-Meier estimator (KM) is widely used in medical research to estimate the survival function from lifetime data. KM is a powerful tool to evaluate clinical trials due to simple computational requirements, a logrank hypothesis test, and the ability to censor patients. However, KM has several constraints and fails to generalize to ordinal variables of interest such as toxicity and ECOG performance. We devised Weighted Trajectory Analysis (WTA) to combine the advantages of KM with the ability to compare treatment groups for ordinal variables and fluctuating outcomes. To assess statistical significance, we developed a new hypothesis test analogous to the logrank test. We demonstrate the functionality of WTA through 1000-fold clinical trial simulations of unique stochastic models of chemotherapy toxicity and schizophrenia progression. At several increments of sample size and hazard ratio, we compare the performance of WTA to both KM and Generalized Estimating Equations (GEE). WTA generally required half the sample size to achieve comparable power to KM; advantages over GEE include its robust non-parametric approach and summary plot. We also apply WTA to real clinical data: the toxicity outcomes of melanoma patients receiving immunotherapy and the disease progression of patients with metastatic breast cancer receiving ramucirumab. The application of WTA demonstrates that using traditional methods such as percent incidence and KM can lead to both Type I and II errors by failing to model illness trajectory. This article outlines a novel method for clinical outcome assessment that extends the advantages of Kaplan-Meier estimates to ordinal outcome variables.|http://arxiv.org/abs/2306.04138v1|Utkarsh Chauhan,Kaiqiong Zhao,John Walker,John R. Mackey
568|Probabilistic Temporal Prediction of Continuous Disease Trajectories and Treatment Effects Using Neural SDEs|Personalized medicine based on medical images, including predicting future individualized clinical disease progression and treatment response, would have an enormous impact on healthcare and drug development, particularly for diseases (e.g. multiple sclerosis (MS)) with long term, complex, heterogeneous evolutions and no cure. In this work, we present the first stochastic causal temporal framework to model the continuous temporal evolution of disease progression via Neural Stochastic Differential Equations (NSDE). The proposed causal inference model takes as input the patient's high dimensional images (MRI) and tabular data, and predicts both factual and counterfactual progression trajectories on different treatments in latent space. The NSDE permits the estimation of high-confidence personalized trajectories and treatment effects. Extensive experiments were performed on a large, multi-centre, proprietary dataset of patient 3D MRI and clinical data acquired during several randomized clinical trials for MS treatments. Our results present the first successful uncertainty-based causal Deep Learning (DL) model to: (a) accurately predict future patient MS disability evolution (e.g. EDSS) and treatment effects leveraging baseline MRI, and (b) permit the discovery of subgroups of patients for which the model has high confidence in their response to treatment even in clinical trials which did not reach their clinical endpoints.|http://arxiv.org/abs/2406.12807v1|Joshua Durso-Finley,Berardino Barile,Jean-Pierre Falet,Douglas L. Arnold,Nick Pawlowski,Tal Arbel
569|Artificial intelligence to improve clinical coding practice in Scandinavia: a crossover randomized controlled trial|\textbf{Trial design} Crossover randomized controlled trial. \textbf{Methods} An AI tool, Easy-ICD, was developed to assist clinical coders and was tested for improving both accuracy and time in a user study in Norway and Sweden. Participants were randomly assigned to two groups, and crossed over between coding complex (longer) texts versus simple (shorter) texts, while using our tool versus not using our tool. \textbf{Results} Based on Mann-Whitney U test, the median coding time difference for complex clinical text sequences was 123 seconds (\emph{P}\textless.001, 95\% CI: 81 to 164), representing a 46\% reduction in median coding time when our tool is used. There was no significant time difference for simpler text sequences. For coding accuracy, the improvement we noted for both complex and simple texts was not significant. \textbf{Conclusions} This study demonstrates the potential of AI to transform common tasks in clinical workflows, with ostensible positive impacts on work efficiencies for complex clinical coding tasks. Further studies within hospital workflows are required before these presumed impacts can be more clearly understood.|http://arxiv.org/abs/2410.23725v1|Taridzo Chomutare,Therese Olsen Svenning,Miguel ngel Tejedor Hernndez,Phuong Dinh Ngo,Andrius Budrionis,Kaisa Markljung,Lill Irene Hind,Torbjrn Torsvik,Karl yvind Mikalsen,Aleksandar Babic,Hercules Dalianis
570|Application of the hierarchical bootstrap to multi-level data in neuroscience|A common feature in many neuroscience datasets is the presence of hierarchical data structures, most commonly recording the activity of multiple neurons in multiple animals across multiple trials. Accordingly, the measurements constituting the dataset are not independent, even though the traditional statistical analyses often applied in such cases (e.g., Students t-test) treat them as such. The hierarchical bootstrap has been shown to be an effective tool to accurately analyze such data and while it has been used extensively in the statistical literature, its use is not widespread in neuroscience - despite the ubiquity of hierarchical datasets. In this paper, we illustrate the intuitiveness and utility of this approach to analyze hierarchically nested datasets. We use simulated neural data to show that traditional statistical tests can result in a false positive rate of over 45%, even if the Type-I error rate is set at 5%. While summarizing data across non-independent points (or lower levels) can potentially fix this problem, this approach greatly reduces the statistical power of the analysis. The hierarchical bootstrap, when applied sequentially over the levels of the hierarchical structure, keeps the Type-I error rate within the intended bound and retains more statistical power than summarizing methods. We conclude by demonstrating the effectiveness of the method in two real-world examples, first analyzing singing data in male Bengalese finches (Lonchura striata var. domestica) and second quantifying changes in behavior under optogenetic control in flies (Drosophila melanogaster).|http://arxiv.org/abs/2007.07797v2|Varun Saravanan,Gordon J Berman,Samuel J Sober
571|Penalized Q-Learning for Dynamic Treatment Regimes|A dynamic treatment regime effectively incorporates both accrued information and long-term effects of treatment from specially designed clinical trials. As these become more and more popular in conjunction with longitudinal data from clinical studies, the development of statistical inference for optimal dynamic treatment regimes is a high priority. This is very challenging due to the difficulties arising form non-regularities in the treatment effect parameters. In this paper, we propose a new reinforcement learning framework called penalized Q-learning (PQ-learning), under which the non-regularities can be resolved and valid statistical inference established. We also propose a new statistical procedure---individual selection---and corresponding methods for incorporating individual selection within PQ-learning. Extensive numerical studies are presented which compare the proposed methods with existing methods, under a variety of non-regular scenarios, and demonstrate that the proposed approach is both inferentially and computationally superior. The proposed method is demonstrated with the data from a depression clinical trial study.|http://arxiv.org/abs/1108.5338v1|Rui Song,Weiwei Wang,Donglin Zeng,Michael R. Kosorok
572|An Information System to Support and Monitor Clinical Trial Process|The demand of transparency of clinical research results, the need of accelerating the process of transferring innovation in the daily medical practice as well as assuring patient safety and product efficacy make it necessary to extend the functionality of traditional trial registries. These new systems should combine different functionalities to track the information exchange, support collaborative work, manage regulatory documents and monitor the entire clinical investigation (CIV) lifecycle. This is the approach used to develop MEDIS, a Medical Device Information System, described in this paper under the perspective of the business process, and the underlining architecture. Moreover, MEDIS was designed on the basis of Health Level 7 (HL7) v.3 standards and methodology to make it interoperable with similar registries, but also to facilitate information exchange between different health information systems.|http://arxiv.org/abs/1301.1886v1|Daniela Luzi,Fabrizio Pecoraro
573|A Two-Stage Patient-Focused Study Design for Rare Disease Controlled Trials|We developed a study design for rare disease clinical trials (RDTs) that efficiently evaluate treatments, promotes access to new treatments during treatment development, and optimizes healthcare resource utilization for future treatment allocation, development, and prioritization. Comprehensive literature review and focus group discussion were conducted. To address the multifaceted challenges facing RDTs, four key issues for RDTs must be addressed, which are 1) the opportunity to access the new treatment; 2) assessment of outcomes where clinically validated outcomes may be lacking; 3) patient heterogeneity; and 4) duration of the study and number of patients required. Our proposed study design has two stages. Stage 1 distinguishes patients who respond to the treatment from those who do not respond to the treatment after assigning them all to the experimental treatment. Stage 2 evaluates the treatment effect comparatively among patients responded in Stage 1. In addition to treatment effect evaluation, our design can greatly benefit rare disease patients and clinical practice by increasing opportunities to access experimental treatments and by providing relevant information that can be used for tailoring treatments to certain subgroups, aiding future research in treatment development, and improving healthcare resource utilization.|http://arxiv.org/abs/1607.00046v1|Jian Yong,Sohaib H. Mohammad,Yan Yuan
574|Evaluation of TRANSFoRm Mobile eHealth Solution for Remote Patient Monitoring during Clinical Trials|Today, in the digital age, the mobile devices are more and more used to aid people in the struggle to improve or maintain their health. In this paper, the mobile eHealth solution for remote patient monitoring during clinical trials is presented, together with the outcomes of quantitative and qualitative performance evaluation. The evaluation is a third step to improve the quality of the application after earlier Good Clinical Practice certification and validation with the participation of 10 patients and three general practitioners. This time, the focus was on the usability which was evaluated by the seventeen participants divided into three age groups (18-28, 29-50, and 50+). The results, from recorded sessions and the eye tracking, show that there is no difference in performance between the first group and the second group, while for the third group the performance was worse, however, it was still good enough to complete task within reasonable time.|http://arxiv.org/abs/1607.06896v1|Jarosaw Jankowski,Stanisaw Saganowski,Piotr Brdka
575|Comparison between continuous and discrete doses using Escalation With Overdose Control|Although there is an extensive statistical literature showing the disadvantages of discretizing continuous variables, categorization is a common practice in clinical research which results in substantial loss of information. A large collection of methods in cancer phase I clinical trial design establishes dose of a new agent as a discrete variable. A noteworthy exception is the Escalation With Overdose Control (EWOC) design, where doses can be defined either as continuous or as a grid of discrete doses. A Monte Carlo simulation study was performed to compare the operating characteristics of continuous and discrete dose EWOC designs. Four equally spaced grids with different interval lengths were considered. The loss of information was measured by several operating characteristics easier for clinicians to interpret, in addition to the usual statistical measures of bias and mean squared error. Based on the simulations, if there is not enough knowledge about the true MTD value as commonly happens in phase I clinical trials, continuous dose scheme arises as an attractive option.|http://arxiv.org/abs/1708.04792v1|Mrcio Augusto Diniz,Mourad Tighiouart,Andr Rogatko
576|Dirac Delta Regression: Conditional Density Estimation with Clinical Trials|Personalized medicine seeks to identify the causal effect of treatment for a particular patient as opposed to a clinical population at large. Most investigators estimate such personalized treatment effects by regressing the outcome of a randomized clinical trial (RCT) on patient covariates. The realized value of the outcome may however lie far from the conditional expectation. We therefore introduce a method called Dirac Delta Regression (DDR) that estimates the entire conditional density from RCT data in order to visualize the probabilities across all possible outcome values. DDR transforms the outcome into a set of asymptotically Dirac delta distributions and then estimates the density using non-linear regression. The algorithm can identify significant differences in patient-specific outcomes even when no population level effect exists. Moreover, DDR outperforms state-of-the-art algorithms in conditional density estimation by a large margin even in the small sample regime. An R package is available at https://github.com/ericstrobl/DDR.|http://arxiv.org/abs/1905.10330v2|Eric V. Strobl,Shyam Visweswaran
577|Decision-making with multiple correlated binary outcomes in clinical trials|Clinical trials often evaluate multiple outcome variables to form a comprehensive picture of the effects of a new treatment. The resulting multidimensional insight contributes to clinically relevant and efficient decision-making about treatment superiority. Common statistical procedures to make these superiority decisions with multiple outcomes have two important shortcomings however: 1) Outcome variables are often modeled individually, and consequently fail to consider the relation between outcomes; and 2) superiority is often defined as a relevant difference on a single, on any, or on all outcomes(s); and lacks a compensatory mechanism that allows large positive effects on one or multiple outcome(s) to outweigh small negative effects on other outcomes. To address these shortcomings, this paper proposes 1) a Bayesian model for the analysis of correlated binary outcomes based on the multivariate Bernoulli distribution; and 2) a flexible decision criterion with a compensatory mechanism that captures the relative importance of the outcomes. A simulation study demonstrates that efficient and unbiased decisions can be made while Type I error rates are properly controlled. The performance of the framework is illustrated for 1) fixed, group sequential, and adaptive designs; and 2) non-informative and informative prior distributions.|http://arxiv.org/abs/1908.10158v2|X. M. Kavelaars,J. Mulder,M. C. Kaptein
578|Generating Digital Twins with Multiple Sclerosis Using Probabilistic Neural Networks|Multiple Sclerosis (MS) is a neurodegenerative disorder characterized by a complex set of clinical assessments. We use an unsupervised machine learning model called a Conditional Restricted Boltzmann Machine (CRBM) to learn the relationships between covariates commonly used to characterize subjects and their disease progression in MS clinical trials. A CRBM is capable of generating digital twins, which are simulated subjects having the same baseline data as actual subjects. Digital twins allow for subject-level statistical analyses of disease progression. The CRBM is trained using data from 2395 subjects enrolled in the placebo arms of clinical trials across the three primary subtypes of MS. We discuss how CRBMs are trained and show that digital twins generated by the model are statistically indistinguishable from their actual subject counterparts along a number of measures.|http://arxiv.org/abs/2002.02779v2|Jonathan R. Walsh,Aaron M. Smith,Yannick Pouliot,David Li-Bland,Anton Loukianov,Charles K. Fisher
579|Tolerance and Prediction Intervals for Non-normal Models|A prediction interval covers a future observation from a random process in repeated sampling, and is typically constructed by identifying a pivotal quantity that is also an ancillary statistic. Analogously, a tolerance interval covers a population percentile in repeated sampling and is often based on a pivotal quantity. One approach we consider in non-normal models leverages a link function resulting in a pivotal quantity that is approximately normally distributed. In settings where this normal approximation does not hold we consider a second approach for tolerance and prediction based on a confidence interval for the mean. These methods are intuitive, simple to implement, have proper operating characteristics, and are computationally efficient compared to Bayesian, re-sampling, and machine learning methods. This is demonstrated in the context of multi-site clinical trial recruitment with staggered site initiation, real-world time on treatment, and end-of-study success for a clinical endpoint.|http://arxiv.org/abs/2011.11583v5|Geoffrey S Johnson
580|Efficiently Discovering Hammock Paths from Induced Similarity Networks|Similarity networks are important abstractions in many information management applications such as recommender systems, corpora analysis, and medical informatics. For instance, by inducing similarity networks between movies rated similarly by users, or between documents containing common terms, and or between clinical trials involving the same themes, we can aim to find the global structure of connectivities underlying the data, and use the network as a basis to make connections between seemingly disparate entities. In the above applications, composing similarities between objects of interest finds uses in serendipitous recommendation, in storytelling, and in clinical diagnosis, respectively. We present an algorithmic framework for traversing similarity paths using the notion of `hammock' paths which are generalization of traditional paths. Our framework is exploratory in nature so that, given starting and ending objects of interest, it explores candidate objects for path following, and heuristics to admissibly estimate the potential for paths to lead to a desired destination. We present three diverse applications: exploring movie similarities in the Netflix dataset, exploring abstract similarities across the PubMed corpus, and exploring description similarities in a database of clinical trials. Experimental results demonstrate the potential of our approach for unstructured knowledge discovery in similarity networks.|http://arxiv.org/abs/1002.3195v1|M. Shahriar Hossain,Michael Narayan,Naren Ramakrishnan
581|Identifying Principal Stratum Causal Effects Conditional on a Post-treatment Intermediate Response|In neoadjuvant trials on early-stage breast cancer, patients are usually randomized into a control group and a treatment group with an additional target therapy. Early efficacy of the new regimen is assessed via the binary pathological complete response (pCR) and the eventual efficacy is assessed via long-term clinical outcomes such as survival. Although pCR is strongly associated with survival, it has not been confirmed as a surrogate endpoint. To fully understand its clinical implication, it is important to establish causal estimands such as the causal effect in survival for patients who would achieve pCR under the new regimen. Under the principal stratification framework, previous studies focus on sensitivity analyses by varying model parameters in an imposed model on counterfactual outcomes. Under mild assumptions, we propose an approach to estimate those model parameters using empirical data and subsequently the causal estimand of interest. We also extend our approach to address censored outcome data. The proposed method is applied to a recent clinical trial and its performance is evaluated via simulation studies.|http://arxiv.org/abs/2103.04175v2|Xiaoqing Tan,Judah Abberbock,Priya Rastogi,Gong Tang
582|Impact of detecting clinical trial elements in exploration of COVID-19 literature|The COVID-19 pandemic has driven ever-greater demand for tools which enable efficient exploration of biomedical literature. Although semi-structured information resulting from concept recognition and detection of the defining elements of clinical trials (e.g. PICO criteria) has been commonly used to support literature search, the contributions of this abstraction remain poorly understood, especially in relation to text-based retrieval. In this study, we compare the results retrieved by a standard search engine with those filtered using clinically-relevant concepts and their relations. With analysis based on the annotations from the TREC-COVID shared task, we obtain quantitative as well as qualitative insights into characteristics of relational and concept-based literature exploration. Most importantly, we find that the relational concept selection filters the original retrieved collection in a way that decreases the proportion of unjudged documents and increases the precision, which means that the user is likely to be exposed to a larger number of relevant documents.|http://arxiv.org/abs/2105.12261v1|Simon uster,Karin Verspoor,Timothy Baldwin,Jey Han Lau,Antonio Jimeno Yepes,David Martinez,Yulia Otmakhova
583|Using principal stratification in analysis of clinical trials|The ICH E9(R1) addendum (2019) proposed principal stratification (PS) as one of five strategies for dealing with intercurrent events. Therefore, understanding the strengths, limitations, and assumptions of PS is important for the broad community of clinical trialists. Many approaches have been developed under the general framework of PS in different areas of research, including experimental and observational studies. These diverse applications have utilized a diverse set of tools and assumptions. Thus, need exists to present these approaches in a unifying manner. The goal of this tutorial is threefold. First, we provide a coherent and unifying description of PS. Second, we emphasize that estimation of effects within PS relies on strong assumptions and we thoroughly examine the consequences of these assumptions to understand in which situations certain assumptions are reasonable. Finally, we provide an overview of a variety of key methods for PS analysis and use a real clinical trial example to illustrate them. Examples of code for implementation of some of these approaches are given in supplemental materials.|http://arxiv.org/abs/2112.03352v3|Ilya Lipkovich,Bohdana Ratitch,Yongming Qu,Xiang Zhang,Mingyang Shan,Craig Mallinckrodt
584|Identification of prognostic and predictive biomarkers in high-dimensional data with PPLasso|In clinical trials, identification of prognostic and predictive biomarkers is essential to precision medicine. Prognostic biomarkers can be useful for the prevention of the occurrence of the disease, and predictive biomarkers can be used to identify patients with potential benefit from the treatment. Previous researches were mainly focused on clinical characteristics, and the use of genomic data in such an area is hardly studied. A new method is required to simultaneously select prognostic and predictive biomarkers in high dimensional genomic data where biomarkers are highly correlated. We propose a novel approach called PPLasso (Prognostic Predictive Lasso) integrating prognostic and predictive effects into one statistical model. PPLasso also takes into account the correlations between biomarkers that can alter the biomarker selection accuracy. Our method consists in transforming the design matrix to remove the correlations between the biomarkers before applying the generalized Lasso. In a comprehensive numerical evaluation, we show that PPLasso outperforms the traditional Lasso approach on both prognostic and predictive biomarker identification in various scenarios. Finally, our method is applied to publicly available transcriptomic data from clinical trial RV144. Our method is implemented in the PPLasso R package available from the Comprehensive R Archive Network (CRAN).|http://arxiv.org/abs/2202.01970v2|Wencan Zhu,Cline Lvy-Leduc,Nils Terns
585|Marginal Regression on Transient State Occupation Probabilities with Clustered Multistate Process Data|Clustered multistate process data are commonly encountered in multicenter observational studies and clinical trials. A clinically important estimand with such data is the marginal probability of being in a particular transient state as a function of time. However, there is currently no method for nonparametric marginal regression analysis of these probabilities with clustered multistate process data. To address this problem, we propose a weighted functional generalized estimating equations approach which does not impose Markov assumptions or assumptions regarding the structure of the within-cluster dependence, and allows for informative cluster size (ICS). The asymptotic properties of the proposed estimators for the functional regression coefficients are rigorously established and a nonparametric hypothesis testing procedure for covariate effects is proposed. Simulation studies show that the proposed method performs well even with a small number of clusters, and that ignoring the within-cluster dependence and the ICS leads to invalid inferences. The proposed method is used to analyze data from a multicenter clinical trial on recurrent or metastatic squamous-cell carcinoma of the head and neck with a stratified randomization design.|http://arxiv.org/abs/2209.00804v1|Wenxian Zhou,Giorgos Bakoyannis,Ying Zhang,Constantin T Yiannoutsos
586|SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials|Large Language Models (LLMs) are at the forefront of NLP achievements but fall short in dealing with shortcut learning, factual inconsistency, and vulnerability to adversarial inputs.These shortcomings are especially critical in medical contexts, where they can misrepresent actual model capabilities. Addressing this, we present SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for ClinicalTrials. Our contributions include the refined NLI4CT-P dataset (i.e., Natural Language Inference for Clinical Trials - Perturbed), designed to challenge LLMs with interventional and causal reasoning tasks, along with a comprehensive evaluation of methods and results for participant submissions. A total of 106 participants registered for the task contributing to over 1200 individual submissions and 25 system overview papers. This initiative aims to advance the robustness and applicability of NLI models in healthcare, ensuring safer and more dependable AI assistance in clinical decision-making. We anticipate that the dataset, models, and outcomes of this task can support future research in the field of biomedical NLI. The dataset, competition leaderboard, and website are publicly available.|http://arxiv.org/abs/2404.04963v1|Mael Jullien,Marco Valentino,Andr Freitas
587|Comparing restricted mean survival times in small sample clinical trials using pseudo-observations|The widely used proportional hazard assumption cannot be assessed reliably in small-scale clinical trials and might often in fact be unjustified, e.g. due to delayed treatment effects. An alternative to the hazard ratio as effect measure is the difference in restricted mean survival time (RMST) that does not rely on model assumptions. Although an asymptotic test for two-sample comparisons of the RMST exists, it has been shown to suffer from an inflated type I error rate in samples of small or moderate sizes. Recently, permutation tests, including the studentized permutation test, have been introduced to address this issue. In this paper, we propose two methods based on pseudo-observations (PO) regression models as alternatives for such scenarios and assess their properties in comparison to previously proposed approaches in an extensive simulation study. Furthermore, we apply the proposed PO methods to data from a clinical trail and, by doing so, point out some extension that might be very useful for practical applications such as covariate adjustments.|http://arxiv.org/abs/2408.15607v1|David Jesse,Cynthia Huber,Tim Friede
588|"6 choose 4": A framework to understand and facilitate discussion of strategies for overall survival safety monitoring|Advances in anticancer therapies have significantly contributed to declining death rates in certain disease and clinical settings. However, they have also made it difficult to power a clinical trial in these settings with overall survival (OS) as the primary efficacy endpoint. A number of statistical approaches have therefore been recently proposed for the pre-specified analysis of OS as a safety endpoint (Shan, 2023; Fleming et al., 2024; Rodriguez et al., 2024). In this paper, we provide a simple, unifying framework that includes the aforementioned approaches (and a couple others) as special cases. By highlighting each approach's focus, priority, tolerance for risk, and strengths or challenges for practical implementation, this framework can help to facilitate discussions between stakeholders on "fit-for-purpose OS data collection and assessment of harm" (American Association for Cancer Research, 2024). We apply this framework to a real clinical trial in large B-cell lymphoma to illustrate its application and value. Several recommendations and open questions are also raised.|http://arxiv.org/abs/2410.04020v1|Godwin Yung,Kaspar Rufibach,Marcel Wolbers,Mark Yan,Jue Wang
589|A cautious use of auxiliary outcomes for decision-making in randomized clinical trials|Clinical trials often collect data on multiple outcomes, such as overall survival (OS), progression-free survival (PFS), and response to treatment (RT). In most cases, however, study designs only use primary outcome data for interim and final decision-making. In several disease settings, clinically relevant outcomes, for example OS, become available years after patient enrollment. Moreover, the effects of experimental treatments on OS might be less pronounced compared to auxiliary outcomes such as RT. We develop a Bayesian decision-theoretic framework that uses both primary and auxiliary outcomes for interim and final decision-making. The framework allows investigators to control standard frequentist operating characteristics, such as the type I error rate, and can be used with auxiliary outcomes from emerging technologies, such as circulating tumor assays. False positive rates and other frequentist operating characteristics are rigorously controlled without any assumption about the concordance between primary and auxiliary outcomes. We discuss algorithms to implement this decision-theoretic approach and show that incorporating auxiliary information into interim and final decision-making can lead to relevant efficiency gains according to established and interpretable metrics.|http://arxiv.org/abs/2501.04187v1|Massimiliano Russo,Steffen Ventz,Lorenzo Trippa
590|Bayesian phase I/II adaptively randomized oncology trials with combined drugs|We propose a new integrated phase I/II trial design to identify the most efficacious dose combination that also satisfies certain safety requirements for drug-combination trials. We first take a Bayesian copula-type model for dose finding in phase I. After identifying a set of admissible doses, we immediately move the entire set forward to phase II. We propose a novel adaptive randomization scheme to favor assigning patients to more efficacious dose-combination arms. Our adaptive randomization scheme takes into account both the point estimate and variability of efficacy. By using a moving reference to compare the relative efficacy among treatment arms, our method achieves a high resolution to distinguish different arms. We also consider groupwise adaptive randomization when efficacy is late-onset. We conduct extensive simulation studies to examine the operating characteristics of the proposed design, and illustrate our method using a phase I/II melanoma clinical trial.|http://arxiv.org/abs/1108.1614v1|Ying Yuan,Guosheng Yin
591|An optimal multi-period crossover design for an application in paediatric nephrology|Crossover clinical trials can provide substantial benefits by eliminating inter-patient variation from treatment comparisons and by allowing multiple observations of each patient. They are particularly useful when sample sizes are necessarily small. These advantages proved particularly valuable in an assessment of clot prevention in children undergoing haemodialysis. Only small numbers of children are treated at any given time in any single unit, but each patient is obliged to attend two or three times each week, suggesting the use of a crossover trial with many periods. Standard crossover trials described in the literature a) typically have fewer than 10 periods and b) are based on a model of questionable applicability to this study. This paper describes the derivation of an optimal crossover trial with 30 periods which was used to compare the treatments using nine patients.|http://arxiv.org/abs/1210.6260v1|John N. S. Matthews
592|Cancer phase I trial design using drug combinations when a fraction of dose limiting toxicities is attributable to one or more agents|Drug combination trials are increasingly common nowadays in clinical research. However, very few methods have been developed to consider toxicity attributions in the dose escalation process. We are motivated by a trial in which the clinician is able to identify certain toxicities that can be attributed to one of the agents. We present a Bayesian adaptive design in which toxicity attributions are modeled via Copula regression and the maximum tolerated dose (MTD) curve is estimated as a function of model parameters. The dose escalation algorithm uses cohorts of two patients, following the continual reassessment method (CRM) scheme, where at each stage of the trial, we search for the dose of one agent given the current dose of the other agent. The performance of the design is studied by evaluating its operating characteristics when the underlying model is either correctly specified or misspecified. We show that this method can be extended to accommodate discrete dose combinations.|http://arxiv.org/abs/1709.00918v3|Jose L. Jimenez,Mourad Tighiouart,Mauro Gasparini
593|Optimality of testing procedures for survival data|Most statistical tests for treatment effects used in randomized clinical trials with survival outcomes are based on the proportional hazards assumption, which often fails in practice. Data from early exploratory studies may provide evidence of non-proportional hazards which can guide the choice of alternative tests in the design of practice-changing confirmatory trials. We study a test to detect treatment effects in a late-stage trial which accounts for the deviations from proportional hazards suggested by early-stage data. Conditional on early-stage data, among all tests which control the frequentist Type I error rate at a fixed $\alpha$ level, our testing procedure maximizes the Bayesian prediction of the finite-sample power. Hence, the proposed test provides a useful benchmark for other tests commonly used in presence of non-proportional hazards, for example weighted log-rank tests. We illustrate the approach in a simulations based on data from a published cancer immunotherapy phase III trial.|http://arxiv.org/abs/1902.00161v3|Andrea Arf,Brian Alexander,Lorenzo Trippa
594|A Nonparametric Bayesian Design for Drug Combination Cancer Trials|We propose an adaptive design for early phase drug combination cancer trials with the goal of estimating the maximum tolerated dose (MTD). A nonparametric Bayesian model, using beta priors truncated to the set of partially ordered dose combinations, is used to describe the probability of dose limiting toxicity (DLT). Dose allocation between successive cohorts of patients is estimated using a modified Continual Reassessment scheme. The updated probabilities of DLT are calculated with a Gibbs sampler that employs a weighting mechanism to calibrate the influence of data versus the prior. At the end of the trial, we recommend one or more dose combinations as the MTD based on our proposed algorithm. The design operating characteristics indicate that our method is comparable with existing methods. As an illustration, we apply our method to a phase I clinical trial of CB-839 and Gemcitabine.|http://arxiv.org/abs/1910.09163v1|Zahra S. Razaee,Galen Wien-Cook,Mourad Tighiouart
595|Design and Analysis of group-sequential clinical trials based on a modestly-weighted log-rank test in anticipation of a delayed separation of survival curves: A practical guidance|A common feature of many recent trials evaluating the effects of immunotherapy on survival is that non-proportional hazards can be anticipated at the design stage. This raises the possibility to use a statistical method tailored towards testing the purported long-term benefit, rather than applying the more standard log-rank test and/or Cox model. Many such proposals have been made in recent years, but there remains a lack of practical guidance on implementation, particularly in the context of group-sequential designs. In this article, we aim to fill this gap. We discuss how the POPLAR trial, which compared immunotherapy versus chemotherapy in non-small-cell lung cancer, might have been re-designed to be more robust to the presence of a delayed effect. We then provide step-by-step instructions on how to analyse a hypothetical realisation of the trial, based on this new design. Basic theory on weighted log-rank tests and group-sequential methods is covered, and an accompanying R package (including vignette) is provided.|http://arxiv.org/abs/2102.05535v3|Dominic Magirr,Jos L. Jimnez
596|Estimating the Efficiency Gain of Covariate-Adjusted Analyses in Future Clinical Trials Using External Data|We present a general framework for using existing data to estimate the efficiency gain from using a covariate-adjusted estimator of a marginal treatment effect in a future randomized trial. We describe conditions under which it is possible to define a mapping from the distribution that generated the existing external data to the relative efficiency of a covariate-adjusted estimator compared to an unadjusted estimator. Under conditions, these relative efficiencies approximate the ratio of sample size needed to achieve a desired power. We consider two situations where the outcome is either fully or partially observed and several treatment effect estimands that are of particular interest in most trials. For each such estimand, we develop a semiparametrically efficient estimator of the relative efficiency that allows for the application of flexible statistical learning tools to estimate the nuisance functions and an analytic form of a corresponding Wald-type confidence interval. We also propose a double bootstrap scheme for constructing confidence intervals. We demonstrate the performance of the proposed methods through simulation studies and apply these methods to data to estimate the relative efficiency of using covariate adjustment in Covid-19 therapeutic trials.|http://arxiv.org/abs/2104.14752v1|Xiudi Li,Sijia Li,Alex Luedtke
597|Causal Analysis of the TOPCAT Trial: Spironolactone for Preserved Cardiac Function Heart Failure|We describe the results of applying causal discovery methods on the data from a multi-site clinical trial, on the Treatment of Preserved Cardiac Function Heart Failure with an Aldosterone Antagonist (TOPCAT). The trial was inconclusive, with no clear benefits consistently shown for the whole cohort. However, there were questions regarding the reliability of the diagnosis and treatment protocol for a geographic subgroup of the cohort. With the inclusion of medical context in the form of domain knowledge, causal discovery is used to demonstrate regional discrepancies and to frame the regional transportability of the results. Furthermore, we show that, globally and especially for some subgroups, the treatment has significant causal effects, thus offering a more refined view of the trial results.|http://arxiv.org/abs/2211.12983v1|Francesca E. D. Raimondi,Tadhg O'Keeffe,Hana Chockler,Andrew R. Lawrence,Tamara Stemberga,Andre Franca,Maksim Sipos,Javed Butler,Shlomo Ben-Haim
598|Time-to-Event Model-Assisted Designs to Accelerate Phase I Clinical Trials|Two useful strategies to speed up drug development are to increase the patient accrual rate and use novel adaptive designs. Unfortunately, these two strategies often conflict when the evaluation of the outcome cannot keep pace with the patient accrual rate and thus the interim data cannot be observed in time to make adaptive decisions. A similar logistic difficulty arises when the outcome is of late onset. Based on a novel formulation and approximation of the likelihood of the observed data, we propose a general methodology for model-assisted designs to handle toxicity data that are pending due to fast accrual or late-onset toxicity, and facilitate seamless decision making in phase I dose-finding trials. The dose escalation/de-escalation rules of the proposed time-to-event model-assisted designs can be tabulated before the trial begins, which greatly simplifies trial conduct in practice compared to that under existing methods. We show that the proposed designs have desirable finite and large-sample properties and yield performance that is superior to that of more complicated model-based designs. We provide user-friendly software for implementing the designs.|http://arxiv.org/abs/1807.08393v1|Ruitao Lin,Ying Yuan
599|Accounting for Inconsistent Use of Covariate Adjustment in Group Sequential Trials|Group sequential designs in clinical trials allow for interim efficacy and futility monitoring. Adjustment for baseline covariates can increase power and precision of estimated effects. However, inconsistently applying covariate adjustment throughout the stages of a group sequential trial can result in inflation of type I error, biased point estimates, and anti-conservative confidence intervals. We propose methods for performing correct interim monitoring, estimation, and inference in this setting that avoid these issues. We focus on two-arm trials with simple, balanced randomization and continuous outcomes. We study the performance of our boundary, estimation, and inference adjustments in simulation studies. We end with recommendations about the application of covariate adjustment in group sequential designs.|http://arxiv.org/abs/2206.12393v2|Marlena S. Bannick,Sonya L. Heltshe,Noah Simon
600|Automatically Summarizing Evidence from Clinical Trials: A Prototype Highlighting Current Challenges|We present TrialsSummarizer, a system that aims to automatically summarize evidence presented in the set of randomized controlled trials most relevant to a given query. Building on prior work, the system retrieves trial publications matching a query specifying a combination of condition, intervention(s), and outcome(s), and ranks these according to sample size and estimated study quality. The top-k such studies are passed through a neural multi-document summarization system, yielding a synopsis of these trials. We consider two architectures: A standard sequence-to-sequence model based on BART and a multi-headed architecture intended to provide greater transparency to end-users. Both models produce fluent and relevant summaries of evidence retrieved for queries, but their tendency to introduce unsupported statements render them inappropriate for use in this domain at present. The proposed architecture may help users verify outputs allowing users to trace generated tokens back to inputs.|http://arxiv.org/abs/2303.05392v1|Sanjana Ramprasad,Denis Jered McInerney,Iain J. Marshal,Byron C. Wallace
601|The Use of Covariate Adjustment in Randomized Controlled Trials: An Overview|There has been a growing interest in covariate adjustment in the analysis of randomized controlled trials in past years. For instance, the U.S. Food and Drug Administration recently issued guidance that emphasizes the importance of distinguishing between conditional and marginal treatment effects. Although these effects coincide in linear models, this is not typically the case in other settings, and this distinction is often overlooked in clinical trial practice. Considering these developments, this paper provides a review of when and how to utilize covariate adjustment to enhance precision in randomized controlled trials. We describe the differences between conditional and marginal estimands and stress the necessity of aligning statistical analysis methods with the chosen estimand. Additionally, we highlight the potential misalignment of current practices in estimating marginal treatment effects. Instead, we advocate for the utilization of standardization, which can improve efficiency by leveraging the information contained in baseline covariates while remaining robust to model misspecification. Finally, we present practical considerations that have arisen in our respective consultations to further clarify the advantages and limitations of covariate adjustment.|http://arxiv.org/abs/2306.05823v1|Kelly Van Lancker,Frank Bretz,Oliver Dukes
602|Adjusting confidence intervals under covariate-adaptive randomization in non-inferiority and equivalence trials|Regulatory authorities guide the use of permutation tests or randomization tests so as not to increase the type-I error rate when applying covariate-adaptive randomization in randomized clinical trials. For non-inferiority and equivalence trials, this paper derives adjusted confidence intervals using permutation and randomization methods, thus controlling the type-I error to be much closer to the pre-specified nominal significance level. We consider three variable types for the outcome of interest, namely normal, binary, and time-to-event variables for the adjusted confidence intervals. For normal variables, we show that the type-I error for the adjusted confidence interval holds the nominal significance level. However, we highlight a unique theoretical challenge for non-inferiority and equivalence trials: binary and time-to-event variables may not hold the nominal significance level when the model parameters are estimated by models that diverge from the data-generating model under the null hypothesis. To clarify these features, we present simulation results and evaluate the performance of the adjusted confidence intervals.|http://arxiv.org/abs/2312.15619v1|Masahiro Kojima,Hirotaka Mano,Kana Yamada,Keisuke Hanada,Yuji Tanaka,Junji Moriya
603|Adaptive Experiment Design with Synthetic Controls|Clinical trials are typically run in order to understand the effects of a new treatment on a given population of patients. However, patients in large populations rarely respond the same way to the same treatment. This heterogeneity in patient responses necessitates trials that investigate effects on multiple subpopulations - especially when a treatment has marginal or no benefit for the overall population but might have significant benefit for a particular subpopulation. Motivated by this need, we propose Syntax, an exploratory trial design that identifies subpopulations with positive treatment effect among many subpopulations. Syntax is sample efficient as it (i) recruits and allocates patients adaptively and (ii) estimates treatment effects by forming synthetic controls for each subpopulation that combines control samples from other subpopulations. We validate the performance of Syntax and provide insights into when it might have an advantage over conventional trial designs through experiments.|http://arxiv.org/abs/2401.17205v2|Alihan Hyk,Zhaozhi Qian,Mihaela van der Schaar
604|Machine Learning Assisted Adjustment Boosts Efficiency of Exact Inference in Randomized Controlled Trials|In this work, we proposed a novel inferential procedure assisted by machine learning based adjustment for randomized control trials. The method was developed under the Rosenbaum's framework of exact tests in randomized experiments with covariate adjustments. Through extensive simulation experiments, we showed the proposed method can robustly control the type I error and can boost the statistical efficiency for a randomized controlled trial (RCT). This advantage was further demonstrated in a real-world example. The simplicity, flexibility, and robustness of the proposed method makes it a competitive candidate as a routine inference procedure for RCTs, especially when nonlinear association or interaction among covariates is expected. Its application may remarkably reduce the required sample size and cost of RCTs, such as phase III clinical trials.|http://arxiv.org/abs/2403.03058v2|Han Yu,Alan D. Hutson,Xiaoyi Ma
605|A Practical Analysis Procedure on Generalizing Comparative Effectiveness in the Randomized Clinical Trial to the Real-world Trialeligible Population|When evaluating the effectiveness of a drug, a Randomized Controlled Trial (RCT) is often considered the gold standard due to its perfect randomization. While RCT assures strong internal validity, its restricted external validity poses challenges in extending treatment effects to the broader real-world population due to possible heterogeneity in covariates. In this paper, we introduce a procedure to generalize the RCT findings to the real-world trial-eligible population based on the adaption of existing statistical methods. We utilized the augmented inversed probability of sampling weighting (AIPSW) estimator for the estimation and omitted variable bias framework to assess the robustness of the estimate against the assumption violation caused by potentially unmeasured confounders. We analyzed an RCT comparing the effectiveness of lowering hypertension between Songling Xuemaikang Capsule (SXC), a traditional Chinese medicine (TCM), and Losartan as an illustration. The generalization results indicated that although SXC is less effective in lowering blood pressure than Losartan on week 2, week 4, and week 6, there is no statistically significant difference among the trial-eligible population at week 8, and the generalization is robust against potential unmeasured confounders.|http://arxiv.org/abs/2406.04107v1|Kuan Jiang,Xin-xing Lai,Shu Yang,Ying Gao,Xiao-Hua Zhou
606|A fast, flexible simulation framework for Bayesian adaptive designs -- the R package BATSS|The use of Bayesian adaptive designs for randomised controlled trials has been hindered by the lack of software readily available to statisticians. We have developed a new software package (Bayesian Adaptive Trials Simulator Software - BATSS for the statistical software R, which provides a flexible structure for the fast simulation of Bayesian adaptive designs for clinical trials. We illustrate how the BATSS package can be used to define and evaluate the operating characteristics of Bayesian adaptive designs for various different types of primary outcomes (e.g., those that follow a normal, binary, Poisson or negative binomial distribution) and can incorporate the most common types of adaptations: stopping treatments (or the entire trial) for efficacy or futility, and Bayesian response adaptive randomisation - based on user-defined adaptation rules. Other important features of this highly modular package include: the use of (Integrated Nested) Laplace approximations to compute posterior distributions, parallel processing on a computer or a cluster, customisability, adjustment for covariates and a wide range of available conditional distributions for the response.|http://arxiv.org/abs/2410.02050v1|Dominique-Laurent Couturier,Elizabeth G Ryan,Rainer Puhr,Thomas Jaki,Stephane Heritier
607|ZIKQ: An innovative centile chart method for utilizing natural history data in rare disease clinical development|Utilizing natural history data as external control plays an important role in the clinical development of rare diseases, since placebo groups in double-blind randomization trials may not be available due to ethical reasons and low disease prevalence. This article proposed an innovative approach for utilizing natural history data to support rare disease clinical development by constructing reference centile charts. Due to the deterioration nature of certain rare diseases, the distributions of clinical endpoints can be age-dependent and have an absorbing state of zero, which can result in censored natural history data. Existing methods of reference centile charts can not be directly used in the censored natural history data. Therefore, we propose a new calibrated zero-inflated kernel quantile (ZIKQ) estimation to construct reference centile charts from censored natural history data. Using the application to Duchenne Muscular Dystrophy drug development, we demonstrate that the reference centile charts using the ZIKQ method can be implemented to evaluate treatment efficacy and facilitate a more targeted patient enrollment in rare disease clinical development.|http://arxiv.org/abs/2405.17684v1|Tianying Wang,Wenfei Zhang,Ying Wei
608|Beyond accuracy: quantifying trial-by-trial behaviour of CNNs and humans by measuring error consistency|A central problem in cognitive science and behavioural neuroscience as well as in machine learning and artificial intelligence research is to ascertain whether two or more decision makers (be they brains or algorithms) use the same strategy. Accuracy alone cannot distinguish between strategies: two systems may achieve similar accuracy with very different strategies. The need to differentiate beyond accuracy is particularly pressing if two systems are near ceiling performance, like Convolutional Neural Networks (CNNs) and humans on visual object recognition. Here we introduce trial-by-trial error consistency, a quantitative analysis for measuring whether two decision making systems systematically make errors on the same inputs. Making consistent errors on a trial-by-trial basis is a necessary condition for similar processing strategies between decision makers. Our analysis is applicable to compare algorithms with algorithms, humans with humans, and algorithms with humans. When applying error consistency to object recognition we obtain three main findings: (1.) Irrespective of architecture, CNNs are remarkably consistent with one another. (2.) The consistency between CNNs and human observers, however, is little above what can be expected by chance alone -- indicating that humans and CNNs are likely implementing very different strategies. (3.) CORnet-S, a recurrent model termed the "current best model of the primate ventral visual stream", fails to capture essential characteristics of human behavioural data and behaves essentially like a standard purely feedforward ResNet-50 in our analysis. Taken together, error consistency analysis suggests that the strategies used by human and machine vision are still very different -- but we envision our general-purpose error consistency analysis to serve as a fruitful tool for quantifying future progress.|http://arxiv.org/abs/2006.16736v3|Robert Geirhos,Kristof Meding,Felix A. Wichmann
609|Batched bandit problems|Motivated by practical applications, chiefly clinical trials, we study the regret achievable for stochastic bandits under the constraint that the employed policy must split trials into a small number of batches. We propose a simple policy, and show that a very small number of batches gives close to minimax optimal regret bounds. As a byproduct, we derive optimal policies with low switching cost for stochastic bandits.|http://arxiv.org/abs/1505.00369v3|Vianney Perchet,Philippe Rigollet,Sylvain Chassang,Erik Snowberg
610|Compliance-Aware Bandits|Motivated by clinical trials, we study bandits with observable non-compliance. At each step, the learner chooses an arm, after, instead of observing only the reward, it also observes the action that took place. We show that such noncompliance can be helpful or hurtful to the learner in general. Unfortunately, naively incorporating compliance information into bandit algorithms loses guarantees on sublinear regret. We present hybrid algorithms that maintain regret bounds up to a multiplicative factor and can incorporate compliance information. Simulations based on real data from the International Stoke Trial show the practical potential of these algorithms.|http://arxiv.org/abs/1602.02852v1|Nicols Della Penna,Mark D. Reid,David Balduzzi
611|Random Measures, ANOVA Models and Quantifying Uncertainty in Randomized Controlled Trials|This short paper introduces a novel approach to global sensitivity analysis, grounded in the variance-covariance structure of random variables derived from random measures. The proposed methodology facilitates the application of information-theoretic rules for uncertainty quantification, offering several advantages. Specifically, the approach provides valuable insights into the decomposition of variance within discrete subspaces, similar to the standard ANOVA analysis. To illustrate this point, the method is applied to datasets obtained from the analysis of randomized controlled trials on evaluating the efficacy of the COVID-19 vaccine and assessing clinical endpoints in a lung cancer study.|http://arxiv.org/abs/2312.10541v1|Caleb Deen Bastian,Herschel Rabitz,Grzegorz A Rempala
612|Testing for similarity of dose response in multi-regional clinical trials|This paper addresses the problem of deciding whether the dose response relationships between subgroups and the full population in a multi-regional trial are similar to each other. Similarity is measured in terms of the maximal deviation between the dose response curves. We consider a parametric framework and develop two powerful bootstrap tests for the similarity between the dose response curves of one subgroup and the full population, and for the similarity between the dose response curves of several subgroups and the full population. We prove the validity of the tests, investigate the finite sample properties by means of a simulation study and finally illustrate the methodology in a case study.|http://arxiv.org/abs/2404.17682v1|Holger Dette,Lukas Koletzko,Frank Bretz
613|Broad versus narrow research questions in evidence synthesis: a parallel to (and plea for) estimands|There has been a transition from broad to more specific research questions in the practice of network meta-analysis (NMA). Such convergence is also taking place in the context of individual registrational trials, following the recent introduction of the estimand framework, which is impacting the design, data collection strategy, analysis and interpretation of clinical trials. The language of estimands has much to offer to NMA, particularly given the "narrow" perspective of treatments and target populations taken in health technology assessment.|http://arxiv.org/abs/2408.12932v1|Antonio Remiro-Azcar,Anders Gorst-Rasmussen
614|Mining Brain Networks using Multiple Side Views for Neurological Disorder Identification|Mining discriminative subgraph patterns from graph data has attracted great interest in recent years. It has a wide variety of applications in disease diagnosis, neuroimaging, etc. Most research on subgraph mining focuses on the graph representation alone. However, in many real-world applications, the side information is available along with the graph data. For example, for neurological disorder identification, in addition to the brain networks derived from neuroimaging data, hundreds of clinical, immunologic, serologic and cognitive measures may also be documented for each subject. These measures compose multiple side views encoding a tremendous amount of supplemental information for diagnostic purposes, yet are often ignored. In this paper, we study the problem of discriminative subgraph selection using multiple side views and propose a novel solution to find an optimal set of subgraph features for graph classification by exploring a plurality of side views. We derive a feature evaluation criterion, named gSide, to estimate the usefulness of subgraph patterns based upon side views. Then we develop a branch-and-bound algorithm, called gMSV, to efficiently search for optimal subgraph features by integrating the subgraph mining process and the procedure of discriminative feature selection. Empirical studies on graph classification tasks for neurological disorders using brain networks demonstrate that subgraph patterns selected by the multi-side-view guided subgraph selection approach can effectively boost graph classification performances and are relevant to disease diagnosis.|http://arxiv.org/abs/1508.04554v1|Bokai Cao,Xiangnan Kong,Jingyuan Zhang,Philip S. Yu,Ann B. Ragin
615|On the role of $\rm CD8^+$ T cells in determining recovery time from influenza virus infection|Myriad experiments have identified an important role for $\rm CD8^+$ T cell response mechanisms in determining recovery from influenza A virus infection. Animal models of influenza infection further implicate multiple elements of the immune response in defining the dynamical characteristics of viral infection. To date, influenza virus models, while capturing particular aspects of the natural infection history, have been unable to reproduce the full gamut of observed viral kinetic behaviour in a single coherent framework. Here, we introduce a mathematical model of influenza viral dynamics incorporating all major immune components (innate, humoral and cellular) and explore its properties with a particular emphasis on the role of cellular immunity. Calibrated against a range of murine data, our model is capable of recapitulating observed viral kinetics from a multitude of experiments. Importantly, the model predicts a robust exponential relationship between the level of effector $\rm CD8^+$ T cells and recovery time, whereby recovery time rapidly decreases to a fixed minimum recovery time with an increasing level of effector $\rm CD8^+$ T cells. We find support for this relationship in recent clinical data from influenza A(H7N9) hospitalised patients. The exponential relationship implies that people with a lower level of naive $\rm CD8^+$ T cells may receive significantly more benefit from induction of additional effector $\rm CD8^+$ T cells arising from immunological memory, itself established through either previous viral infection or T cell-based vaccines.|http://arxiv.org/abs/1609.05977v1|Pengxing Cao,Zhongfang Wang,Ada W. C. Yan,Jodie McVernon,Jianqing Xu,Jane M. Heffernan,Katherine Kedzierska,James M. McCaw
616|Histopathology of Third Trimester Placenta from SARS-CoV-2-Positive Women|Background: This study aims to investigate whether maternal SARS-CoV-2 status affect placental pathology. Methods: A retrospective case-control study was conducted by reviewing charts and slides of placentas between April 1 to July 24, 2020. Clinical history of COVID-19 were searched in Pathology Database (CoPath). Controls were matched with SARS-CoV-2-negative women with singleton deliveries in the 3rd-trimester. Individual and group, pathological features were extracted from placental pathology reports. Results: Twenty-one 3rd-trimester, placentas from SARS-CoV-2-positive women were identified and compared to 20 placentas from SARS-CoV-2-negative women. There were no significant differences in individual or group gross or microscopic pathological features between the groups. Within the SARS-CoV-2+ group, there are no differences between symptomatic and asymptomatic women. Conclusion: Placentas from SARS-CoV-2-positive women do not demonstrate a specific pathological pattern. Pregnancy complicated with COVID-19 during the 3rd trimester does not have a demonstrable effect on placental structure and pathology.|http://arxiv.org/abs/2008.12104v1|Mai He,Priya Skaria,Kasey Kreutz,Ling Chen,Ian Hagemann,Ebony B. Carter,Indira U. Mysorekar,D Michael Nelson,John Pfeifer,Louis P. Dehner
617|Estimating Waning of Vaccine Effectiveness: a Simulation Study|Developing accurate and reliable methods to estimate vaccine protection is a key goal in immunology and public health. While several statistical methods have been proposed, their potential inaccuracy in capturing fast intra-seasonal waning of vaccine-induced protection needs to be rigorously investigated. To compare statistical methods for vaccine effectiveness (VE) estimation, we generated simulated data using a multiscale agent-based model of an epidemic with an acute viral infection and differing extents of VE waning. We extended the previously proposed framework for VE measures based on the observational data richness to assess changes of vaccine-induced protection with time. While VE measures based on hard-to-collect information (e.g. exact timing of exposures) were accurate, usually VE studies rely on time-to-infection data and the Cox proportional hazard model. We found that its extension utilizing scaled Schoenfeld residuals, previously proposed for capturing VE waning, was unreliable in capturing both the degree of waning and its functional form and identified the mathematical factors contributing to this unreliability. We showed that partitioning time and including a time-vaccine interaction term in the Cox model significantly improved estimation of VE waning, even in the case of dramatic, rapid waning. We also proposed how to optimize the partitioning scheme. Using simulated data, we compared different measures of VE for capturing the intra-seasonal waning of vaccine-induced protection. We propose an extension of the Cox model based on including a time-vaccine interaction term with further optimization of partitioning time. These findings may guide future analysis of VE waning in observational data.|http://arxiv.org/abs/2205.12269v1|Ariel Nikas,Hasan Ahmed,Veronika I. Zarnitsyna
618|Integrative Pan-Cancer Analysis of RNMT: a Potential Prognostic and Immunological Biomarker|Background: RNA guanine-7 methyltransferase (RNMT) is one of the main regulators of N7-methylguanosine, and the deregulation of RNMT correlated with tumor development and immune metabolism. However, the specific function of RNMT in pan-cancer remains unclear.   Methods: RNMT expression in different cancers was analyzed using multiple databases, including Cancer Cell Line Encyclopedia (CCLE), Genotype-Tissue Expression Project (GTEx), and The Cancer Genome Atlas (TCGA). Cox regression analysis and Kaplan-Meier analysis were used to estimate the correlation of RNMT expression to prognosis. The data was also used to research the relationship between RNMT expression and common immunoregulators, tumor mutation burden (TMB), microsatellite instability (MSI), mismatch repair (MMR), and DNA methyltransferase (DNMT). Additionally, the cBioPortal website was used to evaluate the characteristics of RNMT alteration. The TISDB database was used to obtain the expression of different subtypes. The Tumor Immune Estimation Resource (TIMER) database was used to analyze the association between RNMT and tumor immune infiltration. Gene set enrichment analysis (GSEA) was used to identify the relevant pathways.   Results: RNMT was ubiquitously highly expressed across cancers and survival analysis revealed that its expression was highly associated with the clinical prognosis of various cancer types. Remarkably, RNMT participates in immune regulation and plays a crucial part in the tumor microenvironment. A positive association was found between RNMT expression and six immune cell types expression in colon adenocarcinoma, kidney renal clear cell carcinoma, and liver hepatocellular carcinoma. Moreover, RNMT expression was highly associated with immunoregulators in most cancer types, and correlated to TMB, MSI, MMR, and DNMT. Finally, GSEA indicated that RNMT may correlate with tumor immunity.|http://arxiv.org/abs/2210.09574v2|Shuqiang Huang,Cuiyu Tan,Jinzhen Zheng,Zhugu Huang,Zhihong Li,Ziyin Lv,Wanru Chen
619|Prediction of a T-cell/MHC-I-based immune profile for colorectal liver metastases from CT images using ensemble learning|Colorectal cancer liver metastases (CLM) are the most common type of distant metastases originating from the abdomen and are characterized by a high recurrence rate after curative resection. It has been previously reported that CLM presenting a low cluster of differentiation 3 (CD3) positive T-cell infiltration density concurrent with a high major histocompatibility complex class I (MHC-I) expression were associated with poor clinical outcomes. In this study, we attempt to noninvasively predict whether a CLM exhibit the CD3LowMHCHigh immunological profile using preoperative CT images. To this end, we propose an ensemble network combining multiple Attentive Interpretable Tabular learning (TabNet) models, trained using CT-derived radiomic features. A total of 160 CLM were included in this study and randomly divided between a training set (n=130) and a hold-out test set (n=30). The proposed model yielded good prediction performance on the test set with an accuracy of 70.0% [95% confidence interval 53.6%-86.4%] and an area under the curve of 69.4% [52.9%-85.9%]. It also outperformed other off-the-shelf machine learning models. We finally demonstrated that the predicted immune profile was associated with a shorter disease-specific survival (p = .023) and time-to-recurrence (p = .020), showing the value of assessing the immune response.|http://arxiv.org/abs/2303.04149v1|Ralph Saber,David Henault,Rolando Rebolledo,Simon Turcotte,Samuel Kadoury
620|A Two-Stage Approach for Segmenting Spatial Point Patterns Applied to Multiplex Imaging|Recent advances in multiplex imaging have enabled researchers to locate different types of cells within a tissue sample. This is especially relevant for tumor immunology, as clinical regimes corresponding to different stages of disease or responses to treatment may manifest as different spatial arrangements of tumor and immune cells. Spatial point pattern modeling can be used to partition multiplex tissue images according to these regimes. To this end, we propose a two-stage approach: first, local intensities and pair correlation functions are estimated from the spatial point pattern of cells within each image, and the pair correlation functions are reduced in dimension via spectral decomposition of the covariance function. Second, the estimates are clustered in a Bayesian hierarchical model with spatially-dependent cluster labels. The clusters correspond to regimes of interest that are present across subjects; the cluster labels segment the spatial point patterns according to those regimes. Through Markov Chain Monte Carlo sampling, we jointly estimate and quantify uncertainty in the cluster assignment and spatial characteristics of each cluster. Simulations demonstrate the performance of the method, and it is applied to a set of multiplex immunofluorescence images of diseased pancreatic tissue.|http://arxiv.org/abs/2412.08828v1|Alvin Sheng,Brian J. Reich,Ana-Maria Staicu,Santhoshi N. Krishnan,Arvind Rao,Timothy L. Frankel
621|Extraction of evidence tables from abstracts of randomized clinical trials using a maximum entropy classifier and global constraints|Systematic use of the published results of randomized clinical trials is increasingly important in evidence-based medicine. In order to collate and analyze the results from potentially numerous trials, evidence tables are used to represent trials concerning a set of interventions of interest. An evidence table has columns for the patient group, for each of the interventions being compared, for the criterion for the comparison (e.g. proportion who survived after 5 years from treatment), and for each of the results. Currently, it is a labour-intensive activity to read each published paper and extract the information for each field in an evidence table. There have been some NLP studies investigating how some of the features from papers can be extracted, or at least the relevant sentences identified. However, there is a lack of an NLP system for the systematic extraction of each item of information required for an evidence table. We address this need by a combination of a maximum entropy classifier, and integer linear programming. We use the later to handle constraints on what is an acceptable classification of the features to be extracted. With experimental results, we demonstrate substantial advantages in using global constraints (such as the features describing the patient group, and the interventions, must occur before the features describing the results of the comparison).|http://arxiv.org/abs/1509.05209v1|Antonio Trenta,Anthony Hunter,Sebastian Riedel
622|A Semiparametric Joint Model for Terminal Trend of Quality of Life and Survival in Palliative Care Research|Palliative medicine is an interdisciplinary specialty focusing on improving quality of life (QOL) for patients with serious illness and their families. Palliative care programs are available or under development at over 80% of large US hospitals (300+ beds). Palliative care clinical trials present unique analytic challenges relative to evaluating the palliative care treatment efficacy which is to improve patients diminishing QOL as disease progresses towards end of life (EOL). A unique feature of palliative care clinical trials is that patients will experience decreasing QOL during the trial despite potentially beneficial treatment. Often longitudinal QOL and survival data are highly correlated which, in the face of censoring, makes it challenging to properly analyze and interpret longitudinal QOL trajectory. To address these issues, we propose a novel semiparametric statistical approach to jointly model longitudinal QOL and survival data. There are two sub-models in our approach: a semiparametric mixed effects model for longitudinal QOL and a Cox model for survival. We use regression splines method to estimate the nonparametric curves and AIC to select knots. We assess the model through simulation and application to establish a novel modeling approach that could be applied in future palliative care treatment research trials.|http://arxiv.org/abs/1603.01851v3|Zhigang Li,H. R. Frost,Tor D. Tosteson,Lihui Zhao,Lei Liu,Kathleen Lyons,Huaihou Chen,Bernard Cole,David Currow,Marie Bakitas
623|The i3+3 Design for Phase I Clinical Trials|Purpose: The 3+3 design has been shown to be less likely to achieve the objectives of phase I dose-finding trials when compared with more advanced model-based designs. One major criticism of the 3+3 design is that it is based on simple rules, does not depend on statistical models for inference, and leads to unsafe and unreliable operating characteristics. On the other hand, being rule-based allows 3+3 to be easily understood and implemented in practice, making it the first choice among clinicians. Is it possible to have a rule-based design with great performance? Methods: We propose a new rule-based design called i3+3, where the letter "i" represents the word "interval". The i3+3 design is based on simple but more advanced rules that account for the variabilities in the observed data. We compare the operating characteristics for the proposed i3+3 design with other popular phase I designs by simulation. Results: The i3+3 design is far superior than the 3+3 design in trial safety and the ability to identify the true MTD. Compared with model-based phase I designs, i3+3 also demonstrates comparable performances. In other words, the i3+3 design possesses both the simplicity and transparency of the rule-based approaches, and the superior operating characteristics seen in model-based approaches. An online R Shiny tool (https://i3design.shinyapps.io/i3plus3/) is provided to illustrate the i3+3 design, although in practice it requires no software to design or conduct a dose-finding trial. Conclusion: The i3+3 design could be a practice-altering method for the clinical community.|http://arxiv.org/abs/1901.01303v2|Meizi Liu,Sue-Jane Wang,Yuan Ji
624|Seamless phase II/III clinical trials using early outcomes for treatment or subgroup selection: Methods and aspects of their implementation|Adaptive seamless designs combine confirmatory testing, a domain of phase III trials, with features such as treatment or subgroup selection, typically associated with phase II trials. They promise to increase the efficiency of development programmes of new drugs, e.g. in terms of sample size and / or development time. It is well acknowledged that adaptive designs are more involved from a logistical perspective and require more upfront planning, often in form of extensive simulation studies, than conventional approaches. Here we present a framework for adaptive treatment and subgroup selection using the same notation, which links the somewhat disparate literature on treatment selection on one side and on subgroup selection on the other. Furthermore, we introduce a flexible and yet efficient simulation model that serves both designs. As primary endpoints often take a long time to observe, interim analyses are frequently informed by early outcomes. Therefore, all methods presented accommodate both, interim analyses informed by the primary outcome or an early outcome. The R package asd, previously developed to simulate designs with treatment selection, was extended to include subpopulation selection (so-called adaptive enrichment designs). Here we describe the functionality of the R package asd and use it to present some worked-up examples motivated by clinical trials in chronic obstructive pulmonary disease and oncology. The examples illustrate various features of the R package providing at the same time insights into the operating characteristics of adaptive seamless studies.|http://arxiv.org/abs/1901.08365v1|Tim Friede,Nigel Stallard,Nicholas Parsons
625|Estimation of ascertainment bias and its effect on power in clinical trials with time-to-event outcomes|While the gold standard for clinical trials is to blind all parties -- participants, researchers, and evaluators -- to treatment assignment, this is not always a possibility. When some or all of the above individuals know the treatment assignment, this leaves the study open to the introduction of post-randomization biases. In the Strategies to Reduce Injuries and Develop Confidence in Elders (STRIDE) trial, we were presented with the potential for the unblinded clinicians administering the treatment, as well as the individuals enrolled in the study, to introduce ascertainment bias into some but not all events comprising the primary outcome. In this manuscript, we present ways to estimate the ascertainment bias for a time-to-event outcome, and discuss its impact on the overall power of a trial versus changing of the outcome definition to a more stringent unbiased definition that restricts attention to measurements less subject to potentially differential assessment. We found that for the majority of situations, it is better to revise the definition to a more stringent definition, as was done in STRIDE, even though fewer events may be observed.|http://arxiv.org/abs/2004.13775v2|E. J. Greene,P. Peduzzi,J. Dziura,C. Meng,M. E. Miller,T. G. Travison,D. Esserman
626|Efficient, Doubly Robust Estimation of the Effect of Dose Switching for Switchers in a Randomised Clinical Trial|Motivated by a clinical trial conducted by Janssen Pharmaceuticals in which a flexible dosing regimen is compared to placebo, we evaluate how switchers in the treatment arm (i.e., patients who were switched to the higher dose) would have fared had they been kept on the low dose. This in order to understand whether flexible dosing is potentially beneficial for them. Simply comparing these patients' responses with those of patients who stayed on the low dose is unsatisfactory because the latter patients are usually in a better health condition. Because the available information in the considered trial is too scarce to enable a reliable adjustment, we will instead transport data from a fixed dosing trial that has been conducted concurrently on the same target, albeit not in an identical patient population. In particular, we will propose an estimator which relies on an outcome model and a propensity score model for the association between study and patient characteristics. The proposed estimator is asymptotically unbiased if at least one of both models is correctly specified, and efficient (under the model defined by the restrictions on the propensity score) when both models are correctly specified. We show that the proposed method for using results from an external study is generically applicable in studies where a classical confounding adjustment is not possible due to positivity violation (e.g., studies where switching takes place in a deterministic manner). Monte Carlo simulations and application to the motivating study demonstrate adequate performance.|http://arxiv.org/abs/2009.02136v1|Kelly Van Lancker,An Vandebosch,Stijn Vansteelandt
627|An Efficient Doubly-robust Imputation Framework for Longitudinal Dropout, with an Application to an Alzheimer's Clinical Trial|We develop a novel doubly-robust (DR) imputation framework for longitudinal studies with monotone dropout, motivated by the informative dropout that is common in FDA-regulated trials for Alzheimer's disease. In this approach, the missing data are first imputed using a doubly-robust augmented inverse probability weighting (AIPW) estimator, then the imputed completed data are substituted into a full-data estimating equation, and the estimate is obtained using standard software. The imputed completed data may be inspected and compared to the observed data, and standard model diagnostics are available. The same imputed completed data can be used for several different estimands, such as subgroup analyses in a clinical trial, allowing for reduced computation and increased consistency across analyses. We present two specific DR imputation estimators, AIPW-I and AIPW-S, study their theoretical properties, and investigate their performance by simulation. AIPW-S has substantially reduced computational burden compared to many other DR estimators, at the cost of some loss of efficiency and the requirement of stronger assumptions. Simulation studies support the theoretical properties and good performance of the DR imputation framework. Importantly, we demonstrate their ability to address time-varying covariates, such as a time by treatment interaction. We illustrate using data from a large randomized Phase III trial investigating the effect of donepezil in Alzheimer's disease, from the Alzheimer's Disease Cooperative Study (ADCS) group.|http://arxiv.org/abs/2305.02849v1|Yuqi Qiu,Karen Messer
628|The R.O.A.D. to precision medicine|We propose a prognostic stratum matching framework that addresses the deficiencies of Randomized trial data subgroup analysis and transforms ObservAtional Data to be used as if they were randomized, thus paving the road for precision medicine. Our approach counters the effects of unobserved confounding in observational data by correcting the estimated probabilities of the outcome under a treatment through a novel two-step process. These probabilities are then used to train Optimal Policy Trees (OPTs), which are decision trees that optimally assign treatments to subgroups of patients based on their characteristics. This facilitates the creation of clinically intuitive treatment recommendations. We applied our framework to observational data of patients with gastrointestinal stromal tumors (GIST) and validated the OPTs in an external cohort using the sensitivity and specificity metrics. We show that these recommendations outperformed those of experts in GIST. We further applied the same framework to randomized clinical trial (RCT) data of patients with extremity sarcomas. Remarkably, despite the initial trial results suggesting that all patients should receive treatment, our framework, after addressing imbalances in patient distribution due to the trial's small sample size, identified through the OPTs a subset of patients with unique characteristics who may not require treatment. Again, we successfully validated our recommendations in an external cohort.|http://arxiv.org/abs/2311.01681v1|Dimitris Bertsimas,Angelos G. Koulouras,Georgios Antonios Margonis
629|Extending Inferences from Randomized Clinical Trials to Target Populations: A Scoping Review of Transportability Methods|Objective: Randomized controlled trial (RCT) results often inform clinical decision-making, but the highly curated populations of trials and the care provided during the trial are often not reflective of real-world practice. The objective of this scoping review is to identify the ability of methods to transport findings from RCTs to target populations. Study design: A scoping review was conducted on the literature focusing on the transportability of the results from RCTs to observational cohorts. Each study was assessed based on the methodology used for transportability and the extent to which the treatment effect from the RCT was estimated in the target population in observational data. Results: A total of 15 published papers were included. The research topics include cardiovascular diseases, infectious diseases, psychiatry, oncology, orthopedics, anesthesiology, and hematology. These studies show that the findings from RCTs could be translated to real-world settings, with varying degrees of effect size and precision. In some cases, the estimated treatment effect for the target population were statistically significantly different from those in RCTs. Conclusion: Despite variations in the magnitude of effects between RCTs and real-world studies, transportability methods play an important role in effectively bridging the RCTs and real-world care delivery, offering valuable insights for evidence-based medicine.|http://arxiv.org/abs/2402.07236v2|Guanbo Wang,Ting-Wei Ernie Liao,David Furfaro,Leo Anthony Celi,Kevin Sheng-Kai Ma
630|Statistical Operating Characteristics of Current Early Phase Dose Finding Designs with Toxicity and Efficacy in Oncology|Traditional phase I dose finding cancer clinical trial designs aim to determine the maximum tolerated dose (MTD) of the investigational cytotoxic agent based on a single toxicity outcome, assuming a monotone dose-response relationship. However, this assumption might not always hold for newly emerging therapies such as immuno-oncology therapies and molecularly targeted therapies, making conventional dose finding trial designs based on toxicity no longer appropriate. To tackle this issue, numerous early phase dose finding clinical trial designs have been developed to identify the optimal biological dose (OBD), which takes both toxicity and efficacy outcomes into account. In this article, we review the current model-assisted dose finding designs, BOIN-ET, BOIN12, UBI, TEPI-2, PRINTE, STEIN, and uTPI to identify the OBD and compare their operating characteristics. Extensive simulation studies and a case study using a CAR T-cell therapy phase I trial have been conducted to compare the performance of the aforementioned designs under different possible dose-response relationship scenarios. The simulation results demonstrate that the performance of different designs varies depending on the particular dose-response relationship and the specific metric considered. Based on our simulation results and practical considerations, STEIN, PRINTE, and BOIN12 outperform the other designs from different perspectives.|http://arxiv.org/abs/2411.08698v1|Hao Sun,Hsin-Yu Lin,Jieqi Tu,Revathi Ananthakrishnan,Eunhee Kim
631|Application of Markov Chains to Multiple Sclerosis Clinical Trial Data to Estimate Disease Trajectories|Background: Multiple Sclerosis (MS), an autoimmune disease affecting millions worldwide, is characterized by its variable course, in which some patients will experience a more benign disease course and others a more active one, with the latter leading to permanent neural damage and disability. Methods: This study uses a Markov Chain model to demonstrate the probability of movement across different states on the Expanded Disability Status Scale (EDSS) and attempted to define worsening, improvement, cycling, and stability of these different pathways. Most importantly we were interested in assessing the lack of impermanence of confirmed disability worsening and if it could be estimated from the Markov model. Results: The study identified only 8.1% were considered worsening, 5.6% consistent improving and 86% cyclers and less than 1% consistently stable. More importantly we also found that many (approximately 30%) of participants with confirmed disability worsening (CDW) regressed to stages that were not considered worsening, on subsequent visits after CDW. Conclusions: These finding are similar to what has been reported previously as predictors of worsening, and also for a lack of durability of CDW, but our results suggest that clinical trial endpoints may need to be modified to more accurately capture differences between the treatment and control groups. Further, this suggests that the rate of worsening in trials that use time to CDW are overestimating the extent of CDW. The trials remain valid since the regressing applies to both treatment and control groups, but that the results may be underestimating the treatment benefit due to misclassification.|http://arxiv.org/abs/2412.08364v1|Uma Sthanu,Gary Cutter PhD
632|A Framework for Generating Realistic Synthetic Tabular Data in a Randomized Controlled Trial Setting|Generation of realistic synthetic data has garnered considerable attention in recent years, particularly in the health research domain due to its utility in, for instance, sharing data while protecting patient privacy or determining optimal clinical trial design. While much work has been concentrated on synthetic image generation, generation of realistic and complex synthetic tabular data of the type most commonly encountered in classic epidemiological or clinical studies is still lacking, especially with regards to generating data for randomized controlled trials (RTCs). There is no consensus regarding the best way to generate synthetic tabular RCT data such that the underlying multivariate data distribution is preserved. Motivated by an RCT in the treatment of Human Immunodeficiency Virus, we empirically compared the ability of several strategies and two generation techniques (one machine learning, the other a more classical statistical method) to faithfully reproduce realistic data. Our results suggest that using a sequential generation approach with a R-vine copula model to generate baseline variables, followed by a simple random treatment allocation to mimic the RCT setting, and subsequent regression models for variables post-treatment allocation (such as the trial outcome) is the most effective way to generate synthetic tabular RCT data that capture important and realistic features of the real data.|http://arxiv.org/abs/2501.17719v1|Niki Z. Petrakos,Erica E. M. Moodie,Nicolas Savy
633|Test Sensitivity in the Computer-Aided Detection of Breast Cancer from Clinical Mammographic Screening: a Meta-analysis|Objectives: To assess evaluative methodologies for comparative measurements of test sensitivity in clinical mammographic screening trials of computer-aided detection (CAD) technologies. Materials and Methods: This meta-analysis was performed by analytically reviewing the relevant literature on the clinical application of computer-aided detection (CAD) technologies as part of a breast cancer screening program based on x-ray mammography. Each clinical study's method for measuring the CAD system's improvement in test sensitivity is examined in this meta-analysis. The impact of the chosen sensitivity measurement on the study's conclusions are analyzed. Results: This meta-analysis demonstrates that some studies have inappropriately compared sensitivity measurements between control groups and CAD enabled groups. The inappropriate comparison of control groups and CAD enabled groups can lead to an underestimation of the benefits of the clinical application of computer-aided detection technologies. Conclusions: The potential for the sensitivity measurement issues raised in this meta-analysis to alter the conclusions of multiple existing large clinical studies is discussed. Two large scale studies are substantially affected by the analysis provided in this study and this meta-analysis demonstrates that computer-aided detection systems are successfully assisting in the breast cancer screening process.|http://arxiv.org/abs/1302.1382v1|Jacob Levman
634|Learning the progression and clinical subtypes of Alzheimer's disease from longitudinal clinical data|Alzheimer's disease (AD) is a degenerative brain disease impairing a person's ability to perform day to day activities. The clinical manifestations of Alzheimer's disease are characterized by heterogeneity in age, disease span, progression rate, impairment of memory and cognitive abilities. Due to these variabilities, personalized care and treatment planning, as well as patient counseling about their individual progression is limited. Recent developments in machine learning to detect hidden patterns in complex, multi-dimensional datasets provides significant opportunities to address this critical need. In this work, we use unsupervised and supervised machine learning approaches for subtype identification and prediction. We apply machine learning methods to the extensive clinical observations available at the Alzheimer's Disease Neuroimaging Initiative (ADNI) data set to identify patient subtypes and to predict disease progression. Our analysis depicts the progression space for the Alzheimer's disease into low, moderate and high disease progression zones. The proposed work will enable early detection and characterization of distinct disease subtypes based on clinical heterogeneity. We anticipate that our models will enable patient counseling, clinical trial design, and ultimately individualized clinical care.|http://arxiv.org/abs/1812.00546v3|Vipul Satone,Rachneet Kaur,Faraz Faghri,Mike A Nalls,Andrew B Singleton,Roy H Campbell
635|Predicting risk of late age-related macular degeneration using deep learning|By 2040, age-related macular degeneration (AMD) will affect approximately 288 million people worldwide. Identifying individuals at high risk of progression to late AMD, the sight-threatening stage, is critical for clinical actions, including medical interventions and timely monitoring. Although deep learning has shown promise in diagnosing/screening AMD using color fundus photographs, it remains difficult to predict individuals' risks of late AMD accurately. For both tasks, these initial deep learning attempts have remained largely unvalidated in independent cohorts. Here, we demonstrate how deep learning and survival analysis can predict the probability of progression to late AMD using 3,298 participants (over 80,000 images) from the Age-Related Eye Disease Studies AREDS and AREDS2, the largest longitudinal clinical trials in AMD. When validated against an independent test dataset of 601 participants, our model achieved high prognostic accuracy (five-year C-statistic 86.4 (95% confidence interval 86.2-86.6)) that substantially exceeded that of retinal specialists using two existing clinical standards (81.3 (81.1-81.5) and 82.0 (81.8-82.3), respectively). Interestingly, our approach offers additional strengths over the existing clinical standards in AMD prognosis (e.g., risk ascertainment above 50%) and is likely to be highly generalizable, given the breadth of training data from 82 US retinal specialty clinics. Indeed, during external validation through training on AREDS and testing on AREDS2 as an independent cohort, our model retained substantially higher prognostic accuracy than existing clinical standards. These results highlight the potential of deep learning systems to enhance clinical decision-making in AMD patients.|http://arxiv.org/abs/2007.09550v1|Yifan Peng,Tiarnan D. Keenan,Qingyu Chen,Elvira Agrn,Alexis Allot,Wai T. Wong,Emily Y. Chew,Zhiyong Lu
636|The Case Records of ChatGPT: Language Models and Complex Clinical Questions|Background: Artificial intelligence language models have shown promise in various applications, including assisting with clinical decision-making as demonstrated by strong performance of large language models on medical licensure exams. However, their ability to solve complex, open-ended cases, which may be representative of clinical practice, remains unexplored. Methods: In this study, the accuracy of large language AI models GPT4 and GPT3.5 in diagnosing complex clinical cases was investigated using published Case Records of the Massachusetts General Hospital. A total of 50 cases requiring a diagnosis and diagnostic test published from January 1, 2022 to April 16, 2022 were identified. For each case, models were given a prompt requesting the top three specific diagnoses and associated diagnostic tests, followed by case text, labs, and figure legends. Model outputs were assessed in comparison to the final clinical diagnosis and whether the model-predicted test would result in a correct diagnosis. Results: GPT4 and GPT3.5 accurately provided the correct diagnosis in 26% and 22% of cases in one attempt, and 46% and 42% within three attempts, respectively. GPT4 and GPT3.5 provided a correct essential diagnostic test in 28% and 24% of cases in one attempt, and 44% and 50% within three attempts, respectively. No significant differences were found between the two models, and multiple trials with identical prompts using the GPT3.5 model provided similar results. Conclusions: In summary, these models demonstrate potential usefulness in generating differential diagnoses but remain limited in their ability to provide a single unifying diagnosis in complex, open-ended cases. Future research should focus on evaluating model performance in larger datasets of open-ended clinical challenges and exploring potential human-AI collaboration strategies to enhance clinical decision-making.|http://arxiv.org/abs/2305.05609v1|Timothy Poterucha,Pierre Elias,Christopher M. Haggerty
637|Accelerating Clinical Evidence Synthesis with Large Language Models|Synthesizing clinical evidence largely relies on systematic reviews of clinical trials and retrospective analyses from medical literature. However, the rapid expansion of publications presents challenges in efficiently identifying, summarizing, and updating clinical evidence. Here, we introduce TrialMind, a generative artificial intelligence (AI) pipeline for facilitating human-AI collaboration in three crucial tasks for evidence synthesis: study search, screening, and data extraction. To assess its performance, we chose published systematic reviews to build the benchmark dataset, named TrialReviewBench, which contains 100 systematic reviews and the associated 2,220 clinical studies. Our results show that TrialMind excels across all three tasks. In study search, it generates diverse and comprehensive search queries to achieve high recall rates (Ours 0.711-0.834 v.s. Human baseline 0.138-0.232). For study screening, TrialMind surpasses traditional embedding-based methods by 30% to 160%. In data extraction, it outperforms a GPT-4 baseline by 29.6% to 61.5%. We further conducted user studies to confirm its practical utility. Compared to manual efforts, human-AI collaboration using TrialMind yielded a 71.4% recall lift and 44.2% time savings in study screening and a 23.5% accuracy lift and 63.4% time savings in data extraction. Additionally, when comparing synthesized clinical evidence presented in forest plots, medical experts favored TrialMind's outputs over GPT-4's outputs in 62.5% to 100% of cases. These findings show the promise of LLM-based approaches like TrialMind to accelerate clinical evidence synthesis via streamlining study search, screening, and data extraction from medical literature, with exceptional performance improvement when working with human experts.|http://arxiv.org/abs/2406.17755v2|Zifeng Wang,Lang Cao,Benjamin Danek,Qiao Jin,Zhiyong Lu,Jimeng Sun
638|Comparing Approaches to Treatment Effect Estimation for Subgroups in Clinical Trials|Identifying subgroups, which respond differently to a treatment, both in terms of efficacy and safety, is an important part of drug development. A well-known challenge in exploratory subgroup analyses is the small sample size in the considered subgroups, which is usually too low to allow for definite comparisons. In early phase trials this problem is further exaggerated, because limited or no clinical prior information on the drug and plausible subgroups is available. We evaluate novel strategies for treatment effect estimation in these settings in a simulation study motivated by real clinical trial situations. We compare several approaches to estimate treatment effects for selected subgroups, employing model averaging, resampling and Lasso regression methods. Two subgroup identification approaches are employed, one based on categorization of covariates and the other based on splines. Our results show that naive estimation of the treatment effect, which ignores that a selection has taken place, leads to bias and overoptimistic conclusions. For the considered simulation scenarios virtually all evaluated novel methods provide more adequate estimates of the treatment effect for selected subgroups, in terms of bias, MSE and confidence interval coverage.|http://arxiv.org/abs/1603.03316v2|Marius Thomas,Bjrn Bornkamp
639|Robust Wald-Type Tests under Random Censoring with Applications to Clinical Trial Analyses|Randomly censored survival data are frequently encountered in applied sciences including biomedical or reliability applications and clinical trial analyses. Testing the significance of statistical hypotheses is crucial in such analyses to get conclusive inference but the existing likelihood based tests, under a fully parametric model, are extremely non-robust against outliers in the data. Although, there exists a few robust parameter estimators (e.g., M-estimators and minimum density power divergence estimators) given randomly censored data, there is hardly any robust testing procedure available in the literature in this context. One of the major difficulties in this context is the construction of a suitable consistent estimator of the asymptotic variance of M estimators; the latter is a function of the unknown censoring distribution. In this paper, we take the first step in this direction by proposing a consistent estimator of asymptotic variance of the M-estimators based on randomly censored data without any assumption on the form of the censoring scheme. We then describe and study a class of robust Wald-type tests for parametric statistical hypothesis, both simple as well as composite, under such set-up, along with their general asymptotic and robustness properties. Robust tests for comparing two independent randomly censored samples and robust tests against one sided alternatives are also discussed. Their advantages and usefulness are demonstrated for the tests based on the minimum density power divergence estimators with specific attention to clinical trial analyses.|http://arxiv.org/abs/1708.09695v2|Abhik Ghosh,Ayanendranath Basu,Leandro Pardo
640|Improving Mild Cognitive Impairment Prediction via Reinforcement Learning and Dialogue Simulation|Mild cognitive impairment (MCI) is a prodromal phase in the progression from normal aging to dementia, especially Alzheimers disease. Even though there is mild cognitive decline in MCI patients, they have normal overall cognition and thus is challenging to distinguish from normal aging. Using transcribed data obtained from recorded conversational interactions between participants and trained interviewers, and applying supervised learning models to these data, a recent clinical trial has shown a promising result in differentiating MCI from normal aging. However, the substantial amount of interactions with medical staff can still incur significant medical care expenses in practice. In this paper, we propose a novel reinforcement learning (RL) framework to train an efficient dialogue agent on existing transcripts from clinical trials. Specifically, the agent is trained to sketch disease-specific lexical probability distribution, and thus to converse in a way that maximizes the diagnosis accuracy and minimizes the number of conversation turns. We evaluate the performance of the proposed reinforcement learning framework on the MCI diagnosis from a real clinical trial. The results show that while using only a few turns of conversation, our framework can significantly outperform state-of-the-art supervised learning approaches.|http://arxiv.org/abs/1802.06428v1|Fengyi Tang,Kaixiang Lin,Ikechukwu Uchendu,Hiroko H. Dodge,Jiayu Zhou
641|Robust Design and Analysis of Clinical Trials With Non-proportional Hazards: A Straw Man Guidance from a Cross-pharma Working Group|Loss of power and clear description of treatment differences are key issues in designing and analyzing a clinical trial where non-proportional hazard is a possibility. A log-rank test may be very inefficient and interpretation of the hazard ratio estimated using Cox regression is potentially problematic. In this case, the current ICH E9 (R1) addendum would suggest designing a trial with a clinically relevant estimand, e.g., expected life gain. This approach considers appropriate analysis methods for supporting the chosen estimand. However, such an approach is case specific and may suffer lack of power for important choices of the underlying alternate hypothesis distribution. On the other hand, there may be a desire to have robust power under different deviations from proportional hazards. Also, we would contend that no single number adequately describes treatment effect under non-proportional hazards scenarios. The cross-pharma working group has proposed a combination test to provide robust power under a variety of alternative hypotheses. These can be specified for primary analysis at the design stage and methods appropriately accounting for combination test correlations are efficient for a variety of scenarios. We have provided design and analysis considerations based on a combination test under different non-proportional hazard types and present a straw man proposal for practitioners. The proposals are illustrated with real life example and simulation.|http://arxiv.org/abs/1908.07112v4|Satrajit Roychoudhury,Keaven M Anderson,Jiabu Ye,Pralay Mukhopadhyay
642|Recruitment prediction for multi-centre clinical trials based on a hierarchical Poisson-gamma model: asymptotic analysis and improved intervals|We analyse predictions of future recruitment to a multi-centre clinical trial based on a maximum-likelihood fitting of a commonly used hierarchical Poisson-Gamma model for recruitments at individual centres. We consider the asymptotic accuracy of quantile predictions in the limit as the number of recruitment centres grows large and find that, in an important sense, the accuracy of the quantiles does not improve as the number of centres increases. When predicting the number of further recruits in an additional time period, the accuracy degrades as the ratio of the additional time to the census time increases, whereas when predicting the amount of additional time to recruit a further $n^+_\bullet$ patients, the accuracy degrades as the ratio of $n^+_\bullet$ to the number recruited up to the census period increases. Our analysis suggests an improved quantile predictor. Simulation studies verify that the predicted pattern holds for typical recruitment scenarios in clinical trials and verify the much improved coverage properties of prediction intervals obtained from our quantile predictor. In the process of extending the applicability of our methodology, we show that in terms of the accuracy of all integer moments it is always better to approximate the sum of independent gamma random variables by a single gamma random variable matched on the first two moments than by the moment-matched Gaussian available from the central limit theorem.|http://arxiv.org/abs/1912.09790v2|Rachael Mountain,Chris Sherlock
643|Evidence Inference 2.0: More Data, Better Models|How do we most effectively treat a disease or condition? Ideally, we could consult a database of evidence gleaned from clinical trials to answer such questions. Unfortunately, no such database exists; clinical trial results are instead disseminated primarily via lengthy natural language articles. Perusing all such articles would be prohibitively time-consuming for healthcare practitioners; they instead tend to depend on manually compiled systematic reviews of medical literature to inform care.   NLP may speed this process up, and eventually facilitate immediate consult of published evidence. The Evidence Inference dataset was recently released to facilitate research toward this end. This task entails inferring the comparative performance of two treatments, with respect to a given outcome, from a particular article (describing a clinical trial) and identifying supporting evidence. For instance: Does this article report that chemotherapy performed better than surgery for five-year survival rates of operable cancers? In this paper, we collect additional annotations to expand the Evidence Inference dataset by 25\%, provide stronger baseline models, systematically inspect the errors that these make, and probe dataset quality. We also release an abstract only (as opposed to full-texts) version of the task for rapid model prototyping. The updated corpus, documentation, and code for new baselines and evaluations are available at http://evidence-inference.ebm-nlp.com/.|http://arxiv.org/abs/2005.04177v2|Jay DeYoung,Eric Lehman,Ben Nye,Iain J. Marshall,Byron C. Wallace
644|Clinical Trial Drug Safety Assessment for Studies and Submissions Impacted by COVID-19|In this paper, we provide guidance on how standard safety analyses and reporting of clinical trial safety data may need to be modified, given the potential impact of the COVID-19 pandemic. The impact could include missed visits, alternative methods for assessments (such as virtual visits), alternative locations for assessments (such as local labs), and study drug interruptions. We focus on safety planning for Phase 2-4 clinical trials and integrated summaries for submissions. Starting from the recommended safety analyses proposed in white papers and a workshop, created as part of an FDA/PHUSE collaboration (PHUSE 2013, 2015, 2017, 2019), we assess what modifications might be needed. Impact from COVID-19 will likely affect treatment arms equally, so analyses of adverse events from controlled data can, to a large extent, remain unchanged. However, interpretation of summaries from uncontrolled data (summaries that include open-label extension data) will require even more caution than usual. Special consideration will be needed for safety topics of interest, especially events expected to have a higher incidence due to a COVID-19 infection or due to quarantine or travel restrictions (e.g., depression). Analyses of laboratory measurements may need to be modified to account for the combination of measurements from local and central laboratories.|http://arxiv.org/abs/2006.05502v1|Mary Nilsson,Brenda Crowe,Greg Anglin,Greg Ball,Melvin Munsaka,Seta Shahin,Wei Wang
645|Regression analysis for covariate-adaptive randomization: A robust and efficient inference perspective|Linear regression is arguably the most fundamental statistical model; however, the validity of its use in randomized clinical trials, despite being common practice, has never been crystal clear, particularly when stratified or covariate-adaptive randomization is used. In this paper, we investigate several of the most intuitive and commonly used regression models for estimating and inferring the treatment effect in randomized clinical trials. By allowing the regression model to be arbitrarily misspecified, we demonstrate that all these regression-based estimators robustly estimate the treatment effect, albeit with possibly different efficiency. We also propose consistent non-parametric variance estimators and compare their performances to those of the model-based variance estimators that are readily available in standard statistical software. Based on the results and taking into account both theoretical efficiency and practical feasibility, we make recommendations for the effective use of regression under various scenarios. For equal allocation, it suffices to use the regression adjustment for the stratum covariates and additional baseline covariates, if available, with the usual ordinary-least-squares variance estimator. For unequal allocation, regression with treatment-by-covariate interactions should be used, together with our proposed variance estimators. These recommendations apply to simple and stratified randomization, and minimization, among others. We hope this work helps to clarify and promote the usage of regression in randomized clinical trials.|http://arxiv.org/abs/2009.02287v1|Wei Ma,Fuyi Tu,Hanzhong Liu
646|A kernel test for quasi-independence|We consider settings in which the data of interest correspond to pairs of ordered times, e.g, the birth times of the first and second child, the times at which a new user creates an account and makes the first purchase on a website, and the entry and survival times of patients in a clinical trial. In these settings, the two times are not independent (the second occurs after the first), yet it is still of interest to determine whether there exists significant dependence {\em beyond} their ordering in time. We refer to this notion as "quasi-(in)dependence". For instance, in a clinical trial, to avoid biased selection, we might wish to verify that recruitment times are quasi-independent of survival times, where dependencies might arise due to seasonal effects. In this paper, we propose a nonparametric statistical test of quasi-independence. Our test considers a potentially infinite space of alternatives, making it suitable for complex data where the nature of the possible quasi-dependence is not known in advance. Standard parametric approaches are recovered as special cases, such as the classical conditional Kendall's tau, and log-rank tests. The tests apply in the right-censored setting: an essential feature in clinical trials, where patients can withdraw from the study. We provide an asymptotic analysis of our test-statistic, and demonstrate in experiments that our test obtains better power than existing approaches, while being more computationally efficient.|http://arxiv.org/abs/2011.08991v1|Tamara Fernndez,Wenkai Xu,Marc Ditzhaus,Arthur Gretton
647|Assessing contribution of treatment phases through tipping point analyses using rank preserving structural failure time models|In clinical trials, an experimental treatment is sometimes added on to a standard of care or control therapy in multiple treatment phases (e.g., concomitant and maintenance phases) to improve patient outcomes. When the new regimen provides meaningful benefit over the control therapy in such cases, it proves difficult to separately assess the contribution of each phase to the overall effect observed. This article provides an approach for assessing the importance of a specific treatment phase in such a situation through tipping point analyses of a time-to-event endpoint using rank-preserving-structural-failure-time (RPSFT) modeling. A tipping-point analysis is commonly used in situations where it is suspected that a statistically significant difference between treatment arms could be a result of missing or unobserved data instead of a real treatment effect. Rank-preserving-structural-failure-time modeling is an approach for causal inference that is typically used to adjust for treatment switching in clinical trials with time to event endpoints. The methodology proposed in this article is an amalgamation of these two ideas to investigate the contribution of a treatment phase of interest to the effect of a regimen comprising multiple treatment phases. We provide two different variants of the method corresponding to two different effects of interest. We provide two different tipping point thresholds depending on inferential goals. The proposed approaches are motivated and illustrated with data from a recently concluded, real-life phase 3 cancer clinical trial. We then conclude with several considerations and recommendations.|http://arxiv.org/abs/2011.09070v1|Sudipta Bhattacharya,Jyotirmoy Dey
648|Optimal multiple testing and design in clinical trials|A central goal in designing clinical trials is to find the test that maximizes power (or equivalently minimizes required sample size) for finding a false null hypothesis subject to the constraint of type I error. When there is more than one test, such as in clinical trials with multiple endpoints, the issues of optimal design and optimal procedures become more complex. In this paper we address the question of how such optimal tests should be defined and how they can be found. We review different notions of power and how they relate to study goals, and also consider the requirements of type I error control and the nature of the procedures. This leads us to an explicit optimization problem with objective and constraints which describe its specific desiderata. We present a complete solution for deriving optimal procedures for two hypotheses, which have desired monotonicity properties, and are computationally simple. For some of the optimization formulations this yields optimal procedures that are identical to existing procedures, such as Hommel's procedure or the procedure of Bittman et al. (2009), while for others it yields completely novel and more powerful policies than existing ones. We demonstrate the nature of our novel policies and their improved power extensively in simulation and on the APEX study (Cohen et al., 2016).|http://arxiv.org/abs/2104.01346v3|Ruth Heller,Abba Krieger,Saharon Rosset
649|A Permutation Test for Assessing the Presence of Individual Differences in Treatment Effects|One size fits all approaches to medicine have become a thing of the past as the understanding of individual differences grows. The paper introduces a test for the presence of heterogeneity in treatment effects in a clinical trial. Heterogeneity is assessed on the basis of the predicted individual treatment effects (PITE) framework and a permutation test is utilized to establish if significant heterogeneity is present. We first use the novel test to show that heterogeneity in the effects of interventions exists in the Amyotrophic Lateral Sclerosis Clinical Trials. We then show, using two different predictive models (linear regression model and Random Forests) that the test has adequate type I error control. Next, we use the ALS data as the basis for simulations to demonstrate the ability of the permutation test to find heterogeneity in treatment effects as a function of both effect size and sample size. We find that the proposed test has good power to detected heterogeneity in treatment effects when the heterogeneity was due primarily to a single predictor, or when it was spread across the predictors. The predictive model, on the other hand is of secondary importance to detect heterogeneity. The non-parametric property of the permutation test can be applied with any predictive method and requires no additional assumptions to obtain PITEs.|http://arxiv.org/abs/1911.07248v1|Chi Chang,Thomas Jaki,Muhammad Saad Sadiq,Alena A. Kuhlemeier,Daniel Feaster,Nathan Cole,Andrea Lamont,Daniel Oberski,Yasin Desai,M. Lee Van Horn
650|Mixture survival models methodology: an application to cancer immunotherapy assessment in clinical trials|Progress in immunotherapy revolutionized the treatment landscape for advanced lung cancer, raising survival expectations beyond those that were historically anticipated with this disease. In the present study, we describe the methods for the adjustment of mixture parametric models of two populations for survival analysis in the presence of long survivors. A methodology is proposed in several five steps: first, it is proposed to use the multimodality test to decide the number of subpopulations to be considered in the model, second to adjust simple parametric survival models and mixture distribution models, to estimate the parameters and to select the best model fitted the data, finally, to test the hypotheses to compare the effectiveness of immunotherapies in the context of randomized clinical trials. The methodology is illustrated with data from a clinical trial that evaluates the effectiveness of the therapeutic vaccine CIMAvaxEGF vs the best supportive care for the treatment of advanced lung cancer. The mixture survival model allows estimating the presence of a subpopulation of long survivors that is 44% for vaccinated patients. The differences between the treated and control group were significant in both subpopulations (population of short-term survival: p = 0.001, the population of long-term survival: p = 0.0002). For cancer therapies, where a proportion of patients achieves long-term control of the disease, the heterogeneity of the population must be taken into account. Mixture parametric models may be more suitable to detect the effectiveness of immunotherapies compared to standard models.|http://arxiv.org/abs/1911.09765v1|Lizet Sanchez,Patricia Lorenzo-Luaces,Claudia Fonte,Agustin Lage
651|Transformer Query-Target Knowledge Discovery (TEND): Drug Discovery from CORD-19|Previous work established skip-gram word2vec models could be used to mine knowledge in the materials science literature for the discovery of thermoelectrics. Recent transformer architectures have shown great progress in language modeling and associated fine-tuned tasks, but they have yet to be adapted for drug discovery. We present a RoBERTa transformer-based method that extends the masked language token prediction using query-target conditioning to treat the specificity challenge. The transformer discovery method entails several benefits over the word2vec method including domain-specific (antiviral) analogy performance, negation handling, and flexible query analysis (specific) and is demonstrated on influenza drug discovery. To stimulate COVID-19 research, we release an influenza clinical trials and antiviral analogies dataset used in conjunction with the COVID-19 Open Research Dataset Challenge (CORD-19) literature dataset in the study. We examine k-shot fine-tuning to improve the downstream analogies performance as well as to mine analogies for model explainability. Further, the query-target analysis is verified in a forward chaining analysis against the influenza drug clinical trials dataset, before adapted for COVID-19 drugs (combinations and side-effects) and on-going clinical trials. In consideration of the present topic, we release the model, dataset, and code.|http://arxiv.org/abs/2012.04682v2|Leo K. Tam,Xiaosong Wang,Daguang Xu
652|Early Recognition of Ball Catching Success in Clinical Trials with RNN-Based Predictive Classification|Motor disturbances can affect the interaction with dynamic objects, such as catching a ball. A classification of clinical catching trials might give insight into the existence of pathological alterations in the relation of arm and ball movements. Accurate, but also early decisions are required to classify a catching attempt before the catcher's first ball contact. To obtain clinically valuable results, a significant decision confidence of at least 75% is required. Hence, three competing objectives have to be optimized at the same time: accuracy, earliness and decision-making confidence. Here we propose a coupled classification and prediction approach for early time series classification: a predictive, generative recurrent neural network (RNN) forecasts the next data points of ball trajectories based on already available observations; a discriminative RNN continuously generates classification guesses based on the available data points and the unrolled sequence predictions. We compare our approach, which we refer to as predictive sequential classification (PSC), to state-of-the-art sequence learners, including various RNN and temporal convolutional network (TCN) architectures. On this hard real-world task we can consistently demonstrate the superiority of PSC over all other models in terms of accuracy and confidence with respect to earliness of recognition. Specifically, PSC is able to confidently classify the success of catching trials as early as 123 milliseconds before the first ball contact. We conclude that PSC is a promising approach for early time series classification, when accurate and confident decisions are required.|http://arxiv.org/abs/2107.02442v1|Jana Lang,Martin A. Giese,Matthis Synofzik,Winfried Ilg,Sebastian Otte
653|Design of an optimal combination therapy with broadly neutralizing antibodies to suppress HIV-1|Broadly neutralizing antibodies (bNAbs) are promising targets for vaccination and therapy against HIV. Passive infusions of bNAbs have shown promise in clinical trials as a potential alternative for anti-retroviral therapy. A key challenge for the potential clinical application of bnAbs is the suppression of viral escape, which is more effectively achieved with a combination of bNAbs. However, identifying an optimal bNAb cocktail is combinatorially complex. Here, we propose a computational approach to predict the efficacy of a bNAb therapy trial based on the population genetics of HIV escape, which we parametrize using high-throughput HIV sequence data from a cohort of untreated bNAb-naive patients. By quantifying the mutational target size and the fitness cost of HIV-1 escape from bNAbs, we reliably predict the distribution of rebound times in three clinical trials. Importantly, we show that early rebounds are dominated by the pre-treatment standing variation of HIV-1 populations, rather than spontaneous mutations during treatment. Lastly, we show that a cocktail of three bNAbs is necessary to suppress the chances of viral escape below 1%, and we predict the optimal composition of such a bNAb cocktail. Our results offer a rational design for bNAb therapy against HIV-1, and more generally show how genetic data could be used to predict treatment outcomes and design new approaches to pathogenic control.|http://arxiv.org/abs/2112.00069v1|Colin LaMont,Jakub Otwinowski,Kanika Vanshylla,Henning Gruell,Florian Klein,Armita Nourmohammad
654|Sensitivity analysis in longitudinal clinical trials via distributional imputation|Missing data is inevitable in longitudinal clinical trials. Conventionally, the missing at random assumption is assumed to handle missingness, which however is unverifiable empirically. Thus, sensitivity analysis is critically important to assess the robustness of the study conclusions against untestable assumptions. Toward this end, regulatory agencies often request using imputation models such as return-to-baseline, control-based, and washout imputation. Multiple imputation is popular in sensitivity analysis; however, it may be inefficient and result in an unsatisfying interval estimation by Rubin's combining rule. We propose distributional imputation (DI) in sensitivity analysis, which imputes each missing value by samples from its target imputation model given the observed data. Drawn on the idea of Monte Carlo integration, the DI estimator solves the mean estimating equations of the imputed dataset. It is fully efficient with theoretical guarantees. Moreover, we propose weighted bootstrap to obtain a consistent variance estimator, taking into account the variabilities due to model parameter estimation and target parameter estimation. The finite-sample performance of DI inference is assessed in the simulation study. We apply the proposed framework to an antidepressant longitudinal clinical trial involving missing data to investigate the robustness of the treatment effect. Our proposed DI approach detects a statistically significant treatment effect in both the primary analysis and sensitivity analysis under certain prespecified sensitivity models in terms of the average treatment effect, the risk difference, and the quantile treatment effect in lower quantiles of the responses, uncovering the benefit of the test drug for curing depression.|http://arxiv.org/abs/2203.09025v1|Siyi Liu,Shu Yang,Yilong Zhang,Guanghan,Liu
655|Robust analyses for longitudinal clinical trials with missing and non-normal continuous outcomes|Missing data is unavoidable in longitudinal clinical trials, and outcomes are not always normally distributed. In the presence of outliers or heavy-tailed distributions, the conventional multiple imputation with the mixed model with repeated measures analysis of the average treatment effect (ATE) based on the multivariate normal assumption may produce bias and power loss. Control-based imputation (CBI) is an approach for evaluating the treatment effect under the assumption that participants in both the test and control groups with missing outcome data have a similar outcome profile as those with an identical history in the control group. We develop a general robust framework to handle non-normal outcomes under CBI without imposing any parametric modeling assumptions. Under the proposed framework, sequential weighted robust regressions are applied to protect the constructed imputation model against non-normality in both the covariates and the response variables. Accompanied by the subsequent mean imputation and robust model analysis, the resulting ATE estimator has good theoretical properties in terms of consistency and asymptotic normality. Moreover, our proposed method guarantees the analysis model robustness of the ATE estimation, in the sense that its asymptotic results remain intact even when the analysis model is misspecified. The superiority of the proposed robust method is demonstrated by comprehensive simulation studies and an AIDS clinical trial data application.|http://arxiv.org/abs/2203.10561v1|Siyi Liu,Yilong Zhang,Gregory T Golm,Guanghan,Liu,Shu Yang
656|Towards Assessing Data Bias in Clinical Trials|Algorithms and technologies are essential tools that pervade all aspects of our daily lives. In the last decades, health care research benefited from new computer-based recruiting methods, the use of federated architectures for data storage, the introduction of innovative analyses of datasets, and so on. Nevertheless, health care datasets can still be affected by data bias. Due to data bias, they provide a distorted view of reality, leading to wrong analysis results and, consequently, decisions. For example, in a clinical trial that studied the risk of cardiovascular diseases, predictions were wrong due to the lack of data on ethnic minorities. It is, therefore, of paramount importance for researchers to acknowledge data bias that may be present in the datasets they use, eventually adopt techniques to mitigate them and control if and how analyses results are impacted. This paper proposes a method to address bias in datasets that: (i) defines the types of data bias that may be present in the dataset, (ii) characterizes and quantifies data bias with adequate metrics, (iii) provides guidelines to identify, measure, and mitigate data bias for different data sources. The method we propose is applicable both for prospective and retrospective clinical trials. We evaluate our proposal both through theoretical considerations and through interviews with researchers in the health care environment.|http://arxiv.org/abs/2212.09633v1|Chiara Criscuolo,Tommaso Dolci,Mattia Salnitri
657|A Dual Cox Model Theory And Its Applications In Oncology|Given the prominence of targeted therapy and immunotherapy in cancer treatment, it becomes imperative to consider heterogeneity in patients' responses to treatments, which contributes greatly to the widely used proportional hazard assumption invalidated as in several clinical trials. To address the challenge, we develop a Dual Cox model theory including a Dual Cox model and a fitting algorithm.   As one of the finite mixture models, the proposed Dual Cox model consists of two independent Cox models based on patients' responses to one designated treatment (usually the experimental one) in the clinical trial. Responses of patients in the designated treatment arm can be observed and hence those patients are known responders or non-responders. From the perspective of subgroup classification, such a phenomenon renders the proposed model as a semi-supervised problem, compared to the typical finite mixture model where the subgroup classification is usually unsupervised.   A specialized expectation-maximization algorithm is utilized for model fitting, where the initial parameter values are estimated from the patients in the designated treatment arm and then the iteratively reweighted least squares (IRLS) is applied. Under mild assumptions, the consistency and asymptotic normality of its estimators of effect parameters in each Cox model are established.   In addition to strong theoretical properties, simulations demonstrate that our theory can provide a good approximation to a wide variety of survival models, is relatively robust to the change of censoring rate and response rate, and has a high prediction accuracy and stability in subgroup classification while it has a fast convergence rate. Finally, we apply our theory to two clinical trials with cross-overed KM plots and identify the subgroups where the subjects benefit from the treatment or not.|http://arxiv.org/abs/2308.04158v1|Powei Chen,Siying Hu,Haojin Zhou
658|A Comprehensive Analysis of HIV Treatment Efficacy in the ACTG 175 Trial Through Multiple-Endpoint Approaches|In the realm of medical research, the intricate interplay of epidemiological risk, genomic activity, adverse events, and clinical response necessitates a nuanced consideration of multiple variables. Clinical trials, designed to meticulously assess the efficacy and safety of interventions, routinely incorporate a diverse array of endpoints. While a primary endpoint is customary, supplemented by key secondary endpoints, the statistical significance is typically evaluated independently for each. To address the inherent challenges in studying multiple endpoints, diverse strategies, including composite endpoints and global testing, have been proposed. This work stands apart by focusing on the evaluation of a clinical trial, deviating from the conventional approach to underscore the efficacy of a multiple-endpoint procedure. A double-blind study was conducted to gauge the treatment efficacy in adults infected with human immunodeficiency virus type 1 (HIV-1), featuring CD4 cell counts ranging from 200 to 500 per cubic millimeter. A total of 2467 HIV-1-infected patients (43 percent without prior antiretroviral treatment) were randomly assigned to one of four daily regimens: 600 mg of zidovudine; 600 mg of zidovudine plus 400 mg of didanosine; 600 mg of zidovudine plus 2.25 mg of zalcitabine; or 400 mg of didanosine. The primary endpoint comprised a >50 percent decline in CD4 cell count, development of acquired immunodeficiency syndrome (AIDS), or death. This study sought to determine the efficacy and safety of zidovudine (AZT) versus didanosine (ddI), AZT plus ddI, and AZT plus zalcitabine (ddC) in preventing disease progression in HIV-infected patients with CD4 counts of 200-500 cells/mm3. By jointly considering all endpoints, the multiple-endpoints approach yields results of greater significance than a single-endpoint approach.|http://arxiv.org/abs/2311.15410v1|Mara Paula Cantarini,Florencia Ayeln Hernndez
|Ishir Rao
660|Machine Learning to Promote Translational Research: Predicting Patent and Clinical Trial Inclusion in Dementia Research|Projected to impact 1.6 million people in the UK by 2040 and costing {\pounds}25 billion annually, dementia presents a growing challenge to society. This study, a pioneering effort to predict the translational potential of dementia research using machine learning, hopes to address the slow translation of fundamental discoveries into practical applications despite dementia's significant societal and economic impact. We used the Dimensions database to extract data from 43,091 UK dementia research publications between the years 1990-2023, specifically metadata (authors, publication year etc.), concepts mentioned in the paper, and the paper abstract. To prepare the data for machine learning we applied methods such as one hot encoding and/or word embeddings. We trained a CatBoost Classifier to predict if a publication will be cited in a future patent or clinical trial. We trained several model variations. The model combining metadata, concept, and abstract embeddings yielded the highest performance: for patent predictions, an Area Under the Receiver Operating Characteristic Curve (AUROC) of 0.84 and 77.17% accuracy; for clinical trial predictions, an AUROC of 0.81 and 75.11% accuracy. The results demonstrate that integrating machine learning within current research methodologies can uncover overlooked publications, expediting the identification of promising research and potentially transforming dementia research by predicting real-world impact and guiding translational strategies.|http://arxiv.org/abs/2401.05145v1|Matilda Beinat,Julian Beinat,Mohammed Shoaib,Jorge Gomez Magenti
661|Survival analysis for AdVerse events with VarYing follow-up times (SAVVY): summary of findings and a roadmap for the future of safety analyses in clinical trials|The SAVVY project aims to improve the analyses of adverse events (AEs) in clinical trials through the use of survival techniques appropriately dealing with varying follow-up times and competing events (CEs). This paper summarizes key features and conclusions from the various SAVVY papers. Through theoretical investigations using simulations and in an empirical study including randomized clinical trials from several sponsor organisations, biases from ignoring varying follow-up times or CEs are investigated. The bias of commonly used estimators of the absolute and relative AE risk is quantified. Furthermore, we provide a cursory assessment of how pertinent guidelines for the analysis of safety data deal with the features of varying follow-up time and CEs. SAVVY finds that for both, avoiding bias and categorization of evidence with respect to treatment effect on AE risk into categories, the choice of the estimator is key and more important than features of the underlying data such as percentage of censoring, CEs, amount of follow-up, or value of the gold-standard. The choice of the estimator of the cumulative AE probability and the definition of CEs are crucial. SAVVY recommends using the Aalen-Johansen estimator (AJE) with an appropriate definition of CEs whenever the risk for AEs is to be quantified. There is an urgent need to improve the guidelines of reporting AEs so that incidence proportions or one minus Kaplan-Meier estimators are finally replaced by the AJE with appropriate definition of CEs.|http://arxiv.org/abs/2402.17692v1|Kaspar Rufibach,Jan Beyersmann,Tim Friede,Claudia Schmoor,Regina Stegherr
662|Eliciting prior information from clinical trials via calibrated Bayes factor|In the Bayesian framework power prior distributions are increasingly adopted in clinical trials and similar studies to incorporate external and past information, typically to inform the parameter associated to a treatment effect. Their use is particularly effective in scenarios with small sample sizes and where robust prior information is actually available. A crucial component of this methodology is represented by its weight parameter, which controls the volume of historical information incorporated into the current analysis. This parameter can be considered as either fixed or random. Although various strategies exist for its determination, eliciting the prior distribution of the weight parameter according to a full Bayesian approach remains a challenge. In general, this parameter should be carefully selected to accurately reflect the available prior information without dominating the posterior inferential conclusions. To this aim, we propose a novel method for eliciting the prior distribution of the weight parameter through a simulation-based calibrated Bayes factor procedure. This approach allows for the prior distribution to be updated based on the strength of evidence provided by the data: The goal is to facilitate the integration of historical data when it aligns with current information and to limit it when discrepancies arise in terms, for instance, of prior-data conflicts. The performance of the proposed method is tested through simulation studies and applied to real data from clinical trials.|http://arxiv.org/abs/2406.19346v2|Roberto Macr Demartino,Leonardo Egidi,Nicola Torelli,Ioannis Ntzoufras
663|A Deep Subgrouping Framework for Precision Drug Repurposing via Emulating Clinical Trials on Real-world Patient Data|Drug repurposing identifies new therapeutic uses for existing drugs, reducing the time and costs compared to traditional de novo drug discovery. Most existing drug repurposing studies using real-world patient data often treat the entire population as homogeneous, ignoring the heterogeneity of treatment responses across patient subgroups. This approach may overlook promising drugs that benefit specific subgroups but lack notable treatment effects across the entire population, potentially limiting the number of repurposable candidates identified. To address this, we introduce STEDR, a novel drug repurposing framework that integrates subgroup analysis with treatment effect estimation. Our approach first identifies repurposing candidates by emulating multiple clinical trials on real-world patient data and then characterizes patient subgroups by learning subgroup-specific treatment effects. We deploy \model to Alzheimer's Disease (AD), a condition with few approved drugs and known heterogeneity in treatment responses. We emulate trials for over one thousand medications on a large-scale real-world database covering over 8 million patients, identifying 14 drug candidates with beneficial effects to AD in characterized subgroups. Experiments demonstrate STEDR's superior capability in identifying repurposing candidates compared to existing approaches. Additionally, our method can characterize clinically relevant patient subgroups associated with important AD-related risk factors, paving the way for precision drug repurposing.|http://arxiv.org/abs/2412.20373v1|Seungyeon Lee,Ruoqi Liu,Feixiong Cheng,Ping Zhang
664|Adding new experimental arms to randomised clinical trials: impact on error rates|Background: Experimental treatments pass through various stages of development. If a treatment passes through early phase experiments, the investigators may want to assess it in a late phase randomised controlled trial. An efficient way to do this is adding it as a new research arm to an ongoing trial. This allows to add the new treatment while the existing arms continue. The familywise type I error rate (FWER) is often a key quantity of interest in any multi-arm trial. We set out to clarify how it should be calculated when new arms are added to a trial some time after it has started.   Methods: We show how the FWER, any-pair and all-pairs powers can be calculated when a new arm is added to a platform trial. We extend the Dunnett probability and derive analytical formulae for the correlation between the test statistics of the existing pairwise comparison and that of the newly added arm. We also verify our analytical derivation via simulations.   Results: Our results indicate that the FWER depends on the shared control arm information (i.e. individuals in continuous and binary outcomes and primary outcome events in time-to-event outcomes) from the common control arm patients and the allocation ratio. The FWER is driven more by the number of pairwise comparisons and the corresponding (pairwise) Type I error rates than by the timing of the addition of the new arms. The FWER can be estimated using \v{S}id\'{a}k's correction if the correlation between the test statistics of pairwise comparisons is less than 0:30.   Conclusions: The findings we present in this article can be used to design trials with pre-planned deferred arms or to design new pairwise comparisons within an ongoing platform trial where control of the pairwise error rate (PWER) or FWER (for a subset of pairwise comparisons) is required.|http://arxiv.org/abs/1902.05336v1|Babak Choodari-Oskooei,Daniel J Bratton,Melissa R Gannon,Angela M Meade,Matthew R Sydes,Mahesh KB Parmar
665|Handling an uncertain control group event risk in non-inferiority trials: non-inferiority frontiers and the power-stabilising transformation|Background. Non-inferiority (NI) trials are increasingly used to evaluate new treatments expected to have secondary advantages over standard of care, but similar efficacy on the primary outcome. When designing a NI trial with a binary primary outcome, the choice of effect measure for the NI margin has an important effect on sample size calculations; furthermore, if the control event risk observed is markedly different from that assumed, the trial can quickly lose power or the results become difficult to interpret. Methods. We propose a new way of designing NI trials to overcome the issues raised by unexpected control event risks by specifying a NI frontier, i.e. a curve defining the most appropriate non-inferiority margin for each possible value of control event risk. We propose a fixed arcsine difference frontier, the power-stabilising transformation for binary outcomes. We propose and compare three ways of designing a trial using this frontier. Results. Testing and reporting on the arcsine scale leads to results which are challenging to interpret clinically. Working on the arcsine scale generally requires a larger sample size compared to the risk difference scale. Therefore, working on the risk difference scale, modifying the margin after observing the control event risk, might be preferable, as it requires a smaller sample size. However, this approach tends to slightly inflate type I error rate; a solution is to use a lower significance level for testing. When working on the risk ratio scale, the same approach leads to power levels above the nominal one, maintaining type I error under control. Conclusions. Our proposed methods of designing NI trials using power-stabilising frontiers make trial design more resilient to unexpected values of the control event risk, at the only cost of requiring larger sample sizes when the goal is to report results on the risk difference scale.|http://arxiv.org/abs/1905.00241v1|Matteo Quartagno,A. Sarah Walker,Abdel G. Babiker,Rebecca M. Turner,Mahesh K. B. Parmar,Andrew Copas,Ian R. White
666|An Efficient Method for Computing Expected Value of Sample Information for Survival Data from an Ongoing Trial|The European Medicines Agency has in recent years allowed licensing of new pharmaceuticals at an earlier stage in the clinical trial process. When trial evidence is obtained at an early stage, the events of interest, such as disease progression or death, may have only been observed in a small proportion of patients. Health care authorities therefore must decide on the adoption of new technologies based on less mature evidence than previously, resulting in greater uncertainty about clinical- and cost-effectiveness. When a trial is ongoing at the point of decision making, there may be value in continuing the trial in order to collect additional data before making an adoption decision. This can be quantified by the Expected Value of Sample Information (EVSI). However, no guidance exists on how to compute the EVSI for survival data from an ongoing trial, nor on how to account for uncertainty about the choice of survival model in the EVSI calculations. In this article, we describe algorithms for computing the EVSI of extending a trial's follow-up, both where a single known survival model is assumed, and where we are uncertain about the true survival model. We compare a nested Markov Chain Monte Carlo procedure with a non-parametric regression-based method in two synthetic case studies, and find close agreement between the two methods. The regression-based method is fast and straightforward to implement, and scales easily to include any number of candidate survival models in the model uncertainty case. EVSI for ongoing trials can help decision makers determine whether early patient access to a new technology can be justified on the basis of the current evidence or whether more mature evidence is needed.|http://arxiv.org/abs/2103.13238v2|Mathyn Vervaart,Mark Strong,Karl P. Claxton,Nicky J. Welton,Torbjrn Wislff,Eline Aas
667|Optimal predictive probability designs for randomized biomarker-guided oncology trials|Efforts to develop biomarker-targeted anti-cancer therapies have progressed rapidly in recent years. Six antibodies acting on programmed death ligand 1 or programmed death 1 pathways were approved in 75 cancer indications between 2015 and 2021. With efforts to expedite regulatory reviews of promising therapies, several targeted cancer therapies have been granted accelerated approval on the basis of single-arm phase II trials. And yet, in the absence of randomization, patient outcomes may not have been studied under standard of care chemotherapies for emerging biomarker subpopulations prior to the submission of an accelerated approval application. Historical control rates used in single arm studies often arise as population averages, lacking specificity to the targeted subgroup. For example, a recent phase III trial of atezolizumab in patients with metastatic urothelial carcinoma found a 21.6% response rate to standard of care chemotherapy in the biomarker subgbroup of interest, much higher than the historical control rate of 10% that had been used to declare success in the preceding phase II trial. Innovations in design methodology are needed to enable efficient implementation of randomized trials for agents that target biomarker subpopulations. This article proposes three randomized designs for early phase biomarker-guided oncology clinical trials. Each design utilizes the optimal efficiency predictive probability method to monitor multiple biomarker subpopulations for futility. A simulation study motivated by the results reported in the atezolizumab trial is used to evaluate the operating characteristics of the various designs. Our findings suggest that efficient statistical design can be conducted with randomization and futility stopping to effectively acquire more evidence pertaining to comparative efficacy before deciding to conduct a phase III confirmatory trial.|http://arxiv.org/abs/2210.02903v1|Emily C. Zabor,Alexander M. Kaizer,Nathan A. Pennell,Brian P. Hobbs
668|Designing a Bayesian adaptive clinical trial to evaluate novel mechanical ventilation strategies in acute respiratory failure using Integrated Nested Laplace Approximations|Background: We aimed to design a Bayesian adaption trial through extensive simulations to determine values for key design parameters, demonstrate error rates, and establish the expected sample size. The complexity of the proposed outcome and analysis meant that Markov Chain Monte Carlo methods were required, resulting in an infeasible computational burden. Thus, we leveraged the Integrated Nested Laplace Approximations (INLA) algorithm, a fast approximation method, to ensure the feasibility of these simulations. Methods: We simulated Bayesian adaptive two-arm superiority trials that stratified participants into two disease severity states. The outcome was analyzed with proportional odds logistic regression. Trials were stopped for superiority or futility, separately for each state. We calculated the type I error and power across 64 scenarios that varied the stopping thresholds and the minimum sample size before commencing adaptive analyses. We incorporated dynamic borrowing and used INLA to compute the posterior distributions at each adaptive analysis. Designs that maintained a type I error below 5%, a power above 80%, and a feasible mean sample size were then evaluated across 22 scenarios that varied the odds ratios for the two severity states. Results: Power generally increased as the initial sample size and the threshold for declaring futility increased. Two designs were selected for further analysis. In the comprehensive simulations, the one design had a higher chance of reaching a trial conclusion before the maximum sample size and higher probability of declaring superiority when appropriate without a substantial increase in sample size for the more realistic scenarios and was selected as the trial design. Conclusions: We designed a Bayesian adaptive trial to evaluate novel strategies for ventilation using the INLA algorithm to and optimize the trial design through simulation.|http://arxiv.org/abs/2303.18093v1|Reyhaneh Hosseini,Ziming Chen,Ewan Goligher,Eddy Fan,Niall D. Ferguson,Michael O. Harhay,Sarina Sahetya,Martin Urner,Christopher J. Yarnell,Anna Heath
669|Adaptive clinical trial designs with blinded selection of binary composite endpoints and sample size reassessment|For randomized clinical trials where a single, primary, binary endpoint would require unfeasibly large sample sizes, composite endpoints are widely chosen as the primary endpoint. Despite being commonly used, composite endpoints entail challenges in designing and interpreting results. Given that the components may be of different relevance and have different effect sizes, the choice of components must be made carefully. Especially, sample size calculations for composite binary endpoints depend not only on the anticipated effect sizes and event probabilities of the composite components, but also on the correlation between them. However, information on the correlation between endpoints is usually not reported in the literature which can be an obstacle for planning of future sound trial design. We consider two-arm randomized controlled trials with a primary composite binary endpoint and an endpoint that consists only of the clinically more important component of the composite endpoint. We propose a trial design that allows an adaptive modification of the primary endpoint based on blinded information obtained at an interim analysis. We consider a decision rule to select between a composite endpoint and its most relevant component as primary endpoint. The decision rule chooses the endpoint with the lower estimated required sample size. Additionally, the sample size is reassessed using the estimated event probabilities and correlation, and the expected effect sizes of the composite components. We investigate the statistical power and significance level under the proposed design through simulations. We show that the adaptive design is equally or more powerful than designs without adaptive modification on the primary endpoint. The targeted power is achieved even if the correlation is misspecified while maintaining the type 1 error. We illustrated the proposal by means of two case studies.|http://arxiv.org/abs/2206.09639v1|Marta Bofill Roig,Guadalupe Gmez Melis,Martin Posch,Franz Koenig
670|Efficient, broadly-tunable source of megawatt pulses for multiphoton microscopy based on self-phase modulation in argon-filled hollow-core fiber|An exciting recent development for deep-tissue imaging with cellular resolution is three-photon fluorescence microscopy (3PM) with excitation at long wavelengths (1300 and 1700 nm). In the last few years, long-wavelength 3PM has driven rapid progress in deep-tissue imaging beyond the depth limit of two-photon microscopy, with impacts in neuroscience, immunology, and cancer biology. However, wide adoption of 3PM faces challenges. Three-photon excitation (3PE) is naturally weaker than two-photon excitation, which places a premium on ultrashort pulses with high peak power. The inefficiency, complexity, and cost of current sources of these pulses present major barriers to the use of 3PM in typical biomedical research labs. Here, we describe a fiber-based source of femtosecond pulses with multi-megawatt peak power, tunable from 850 nm to 1700 nm. Compressed pulses from a fiber amplifier at 1030~nm are launched into an antiresonant hollow-core fiber filled with argon. By varying only the gas pressure, pulses with hundreds of nanojoules of energy and sub-100 fs duration are obtained at wavelengths between 850 and 1700 nm. This approach is a new route to an efficient, robust, and potentially low-cost source for multiphoton deep-tissue imaging. In particular, 960-nJ and 50-fs pulses are generated at 1300 nm with a conversion efficiency of 10\%. The nearly 20-MW peak power is an order of magnitude higher than the previous best from femtosecond fiber source at 1300~nm. As an example of the capabilities of the source, these pulses are used to image structure and neuronal activity in mouse brain as deep as 1.1 mm below the dura.|http://arxiv.org/abs/2410.00889v1|Yishai Eisenberg,Wenchao Wang,Shitong Zhao,Eric S. Hebert,Yi-Hao Chen,Dimitre G. Ouzounov,Hazuki Takahashi,Anna Gruzdeva,Aaron K. LaViolette,Moshe Labaz,Pavel Sidorenko,Enrique Antonio-Lopez,Rodrigo Amezcua-Correa,Nilay Yapici,Chris Xu,Frank Wise
671|Clinica: an open source software platform for reproducible clinical neuroscience studies|We present Clinica (www.clinica.run), an open-source software platform designed to make clinical neuroscience studies easier and more reproducible. Clinica aims for researchers to i) spend less time on data management and processing, ii) perform reproducible evaluations of their methods, and iii) easily share data and results within their institution and with external collaborators. The core of Clinica is a set of automatic pipelines for processing and analysis of multimodal neuroimaging data (currently, T1-weighted MRI, diffusion MRI and PET data), as well as tools for statistics, machine learning and deep learning. It relies on the brain imaging data structure (BIDS) for the organization of raw neuroimaging datasets and on established tools written by the community to build its pipelines. It also provides converters of public neuroimaging datasets to BIDS (currently ADNI, AIBL, OASIS and NIFD). Processed data include image-valued scalar fields (e.g. tissue probability maps), meshes, surface-based scalar fields (e.g. cortical thickness maps) or scalar outputs (e.g. regional averages). These data follow the ClinicA Processed Structure (CAPS) format which shares the same philosophy as BIDS. Consistent organization of raw and processed neuroimaging files facilitates the execution of single pipelines and of sequences of pipelines, as well as the integration of processed data into statistics or machine learning frameworks. The target audience of Clinica is neuroscientists or clinicians conducting clinical neuroscience studies involving multimodal imaging, and researchers developing advanced machine learning algorithms applied to neuroimaging data.|http://arxiv.org/abs/2107.10256v1|Alexandre Routier,Ninon Burgos,Mauricio Daz,Michael Bacci,Simona Bottani,Omar El-Rifai,Sabrina Fontanella,Pietro Gori,Jrmy Guillon,Alexis Guyot,Ravi Hassanaly,Thomas Jacquemont,Pascal Lu,Arnaud Marcoux,Tristan Moreau,Jorge Samper-Gonzlez,Marc Teichmann,Elina Thibeau--Sutre,Ghislain Vaillant,Junhao Wen,Adam Wild,Marie-Odile Habert,Stanley Durrleman,Olivier Colliot
672|Statistical adjustment for a measure of healthy lifestyle doesn't yield the truth about hormone therapy|The Women's Health Initiative randomized clinical trial of hormone therapy found no benefit of hormones in preventive cardiovascular disease, a finding in striking contrast with a large body of observational research. Understanding whether better methodology and/or statistical adjustment might have prevented the erroneous conclusions of observational research is important. This is a re-analysis of data from a case-control study examining the relationship of postmenopausal hormone therapy and the risks of myocardial infarction (MI) and ischemic stroke in which we reported no overall increase or decrease in the risk of either event. Variables measuring health behavior/lifestyle that are not likely to be causally with the risks of MI and stroke (e.g., sunscreen use) were included in multivariate analysis along with traditional confounders (age, hypertension, diabetes, smoking, body mass index, ethnicity, education, prior coronary heart disease for MI and prior stroke/TIA for stroke) to determine whether adjustment for the health behavior/lifestyle variables could reproduce or bring the results closer to the findings in a large and definitive randomized clinical trial of hormone therapy, the Women's Health Initiative. For both MI and stroke, measures of health behavior/lifestyle were associated with odds ratios (ORs) less than 1.0. Adjustment for traditional cardiovascular disease confounders did not alter the magnitude of the ORs for MI or stroke. Addition of a subset of these variables selected using stepwise regression to the final MI or stroke models along with the traditional cardiovascular disease confounders moved the ORs for estrogen and estrogen/progestin use closer to values observed in the Women Health Initiative clinical trial, but did not reliably reproduce the clinical trial results for these two endpoints.|http://arxiv.org/abs/0805.2845v1|Diana B. Petitti,Wansu Chen
673|An Answer to Multiple Problems with Analysis of Data on Harms?|Discussion of "Multivariate Bayesian Logistic Regression for Analysis of Clinical Trial Safety Issues" by W. DuMouchel [arXiv:1210.0385].|http://arxiv.org/abs/1210.0663v1|Stephen Evans
674|On asymptotic normality of certain linear rank statistics|We consider asymptotic normality of linear rank statistics under various randomization rules met in clinical trials and designed for patients' allocation into treatment and placebo arms. Exposition relies on some general limit theorem due to McLeish (1974) which appears to be well suited for the problem considered and may be employed for other similar rules undis- cussed in the paper. Examples of applications include well known results as well as several new ones.|http://arxiv.org/abs/1709.02637v1|Viktor Skorniakov
675|Regulation of interferon production as a potential strategy for COVID-19 treatment|Regulating the upstream of the cytokines production could be a promising strategy to the treatment of COVID-19. We suggest to pay more attention to the dysregulated IFN-I production in COVID-19 and to considerate cGAS, ALK and STING as potential therapeutic targets preventing cytokine storm. Approved drugs like suramin and ALK inhibitors are worthy of clinical trials.|http://arxiv.org/abs/2003.00751v1|Xiaobing Deng,Xiaoyu Yu,Jianfeng Pei
676|Clinical trial of an AI-augmented intervention for HIV prevention in youth experiencing homelessness|Youth experiencing homelessness (YEH) are subject to substantially greater risk of HIV infection, compounded both by their lack of access to stable housing and the disproportionate representation of youth of marginalized racial, ethnic, and gender identity groups among YEH. A key goal for health equity is to improve adoption of protective behaviors in this population. One promising strategy for intervention is to recruit peer leaders from the population of YEH to promote behaviors such as condom usage and regular HIV testing to their social contacts. This raises a computational question: which youth should be selected as peer leaders to maximize the overall impact of the intervention? We developed an artificial intelligence system to optimize such social network interventions in a community health setting. We conducted a clinical trial enrolling 713 YEH at drop-in centers in a large US city. The clinical trial compared interventions planned with the algorithm to those where the highest-degree nodes in the youths' social network were recruited as peer leaders (the standard method in public health) and to an observation-only control group. Results from the clinical trial show that youth in the AI group experience statistically significant reductions in key risk behaviors for HIV transmission, while those in the other groups do not. This provides, to our knowledge, the first empirical validation of the usage of AI methods to optimize social network interventions for health. We conclude by discussing lessons learned over the course of the project which may inform future attempts to use AI in community-level interventions.|http://arxiv.org/abs/2009.09559v2|Bryan Wilder,Laura Onasch-Vera,Graham Diguiseppi,Robin Petering,Chyna Hill,Amulya Yadav,Eric Rice,Milind Tambe
677|Closed testing procedures for treatment-versus-control comparisons and multiple correlated endpoint|Preferably in two- or three-arm randomized clinical trials, a few (2,3) correlated multiple primary endpoints are considered. In addition to the closed testing principle based on different global tests, two max(maxT) tests are compared with respect to any-pairs, all-pairs and individual power in a simulation study.|http://arxiv.org/abs/2103.07661v1|Ludwig A. Hothorn,Siegfried Kropf
678|Design and Validation of an Open-Source Closed-Loop Testbed for Artificial Pancreas Systems|The development of a fully autonomous artificial pancreas system (APS) to independently regulate the glucose levels of a patient with Type 1 diabetes has been a long-standing goal of diabetes research. A significant barrier to progress is the difficulty of testing new control algorithms and safety features, since clinical trials are time- and resource-intensive. To facilitate ease of validation, we propose an open-source APS testbed by integrating APS controllers with two state-of-the-art glucose simulators and a novel fault injection engine. The testbed is able to reproduce the blood glucose trajectories of real patients from a clinical trial conducted over six months. We evaluate the performance of two closed-loop control algorithms (OpenAPS and Basal Bolus) using the testbed and find that more advanced control algorithms are able to keep blood glucose in a safe region 93.49% and 79.46% of the time on average, compared with 66.18% of the time for the clinical trial. The fault injection engine simulates the real recalls and adverse events reported to the U.S. Food and Drug Administration (FDA) and demonstrates the resilience of the controller in hazardous conditions. We used the testbed to generate 25 years of synthetic data representing 20 different patient profiles with realistic adverse event scenarios, which would have been expensive and risky to collect in a clinical trial. The proposed testbed is a valid tool that can be used by the research community to demonstrate the effectiveness of different control algorithms and safety features for APS.|http://arxiv.org/abs/2208.06479v4|Xugui Zhou,Maxfield Kouzel,Haotian Ren,Homa Alemzadeh
679|Enhancing Longitudinal Clinical Trial Efficiency with Digital Twins and Prognostic Covariate-Adjusted Mixed Models for Repeated Measures (PROCOVA-MMRM)|Clinical trials are critical in advancing medical treatments but often suffer from immense time and financial burden. Advances in statistical methodologies and artificial intelligence (AI) present opportunities to address these inefficiencies. Here we introduce Prognostic Covariate-Adjusted Mixed Models for Repeated Measures (PROCOVA-MMRM) as an advantageous combination of prognostic covariate adjustment (PROCOVA) and Mixed Models for Repeated Measures (MMRM). PROCOVA-MMRM utilizes time-matched prognostic scores generated from AI models to enhance the precision of treatment effect estimators for longitudinal continuous outcomes, enabling reductions in sample size and enrollment times. We first provide a description of the background and implementation of PROCOVA-MMRM, followed by two case study reanalyses where we compare the performance of PROCOVA-MMRM versus the unadjusted MMRM. These reanalyses demonstrate significant improvements in statistical power and precision in clinical indications with unmet medical need, specifically Alzheimer's Disease (AD) and Amyotrophic Lateral Sclerosis (ALS). We also explore the potential for sample size reduction with the prospective implementation of PROCOVA-MMRM, finding that the same or better results could have been achieved with fewer participants in these historical trials if the enhanced precision provided by PROCOVA-MMRM had been prospectively leveraged. We also confirm the robustness of the statistical properties of PROCOVA-MMRM in a variety of realistic simulation scenarios. Altogether, PROCOVA-MMRM represents a rigorous method of incorporating advances in the prediction of time-matched prognostic scores generated by AI into longitudinal analysis, potentially reducing both the cost and time required to bring new treatments to patients while adhering to regulatory standards.|http://arxiv.org/abs/2404.17576v1|Jessica L. Ross,Arman Sabbaghi,Run Zhuang,Daniele Bertolini,the Alzheimer's Disease Cooperative Study,Alzheimer's Disease Neuroimaging Initiative,the Critical Path for Alzheimer's Disease Database,the European Prevention of Alzheimer's Disease,Consortium,the Pooled Resource Open-Access ALS Clinical Trials Consortium
680|CRCL at SemEval-2024 Task 2: Simple prompt optimizations|We present a baseline for the SemEval 2024 task 2 challenge, whose objective is to ascertain the inference relationship between pairs of clinical trial report sections and statements. We apply prompt optimization techniques with LLM Instruct models provided as a Language Model-as-a-Service (LMaaS). We observed, in line with recent findings, that synthetic CoT prompts significantly enhance manually crafted ones.|http://arxiv.org/abs/2405.01942v1|Clment Brutti-Mairesse,Loc Verlingue
681|Arges: Spatio-Temporal Transformer for Ulcerative Colitis Severity Assessment in Endoscopy Videos|Accurate assessment of disease severity from endoscopy videos in ulcerative colitis (UC) is crucial for evaluating drug efficacy in clinical trials. Severity is often measured by the Mayo Endoscopic Subscore (MES) and Ulcerative Colitis Endoscopic Index of Severity (UCEIS) score. However, expert MES/UCEIS annotation is time-consuming and susceptible to inter-rater variability, factors addressable by automation. Automation attempts with frame-level labels face challenges in fully-supervised solutions due to the prevalence of video-level labels in clinical trials. CNN-based weakly-supervised models (WSL) with end-to-end (e2e) training lack generalization to new disease scores and ignore spatio-temporal information crucial for accurate scoring. To address these limitations, we propose "Arges", a deep learning framework that utilizes a transformer with positional encoding to incorporate spatio-temporal information from frame features to estimate disease severity scores in endoscopy video. Extracted features are derived from a foundation model (ArgesFM), pre-trained on a large diverse dataset from multiple clinical trials (61M frames, 3927 videos). We evaluate four UC disease severity scores, including MES and three UCEIS component scores. Test set evaluation indicates significant improvements, with F1 scores increasing by 4.1% for MES and 18.8%, 6.6%, 3.8% for the three UCEIS component scores compared to state-of-the-art methods. Prospective validation on previously unseen clinical trial data further demonstrates the model's successful generalization.|http://arxiv.org/abs/2410.00536v1|Krishna Chaitanya,Pablo F. Damasceno,Shreyas Fadnavis,Pooya Mobadersany,Chaitanya Parmar,Emily Scherer,Natalia Zemlianskaia,Lindsey Surace,Louis R. Ghanem,Oana Gabriela Cula,Tommaso Mansi,Kristopher Standish
682|Spatiotemporal Dissociation of Brain Activity underlying Subjective Awareness, Objective Performance and Confidence|Despite intense recent research, the neural correlates of conscious visual perception remain elusive. The most established paradigm for studying brain mechanisms underlying conscious perception is to keep the physical sensory inputs constant and identify brain activities that correlate with the changing content of conscious awareness. However, such a contrast based on conscious content alone would not only reveal brain activities directly contributing to conscious perception, but also include brain activities that precede or follow it. To address this issue, we devised a paradigm whereby we collected, trial-by-trial, measures of objective performance, subjective awareness, and the confidence level of subjective awareness. Using magnetoencephalography recordings in healthy human volunteers, we dissociated brain activities underlying these different cognitive phenomena. Our results provide strong evidence that widely distributed slow cortical potentials (SCPs) correlate with subjective awareness, even after the effects of objective performance and confidence were both removed. The SCP correlate of conscious perception manifests strongly in its waveform, phase, and power. In contrast, objective performance and confidence were both contributed by relatively transient brain activity. These results shed new light on the brain mechanisms of conscious, unconscious, and metacognitive processing.|http://arxiv.org/abs/1403.5241v1|Qi Li,Zachary Hill,Biyu J. He
683|LFADS - Latent Factor Analysis via Dynamical Systems|Neuroscience is experiencing a data revolution in which many hundreds or thousands of neurons are recorded simultaneously. Currently, there is little consensus on how such data should be analyzed. Here we introduce LFADS (Latent Factor Analysis via Dynamical Systems), a method to infer latent dynamics from simultaneously recorded, single-trial, high-dimensional neural spiking data. LFADS is a sequential model based on a variational auto-encoder. By making a dynamical systems hypothesis regarding the generation of the observed data, LFADS reduces observed spiking to a set of low-dimensional temporal factors, per-trial initial conditions, and inferred inputs. We compare LFADS to existing methods on synthetic data and show that it significantly out-performs them in inferring neural firing rates and latent dynamics.|http://arxiv.org/abs/1608.06315v1|David Sussillo,Rafal Jozefowicz,L. F. Abbott,Chethan Pandarinath
684|FPGA Deployment of LFADS for Real-time Neuroscience Experiments|Large-scale recordings of neural activity are providing new opportunities to study neural population dynamics. A powerful method for analyzing such high-dimensional measurements is to deploy an algorithm to learn the low-dimensional latent dynamics. LFADS (Latent Factor Analysis via Dynamical Systems) is a deep learning method for inferring latent dynamics from high-dimensional neural spiking data recorded simultaneously in single trials. This method has shown a remarkable performance in modeling complex brain signals with an average inference latency in milliseconds. As our capacity of simultaneously recording many neurons is increasing exponentially, it is becoming crucial to build capacity for deploying low-latency inference of the computing algorithms. To improve the real-time processing ability of LFADS, we introduce an efficient implementation of the LFADS models onto Field Programmable Gate Arrays (FPGA). Our implementation shows an inference latency of 41.97 $\mu$s for processing the data in a single trial on a Xilinx U55C.|http://arxiv.org/abs/2402.04274v1|Xiaohan Liu,ChiJui Chen,YanLun Huang,LingChi Yang,Elham E Khoda,Yihui Chen,Scott Hauck,Shih-Chieh Hsu,Bo-Cheng Lai
685|Decoding Continuous Character-based Language from Non-invasive Brain Recordings|Deciphering natural language from brain activity through non-invasive devices remains a formidable challenge. Previous non-invasive decoders either require multiple experiments with identical stimuli to pinpoint cortical regions and enhance signal-to-noise ratios in brain activity, or they are limited to discerning basic linguistic elements such as letters and words. We propose a novel approach to decoding continuous language from single-trial non-invasive fMRI recordings, in which a three-dimensional convolutional network augmented with information bottleneck is developed to automatically identify responsive voxels to stimuli, and a character-based decoder is designed for the semantic reconstruction of continuous language characterized by inherent character structures. The resulting decoder can produce intelligible textual sequences that faithfully capture the meaning of perceived speech both within and across subjects, while existing decoders exhibit significantly inferior performance in cross-subject contexts. The ability to decode continuous language from single trials across subjects demonstrates the promising applications of non-invasive language brain-computer interfaces in both healthcare and neuroscience.|http://arxiv.org/abs/2403.11183v2|Cenyuan Zhang,Xiaoqing Zheng,Ruicheng Yin,Shujie Geng,Jianhan Xu,Xuan Gao,Changze Lv,Zixuan Ling,Xuanjing Huang,Miao Cao,Jianfeng Feng
686|Visualizing adverse events in clinical trials using correspondence analysis with R-package visae|We propose to apply stacked CA using contribution biplots as a tool to explore differences in AE data among treatments in clinical trials. We defined five levels of refinement for the analysis based on data derived from the Common Terminology Criteria for Adverse Events (CTCAE) grades, domains, terms and their combinations. In addition, we developed a Shiny app built in an R-package, publicly available on Comprehensive R Archive Network (CRAN), to interactively investigate CA configurations. Data from two randomized controlled trials (RCT) were used to illustrate the proposed methods: NSABP R-04, a neoadjuvant rectal 2x2 factorial trial comparing radiation therapy with either capecitabine (Cape) or 5-fluorouracil (5-FU) alone with or without oxaliplatin (Oxa), and NSABP B-35, a double-blind RCT comparing tamoxifen to anastrozole in postmenopausal women with hormone-positive ductal carcinoma in situ. In the R04 trial (n=1308), CA biplots displayed the discrepancies between single agent treatments and their combinations with Oxa at all levels of AE classes, such that these discrepancies were responsible for the largest portion of the explained variability among treatments. In addition, an interaction effect when adding Oxa to Cape/5-FU was identified when the distance between Cape+Oxa and 5-FU+Oxa was observed to be larger than the distance between 5-FU and Cape, with Cape+Oxa and 5-FU+Oxa in different quadrants of the CA biplots. In the B35 trial (n=3009), CA biplots showed different patterns for non-adherent Anastrozole and Tamoxifen compared with their adherent counterparts. CA with contribution biplot is an effective tool that can be used to summarize AE data in a two-dimensional display while minimizing the loss of information and interpretation.|http://arxiv.org/abs/2101.03454v3|Mrcio A. Diniz,Gillian Gresham,Sungjin Kim,Michael Luu,N. Lynn Henry,Mourad Tighiouart,Greg Yothers,Patricia A. Ganz,Andr Rogatko
687|Effective Matching of Patients to Clinical Trials using Entity Extraction and Neural Re-ranking|Clinical trials (CTs) often fail due to inadequate patient recruitment. This paper tackles the challenges of CT retrieval by presenting an approach that addresses the patient-to-trials paradigm. Our approach involves two key components in a pipeline-based model: (i) a data enrichment technique for enhancing both queries and documents during the first retrieval stage, and (ii) a novel re-ranking schema that uses a Transformer network in a setup adapted to this task by leveraging the structure of the CT documents. We use named entity recognition and negation detection in both patient description and the eligibility section of CTs. We further classify patient descriptions and CT eligibility criteria into current, past, and family medical conditions. This extracted information is used to boost the importance of disease and drug mentions in both query and index for lexical retrieval. Furthermore, we propose a two-step training schema for the Transformer network used to re-rank the results from the lexical retrieval. The first step focuses on matching patient information with the descriptive sections of trials, while the second step aims to determine eligibility by matching patient information with the criteria section. Our findings indicate that the inclusion criteria section of the CT has a great influence on the relevance score in lexical models, and that the enrichment techniques for queries and documents improve the retrieval of relevant trials. The re-ranking strategy, based on our training schema, consistently enhances CT retrieval and shows improved performance by 15\% in terms of precision at retrieving eligible trials. The results of our experiments suggest the benefit of making use of extracted entities. Moreover, our proposed re-ranking schema shows promising effectiveness compared to larger neural models, even with limited training data.|http://arxiv.org/abs/2307.00381v1|Wojciech Kusa,scar E. Mendoza,Petr Knoth,Gabriella Pasi,Allan Hanbury
688|METRIK: Measurement-Efficient Randomized Controlled Trials using Transformers with Input Masking|Clinical randomized controlled trials (RCTs) collect hundreds of measurements spanning various metric types (e.g., laboratory tests, cognitive/motor assessments, etc.) across 100s-1000s of subjects to evaluate the effect of a treatment, but do so at the cost of significant trial expense. To reduce the number of measurements, trial protocols can be revised to remove metrics extraneous to the study's objective, but doing so requires additional human labor and limits the set of hypotheses that can be studied with the collected data. In contrast, a planned missing design (PMD) can reduce the amount of data collected without removing any metric by imputing the unsampled data. Standard PMDs randomly sample data to leverage statistical properties of imputation algorithms, but are ad hoc, hence suboptimal. Methods that learn PMDs produce more sample-efficient PMDs, but are not suitable for RCTs because they require ample prior data (150+ subjects) to model the data distribution. Therefore, we introduce a framework called Measurement EfficienT Randomized Controlled Trials using Transformers with Input MasKing (METRIK), which, for the first time, calculates a PMD specific to the RCT from a modest amount of prior data (e.g., 60 subjects). Specifically, METRIK models the PMD as a learnable input masking layer that is optimized with a state-of-the-art imputer based on the Transformer architecture. METRIK implements a novel sampling and selection algorithm to generate a PMD that satisfies the trial designer's objective, i.e., whether to maximize sampling efficiency or imputation performance for a given sampling budget. Evaluated across five real-world clinical RCT datasets, METRIK increases the sampling efficiency of and imputation performance under the generated PMD by leveraging correlations over time and across metrics, thereby removing the need to manually remove metrics from the RCT.|http://arxiv.org/abs/2406.16351v1|Sayeri Lala,Niraj K. Jha
689|Subtyping brain diseases from imaging data|The imaging community has increasingly adopted machine learning (ML) methods to provide individualized imaging signatures related to disease diagnosis, prognosis, and response to treatment. Clinical neuroscience and cancer imaging have been two areas in which ML has offered particular promise. However, many neurologic and neuropsychiatric diseases, as well as cancer, are often heterogeneous in terms of their clinical manifestations, neuroanatomical patterns or genetic underpinnings. Therefore, in such cases, seeking a single disease signature might be ineffectual in delivering individualized precision diagnostics. The current chapter focuses on ML methods, especially semi-supervised clustering, that seek disease subtypes using imaging data. Work from Alzheimer Disease and its prodromal stages, psychosis, depression, autism, and brain cancer are discussed. Our goal is to provide the readers with a broad overview in terms of methodology and clinical applications.|http://arxiv.org/abs/2202.10945v1|Junhao Wen,Erdem Varol,Zhijian Yang,Gyujoon Hwang,Dominique Dwyer,Anahita Fathi Kazerooni,Paris Alexandros Lalousis,Christos Davatzikos
690|Investigating the Use of Mastery-Style Online Homework Exercises in Introductory Algebra-based Mechanics in a Controlled Clinical Study|Homework in introductory physics represents an important part of a student's learning experience; therefore choosing the manner in which homework is presented merits investigation. We performed three rounds of clinical trials comparing the effects of mastery-style homework vs traditional-style homework with students in both algebra-based and calculus-based introductory mechanics. Results indicate a benefit from mastery-style over traditional-style homework, principally for weaker students who are less familiar with the material being covered and on questions that are nearer transfer to the study materials.|http://arxiv.org/abs/1611.09377v1|William R. Evans,Mats A. Selen
691|Statistical inference methods for cumulative incidence function curves at a fixed point in time|Competing risks data arise frequently in clinical trials. When the proportional subdistribution hazard assumption is violated or two cumulative incidence function (CIF) curves cross, rather than comparing the overall treatment effects, researchers may be interested in focusing on a comparison of clinical utility at some fixed time points. This paper extend a series of tests that are constructed based on a pseudo-value regression technique or different transformation functions for CIFs and their variances based on Gaynor's or Aalen's work, and the differences among CIFs at a given time point are compared.|http://arxiv.org/abs/1807.05846v2|Jinbao Chen,Yawen Hou,Zheng Chen
692|Revolutionizing Pharma: Unveiling the AI and LLM Trends in the Pharmaceutical Industry|This document offers a critical overview of the emerging trends and significant advancements in artificial intelligence (AI) within the pharmaceutical industry. Detailing its application across key operational areas, including research and development, animal testing, clinical trials, hospital clinical stages, production, regulatory affairs, quality control and other supporting areas, the paper categorically examines AI's role in each sector. Special emphasis is placed on cutting-edge AI technologies like machine learning algorithms and their contributions to various aspects of pharmaceutical operations. Through this comprehensive analysis, the paper highlights the transformative potential of AI in reshaping the pharmaceutical industry's future.|http://arxiv.org/abs/2401.10273v2|Yu Han,Jingwen Tao
693|Towards Outcome-Driven Patient Subgroups: A Machine Learning Analysis Across Six Depression Treatment Studies|Major depressive disorder (MDD) is a heterogeneous condition; multiple underlying neurobiological substrates could be associated with treatment response variability. Understanding the sources of this variability and predicting outcomes has been elusive. Machine learning has shown promise in predicting treatment response in MDD, but one limitation has been the lack of clinical interpretability of machine learning models. We analyzed data from six clinical trials of pharmacological treatment for depression (total n = 5438) using the Differential Prototypes Neural Network (DPNN), a neural network model that derives patient prototypes which can be used to derive treatment-relevant patient clusters while learning to generate probabilities for differential treatment response. A model classifying remission and outputting individual remission probabilities for five first-line monotherapies and three combination treatments was trained using clinical and demographic data. Model validity and clinical utility were measured based on area under the curve (AUC) and expected improvement in sample remission rate with model-guided treatment, respectively. Post-hoc analyses yielded clusters (subgroups) based on patient prototypes learned during training. Prototypes were evaluated for interpretability by assessing differences in feature distributions and treatment-specific outcomes. A 3-prototype model achieved an AUC of 0.66 and an expected absolute improvement in population remission rate compared to the sample remission rate. We identified three treatment-relevant patient clusters which were clinically interpretable. It is possible to produce novel treatment-relevant patient profiles using machine learning models; doing so may improve precision medicine for depression. Note: This model is not currently the subject of any active clinical trials and is not intended for clinical use.|http://arxiv.org/abs/2303.15202v2|David Benrimoh,Akiva Kleinerman,Toshi A. Furukawa,Charles F. Reynolds III,Eric Lenze,Jordan Karp,Benoit Mulsant,Caitrin Armstrong,Joseph Mehltretter,Robert Fratila,Kelly Perlman,Sonia Israel,Myriam Tanguay-Sela,Christina Popescu,Grace Golden,Sabrina Qassim,Alexandra Anacleto,Adam Kapelner,Ariel Rosenfeld,Gustavo Turecki
694|Optimal curtailed designs for single arm phase II clinical trials|In single-arm phase II oncology trials, the most popular choice of design is Simon's two-stage design, which allows early stopping at one interim analysis. However, the expected trial sample size can be reduced further by allowing curtailment. Curtailment is stopping when the final go or no-go decision is certain, so-called non-stochastic curtailment, or very likely, known as stochastic curtailment.   In the context of single-arm phase II oncology trials, stochastic curtailment has previously been restricted to stopping in the second stage and/or stopping for a no-go decision only. We introduce two designs that incorporate stochastic curtailment and allow stopping after every observation, for either a go or no-go decision. We obtain optimal stopping boundaries by searching over a range of potential conditional powers, beyond which the trial will stop for a go or no-go decision. This search is novel: firstly, the search is undertaken over a range of values unique to each possible design realisation. Secondly, these values are evaluated taking into account the possibility of early stopping. Finally, each design realisation's operating characteristics are obtained exactly.   The proposed designs are compared to existing designs in a real data example. They are also compared under three scenarios, both with respect to four single optimality criteria and using a loss function.   The proposed designs are superior in almost all cases. Optimising for the expected sample size under either the null or alternative hypothesis, the saving compared to the popular Simon's design ranges from 22% to 55%.|http://arxiv.org/abs/1909.03017v1|Martin Law,Michael J. Grayling,Adrian P. Mander
695|Bayesian Nonparametric Common Atoms Regression for Generating Synthetic Controls in Clinical Trials|The availability of electronic health records (EHR) has opened opportunities to supplement increasingly expensive and difficult to carry out randomized controlled trials (RCT) with evidence from readily available real world data. In this paper, we use EHR data to construct synthetic control arms for treatment-only single arm trials. We propose a novel nonparametric Bayesian common atoms mixture model that allows us to find equivalent population strata in the EHR and the treatment arm and then resample the EHR data to create equivalent patient populations under both the single arm trial and the resampled EHR. Resampling is implemented via a density-free importance sampling scheme. Using the synthetic control arm, inference for the treatment effect can then be carried out using any method available for RCTs. Alternatively the proposed nonparametric Bayesian model allows straightforward model-based inference. In simulation experiments, the proposed method exhibits higher power than alternative methods in detecting treatment effects, specifically for non-linear response functions. We apply the method to supplement single arm treatment-only glioblastoma studies with a synthetic control arm based on historical trials.|http://arxiv.org/abs/2201.00068v3|Noirrit Kiran Chandra,Abhra Sarkar,John F. de Groot,Ying Yuan,Peter Mller
696|The Backfill i3+3 Design for Dose-Finding Trials in Oncology|We consider a formal statistical design that allows simultaneous enrollment of a main cohort and a backfill cohort of patients in a dose-finding trial. The goal is to accumulate more information at various doses to facilitate dose optimization. The proposed design, called Bi3+3, combines the simple dose-escalation algorithm in the i3+3 design and a model-based inference under the framework of probability of decisions (POD), both previously published. As a result, Bi3+3 provides a simple algorithm for backfilling patients to lower doses in a dose-finding trial once these doses exhibit safety profile in patients. The POD framework allows dosing decisions to be made when some backfill patients are still being followed with incomplete toxicity outcomes, thereby potentially expediting the clinical trial. At the end of the trial, Bi3+3 uses both toxicity and efficacy outcomes to estimate an optimal biological dose (OBD). The proposed inference is based on a dose-response model that takes into account either a monotone or plateau dose-efficacy relationship, which are frequently encountered in modern oncology drug development. Simulation studies show promising operating characteristics of the Bi3+3 design in comparison to existing designs.|http://arxiv.org/abs/2303.15798v4|Jiaxin Liu,Shijie Yuan,B. Nebiyou Bekele,Yuan Ji
697|Bayesian sample size determination using robust commensurate priors with interpretable discrepancy weights|Randomized controlled clinical trials provide the gold standard for evidence generation in relation to the efficacy of a new treatment in medical research. Relevant information from previous studies may be desirable to incorporate in the design and analysis of a new trial, with the Bayesian paradigm providing a coherent framework to formally incorporate prior knowledge. Many established methods involve the use of a discounting factor, sometimes related to a measure of `similarity' between historical and the new trials. However, it is often the case that the sample size is highly nonlinear in those discounting factors. This hinders communication with subject-matter experts to elicit sensible values for borrowing strength at the trial design stage. Focusing on a commensurate predictive prior method that can incorporate historical data from multiple sources, we highlight a particular issue of nonmonotonicity and explain why this causes issues with interpretability of the discounting factors (hereafter referred to as `weights'). We propose a solution for this, from which an analytical sample size formula is derived. We then propose a linearization technique such that the sample size changes uniformly over the weights. Our approach leads to interpretable weights that represent the probability that historical data are (ir)relevant to the new trial, and could therefore facilitate easier elicitation of expert opinion on their values.   Keywords: Bayesian sample size determination; Commensurate priors; Historical borrowing; Prior aggregation; Uniform shrinkage.|http://arxiv.org/abs/2401.10592v2|Lou E. Whitehead,James M. S. Wason,Oliver Sailer,Haiyan Zheng
698|Assessment of contextualised representations in detecting outcome phrases in clinical trials|Automating the recognition of outcomes reported in clinical trials using machine learning has a huge potential of speeding up access to evidence necessary in healthcare decision-making. Prior research has however acknowledged inadequate training corpora as a challenge for the Outcome detection (OD) task. Additionally, several contextualized representations like BERT and ELMO have achieved unparalleled success in detecting various diseases, genes, proteins, and chemicals, however, the same cannot be emphatically stated for outcomes, because these models have been relatively under-tested and studied for the OD task. We introduce "EBM-COMET", a dataset in which 300 PubMed abstracts are expertly annotated for clinical outcomes. Unlike prior related datasets that use arbitrary outcome classifications, we use labels from a taxonomy recently published to standardize outcome classifications. To extract outcomes, we fine-tune a variety of pre-trained contextualized representations, additionally, we use frozen contextualized and context-independent representations in our custom neural model augmented with clinically informed Part-Of-Speech embeddings and a cost-sensitive loss function. We adopt strict evaluation for the trained models by rewarding them for correctly identifying full outcome phrases rather than words within the entities i.e. given an outcome "systolic blood pressure", the models are rewarded a classification score only when they predict all 3 words in sequence, otherwise, they are not rewarded. We observe our best model (BioBERT) achieve 81.5\% F1, 81.3\% sensitivity and 98.0\% specificity. We reach a consensus on which contextualized representations are best suited for detecting outcomes from clinical-trial abstracts. Furthermore, our best model outperforms scores published on the original EBM-NLP dataset leader-board scores.|http://arxiv.org/abs/2203.03547v2|Micheal Abaho,Danushka Bollegala,Paula R Williamson,Susanna Dodd
699|Deep Few-view High-resolution Photon-counting Extremity CT at Halved Dose for a Clinical Trial|The latest X-ray photon-counting computed tomography (PCCT) for extremity allows multi-energy high-resolution (HR) imaging for tissue characterization and material decomposition. However, both radiation dose and imaging speed need improvement for contrast-enhanced and other studies. Despite the success of deep learning methods for 2D few-view reconstruction, applying them to HR volumetric reconstruction of extremity scans for clinical diagnosis has been limited due to GPU memory constraints, training data scarcity, and domain gap issues. In this paper, we propose a deep learning-based approach for PCCT image reconstruction at halved dose and doubled speed in a New Zealand clinical trial. Particularly, we present a patch-based volumetric refinement network to alleviate the GPU memory limitation, train network with synthetic data, and use model-based iterative refinement to bridge the gap between synthetic and real-world data. The simulation and phantom experiments demonstrate consistently improved results under different acquisition conditions on both in- and off-domain structures using a fixed network. The image quality of 8 patients from the clinical trial are evaluated by three radiologists in comparison with the standard image reconstruction with a full-view dataset. It is shown that our proposed approach is essentially identical to or better than the clinical benchmark in terms of diagnostic image quality scores. Our approach has a great potential to improve the safety and efficiency of PCCT without compromising image quality.|http://arxiv.org/abs/2403.12331v1|Mengzhou Li,Chuang Niu,Ge Wang,Maya R Amma,Krishna M Chapagain,Stefan Gabrielson,Andrew Li,Kevin Jonker,Niels de Ruiter,Jennifer A Clark,Phil Butler,Anthony Butler,Hengyong Yu
700|Developing and Using Special-Purpose Lexicons for Cohort Selection from Clinical Notes|Background and Significance: Selecting cohorts for a clinical trial typically requires costly and time-consuming manual chart reviews resulting in poor participation. To help automate the process, National NLP Clinical Challenges (N2C2) conducted a shared challenge by defining 13 criteria for clinical trial cohort selection and by providing training and test datasets. This research was motivated by the N2C2 challenge.   Methods: We broke down the task into 13 independent subtasks corresponding to each criterion and implemented subtasks using rules or a supervised machine learning model. Each task critically depended on knowledge resources in the form of task-specific lexicons, for which we developed a novel model-driven approach. The approach allowed us to first expand the lexicon from a seed set and then remove noise from the list, thus improving the accuracy.   Results: Our system achieved an overall F measure of 0.9003 at the challenge, and was statistically tied for the first place out of 45 participants. The model-driven lexicon development and further debugging the rules/code on the training set improved overall F measure to 0.9140, overtaking the best numerical result at the challenge.   Discussion: Cohort selection, like phenotype extraction and classification, is amenable to rule-based or simple machine learning methods, however, the lexicons involved, such as medication names or medical terms referring to a medical problem, critically determine the overall accuracy. Automated lexicon development has the potential for scalability and accuracy.|http://arxiv.org/abs/1902.09674v1|Samarth Rawal,Ashok Prakash,Soumya Adhya,Sidharth Kulkarni,Saadat Anwar,Chitta Baral,Murthy Devarakonda
701|Forecasting the Progression of Alzheimer's Disease Using Neural Networks and a Novel Pre-Processing Algorithm|Alzheimer's disease (AD) is the most common neurodegenerative disease in older people. Despite considerable efforts to find a cure for AD, there is a 99.6% failure rate of clinical trials for AD drugs, likely because AD patients cannot easily be identified at early stages. This project investigated machine learning approaches to predict the clinical state of patients in future years to benefit AD research. Clinical data from 1737 patients was obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database and was processed using the "All-Pairs" technique, a novel methodology created for this project involving the comparison of all possible pairs of temporal data points for each patient. This data was then used to train various machine learning models. Models were evaluated using 7-fold cross-validation on the training dataset and confirmed using data from a separate testing dataset (110 patients). A neural network model was effective (mAUC = 0.866) at predicting the progression of AD on a month-by-month basis, both in patients who were initially cognitively normal and in patients suffering from mild cognitive impairment. Such a model could be used to identify patients at early stages of AD and who are therefore good candidates for clinical trials for AD therapeutics.|http://arxiv.org/abs/1903.07510v2|Jack Albright
702|Quantifying treatment differences in confirmatory trials under non-proportional hazards|Proportional hazards are a common assumption when designing confirmatory clinical trials in oncology. With the emergence of immunotherapy and novel targeted therapies, departure from the proportional hazard assumption is not rare in nowadays clinical research. Under non-proportional hazards, the hazard ratio does not have a straightforward clinical interpretation, and the log-rank test is no longer the most powerful statistical test even though it is still valid. Nevertheless, the log-rank test and the hazard ratio are still the primary analysis tools, and traditional approaches such as sample size increase are still proposed to account for the impact of non-proportional hazards. The weighed log-rank test and the test based on the restricted mean survival time (RMST) are receiving a lot of attention as a potential alternative to the log-rank test. We conduct a simulation study comparing the performance and operating characteristics of the log-rank test, the weighted log-rank test and the test based on the RMST, including a treatment effect estimation, under different non-proportional hazards patterns. Results show that, under non-proportional hazards, the hazard ratio and weighted hazard ratio have no straightforward clinical interpretation whereas the RMST ratio can be interpreted regardless of the proportional hazards assumption. In terms of power, the RMST achieves a similar performance when compared to the log-rank test.|http://arxiv.org/abs/1908.10502v3|Jos L. Jimnez
703|3-D Stochastic Numerical Breast Phantoms for Enabling Virtual Imaging Trials of Ultrasound Computed Tomography|Ultrasound computed tomography (USCT) is an emerging imaging modality for breast imaging that can produce quantitative images that depict the acoustic properties of tissues. Computer-simulation studies, also known as virtual imaging trials, provide researchers with an economical and convenient route to systematically explore imaging system designs and image reconstruction methods. When simulating an imaging technology intended for clinical use, it is essential to employ realistic numerical phantoms that can facilitate the objective, or task-based, assessment of image quality. Moreover, when computing objective image quality measures, an ensemble of such phantoms should be employed that display the variability in anatomy and object properties that is representative of the to-be-imaged patient cohort. Such stochastic phantoms for clinically relevant applications of USCT are currently lacking. In this work, a methodology for producing realistic three-dimensional (3D) numerical breast phantoms for enabling clinically relevant computer-simulation studies of USCT breast imaging is presented. By extending and adapting an existing stochastic 3D breast phantom for use withUSCT, methods for creating ensembles of numerical acoustic breast phantoms are established. These breast phantoms will possess clinically relevant variations in breast size, composition, acoustic properties, tumor locations, and tissue textures. To demonstrate the use of the phantoms in virtual USCT studies, two brief case studies are presented that address the development and assessment of image reconstruction procedures. Examples of breast phantoms produced by use of the proposed methods and a collection of 52 sets of simulated USCT measurement data have been made open source for use in image reconstruction development|http://arxiv.org/abs/2106.02744v2|Fu Li,Umberto Villa,Seonyeong Park,Mark A. Anastasio
704|A continued learning approach for model-informed precision dosing: updating models in clinical practice|Model-informed precision dosing (MIPD) is a quantitative dosing framework that combines prior knowledge on the drug-disease-patient system with patient data from therapeutic drug/ biomarker monitoring (TDM) to support individualized dosing in ongoing treatment.Structural models and prior parameter distributions used in MIPD approaches typically build on prior clinical trials that involve only a limited number of patients selected according to some exclusion/inclusion criteria. Compared to the prior clinical trial population, the patient population in clinical practice can be expected to include also altered behavior and/or increased interindividual variability, the extent of which, however, is typically unknown. Here, we address the question of how to adapt and refine models on the level of the model parameters to better reflect this real-world diversity. We propose an approach for continued learning across patients during MIPD using a sequential hierarchical Bayesian framework. The approach builds on two stages to separate the update of the individual patient parameters from updating the population parameters. Consequently, it enables continued learning across hospitals or study centers, since only summary patient data (on the level of model parameters) need to be shared, but no individual TDM data. We illustrate this continued learning approach with neutrophil-guided dosing of paclitaxel. The present study constitutes an important step towards building confidence in MIPD and eventually establishing MIPD increasingly in everyday therapeutic use.|http://arxiv.org/abs/2106.03752v1|Corinna Maier,Jana de Wiljes,Niklas Hartung,Charlotte Kloft,Wilhelm Huisinga
705|Assessing contribution of treatment phases through tipping point analyses via counterfactual elicitation using rank preserving structural failure time models|This article provides a novel approach to assess the importance of specific treatment phases within a treatment regimen through tipping point analyses (TPA) of a time-to-event endpoint using rank-preserving-structural-failure-time (RPSFT) modelling. In oncology clinical research, an experimental treatment is often added to the standard of care therapy in multiple treatment phases to improve patient outcomes. When the resulting new regimen provides a meaningful benefit over standard of care, gaining insights into the contribution of each treatment phase becomes important to properly guide clinical practice. New statistical approaches are needed since traditional methods are inadequate in answering such questions. RPSFT modelling is an approach for causal inference, typically used to adjust for treatment switching in randomized clinical trials with time-to-event endpoints. A tipping-point analysis is commonly used in situations where a statistically significant treatment effect is suspected to be an artifact of missing or unobserved data rather than a real treatment difference. The methodology proposed in this article is an amalgamation of these two ideas to investigate the contribution of a specific component of a regimen comprising multiple treatment phases. We provide different variants of the method and construct indices of contribution of a treatment phase to the overall benefit of a regimen that facilitates interpretation of results. The proposed approaches are illustrated with findings from a recently concluded, real-life phase 3 cancer clinical trial. We conclude with several considerations and recommendations for practical implementation of this new methodology.|http://arxiv.org/abs/2107.01480v1|Sudipta Bhattacharya,Jyotirmoy Dey
706|P2T2: a Physically-primed deep-neural-network approach for robust $T_{2}$ distribution estimation from quantitative $T_{2}$-weighted MRI|Estimating $T_2$ relaxation time distributions from multi-echo $T_2$-weighted MRI ($T_2W$) data can provide valuable biomarkers for assessing inflammation, demyelination, edema, and cartilage composition in various pathologies, including neurodegenerative disorders, osteoarthritis, and tumors. Deep neural network (DNN) based methods have been proposed to address the complex inverse problem of estimating $T_2$ distributions from MRI data, but they are not yet robust enough for clinical data with low Signal-to-Noise ratio (SNR) and are highly sensitive to distribution shifts such as variations in echo-times (TE) used during acquisition. Consequently, their application is hindered in clinical practice and large-scale multi-institutional trials with heterogeneous acquisition protocols. We propose a physically-primed DNN approach, called $P_2T_2$, that incorporates the signal decay forward model in addition to the MRI signal into the DNN architecture to improve the accuracy and robustness of $T_2$ distribution estimation. We evaluated our $P_2T_2$ model in comparison to both DNN-based methods and classical methods for $T_2$ distribution estimation using 1D and 2D numerical simulations along with clinical data. Our model improved the baseline model's accuracy for low SNR levels ($SNR<80$) which are common in the clinical setting. Further, our model achieved a $\sim$35\% improvement in robustness against distribution shifts in the acquisition process compared to previously proposed DNN models. Finally, Our $P_2T_2$ model produces the most detailed Myelin-Water fraction maps compared to baseline approaches when applied to real human MRI data. Our $P_2T_2$ model offers a reliable and precise means of estimating $T_2$ distributions from MRI data and shows promise for use in large-scale multi-institutional trials with heterogeneous acquisition protocols.|http://arxiv.org/abs/2212.04928v2|Hadas Ben-Atya,Moti Freiman
707|Investigating Deep-Learning NLP for Automating the Extraction of Oncology Efficacy Endpoints from Scientific Literature|Benchmarking drug efficacy is a critical step in clinical trial design and planning. The challenge is that much of the data on efficacy endpoints is stored in scientific papers in free text form, so extraction of such data is currently a largely manual task. Our objective is to automate this task as much as possible. In this study we have developed and optimised a framework to extract efficacy endpoints from text in scientific papers, using a machine learning approach. Our machine learning model predicts 25 classes associated with efficacy endpoints and leads to high F1 scores (harmonic mean of precision and recall) of 96.4% on the test set, and 93.9% and 93.7% on two case studies. These methods were evaluated against - and showed strong agreement with - subject matter experts and show significant promise in the future of automating the extraction of clinical endpoints from free text. Clinical information extraction from text data is currently a laborious manual task which scales poorly and is prone to human error. Demonstrating the ability to extract efficacy endpoints automatically shows great promise for accelerating clinical trial design moving forwards.|http://arxiv.org/abs/2311.04925v1|Aline Gendrin-Brokmann,Eden Harrison,Julianne Noveras,Leonidas Souliotis,Harris Vince,Ines Smit,Francisco Costa,David Milward,Sashka Dimitrievska,Paul Metcalfe,Emilie Louvet
708|Pre-processing and quality control of large clinical CT head datasets for intracranial arterial calcification segmentation|As a potential non-invasive biomarker for ischaemic stroke, intracranial arterial calcification (IAC) could be used for stroke risk assessment on CT head scans routinely acquired for other reasons (e.g. trauma, confusion). Artificial intelligence methods can support IAC scoring, but they have not yet been developed for clinical imaging. Large heterogeneous clinical CT datasets are necessary for the training of such methods, but they exhibit expected and unexpected data anomalies. Using CTs from a large clinical trial, the third International Stroke Trial (IST-3), we propose a pipeline that uses as input non-enhanced CT scans to output regions of interest capturing selected large intracranial arteries for IAC scoring. Our method uses co-registration with templates. We focus on quality control, using information presence along the z-axis of the imaging to group and apply similarity measures (structural similarity index measure) to triage assessment of individual image series. Additionally, we propose superimposing thresholded binary masks of the series to inspect large quantities of data in parallel. We identify and exclude unrecoverable samples and registration failures. In total, our pipeline processes 10,659 CT series, rejecting 4,322 (41%) in the entire process, 1,450 (14% of the total) during quality control, and outputting 6,337 series. Our pipeline enables effective and efficient region of interest localisation for targeted IAC segmentation.|http://arxiv.org/abs/2408.01199v1|Benjamin Jin,Maria del C. Valds Hernndez,Alessandro Fontanella,Wenwen Li,Eleanor Platt,Paul Armitage,Amos Storkey,Joanna M. Wardlaw,Grant Mair
709|Online detection and sorting of extracellularly recorded action potentials in human medial temporal lobe recordings, in vivo|Understanding the function of complex cortical circuits requires the simultaneous recording of action potentials from many neurons in awake and behaving animals. Practically, this can be achieved by extracellularly recording from multiple brain sites using single wire electrodes. However, in densely packed neural structures such as the human hippocampus, a single electrode can record the activity of multiple neurons. Thus, analytic techniques that differentiate action potentials of different neurons are required. Offline spike sorting approaches are currently used to detect and sort action potentials after finishing the experiment. Because the opportunities to record from the human brain are relatively rare, it is desirable to analyze large numbers of simultaneous recordings quickly using online sorting and detection algorithms. In this way, the experiment can be optimized for the particular response properties of the recorded neurons. Here we present and evaluate a method that is capable of detecting and sorting extracellular single-wire recordings in realtime. We demonstrate the utility of the method by applying it to an extensive data set we acquired from chronically-implanted depth electrodes in the hippocampus of human epilepsy patients. This dataset is particularly challenging because it was recorded in a noisy clinical environment. This method will allow the development of closed-loop experiments, which immediately adapt the experimental stimuli and/or tasks to the neural response observed.|http://arxiv.org/abs/q-bio/0604033v1|Ueli Rutishauser,Erin M. Schuman,Adam N. Mamelak
710|How does artificial intelligence contribute to iEEG research?|Artificial intelligence (AI) is a fast-growing field focused on modeling and machine implementation of various cognitive functions with an increasing number of applications in computer vision, text processing, robotics, neurotechnology, bio-inspired computing and others. In this chapter, we describe how AI methods can be applied in the context of intracranial electroencephalography (iEEG) research. IEEG data is unique as it provides extremely high-quality signals recorded directly from brain tissue. Applying advanced AI models to these data carries the potential to further our understanding of many fundamental questions in neuroscience. At the same time, as an invasive technique, iEEG lends itself well to long-term, mobile brain-computer interface applications, particularly for communication in severely paralyzed individuals. We provide a detailed overview of these two research directions in the application of AI techniques to iEEG. That is, (1) the development of computational models that target fundamental questions about the neurobiological nature of cognition (AI-iEEG for neuroscience) and (2) applied research on monitoring and identification of event-driven brain states for the development of clinical brain-computer interface systems (AI-iEEG for neurotechnology). We explain key machine learning concepts, specifics of processing and modeling iEEG data and details of state-of-the-art iEEG-based neurotechnology and brain-computer interfaces.|http://arxiv.org/abs/2207.13190v1|Julia Berezutskaya,Anne-Lise Saive,Karim Jerbi,Marcel van Gerven
711|Pennsieve: A Collaborative Platform for Translational Neuroscience and Beyond|The exponential growth of neuroscientific data necessitates platforms that facilitate data management and multidisciplinary collaboration. In this paper, we introduce Pennsieve - an open-source, cloud-based scientific data management platform built to meet these needs. Pennsieve supports complex multimodal datasets and provides tools for data visualization and analyses. It takes a comprehensive approach to data integration, enabling researchers to define custom metadata schemas and utilize advanced tools to filter and query their data. Pennsieve's modular architecture allows external applications to extend its capabilities, and collaborative workspaces with peer-reviewed data publishing mechanisms promote high-quality datasets optimized for downstream analysis, both in the cloud and on-premises.   Pennsieve forms the core for major neuroscience research programs including NIH SPARC Initiative, NIH HEAL Initiative's PRECISION Human Pain Network, and NIH HEAL RE-JOIN Initiative. It serves more than 80 research groups worldwide, along with several large-scale, inter-institutional projects at clinical sites through the University of Pennsylvania. Underpinning the SPARC.Science, Epilepsy.Science, and Pennsieve Discover portals, Pennsieve stores over 125 TB of scientific data, with 35 TB of data publicly available across more than 350 high-impact datasets. It adheres to the findable, accessible, interoperable, and reusable (FAIR) principles of data sharing and is recognized as one of the NIH-approved Data Repositories. By facilitating scientific data management, discovery, and analysis, Pennsieve fosters a robust and collaborative research ecosystem for neuroscience and beyond.|http://arxiv.org/abs/2409.10509v2|Zack Goldblum,Zhongchuan Xu,Haoer Shi,Patryk Orzechowski,Jamaal Spence,Kathryn A Davis,Brian Litt,Nishant Sinha,Joost Wagenaar
712|Testing the Odds of Inherent versus Observed Over-dispersion in Neural Spike Counts Odds of Inherent versus Observed Over-dispersion|The repeated presentation of an identical visual stimulus in the receptive field of a neuron may evoke different spiking patterns at each trial. Probabilistic methods are essential to understand the functional role of this variance within the neural activity. In that case, a Poisson process is the most common model of trial-to-trial variability. For a Poisson process, the variance of the spike count is constrained to be equal to the mean, irrespective of the duration of measurements. Numerous studies have shown that this relationship does not generally hold. Specifically, a majority of electrophysiological recordings show an " over-dispersion " effect: Responses that exhibit more inter-trial variability than expected from a Poisson process alone. A model that is particularly well suited to quantify over-dispersion is the Negative-Binomial distribution model. This model is well-studied and widely used but has only recently been applied to neuroscience. In this paper, we address three main issues. First, we describe how the Negative-Binomial distribution provides a model apt to account for overdispersed spike counts. Second, we quantify the significance of this model for any neurophysiological data by proposing a statistical test, which quantifies the odds that over-dispersion could be due to the limited number of repetitions (trials). We apply this test to three neurophysiological tests along the visual pathway. Finally, we compare the performance of this model to the Poisson model on a population decoding task. We show that the decoding accuracy is improved when accounting for over-dispersion, especially under the hypothesis of tuned over-dispersion.|http://arxiv.org/abs/1611.04364v1|Wahiba Taouali,Giacomo Benvenuti,Pascal Wallisch,Frdric Chavane,Laurent Perrinet
713|Analyzing second order stochasticity of neural spiking under stimuli-bundle exposure|Conventional analysis of neuroscience data involves computing average neural activity over a group of trials and/or a period of time. This approach may be particularly problematic when assessing the response patterns of neurons to more than one simultaneously presented stimulus. In such cases, the brain must represent each individual component of the stimuli bundle, but trial-and-time-pooled averaging methods are fundamentally unequipped to address the means by which multi-item representation occurs. We introduce and investigate a novel statistical analysis framework that relates the firing pattern of a single cell, exposed to a stimuli bundle, to the ensemble of its firing patterns under each constituent stimulus. Existing statistical tools focus on what may be called "first order stochasticity" in trial-to-trial variation in the form of unstructured noise around a fixed firing rate curve associated with a given stimulus. Our analysis is based upon the theoretical premise that exposure to a stimuli bundle induces additional stochasticity in the cell's response pattern, in the form of a stochastically varying recombination of its single stimulus firing rate curves. We discuss challenges to statistical estimation of such "second order stochasticity" and address them with a novel dynamic admixture Poisson process (DAPP) model. DAPP is a hierarchical point process model that decomposes second order stochasticity into a Gaussian stochastic process and a random vector of interpretable features, and, facilitates borrowing of information on the latter across repeated trials through latent clustering. We present empirical evidence of the utility of the DAPP analysis with synthetic and real neural recordings.|http://arxiv.org/abs/1911.04387v1|Chris Glynn,Surya T Tokdar,Azeem Zaman,Valeria C Caruso,Jeffrey T Mohl,Shawn M Willett,Jennifer M Groh
714|Immunological determinants of clinical outcomes in COVID-19: A quantitative perspective|Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has a variable clinical presentation that ranges from asymptomatic, to severe disease with cytokine storm. The mortality rates also differ across the globe, ranging from 0.5-13%. This variation is likely due to both pathogen and host factors. Host factors may include genetic differences in the immune response genes as well as variation in HLA and KIR allotypes. To better understand what impact these genetic variants in immune response genes may have in the differences observed in the immune response to SARS-CoV-2, a quantitative analysis of a dynamical systems model that considers both, the magnitude of viral growth, and the subsequent innate and adaptive response required to achieve control of infection is considered. Based on this broad quantitative framework it may be posited that the spectrum of symptomatic to severely symptomatic presentations of COVID19 represents the balance between innate and adaptive immune responses. In asymptomatic patients, prompt and adequate adaptive immune response quells infection, whereas in those with severe symptoms a slower inadequate adaptive response leads to a runaway cytokine cascade fueled by ongoing viral replication. Polymorphisms in the various components of the innate and adaptive immune response may cause altered immune response kinetics that would result in variable severity of illness. Understanding how this genetic variation may alter the response to SARS-CoV-2 infection is critical to develop successful treatment strategies.|http://arxiv.org/abs/2005.06541v2|Elizabeth Krieger,Nicole Vissichelli,Stefan Leichtle,Markos Kashioris,Roy Sabo,Don Brophy,Xiang-Yang Wang,Pamela Kimbal,Michael Neale,Myrna G. Serrano,Gregory A. Buck,Catherine Roberts,Rehan Qayyum,Daniel Nixon,Steven Grossman,Amir A. Toor
715|Mind Control as a Guide for the Mind|The human brain is a complex network that supports mental function. The nascent field of network neuroscience applies tools from mathematics to neuroimaging data in the hopes of shedding light on cognitive function. A critical question arising from these empirical studies is how to modulate a human brain network to treat cognitive deficits or enhance mental abilities. While historically a number of tools have been employed to modulate mental states (such as cognitive behavioral therapy and brain stimulation), theoretical frameworks to guide these interventions - and to optimize them for clinical use - are fundamentally lacking. One promising and as-yet underexplored approach lies in a sub-discipline of engineering known as network control theory. Here, we posit that network control fundamentally relates to mind control, and that this relationship highlights important areas for future empirical research and opportunities to translate knowledge in practical domains. We clarify the conceptual intersection between neuroanatomy, cognition, and control engineering in the context of network neuroscience. Finally, we discuss the challenges, ethics, and promises of mind control.|http://arxiv.org/abs/1610.04134v2|John D. Medaglia,Perry Zurn,Walter Sinnott-Armstrong,Danielle S. Bassett
716|Event-based Gesture Recognition with Dynamic Background Suppression using Smartphone Computational Capabilities|This paper introduces a framework of gesture recognition operating on the output of an event based camera using the computational resources of a mobile phone. We will introduce a new development around the concept of time-surfaces modified and adapted to run on the limited computational resources of a mobile platform. We also introduce a new method to remove dynamically backgrounds that makes full use of the high temporal resolution of event-based cameras. We assess the performances of the framework by operating on several dynamic scenarios in uncontrolled lighting conditions indoors and outdoors. We also introduce a new publicly available event-based dataset for gesture recognition selected through a clinical process to allow human-machine interactions for the visually-impaired and the elderly. We finally report comparisons with prior works that tackled event-based gesture recognition reporting comparable if not superior results if taking into account the limited computational and memory constraints of the used hardware.|http://arxiv.org/abs/1811.07802v3|Jean-Matthieu Maro,Ryad Benosman
717|Computational models in Electroencephalography|Computational models lie at the intersection of basic neuroscience and healthcare applications because they allow researchers to test hypotheses \textit{in silico} and predict the outcome of experiments and interactions that are very hard to test in reality. Yet, what is meant by "computational model" is understood in many different ways by researchers in different fields of neuroscience and psychology, hindering communication and collaboration. In this review, we point out the state of the art of computational modeling in Electroencephalography (EEG) and outline how these models can be used to integrate findings from electrophysiology, network-level models, and behavior. On the one hand, computational models serve to investigate the mechanisms that generate brain activity, for example measured with EEG, such as the transient emergence of oscillations at different frequency bands and/or with different spatial topographies. On the other hand, computational models serve to design experiments and test hypotheses \emph{in silico}. The final purpose of computational models of EEG is to obtain a comprehensive understanding of the mechanisms that underlie the EEG signal. This is crucial for an accurate interpretation of EEG measurements that may ultimately serve in the development of novel clinical applications.|http://arxiv.org/abs/2009.08385v1|Katharina Glomb,Joana Cabral,Anna Cattani,Alberto Mazzoni,Ashish Raj,Benedetta Franceschiello
718|A computational medical XR discipline|Computational Medical Extended Reality (CMXR), brings together life sciences and neuroscience with mathematics, engineering and computer science. It unifies computational science (scientific computing) with intelligent extended reality and spatial computing for the medical field. It significantly differs from previous "Clinical XR" or "Medical XR" terms, as it is focusing on how to integrate computational methods from neural simulation to computational geometry, computational vision and computer graphics with deep learning models to solve specific hard problems in medicine and neuroscience: from low/no-code/genAI authoring platforms to deep learning XR systems for training, planning, operative navigation, therapy and rehabilitation.|http://arxiv.org/abs/2108.04136v6|George Papagiannakis,Walter Greenleaf,Michael Cole,Mark Zhang,Rabi Datta,Mathias Delahaye,Eleni Grigoriou,Manos Kamarianakis,Antonis Protopsaltis,Philippe Bijlenga,Nadia Magnenat-Thalmann,Eleftherios Tsiridis,Eustathios Kenanidis,Kyriakos Vamvakidis,Ioannis Koutelidakis,Oliver A Kannape
719|Latent Representation Learning for Multimodal Brain Activity Translation|Neuroscience employs diverse neuroimaging techniques, each offering distinct insights into brain activity, from electrophysiological recordings such as EEG, which have high temporal resolution, to hemodynamic modalities such as fMRI, which have increased spatial precision. However, integrating these heterogeneous data sources remains a challenge, which limits a comprehensive understanding of brain function. We present the Spatiotemporal Alignment of Multimodal Brain Activity (SAMBA) framework, which bridges the spatial and temporal resolution gaps across modalities by learning a unified latent space free of modality-specific biases. SAMBA introduces a novel attention-based wavelet decomposition for spectral filtering of electrophysiological recordings, graph attention networks to model functional connectivity between functional brain units, and recurrent layers to capture temporal autocorrelations in brain signal. We show that the training of SAMBA, aside from achieving translation, also learns a rich representation of brain information processing. We showcase this classify external stimuli driving brain activity from the representation learned in hidden layers of SAMBA, paving the way for broad downstream applications in neuroscience research and clinical contexts.|http://arxiv.org/abs/2409.18462v1|Arman Afrasiyabi,Dhananjay Bhaskar,Erica L. Busch,Laurent Caplette,Rahul Singh,Guillaume Lajoie,Nicholas B. Turk-Browne,Smita Krishnaswamy
720|Using Decision Lists to Construct Interpretable and Parsimonious Treatment Regimes|A treatment regime formalizes personalized medicine as a function from individual patient characteristics to a recommended treatment. A high-quality treatment regime can improve patient outcomes while reducing cost, resource consumption, and treatment burden. Thus, there is tremendous interest in estimating treatment regimes from observational and randomized studies. However, the development of treatment regimes for application in clinical practice requires the long-term, joint effort of statisticians and clinical scientists. In this collaborative process, the statistician must integrate clinical science into the statistical models underlying a treatment regime and the clinician must scrutinize the estimated treatment regime for scientific validity. To facilitate meaningful information exchange, it is important that estimated treatment regimes be interpretable in a subject-matter context. We propose a simple, yet flexible class of treatment regimes whose members are representable as a short list of if-then statements. Regimes in this class are immediately interpretable and are therefore an appealing choice for broad application in practice. We derive a robust estimator of the optimal regime within this class and demonstrate its finite sample performance using simulation experiments. The proposed method is illustrated with data from two clinical trials.|http://arxiv.org/abs/1504.07715v1|Yichi Zhang,Eric B. Laber,Anastasios Tsiatis,Marie Davidian
721|Correlational Dueling Bandits with Application to Clinical Treatment in Large Decision Spaces|We consider sequential decision making under uncertainty, where the goal is to optimize over a large decision space using noisy comparative feedback. This problem can be formulated as a $K$-armed Dueling Bandits problem where $K$ is the total number of decisions. When $K$ is very large, existing dueling bandits algorithms suffer huge cumulative regret before converging on the optimal arm. This paper studies the dueling bandits problem with a large number of arms that exhibit a low-dimensional correlation structure. Our problem is motivated by a clinical decision making process in large decision space. We propose an efficient algorithm CorrDuel which optimizes the exploration/exploitation tradeoff in this large decision space of clinical treatments. More broadly, our approach can be applied to other sequential decision problems with large and structured decision spaces. We derive regret bounds, and evaluate performance in simulation experiments as well as on a live clinical trial of therapeutic spinal cord stimulation. To our knowledge, this marks the first time an online learning algorithm was applied towards spinal cord injury treatments. Our experimental results show the effectiveness and efficiency of our approach.|http://arxiv.org/abs/1707.02375v1|Yanan Sui,Yisong Yue,Joel W. Burdick
722|An Interpretable End-to-end Fine-tuning Approach for Long Clinical Text|Unstructured clinical text in EHRs contains crucial information for applications including decision support, trial matching, and retrospective research. Recent work has applied BERT-based models to clinical information extraction and text classification, given these models' state-of-the-art performance in other NLP domains. However, BERT is difficult to apply to clinical notes because it doesn't scale well to long sequences of text. In this work, we propose a novel fine-tuning approach called SnipBERT. Instead of using entire notes, SnipBERT identifies crucial snippets and then feeds them into a truncated BERT-based model in a hierarchical manner. Empirically, SnipBERT not only has significant predictive performance gain across three tasks but also provides improved interpretability, as the model can identify key pieces of text that led to its prediction.|http://arxiv.org/abs/2011.06504v1|Kexin Huang,Sankeerth Garapati,Alexander S. Rich
723|Prediction of individual progression rate in Parkinson's disease using clinical measures and biomechanical measures of gait and postural stability|Parkinson's disease (PD) is a common neurological disorder characterized by gait impairment. PD has no cure, and an impediment to developing a treatment is the lack of any accepted method to predict disease progression rate. The primary aim of this study was to develop a model using clinical measures and biomechanical measures of gait and postural stability to predict an individual's PD progression over two years. Data from 160 PD subjects were utilized. Machine learning models, including XGBoost and Feed Forward Neural Networks, were developed using extensive model optimization and cross-validation. The highest performing model was a neural network that used a group of clinical measures, achieved a PPV of 71% in identifying fast progressors, and explained a large portion (37%) of the variance in an individual's progression rate on held-out test data. This demonstrates the potential to predict individual PD progression rate and enrich trials by analyzing clinical and biomechanical measures with machine learning.|http://arxiv.org/abs/1911.10227v1|Vyom Raval,Kevin P. Nguyen,Ashley Gerald,Richard B. Dewey Jr.,Albert Montillo
724|A Dynamic Deep Neural Network For Multimodal Clinical Data Analysis|Clinical data from electronic medical records, registries or trials provide a large source of information to apply machine learning methods in order to foster precision medicine, e.g. by finding new disease phenotypes or performing individual disease prediction. However, to take full advantage of deep learning methods on clinical data, architectures are necessary that 1) are robust with respect to missing and wrong values, and 2) can deal with highly variable-sized lists and long-term dependencies of individual diagnosis, procedures, measurements and medication prescriptions. In this work, we elaborate limitations of fully-connected neural networks and classical machine learning methods in this context and propose AdaptiveNet, a novel recurrent neural network architecture, which can deal with multiple lists of different events, alleviating the aforementioned limitations. We employ the architecture to the problem of disease progression prediction in rheumatoid arthritis using the Swiss Clinical Quality Management registry, which contains over 10.000 patients and more than 65.000 patient visits. Our proposed approach leads to more compact representations and outperforms the classical baselines.|http://arxiv.org/abs/2008.06294v1|Maria Hgle,Gabriel Kalweit,Thomas Huegle,Joschka Boedecker
725|A survey of open questions in adaptive therapy: bridging mathematics and clinical translation|Adaptive therapy is a dynamic cancer treatment protocol that updates (or "adapts") treatment decisions in anticipation of evolving tumor dynamics. This broad term encompasses many possible dynamic treatment protocols of patient-specific dose modulation or dose timing. Adaptive therapy maintains high levels of tumor burden to benefit from the competitive suppression of treatment-sensitive subpopulations on treatment-resistant subpopulations. This evolution-based approach to cancer treatment has been integrated into several ongoing or planned clinical trials, including treatment of metastatic castrate resistant prostate cancer, ovarian cancer, and BRAF-mutant melanoma. In the previous few decades, experimental and clinical investigation of adaptive therapy has progressed synergistically with mathematical and computational modeling. In this work, we discuss 11 open questions in cancer adaptive therapy mathematical modeling. The questions are split into three sections: 1) the necessary components of mathematical models of adaptive therapy 2) design and validation of dosing protocols, and 3) challenges and opportunities in clinical translation.|http://arxiv.org/abs/2210.12062v1|Jeffrey West,Fred Adler,Jill Gallaher,Maximilian Strobl,Renee Brady-Nicholls,Joel S. Brown,Mark Robertson-Tessi,Eunjung Kim,Robert Noble,Yannick Viossat,David Basanta,Alexander R. A. Anderson
726|Anonymising Clinical Data for Secondary Use|Secondary use of data already collected in clinical studies has become more and more popular in recent years, with the commitment of the pharmaceutical industry and many academic institutions in Europe and the US to provide access to their clinical trial data. Whilst this clearly provides societal benefit in helping to progress medical research, this has to be balanced against protection of subjects' privacy. There are two main scenarios for sharing subject data: within Clinical Study Reports and Individual Patient Level Data, and these scenarios have different associated risks and generally require different approaches. In any data sharing scenario, there is a trade-off between data utility and the risk of subject re-identification, and achieving this balance is key. Quantitative metrics can guide the amount of de-identification required and new technologies may also start to provide alternative ways to achieve the risk-utility balance.|http://arxiv.org/abs/2307.03682v1|Irene Ferreira,Chris Harbron,Alex Hughes,Tamsin Sargood,Christoph Gerlinger
727|TLDR at SemEval-2024 Task 2: T5-generated clinical-Language summaries for DeBERTa Report Analysis|This paper introduces novel methodologies for the Natural Language Inference for Clinical Trials (NLI4CT) task. We present TLDR (T5-generated clinical-Language summaries for DeBERTa Report Analysis) which incorporates T5-model generated premise summaries for improved entailment and contradiction analysis in clinical NLI tasks. This approach overcomes the challenges posed by small context windows and lengthy premises, leading to a substantial improvement in Macro F1 scores: a 0.184 increase over truncated premises. Our comprehensive experimental evaluation, including detailed error analysis and ablations, confirms the superiority of TLDR in achieving consistency and faithfulness in predictions against semantically altered inputs.|http://arxiv.org/abs/2404.09136v1|Spandan Das,Vinay Samuel,Shahriar Noroozizadeh
728|Causal machine learning for predicting treatment outcomes|Causal machine learning (ML) offers flexible, data-driven methods for predicting treatment outcomes including efficacy and toxicity, thereby supporting the assessment and safety of drugs. A key benefit of causal ML is that it allows for estimating individualized treatment effects, so that clinical decision-making can be personalized to individual patient profiles. Causal ML can be used in combination with both clinical trial data and real-world data, such as clinical registries and electronic health records, but caution is needed to avoid biased or incorrect predictions. In this Perspective, we discuss the benefits of causal ML (relative to traditional statistical or ML approaches) and outline the key components and steps. Finally, we provide recommendations for the reliable use of causal ML and effective translation into the clinic.|http://arxiv.org/abs/2410.08770v1|Stefan Feuerriegel,Dennis Frauen,Valentyn Melnychuk,Jonas Schweisthal,Konstantin Hess,Alicia Curth,Stefan Bauer,Niki Kilbertus,Isaac S. Kohane,Mihaela van der Schaar
729|The Temple University Hospital Seizure Detection Corpus|We introduce the TUH EEG Seizure Corpus (TUSZ), which is the largest open source corpus of its type, and represents an accurate characterization of clinical conditions. In this paper, we describe the techniques used to develop TUSZ, evaluate their effectiveness, and present some descriptive statistics on the resulting corpus.|http://arxiv.org/abs/1801.08085v1|Vinit Shah,Eva von Weltin,Silvia Lopez,James Riley McHugh,Lily Veloso,Meysam Golmohammadi,Iyad Obeid,Joseph Picone
730|Effective and Interpretable fMRI Analysis via Functional Brain Network Generation|Recent studies in neuroscience show great potential of functional brain networks constructed from fMRI data for popularity modeling and clinical predictions. However, existing functional brain networks are noisy and unaware of downstream prediction tasks, while also incompatible with recent powerful machine learning models of GNNs. In this work, we develop an end-to-end trainable pipeline to extract prominent fMRI features, generate brain networks, and make predictions with GNNs, all under the guidance of downstream prediction tasks. Preliminary experiments on the PNC fMRI data show the superior effectiveness and unique interpretability of our framework.|http://arxiv.org/abs/2107.11247v1|Xuan Kan,Hejie Cui,Ying Guo,Carl Yang
731|OLIVES Dataset: Ophthalmic Labels for Investigating Visual Eye Semantics|Clinical diagnosis of the eye is performed over multifarious data modalities including scalar clinical labels, vectorized biomarkers, two-dimensional fundus images, and three-dimensional Optical Coherence Tomography (OCT) scans. Clinical practitioners use all available data modalities for diagnosing and treating eye diseases like Diabetic Retinopathy (DR) or Diabetic Macular Edema (DME). Enabling usage of machine learning algorithms within the ophthalmic medical domain requires research into the relationships and interactions between all relevant data over a treatment period. Existing datasets are limited in that they neither provide data nor consider the explicit relationship modeling between the data modalities. In this paper, we introduce the Ophthalmic Labels for Investigating Visual Eye Semantics (OLIVES) dataset that addresses the above limitation. This is the first OCT and near-IR fundus dataset that includes clinical labels, biomarker labels, disease labels, and time-series patient treatment information from associated clinical trials. The dataset consists of 1268 near-IR fundus images each with at least 49 OCT scans, and 16 biomarkers, along with 4 clinical labels and a disease diagnosis of DR or DME. In total, there are 96 eyes' data averaged over a period of at least two years with each eye treated for an average of 66 weeks and 7 injections. We benchmark the utility of OLIVES dataset for ophthalmic data as well as provide benchmarks and concrete research directions for core and emerging machine learning paradigms within medical image analysis.|http://arxiv.org/abs/2209.11195v1|Mohit Prabhushankar,Kiran Kokilepersaud,Yash-yee Logan,Stephanie Trejo Corona,Ghassan AlRegib,Charles Wykoff
732|Ethical considerations of use of hold-out sets in clinical prediction model management|Clinical prediction models are statistical or machine learning models used to quantify the risk of a certain health outcome using patient data. These can then inform potential interventions on patients, causing an effect called performative prediction: predictions inform interventions which influence the outcome they were trying to predict, leading to a potential underestimation of risk in some patients if a model is updated on this data. One suggested resolution to this is the use of hold-out sets, in which a set of patients do not receive model derived risk scores, such that a model can be safely retrained. We present an overview of clinical and research ethics regarding potential implementation of hold-out sets for clinical prediction models in health settings. We focus on the ethical principles of beneficence, non-maleficence, autonomy and justice. We also discuss informed consent, clinical equipoise, and truth-telling. We present illustrative cases of potential hold-out set implementations and discuss statistical issues arising from different hold-out set sampling methods. We also discuss differences between hold-out sets and randomised control trials, in terms of ethics and statistical issues. Finally, we give practical recommendations for researchers interested in the use hold-out sets for clinical prediction models.|http://arxiv.org/abs/2406.03161v1|Louis Chislett,Louis JM Aslett,Alisha R Davies,Catalina A Vallejos,James Liley
733|The relationship between parameters and effects in transcranial ultrasonic stimulation|Transcranial ultrasonic stimulation (TUS) is rapidly gaining traction for non-invasive human neuromodulation, with a pressing need to establish protocols that maximise neuromodulatory efficacy. In this review, we aggregate and examine empirical evidence for the relationship between tunable TUS parameters and in vitro and in vivo outcomes. Based on this multiscale approach, TUS researchers can make better informed decisions about optimal parameter settings. Importantly, we also discuss the challenges involved in extrapolating results from prior empirical work to future interventions, including the translation of protocols between models and the complex interaction between TUS protocols and the brain. A synthesis of the empirical evidence suggests that larger effects will be observed at lower frequencies within the sub-MHz range, higher intensities and pressures than commonly administered thus far, and longer pulses and pulse train durations. Nevertheless, we emphasise the need for cautious interpretation of empirical data from different experimental paradigms when basing protocols on prior work as we advance towards refined TUS parameters for human neuromodulation.|http://arxiv.org/abs/2407.01232v2|Tulika Nandi,Benjamin R. Kop,Kim Butts Pauly,Charlotte J. Stagg,Lennart Verhagen
734|A Scoping Review of Publicly Available Language Tasks in Clinical Natural Language Processing|Objective: to provide a scoping review of papers on clinical natural language processing (NLP) tasks that use publicly available electronic health record data from a cohort of patients. Materials and Methods: We searched six databases, including biomedical research and computer science literature database. A round of title/abstract screening and full-text screening were conducted by two reviewers. Our method followed the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) guidelines. Results: A total of 35 papers with 47 clinical NLP tasks met inclusion criteria between 2007 and 2021. We categorized the tasks by the type of NLP problems, including name entity recognition, summarization, and other NLP tasks. Some tasks were introduced with a topic of clinical decision support applications, such as substance abuse, phenotyping, cohort selection for clinical trial. We summarized the tasks by publication and dataset information. Discussion: The breadth of clinical NLP tasks keeps growing as the field of NLP evolves with advancements in language systems. However, gaps exist in divergent interests between general domain NLP community and clinical informatics community, and in generalizability of the data sources. We also identified issues in data selection and preparation including the lack of time-sensitive data, and invalidity of problem size and evaluation. Conclusions: The existing clinical NLP tasks cover a wide range of topics and the field will continue to grow and attract more attention from both general domain NLP and clinical informatics community. We encourage future work to incorporate multi-disciplinary collaboration, reporting transparency, and standardization in data preparation.|http://arxiv.org/abs/2112.05780v1|Yanjun Gao,Dmitriy Dligach,Leslie Christensen,Samuel Tesch,Ryan Laffin,Dongfang Xu,Timothy Miller,Ozlem Uzuner,Matthew M Churpek,Majid Afshar
735|Exact properties of Efron's biased coin randomization procedure|Efron [Biometrika 58 (1971) 403--417] developed a restricted randomization procedure to promote balance between two treatment groups in a sequential clinical trial. He called this the biased coin design. He also introduced the concept of accidental bias, and investigated properties of the procedure with respect to both accidental and selection bias, balance, and randomization-based inference using the steady-state properties of the induced Markov chain. In this paper we revisit this procedure, and derive closed-form expressions for the exact properties of the measures derived asymptotically in Efron's paper. In particular, we derive the exact distribution of the treatment imbalance and the variance-covariance matrix of the treatment assignments. These results have application in the design and analysis of clinical trials, by providing exact formulas to determine the role of the coin's bias probability in the context of selection and accidental bias, balancing properties and randomization-based inference.|http://arxiv.org/abs/1010.0483v1|Tigran Markaryan,William F. Rosenberger
736|Randomized Urn Models revisited using Stochastic Approximation|This paper presents the link between stochastic approximation and clinical trials based on randomized urn models investigated in Bai and Hu (1999,2005) and Bai, Hu and Shen (2002). We reformulate the dynamics of both the urn composition and the assigned treatments as standard stochastic approximation (SA) algorithms with remainder. Then, we derive the a.s. convergence and the asymptotic normality (CLT) of the normalized procedure under less stringent assumptions by calling upon the ODE and SDE methods. As a second step, we investigate a more involved family of models, known as multi-arm clinical trials, where the urn updating depends on the past performances of the treatments. By increasing the dimension of the state vector, our SA approach provides this time a new asymptotic normality result.|http://arxiv.org/abs/1101.2786v6|Sophie Laruelle,Gilles Pags
737|Addressing missing data mechanism uncertainty using multiple-model multiple imputation: Application to a longitudinal clinical trial|We present a framework for generating multiple imputations for continuous data when the missing data mechanism is unknown. Imputations are generated from more than one imputation model in order to incorporate uncertainty regarding the missing data mechanism. Parameter estimates based on the different imputation models are combined using rules for nested multiple imputation. Through the use of simulation, we investigate the impact of missing data mechanism uncertainty on post-imputation inferences and show that incorporating this uncertainty can increase the coverage of parameter estimates. We apply our method to a longitudinal clinical trial of low-income women with depression where nonignorably missing data were a concern. We show that different assumptions regarding the missing data mechanism can have a substantial impact on inferences. Our method provides a simple approach for formalizing subjective notions regarding nonresponse so that they can be easily stated, communicated and compared.|http://arxiv.org/abs/1301.2490v1|Juned Siddique,Ofer Harel,Catherine M. Crespi
738|A Two-Stage, Phase II Clinical Trial Design with Nested Criteria for Early Stopping and Efficacy|We propose a two-stage design for a clinical trial with an early stopping rule for safety. We use different criteria to assess early stopping and efficacy. The early stopping rule is based on a criteria that can be determined more quickly than that of efficacy. These separate criteria are also nested in the sense that efficacy is a special case of, but not identical to, the early stopping criteria. The design readily allows for planning in terms of statistical significance, power, and expected sample size necessary to assess an early stopping rule. This method is illustrated with a Phase II design comparing patients treated for lung cancer with a novel drug combination to those treated using historical control. In this example, the early stopping rule is based on the numbers of patients who exhibit progression-free survival (PFS) at 2 months post treatment follow-up and efficacy is judged by the number of patients who have PFS at 6 months.|http://arxiv.org/abs/1307.6275v1|Daniel Zelterman
739|Sequential Advantage Selection for Optimal Treatment Regimes|Variable selection for optimal treatment regime in a clinical trial or an observational study is getting more attention. Most existing variable selection techniques focused on selecting variables that are important for prediction, therefore some variables that are poor in prediction but are critical for decision-making may be ignored. A qualitative interaction of a variable with treatment arises when treatment effect changes direction as the value of this variable varies. The qualitative interaction indicates the importance of this variable for decision-making. Gunter et al. (2011) proposed S-score which characterizes the magnitude of qualitative interaction of each variable with treatment individually. In this article, we developed a sequential advantage selection method based on the modified S-score. Our method selects qualitatively interacted variables sequentially, and hence excludes marginally important but jointly unimportant variables {or vice versa}. The optimal treatment regime based on variables selected via joint model is more comprehensive and reliable. With the proposed stopping criteria, our method can handle a large amount of covariates even if sample size is small. Simulation results show our method performs well in practical settings. We further applied our method to data from a clinical trial for depression.|http://arxiv.org/abs/1405.5239v1|Ailin Fan,Wenbin Lu,Rui Song
740|Central limit Theorem for an Adaptive Randomly Reinforced Urn Model|The generalized P\`olya urn (GPU) models and their variants have been investigated in several disciplines. However, typical assumptions made with respect to the GPU do not include urn models with diagonal replacement matrix, which arise in several applications, specifically in clinical trials. To facilitate mathematical analyses of models in these applications, we introduce an adaptive randomly reinforced urn model that uses accruing statistical information to adaptively skew the urn proportion toward specific targets. We study several probabilistic aspects that are important in implementing the urn model in practice. Specifically, we establish the law of large numbers and a central limit theorem for the number of sampled balls. To establish these results, we develop new techniques involving last exit times and crossing time analyses of the proportion of balls in the urn. To obtain precise estimates in these techniques, we establish results on the harmonic moments of the total number of balls in the urn. Finally, we describe our main results in the context an application to response-adaptive randomization in clinical trials. Our simulation experiments in this context demonstrate the ease and scope of our model.|http://arxiv.org/abs/1502.06130v1|Andrea Ghiglietti,Anand N. Vidyashankar,William F. Rosenberger
741|Dynamic Prediction for Multiple Repeated Measures and Event Time Data: An Application to Parkinson's Disease|In many clinical trials studying neurodegenerative diseases such as Parkinson's disease (PD), multiple longitudinal outcomes are collected to fully explore the multidimensional impairment caused by this disease. If the outcomes deteriorate rapidly, patients may reach a level of functional disability sufficient to initiate levodopa therapy for ameliorating disease symptoms. An accurate prediction of the time to functional disability is helpful for clinicians to monitor patients' disease progression and make informative medical decisions. In this article, we first propose a joint model that consists of a semiparametric multilevel latent trait model (MLLTM) for the multiple longitudinal outcomes, and a survival model for event time. The two submodels are linked together by an underlying latent variable. We develop a Bayesian approach for parameter estimation and a dynamic prediction framework for predicting target patients' future outcome trajectories and risk of a survival event, based on their multivariate longitudinal measurements. Our proposed model is evaluated by simulation studies and is applied to the DATATOP study, a motivating clinical trial assessing the effect of deprenyl among patients with early PD.|http://arxiv.org/abs/1603.06476v2|Jue Wang,Sheng Luo,Liang Li
742|Pairwise Sequential Randomization and Its Properties|In comparative studies, such as in causal inference and clinical trials, balancing important covariates is often one of the most important concerns for both efficient and credible comparison. However, chance imbalance still exists in many randomized experiments. This phenomenon of covariate imbalance becomes much more serious as the number of covariates $p$ increases. To address this issue, we introduce a new randomization procedure, called pairwise sequential randomization (PSR). The proposed method allocates the units sequentially and adaptively, using information on the current level of imbalance and the incoming unit's covariate. With a large number of covariates or a large number of units, the proposed method shows substantial advantages over the traditional methods in terms of the covariate balance, estimation accuracy, and computational time, making it an ideal technique in the era of big data. The proposed method attains the optimal covariate balance, in the sense that the estimated treatment effect under the proposed method attains its minimum variance asymptotically. Also the proposed method is widely applicable in both causal inference and clinical trials. Numerical studies and real data analysis provide further evidence of the advantages of the proposed method.|http://arxiv.org/abs/1611.02802v2|Yichen Qin,Yang Li,Wei Ma,Feifang Hu
743|An information-theoretic approach for selecting arms in clinical trials|The question of selecting the "best" amongst different choices is a common problem in statistics. In drug development, our motivating setting, the question becomes, for example: what is the dose that gives me a pre-specified risk of toxicity or which treatment gives the best response rate. Motivated by a recent development in the weighted information measures theory, we propose an experimental design based on a simple and intuitive criterion which governs arm selection in the experiment with multinomial outcomes. The criterion leads to accurate arm selection without any parametric or monotonicity assumption. The asymptotic properties of the design are studied for different allocation rules and the small sample size behaviour is evaluated in simulations in the context of Phase I and Phase II clinical trials with binary endpoints. We compare the proposed design to currently used alternatives and discuss its practical implementation.|http://arxiv.org/abs/1708.02426v2|Pavel Mozgunov,Thomas Jaki
744|Bayesian Nonparametric Inference for Panel Count Data with an Informative Observation Process|In this paper, the panel count data analysis for recurrent events is considered. Such analysis is useful for studying tumor or infection recurrences in both clinical trial and observational studies. A bivariate Gaussian Cox process model is proposed to jointly model the observation process and the recurrent event process. Bayesian nonparametric inference is proposed for simultaneously estimating regression parameters, bivariate frailty effects and baseline intensity functions. Inference is done through Markov chain Monte Carlo, with fully developed computational techniques. Predictive inference is also discussed under the Bayesian setting. The proposed method is shown to be efficient via simulation studies. A clinical trial dataset on skin cancer patients is analyzed to illustrate the proposed approach.|http://arxiv.org/abs/1709.05275v2|Ye Liang,Yang Li,Bin Zhang
745|Learning Disentangled Representations of Texts with Application to Biomedical Abstracts|We propose a method for learning disentangled representations of texts that code for distinct and complementary aspects, with the aim of affording efficient model transfer and interpretability. To induce disentangled embeddings, we propose an adversarial objective based on the (dis)similarity between triplets of documents with respect to specific aspects. Our motivating application is embedding biomedical abstracts describing clinical trials in a manner that disentangles the populations, interventions, and outcomes in a given trial. We show that our method learns representations that encode these clinically salient aspects, and that these can be effectively used to perform aspect-specific retrieval. We demonstrate that the approach generalizes beyond our motivating application in experiments on two multi-aspect review corpora.|http://arxiv.org/abs/1804.07212v2|Sarthak Jain,Edward Banner,Jan-Willem van de Meent,Iain J. Marshall,Byron C. Wallace
746|Subgroup Identification using Covariate Adjusted Interaction Trees|We consider the problem of identifying sub-groups of participants in a clinical trial that have enhanced treatment effect. Recursive partitioning methods that recursively partition the covariate space based on some measure of between groups treatment effect difference are popular for such sub-group identification. The most commonly used recursive partitioning method, the classification and regression tree algorithm, first creates a large tree by recursively partitioning the covariate space using some splitting criteria and then selects the final tree from all subtrees of the large tree. In the context of subgroup identification, calculation of the splitting criteria and the evaluation measure used for final tree selection rely on comparing differences in means between the treatment and control arm. When covariates are prognostic for the outcome, covariate adjusted estimators have the ability to improve efficiency compared to using differences in means between the treatment and control group. This manuscript develops two covariate adjusted estimators that can be used to both make splitting decisions and for final tree selection. The performance of the resulting covariate adjusted recursive partitioning algorithm is evaluated using simulations and by analyzing a clinical trial that evaluates if motivational interviews improve treatment engagement for substance abusers.|http://arxiv.org/abs/1806.08258v1|Jon Arni Steingrimsson,Jiabei Yang
747|Equivalence of regression curves sharing common parameters|In clinical trials the comparison of two different populations is a frequently addressed problem. Non-linear (parametric) regression models are commonly used to describe the relationship between covariates as the dose and a response variable in the two groups. In some situations it is reasonable to assume some model parameters to be the same, for instance the placebo effect or the maximum treatment effect. In this paper we develop a (parametric) bootstrap test to establish the similarity of two regression curves sharing some common parameters. We show by theoretical arguments and by means of a simulation study that the new test controls its level and achieves a reasonable power. Moreover, it is demonstrated that under the assumption of common parameters a considerable more powerful test can be constructed compared to the test which does not use this assumption. Finally, we illustrate potential applications of the new methodology by a clinical trial example.|http://arxiv.org/abs/1902.03456v1|Kathrin Mllenhoff,Frank Bretz,Holger Dette
748|Teasing out the overall survival benefit with adjustment for treatment switching to other therapies|In oncology clinical trials, characterizing the long-term overall survival (OS) benefit for an experimental drug or treatment regimen (experimental group) is often unobservable if some patients in the control group switch to drugs in the experimental group and/or other cancer treatments after disease progression. A key question often raised by payers and reimbursement agencies is how to estimate the true benefit of the experimental drug group on overall survival that would have been estimated if there were no treatment switches. Several commonly used statistical methods are available to estimate overall survival benefit while adjusting for treatment switching, ranging from naive exclusion or censoring approaches to more advanced methods including inverse probability of censoring weighting (IPCW), iterative parameter estimation (IPE) algorithm or rank-preserving structural failure time models (RPSFTM). However, many clinical trials now have patients switching to different treatment regimens other than the test drugs, and the existing methods cannot handle more complicated scenarios. To address this challenge, we propose two additional methods: stratified RPSFTM and random-forest-based prediction. A simulation study is conducted to assess the properties of the existing methods along with the two newly proposed approaches.|http://arxiv.org/abs/1908.00654v1|Yuqing Xu,Meijing Wu,Weili He,Qiming Liao,Yabing Mai
749|Efficient and flexible simulation-based sample size determination for clinical trials with multiple design parameters|Simulation offers a simple and flexible way to estimate the power of a clinical trial when analytic formulae are not available. The computational burden of using simulation has, however, restricted its application to only the simplest of sample size determination problems, minimising a single parameter (the overall sample size) subject to power being above a target level. We describe a general framework for solving simulation-based sample size determination problems with several design parameters over which to optimise and several conflicting criteria to be minimised. The method is based on an established global optimisation algorithm widely used in the design and analysis of computer experiments, using a non-parametric regression model as an approximation of the true underlying power function. The method is flexible, can be used for almost any problem for which power can be estimated using simulation, and can be implemented using existing statistical software packages. We illustrate its application to three increasingly complicated sample size determination problems involving complex clustering structures, co-primary endpoints, and small sample considerations.|http://arxiv.org/abs/1908.07769v1|Duncan T. Wilson,Rebecca E. A. Walwyn,Richard Hooper,Julia Brown,Amanda J. Farrin
750|Towards Understanding of Medical Randomized Controlled Trials by Conclusion Generation|Randomized controlled trials (RCTs) represent the paramount evidence of clinical medicine. Using machines to interpret the massive amount of RCTs has the potential of aiding clinical decision-making. We propose a RCT conclusion generation task from the PubMed 200k RCT sentence classification dataset to examine the effectiveness of sequence-to-sequence models on understanding RCTs. We first build a pointer-generator baseline model for conclusion generation. Then we fine-tune the state-of-the-art GPT-2 language model, which is pre-trained with general domain data, for this new medical domain task. Both automatic and human evaluation show that our GPT-2 fine-tuned models achieve improved quality and correctness in the generated conclusions compared to the baseline pointer-generator model. Further inspection points out the limitations of this current approach and future directions to explore.|http://arxiv.org/abs/1910.01462v1|Alexander Te-Wei Shieh,Yung-Sung Chuang,Shang-Yu Su,Yun-Nung Chen
751|An evaluation of machine learning techniques to predict the outcome of children treated for Hodgkin-Lymphoma on the AHOD0031 trial: A report from the Children's Oncology Group|In this manuscript we analyze a data set containing information on children with Hodgkin Lymphoma (HL) enrolled on a clinical trial. Treatments received and survival status were collected together with other covariates such as demographics and clinical measurements. Our main task is to explore the potential of machine learning (ML) algorithms in a survival analysis context in order to improve over the Cox Proportional Hazard (CoxPH) model. We discuss the weaknesses of the CoxPH model we would like to improve upon and then we introduce multiple algorithms, from well-established ones to state-of-the-art models, that solve these issues. We then compare every model according to the concordance index and the brier score. Finally, we produce a series of recommendations, based on our experience, for practitioners that would like to benefit from the recent advances in artificial intelligence.|http://arxiv.org/abs/2001.05534v2|Cdric Beaulac,Jeffrey S. Rosenthal,Qinglin Pei,Debra Friedman,Suzanne Wolden,David Hodgson
752|On the Theory of Covariate-Adaptive Designs|Pocock and Simon's marginal procedure (Pocock and Simon, 1975) is often implemented forbalancing treatment allocation over influential covariates in clinical trials. However, the theoretical properties of Pocock and Simion's procedure have remained largely elusive for decades. In this paper, we propose a general framework for covariate-adaptive designs and establish the corresponding theory under widely satisfied conditions. As a special case, we obtain the theoretical properties of Pocock and Simon's marginal procedure: the marginal imbalances and overall imbalance are bounded in probability, but the within-stratum imbalances increase with the rate of $\sqrt{n}$ as the sample size increases. The theoretical results provide new insights about balance properties of covariate-adaptive randomization procedures and open a door to study the theoretical properties of statistical inference for clinical trials based on covariate-adaptive randomization procedures.|http://arxiv.org/abs/2004.02994v1|Feifang Hu,Li-Xin Zhang
753|Power Calculations for Replication Studies|The reproducibility crisis has led to an increasing number of replication studies being conducted. Sample sizes for replication studies are often calculated using conditional power based on the effect estimate from the original study. However, this approach is not well suited as it ignores the uncertainty of the original result. Bayesian methods are used in clinical trials to incorporate prior information into power calculations. We propose to adapt this methodology to the replication framework and promote the use of predictive instead of conditional power in the design of replication studies. Moreover, we describe how extensions of the methodology to sequential clinical trials can be tailored to replication studies. Conditional and predictive power calculated at an interim analysis are compared and we argue that predictive power is a useful tool to decide whether to stop a replication study prematurely. A recent project on the replicability of social sciences is used to illustrate the properties of the different methods.|http://arxiv.org/abs/2004.10814v4|Charlotte Micheloud,Leonhard Held
754|Functional additive models for optimizing individualized treatment rules|A novel functional additive model is proposed which is uniquely modified and constrained to model nonlinear interactions between a treatment indicator and a potentially large number of functional and/or scalar pretreatment covariates. The primary motivation for this approach is to optimize individualized treatment rules based on data from a randomized clinical trial. We generalize functional additive regression models by incorporating treatment-specific components into additive effect components. A structural constraint is imposed on the treatment-specific components in order to provide a class of additive models with main effects and interaction effects that are orthogonal to each other. If primary interest is in the interaction between treatment and the covariates, as is generally the case when optimizing individualized treatment rules, we can thereby circumvent the need to estimate the main effects of the covariates, obviating the need to specify their form and thus avoiding the issue of model misspecification. The methods are illustrated with data from a depression clinical trial with electroencephalogram functional data as patients' pretreatment covariates.|http://arxiv.org/abs/2006.00266v2|Hyung Park,Eva Petkova,Thaddeus Tarpey,R. Todd Ogden
755|From conception to clinical trial: IViST -- the first multi-sensor-based platform for real-time In Vivo dosimetry and Source Tracking in HDR brachytherapy|This study aims to introduce IViST (In Vivo Source Tracking), a novel multi-sensors dosimetry platform for real-time treatment monitoring in HDR brachytherapy. IViST is a platform that comprises 3 parts: 1) an optimized and characterized multi-point plastic scintillator dosimeter (3 points mPSD; using BCF-60, BCF-12, and BCF-10 scintillators), 2) a compact assembly of photomultiplier tubes (PMTs) coupled to dichroic mirrors and filters for high-sensitivity scintillation light collection, and 3) a Python-based graphical user interface used for system management and signal processing. IViST can simultaneously measure dose, triangulate source position, and measure dwell time. By making 100 000 measurements/s, IViST samples enough data to quickly perform key QA/QC tasks such as identifying wrong individual dwell time or interchanged transfer tubes. By using 3 co-linear sensors and planned information for an implant geometry (from DICOM RT), the platform can also triangulate source position in real-time. A clinical trial is presently on-going using the IViST system.|http://arxiv.org/abs/2006.08031v1|Haydee M. Linares Rosales,Audrey Cantin,Sylviane Aubin,Sam Beddar,Luc Beaulieu
756|SMIM: a unified framework of Survival sensitivity analysis using Multiple Imputation and Martingale|Censored survival data are common in clinical trial studies. We propose a unified framework for sensitivity analysis to censoring at random in survival data using multiple imputation and martingale, called SMIM. The proposed framework adopts the \delta-adjusted and control-based models, indexed by the sensitivity parameter, entailing censoring at random and a wide collection of censoring not at random assumptions. Also, it targets for a broad class of treatment effect estimands defined as functionals of treatment-specific survival functions, taking into account of missing data due to censoring. Multiple imputation facilitates the use of simple full-sample estimation; however, the standard Rubin's combining rule may overestimate the variance for inference in the sensitivity analysis framework. We decompose the multiple imputation estimator into a martingale series based on the sequential construction of the estimator and propose the wild bootstrap inference by resampling the martingale series. The new bootstrap inference has a theoretical guarantee for consistency and is computationally efficient compared to the non-parametric bootstrap counterpart. We evaluate the finite-sample performance of the proposed SMIM through simulation and an application on a HIV clinical trial.|http://arxiv.org/abs/2007.02339v2|Shu Yang,Yilong Zhang,Guanghan Frank Liu,Qian Guan
757|Adaptive dose-response studies to establish proof-of-concept in learning-phase clinical trials|In learning-phase clinical trials in drug development, adaptive designs can be efficient and highly informative when used appropriately. In this article, we extend the multiple comparison procedures with modeling techniques (MCP-Mod) procedure with generalized multiple contrast tests (GMCTs) to two-stage adaptive designs for establishing proof-of-concept. The results of an interim analysis of first-stage data are used to adapt the candidate dose-response models and the dosages studied in the second stage. GMCTs are used in both stages to obtain stage-wise p-values, which are then combined to determine an overall p-value. An alternative approach is also considered that combines the t-statistics across stages, employing the conditional rejection probability (CRP) principle to preserve the Type I error probability. Simulation studies demonstrate that the adaptive designs are advantageous compared to the corresponding tests in a non-adaptive design if the selection of the candidate set of dose-response models is not well informed by evidence from preclinical and early-phase studies.|http://arxiv.org/abs/2102.10434v2|Shiyang Ma,Michael P. McDermott
758|Collaborative causal inference with a distributed data-sharing management|Data sharing barriers are paramount challenges arising from multicenter clinical trials where multiple data sources are stored in a distributed fashion at different local study sites. Merging such data sources into a common data storage for a centralized statistical analysis requires a data use agreement, which is often time-consuming. Data merging may become more burdensome when causal inference is of primary interest because propensity score modeling involves combining many confounding variables, and systematic incorporation of this additional modeling in meta-analysis has not been thoroughly investigated in the literature. We propose a new causal inference framework that avoids the merging of subject-level raw data from multiple sites but needs only the sharing of summary statistics. The proposed collaborative inference enjoys maximal protection of data privacy and minimal sensitivity to unbalanced data distributions across data sources. We show theoretically and numerically that the new distributed causal inference approach has little loss of statistical power compared to the centralized method that requires merging the entire data. We present large-sample properties and algorithms for the proposed method. We illustrate its performance by simulation experiments and a real-world data example on a multicenter clinical trial of basal insulin treatment for reducing the risk of post-transplantation diabetes among kidney-transplant patients.|http://arxiv.org/abs/2204.00857v1|Mengtong Hu,Xu Shi,Peter X. -K. Song
759|Estimating optimal individualized treatment rules with multistate processes|Multistate process data are common in studies of chronic diseases such as cancer. These data are ideal for precision medicine purposes as they can be leveraged to improve more refined health outcomes, compared to standard survival outcomes, as well as incorporate patient preferences regarding quantity versus quality of life. However, there are currently no methods for the estimation of optimal individualized treatment rules with such data. In this article, we propose a nonparametric outcome weighted learning approach for this problem in randomized clinical trial settings. The theoretical properties of the proposed methods, including Fisher consistency and asymptotic normality of the estimated expected outcome under the estimated optimal individualized treatment rule, are rigorously established. A consistent closed-form variance estimator is provided and methodology for the calculation of simultaneous confidence intervals is proposed. Simulation studies show that the proposed methodology and inference procedures work well even with small sample sizes and high rates of right censoring. The methodology is illustrated using data from a randomized clinical trial on the treatment of metastatic squamous-cell carcinoma of the head and neck.|http://arxiv.org/abs/2204.09785v2|Giorgos Bakoyannis
760|Benefits of Monotonicity in Safe Exploration with Gaussian Processes|We consider the problem of sequentially maximising an unknown function over a set of actions while ensuring that every sampled point has a function value below a given safety threshold. We model the function using kernel-based and Gaussian process methods, while differing from previous works in our assumption that the function is monotonically increasing with respect to a \emph{safety variable}. This assumption is motivated by various practical applications such as adaptive clinical trial design and robotics. Taking inspiration from the \textsc{\sffamily GP-UCB} and \textsc{\sffamily SafeOpt} algorithms, we propose an algorithm, monotone safe {\sffamily UCB} (\textsc{\sffamily M-SafeUCB}) for this task. We show that \textsc{\sffamily M-SafeUCB} enjoys theoretical guarantees in terms of safety, a suitably-defined regret notion, and approximately finding the entire safe boundary. In addition, we illustrate that the monotonicity assumption yields significant benefits in terms of the guarantees obtained, as well as algorithmic simplicity and efficiency. We support our theoretical findings by performing empirical evaluations on a variety of functions, including a simulated clinical trial experiment.|http://arxiv.org/abs/2211.01561v2|Arpan Losalka,Jonathan Scarlett
761|Joint modelling of longitudinal and time-to-event data applied to group sequential clinical trials|Often in Phase 3 clinical trials measuring a long-term time-to-event endpoint, such as overall survival or progression-free survival, investigators also collect repeated measures on biomarkers which may be predictive of the primary endpoint. Although these data may not be leveraged directly to support early stopping decisions, can we make greater use of these data to increase efficiency and improve interim decision making? We present a joint model for longitudinal and time-to-event data and a method which establishes the distribution of successive estimates of parameters in the joint model across interim analyses. With this in place, we can use the estimates to define both efficacy and futility stopping rules. Using simulation, we evaluate the benefits of incorporating biomarker information and the affects on interim decision making.|http://arxiv.org/abs/2211.16138v1|Abigail J. Burdon,Lisa V. Hampson,Christopher Jennison
762|Predicting subgroup treatment effects for a new study: Motivations, results and learnings from running a data challenge in a pharmaceutical corporation|We present the motivation, experience and learnings from a data challenge conducted at a large pharmaceutical corporation on the topic of subgroup identification. The data challenge aimed at exploring approaches to subgroup identification for future clinical trials. To mimic a realistic setting, participants had access to 4 Phase III clinical trials to derive a subgroup and predict its treatment effect on a future study not accessible to challenge participants. 30 teams registered for the challenge with around 100 participants, primarily from Biostatistics organisation. We outline the motivation for running the challenge, the challenge rules and logistics. Finally, we present the results of the challenge, the participant feedback as well as the learnings, and how these learnings can be translated into statistical practice.|http://arxiv.org/abs/2304.05658v1|Bjrn Bornkamp,Silvia Zaoli,Michela Azzarito,Ruvie Martin,Carsten Philipp Mller,Conor Moloney,Giulia Capestro,David Ohlssen,Mark Baillie
763|Clinical trials with rescue medication applied according to a deterministic rule|Clinical trials in specific indications require the administration of rescue medication in case a patient does not sufficiently respond to investigational treatment. The application of additional treatment on an as needed basis causes problems to the analysis and interpretation of the results of these studies since the effect of the investigational treatment can be confounded by the additional medication. Following-up all patients until study end and capturing all data is not fully addressing the issue. We present an analysis that takes care of the fact that rescue is a study outcome and not a covariate when rescue medication is administered according to a deterministic rule. This approach allows to clearly define a biological effect. For normally distributed longitudinal data a practically unbiased estimator of the biological effect can be obtained. The results are compared to an ITT analysis and an analysis on all patients not receiving rescue.|http://arxiv.org/abs/1608.08096v1|Gerd K. Rosenkranz
764|A Benchmark for Dose Finding Studies with Continuous Outcomes|An important tool to evaluate the performance of any design is an optimal benchmark proposed by O'Quigley and others (2002, Biostatistics 3(1), 51-56) that provides an upper bound on the performance of a design under a given scenario. The original benchmark can be applied to dose finding studies with a binary endpoint only. However, there is a growing interest in dose finding studies involving continuous outcomes, but no benchmark for such studies has been developed. We show that the original benchmark and its extension by Cheung (2014, Biometrics 70(2), 389-397), when looked at from a different perspective, can be generalised to various settings with several discrete and continuous outcomes. We illustrate and compare the benchmark performance in the setting of a Phase I clinical trial with continuous toxicity endpoint and in the setting of a Phase I/II clinical trial with continuous efficacy outcome. We show that the proposed benchmark provides an accurate upper bound for model-based dose finding methods and serves as a powerful tool for evaluating designs.|http://arxiv.org/abs/1712.08823v2|Pavel Mozgunov,Thomas Jaki,Xavier Paoletti
765|#neverforget - Photobiomodulation Against Alzheimer's Disease: A Systematic Review|Alzheimer's disease affects an ever-increasing number of people in the aging population. Current treatment options are limited to a narrow time frame at the mild to moderate stage of dementia, and patients are confronted with the inevitable progression of their disease. Most investigational drugs fail to prove their efficacy in clinical trials, and there are but a few preventative measures that one can take. A novel treatment approach, using photobiomodulation to increase the brain's mitochondrial function and prevent neuronal apoptosis, has shown promising results in in vitro and in vivo experiments. This systematic review aims at providing a comprehensive summary on the available research on photobiomodulation against Alzheimer's disease to support the translation of this modality from bench to bedside. It shows that the mechanistic action has been largely understood on a cellular level, safe and effective doses have been found in animal models, and first human case studies provide reason to enter large scale clinical trials. A brief outlook on study design concludes this review and provides a basis for further work on the topic.|http://arxiv.org/abs/1906.02501v1|Joachim Enengl,Peter Dungel
766|Application of Bayesian Dynamic Linear Models to Random Allocation Clinical Trials|Random allocation models used in clinical trials aid researchers in determining which of a particular treatment provides the best results by reducing bias between groups. Often however, this determination leaves researchers battling ethical issues of providing patients with unfavorable treatments. Many methods such as Play the Winner and Randomized Play the Winner Rule have historically been utilized to determine patient allocation, however, these methods are prone to the increased assignment of unfavorable treatments. Recently a new Bayesian Method using Decreasingly Informative Priors has been proposed by \citep{sabo2014adaptive}, and later \citep{donahue2020allocation}. Yet this method can be time consuming if MCMC methods are required. We propose the use of a new method which uses Dynamic Linear Model (DLM) \citep{harrison1999bayesian} to increase allocation speed while also decreasing patient allocation samples necessary to identify the more favorable treatment. Furthermore, a sensitivity analysis is conducted on multiple parameters. Finally, a Bayes Factor is calculated to determine the proportion of unused patient budget remaining at a specified cut off and this will be used to determine decisive evidence in favor of the better treatment.|http://arxiv.org/abs/2008.00339v1|Albert. H. Lee III,Edward L Boone,Roy T. Sabo,Erin Donahue
767|HOPES -- An Integrative Digital Phenotyping Platform for Data Collection, Monitoring and Machine Learning|We describe the development of, and early experiences with, comprehensive Digital Phenotyping platform: Health Outcomes through Positive Engagement and Self-Empowerment (HOPES). HOPES is based on the open-source Beiwe platform but adds a much wider range of data collection, including the integration of wearable data sources and further sensor collection from the smartphone. Requirements were in part derived from a concurrent clinical trial for schizophrenia. This trial required development of significant capabilities in HOPES in security, privacy, ease-of-use and scalability, based on a careful combination of public cloud and on-premises operation. We describe new data pipelines to clean, process, present and analyze data. This includes a set of dashboards customized to the needs of the research study operations and for clinical care. A test use of HOPES is described by analyzing the digital behaviors of 20 participants during the SARS-CoV-2 pandemic.|http://arxiv.org/abs/2008.12431v1|Xuancong Wang,Nikola Vouk,Creighton Heaukulani,Thisum Buddhika,Wijaya Martanto,Jimmy Lee,Robert JT Morris
768|Testing the Equality of Proportions for Combined Unilateral and Bilateral Data|Measurements are generally collected as unilateral or bilateral data in clinical trials or observational studies. For example, in ophthalmologic studies, statistical tests are often based on one or two eyes of an individual. For bilateral data, recent literatures have shown some testing procedures that take into account the intra-class correlation between two eyes of the same person. Ma et al. (2015) investigated three testing procedures under Rosner's model. In this paper, we extend Ma's work for bilateral data to combined bilateral and unilateral data. The proposed procedures are based on the likelihood estimate algorithm derived from the root of 4th order polynomial equations and fisher scoring iterations. Simulation studies are performed to compare the testing procedures under different parameter configurations. The result shows that score test has satisfactory type I error rates and powers. Therefore, we recommend score test for testing the equality of proportions. We illustrate the application of the proposed methods with a double-blind randomized clinical trial.|http://arxiv.org/abs/2010.03501v1|Chang-Xing Ma,Kejia Wang
769|CauchyCP: a powerful test under non-proportional hazards using Cauchy combination of change-point Cox regressions|Non-proportional hazards data are routinely encountered in randomized clinical trials. In such cases, classic Cox proportional hazards model can suffer from severe power loss, with difficulty in interpretation of the estimated hazard ratio since the treatment effect varies over time. We propose CauchyCP, an omnibus test of change-point Cox regression models, to overcome both challenges while detecting signals of non-proportional hazards patterns. Extensive simulation studies demonstrate that, compared to existing treatment comparison tests under non-proportional hazards, the proposed CauchyCP test 1) controls the type I error better at small $\alpha$ levels ($< 0.01$); 2) increases the power of detecting time-varying effects; and 3) is more computationally efficient. The superior performance of CauchyCP is further illustrated using retrospective analyses of two randomized clinical trial datasets and a pharmacogenetic biomarker study dataset. The R package $\textit{CauchyCP}$ is publicly available on CRAN.|http://arxiv.org/abs/2101.00059v1|Hong Zhang,Qing Li,Devan V. Mehrotra,Judong Shen
770|Low incidence rate of COVID-19 undermines confidence in estimation of the vaccine efficacy|Knowing the true effect size of clinical interventions in randomised clinical trials is key to informing the public health policies. Vaccine efficacy is defined in terms of the relative risk or the ratio of two disease risks. However, only approximate methods are available for estimating the variance of the relative risk. In this article, we show using a probabilistic model that uncertainty in the efficacy rate could be underestimated when the disease risk is low. Factoring in the baseline rate of the disease, we estimate broader confidence intervals for the efficacy rates of the vaccines recently developed for COVID-19. We propose new confidence intervals for the relative risk. We further show that sample sizes required for phase 3 efficacy trials are routinely underestimated and propose a new method for sample size calculation where the efficacy is of interest. We also discuss the deleterious effects of classification bias which is particularly relevant at low disease prevalence.|http://arxiv.org/abs/2101.10005v2|Yasin Memari
771|Randomization-based joint central limit theorem and efficient covariate adjustment in stratified $2^K$ factorial experiments|Randomized block factorial experiments are widely used in industrial engineering, clinical trials, and social science. Researchers often use a linear model and analysis of covariance to analyze experimental results; however, limited studies have addressed the validity and robustness of the resulting inferences because assumptions for a linear model might not be justified by randomization in randomized block factorial experiments. In this paper, we establish a new finite population joint central limit theorem for usual (unadjusted) factorial effect estimators in randomized block $2^K$ factorial experiments. Our theorem is obtained under a randomization-based inference framework, making use of an extension of the vector form of the Wald--Wolfowitz--Hoeffding theorem for a linear rank statistic. It is robust to model misspecification, numbers of blocks, block sizes, and propensity scores across blocks. To improve the estimation and inference efficiency, we propose four covariate adjustment methods. We show that under mild conditions, the resulting covariate-adjusted factorial effect estimators are consistent, jointly asymptotically normal, and generally more efficient than the unadjusted estimator. In addition, we propose Neyman-type conservative estimators for the asymptotic covariances to facilitate valid inferences. Simulation studies and a clinical trial data analysis demonstrate the benefits of the covariate adjustment methods.|http://arxiv.org/abs/2103.04050v2|Hanzhong Liu,Jiyang Ren,Yuehan Yang
772|A unified framework for weighted parametric group sequential design (WPGSD)|Group sequential design (GSD) is widely used in clinical trials in which correlated tests of multiple hypotheses are used. Multiple primary objectives resulting in tests with known correlations include evaluating 1) multiple experimental treatment arms, 2) multiple populations, 3) the combination of multiple arms and multiple populations, or 4) any asymptotically multivariate normal tests. In this paper, we focus on the first 3 of these and extend the framework of the weighted parametric multiple test procedure from fixed designs with a single analysis per objective to a GSD setting where different objectives may be assessed at the same or different times, each in a group sequential fashion. Pragmatic methods for design and analysis of weighted parametric group sequential design(WPGSD) under closed testing procedures are proposed to maintain the strong control of familywise Type I error rate (FWER) when correlations between tests are incorporated. This results in the ability to relax testing bounds compared to designs not fully adjusting for known correlations, increasing power or allowing decreased sample size. We illustrate the proposed methods using clinical trial examples and conduct a simulation study to evaluate the operating characteristics.|http://arxiv.org/abs/2103.10537v1|Keaven M. Anderson,Zifang Guo,Jing Zhao,Linda Z. Sun
773|Classification of hierarchical text using geometric deep learning: the case of clinical trials corpus|We consider the hierarchical representation of documents as graphs and use geometric deep learning to classify them into different categories. While graph neural networks can efficiently handle the variable structure of hierarchical documents using the permutation invariant message passing operations, we show that we can gain extra performance improvements using our proposed selective graph pooling operation that arises from the fact that some parts of the hierarchy are invariable across different documents. We applied our model to classify clinical trial (CT) protocols into completed and terminated categories. We use bag-of-words based, as well as pre-trained transformer-based embeddings to featurize the graph nodes, achieving f1-scores around 0.85 on a publicly available large scale CT registry of around 360K protocols. We further demonstrate how the selective pooling can add insights into the CT termination status prediction. We make the source code and dataset splits accessible.|http://arxiv.org/abs/2110.15710v1|Sohrab Ferdowsi,Nikolay Borissov,Julien Knafou,Poorya Amini,Douglas Teodoro
774|Return-to-baseline multiple imputation for missing values in clinical trials|Return-to-baseline is an important method to impute missing values or unobserved potential outcomes when certain hypothetical strategies are used to handle intercurrent events in clinical trials. Current return-to-baseline approaches seen in literature and in practice inflate the variability of the "complete" dataset after imputation and lead to biased mean estimators {when the probability of missingness depends on the observed baseline and/or postbaseline intermediate outcomes}. In this article, we first provide a set of criteria a return-to-baseline imputation method should satisfy. Under this framework, we propose a novel return-to-baseline imputation method. Simulations show the completed data after the new imputation approach have the proper distribution, and the estimators based on the new imputation method outperform the traditional method in terms of both bias and variance, when missingness depends on the observed values. The new method can be implemented easily with the existing multiple imputation procedures in commonly used statistical packages.|http://arxiv.org/abs/2111.09423v1|Yongming Qu,Biyue Dai
775|Temporal Subtyping of Alzheimer's Disease Using Medical Conditions Preceding Alzheimer's Disease Onset in Electronic Health Records|Subtyping of Alzheimer's disease (AD) can facilitate diagnosis, treatment, prognosis and disease management. It can also support the testing of new prevention and treatment strategies through clinical trials. In this study, we employed spectral clustering to cluster 29,922 AD patients in the OneFlorida Data Trust using their longitudinal EHR data of diagnosis and conditions into four subtypes. These subtypes exhibit different patterns of progression of other conditions prior to the first AD diagnosis. In addition, according to the results of various statistical tests, these subtypes are also significantly different with respect to demographics, mortality, and prescription medications after the AD diagnosis. This study could potentially facilitate early detection and personalized treatment of AD as well as data-driven generalizability assessment of clinical trials for AD.|http://arxiv.org/abs/2202.10991v1|Zhe He,Shubo Tian,Arslan Erdengasileng,Neil Charness,Jiang Bian
776|A systematic analysis of biotech startups that went public in the first half of 2021|Biotechnologies are being commercialized at historic rates. In 2020, 74 biotech startups went public through an Initial Public Offering (IPO), and 60 went through the IPO process in the first six months of 2021. However, the traits associated with biotech startups obtaining recent IPOs have not been reported. Here we build a database of biotechs that underwent an IPO in the first half of 2021. By analyzing leadership, technological focus, clinical trials, and financing, we found that advanced degrees among the leadership, clinical trials, and intellectual property are important factors for biotech startups. The data also suggest that large private rounds can decrease time-to-IPO and affect post-IPO stock performance. Notably, these traits were often exhibited by the 138 biotech IPOs in 2018-2019, suggesting 2021 data were not driven by COVID.|http://arxiv.org/abs/2205.00993v1|Sebastian G. Huayamares,Melissa P. Lokugamage,Alejandro J. Da Silva Sanchez,James E. Dahlman
777|Restricted mean survival time estimate using covariate adjusted pseudovalue regression to improve precision|Covariate adjustment is desired by both practitioners and regulators of randomized clinical trials because it improves precision for estimating treatment effects. However, covariate adjustment presents a particular challenge in time-to-event analysis. We propose to apply covariate adjusted pseudovalue regression to estimate between-treatment difference in restricted mean survival times (RMST). Our proposed method incorporates a prognostic covariate to increase precision of treatment effect estimate, maintaining strict type I error control without introducing bias. In addition, the amount of increase in precision can be quantified and taken into account in sample size calculation at the study design stage. Consequently, our proposed method provides the ability to design smaller randomized studies at no expense to statistical power.|http://arxiv.org/abs/2208.04495v3|Yunfan Li,Jessica L. Ross,Aaron M. Smith,David P. Miller
778|Bayesian Arc Length Survival Analysis Model (BALSAM): Theory and Application to an HIV/AIDS Clinical Trial|Stochastic volatility often implies increasing risks that are difficult to capture given the dynamic nature of real-world applications. We propose using arc length, a mathematical concept, to quantify cumulative variations (the total variability over time) to more fully characterize stochastic volatility. The hazard rate, as defined by the Cox proportional hazards model in survival analysis, is assumed to be impacted by the instantaneous value of a longitudinal variable. However, when cumulative variations pose a significant impact on the hazard, this assumption is questionable. Our proposed Bayesian Arc Length Survival Analysis Model (BALSAM) infuses arc length into a united statistical framework by synthesizing three parallel components (joint models, distributed lag models, and arc length). We illustrate the use of BALSAM in simulation studies and also apply it to an HIV/AIDS clinical trial to assess the impact of cumulative variations of CD4 count (a critical longitudinal biomarker) on mortality while accounting for measurement errors and relevant variables.|http://arxiv.org/abs/2212.06353v1|Yan Gao,Rodney A. Sparapani,Sanjib Basu
779|Pragmatic Estimation of Sample Size for Number of Interviews for PRO development in the 2009 FDA PRO guidance|PROs developed de novo, using the FDA guidance may involve structured patient interviews or focus groups. Qualitative Research is a methodology for eliciting and coding interviews and produces concepts or themes. These concepts are used to develop items in a PRO for use as an endpoint in Clinical trials. A convention in the field is that interviews and code/concept elicitation are considered complete when subsequent interviews produces "no new concepts" -termed "saturation". FDA reviewers frequently challenge PRO developers whether there are sufficient patient interviews to confirm that saturation is achieved after occurrence of zero new concepts. Several authors have reported that concrete criteria are need for confirming that saturation is achieved (Francis 2010, Mason 2010, Marshall 2013). I provide statistical methodology for confirming saturation, suitable for review by a regulatory authority. Type I error for saturation, may occur if further interviews elicited more concepts after first occurrence of saturation. I use published data set on code elicitation (Guest, 2006) to demonstrate that saturation may occur more than once in a sequence of interviews. I provide a statistical definition for saturation in qualitative research, that addresses regulatory concerns for PRO's developed for use as a clinical trial endpoint in a regulatory submission.|http://arxiv.org/abs/2301.04760v1|Chris Barker
780|Estimating the false discovery risk of (randomized) clinical trials in medical journals based on published p-values|The influential claim that most published results are false raised concerns about the trustworthiness and integrity of science. Since then, there have been numerous attempts to examine the rate of false-positive results that have failed to settle this question empirically. Here we propose a new way to estimate the false positive risk and apply the method to the results of (randomized) clinical trials in top medical journals. Contrary to claims that most published results are false, we find that the traditional significance criterion of $\alpha = .05$ produces a false positive risk of 13%. Adjusting $\alpha$ to .01 lowers the false positive risk to less than 5%. However, our method does provide clear evidence of publication bias that leads to inflated effect size estimates. These results provide a solid empirical foundation for evaluations of the trustworthiness of medical research.|http://arxiv.org/abs/2302.00774v1|Ulrich Schimmack,Frantiek Barto
781|Augmented Learning of Heterogeneous Treatment Effects via Gradient Boosting Trees|Heterogeneous treatment effects (HTE) based on patients' genetic or clinical factors are of significant interest to precision medicine. Simultaneously modeling HTE and corresponding main effects for randomized clinical trials with high-dimensional predictive markers is challenging. Motivated by the modified covariates approach, we propose a two-stage statistical learning procedure for estimating HTE with optimal efficiency augmentation, generalizing to arbitrary interaction model and exploiting powerful extreme gradient boosting trees (XGBoost). Target estimands for HTE are defined in the scale of mean difference for quantitative outcomes, or risk ratio for binary outcomes, which are the minimizers of specialized loss functions. The first stage is to estimate the main-effect equivalency of the baseline markers on the outcome, which is then used as an augmentation term in the second stage estimation for HTE. The proposed two-stage procedure is robust to model mis-specification of main effects and improves efficiency for estimating HTE through nonparametric function estimation, e.g., XGBoost. A permutation test is proposed for global assessment of evidence for HTE. An analysis of a genetic study in Prostate Cancer Prevention Trial led by the SWOG Cancer Research Network, is conducted to showcase the properties and the utilities of the two-stage method.|http://arxiv.org/abs/2302.01367v1|Heng Chen,Michael L. LeBlanc,James Y. Dai
782|Dynamic Borrowing Method for Historical Information Using a Frequentist Approach for Hybrid Control Design|Information borrowing from historical data is gaining attention in clinical trials of rare and pediatric diseases, where statistical power may be insufficient for confirmation of efficacy if the sample size is small. Although Bayesian information borrowing methods are well established, test-then-pool and equivalence-based test-then-pool methods have recently been proposed as frequentist methods to determine whether historical data should be used for statistical hypothesis testing. Depending on the results of the hypothesis testing, historical data may not be usable. This paper proposes a dynamic borrowing method for historical information based on the similarity between current and historical data. In our proposed method of dynamic information borrowing, as in Bayesian dynamic borrowing, the amount of borrowing ranges from 0% to 100%. We propose two methods using the density function of the t-distribution and a logistic function as a similarity measure. We evaluate the performance of the proposed methods through Monte Carlo simulations. We demonstrate the usefulness of borrowing information by reanalyzing actual clinical trial data.|http://arxiv.org/abs/2305.14679v1|Masahiro Kojima
783|A General Form of Covariate Adjustment in Randomized Clinical Trials|In randomized clinical trials, adjusting for baseline covariates can improve credibility and efficiency for demonstrating and quantifying treatment effects. This article studies the augmented inverse propensity weighted (AIPW) estimator, which is a general form of covariate adjustment that uses linear, generalized linear, and non-parametric or machine learning models for the conditional mean of the response given covariates. Under covariate-adaptive randomization, we establish general theorems that show a complete picture of the asymptotic normality, {efficiency gain, and applicability of AIPW estimators}. In particular, we provide for the first time a rigorous theoretical justification of using machine learning methods with cross-fitting for dependent data under covariate-adaptive randomization. Based on the general theorems, we offer insights on the conditions for guaranteed efficiency gain and universal applicability {under different randomization schemes}, which also motivate a joint calibration strategy using some constructed covariates after applying AIPW. Our methods are implemented in the R package RobinCar.|http://arxiv.org/abs/2306.10213v2|Marlena S. Bannick,Jun Shao,Jingyi Liu,Yu Du,Yanyao Yi,Ting Ye
784|Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data|Intracerebral hemorrhage (ICH) is the second most common and deadliest form of stroke. Despite medical advances, predicting treat ment outcomes for ICH remains a challenge. This paper proposes a novel prognostic model that utilizes both imaging and tabular data to predict treatment outcome for ICH. Our model is trained on observational data collected from non-randomized controlled trials, providing reliable predictions of treatment success. Specifically, we propose to employ a variational autoencoder model to generate a low-dimensional prognostic score, which can effectively address the selection bias resulting from the non-randomized controlled trials. Importantly, we develop a variational distributions combination module that combines the information from imaging data, non-imaging clinical data, and treatment assignment to accurately generate the prognostic score. We conducted extensive experiments on a real-world clinical dataset of intracerebral hemorrhage. Our proposed method demonstrates a substantial improvement in treatment outcome prediction compared to existing state-of-the-art approaches. Code is available at https://github.com/med-air/TOP-GPM|http://arxiv.org/abs/2307.12858v1|Wenao Ma,Cheng Chen,Jill Abrigo,Calvin Hoi-Kwan Mak,Yuqi Gong,Nga Yan Chan,Chu Han,Zaiyi Liu,Qi Dou
785|Covariate adjustment and estimation of difference in proportions in randomized clinical trials|Difference in proportions is frequently used to measure treatment effect for binary outcomes in randomized clinical trials. The estimation of difference in proportions can be assisted by adjusting for prognostic baseline covariates to enhance precision and bolster statistical power. Standardization or G-computation is a widely used method for covariate adjustment in estimating unconditional difference in proportions, because of its robustness to model misspecification. Various inference methods have been proposed to quantify the uncertainty and confidence intervals based on large-sample theories. However, their performances under small sample sizes and model misspecification have not been comprehensively evaluated. We propose an alternative approach to estimate the unconditional variance of the standardization estimator based on the robust sandwich estimator to further enhance the finite sample performance. Extensive simulations are provided to demonstrate the performances of the proposed method, spanning a wide range of sample sizes, randomization ratios, and model misspecification. We apply the proposed method in a real data example to illustrate the practical utility.|http://arxiv.org/abs/2308.15688v1|Jialuo Liu,Dong Xi
786|Designing and evaluating an online reinforcement learning agent for physical exercise recommendations in N-of-1 trials|Personalized adaptive interventions offer the opportunity to increase patient benefits, however, there are challenges in their planning and implementation. Once implemented, it is an important question whether personalized adaptive interventions are indeed clinically more effective compared to a fixed gold standard intervention. In this paper, we present an innovative N-of-1 trial study design testing whether implementing a personalized intervention by an online reinforcement learning agent is feasible and effective. Throughout, we use a new study on physical exercise recommendations to reduce pain in endometriosis for illustration. We describe the design of a contextual bandit recommendation agent and evaluate the agent in simulation studies. The results show that, first, implementing a personalized intervention by an online reinforcement learning agent is feasible. Second, such adaptive interventions have the potential to improve patients' benefits even if only few observations are available. As one challenge, they add complexity to the design and implementation process. In order to quantify the expected benefit, data from previous interventional studies is required. We expect our approach to be transferable to other interventions and clinical interventions.|http://arxiv.org/abs/2309.14156v2|Dominik Meier,Ipek Ensari,Stefan Konigorski
787|Estimands and cumulative incidence function regression in clinical trials: some new results on interpretability and robustness|Regression analyses based on transformations of cumulative incidence functions are often adopted when modeling and testing for treatment effects in clinical trial settings involving competing and semi-competing risks. Common frameworks include the Fine-Gray model and models based on direct binomial regression. Using large sample theory we derive the limiting values of treatment effect estimators based on such models when the data are generated according to multiplicative intensity-based models, and show that the estimand is sensitive to several process features. The rejection rates of hypothesis tests based on cumulative incidence function regression models are also examined for null hypotheses of different types, based on which a robustness property is established. In such settings supportive secondary analyses of treatment effects are essential to ensure a full understanding of the nature of treatment effects. An application to a palliative study of individuals with breast cancer metastatic to bone is provided for illustration.|http://arxiv.org/abs/2401.04863v1|Alexandra Bhler,Richard J Cook,Jerald F Lawless
788|Constrained Markov decision processes for response-adaptive procedures in clinical trials with binary outcomes|A constrained Markov decision process (CMDP) approach is developed for response-adaptive procedures in clinical trials with binary outcomes. The resulting CMDP class of Bayesian response -- adaptive procedures can be used to target a certain objective, e.g., patient benefit or power while using constraints to keep other operating characteristics under control. In the CMDP approach, the constraints can be formulated under different priors, which can induce a certain behaviour of the policy under a given statistical hypothesis, or given that the parameters lie in a specific part of the parameter space. A solution method is developed to find the optimal policy, as well as a more efficient method, based on backward recursion, which often yields a near-optimal solution with an available optimality gap. Three applications are considered, involving type I error and power constraints, constraints on the mean squared error, and a constraint on prior robustness. While the CMDP approach slightly outperforms the constrained randomized dynamic programming (CRDP) procedure known from literature when focussing on type I and II error and mean squared error, showing the general quality of CRDP, CMDP significantly outperforms CRDP when the focus is on type I and II error only.|http://arxiv.org/abs/2401.15694v1|Stef Baas,Aleida Braaksma,Richard J. Boucherie
789|Publication bias adjustment in network meta-analysis: an inverse probability weighting approach using clinical trial registries|Network meta-analysis (NMA) is a useful tool to compare multiple interventions simultaneously in a single meta-analysis, it can be very helpful for medical decision making when the study aims to find the best therapy among several active candidates. However, the validity of its results is threatened by the publication bias issue. Existing methods to handle the publication bias issue in the standard pairwise meta-analysis are hard to extend to this area with the complicated data structure and the underlying assumptions for pooling the data. In this paper, we aimed to provide a flexible inverse probability weighting (IPW) framework along with several t-type selection functions to deal with the publication bias problem in the NMA context. To solve these proposed selection functions, we recommend making use of the additional information from the unpublished studies from multiple clinical trial registries. A comprehensive numerical study and a real example showed that our methodology can help obtain more accurate estimates and higher coverage probabilities, and improve other properties of an NMA (e.g., ranking the interventions).|http://arxiv.org/abs/2402.00239v1|Ao Huang,Yi Zhou,Satoshi Hattori
790|A two-step approach for analyzing time to event data under non-proportional hazards|The log-rank test and the Cox proportional hazards model are commonly used to compare time-to-event data in clinical trials, as they are most powerful under proportional hazards. But there is a loss of power if this assumption is violated, which is the case for some new oncology drugs like immunotherapies. We consider a two-stage test procedure, in which the weighting of the log-rank test statistic depends on a pre-test of the proportional hazards assumption. I.e., depending on the pre-test either the log-rank or an alternative test is used to compare the survival probabilities. We show that if naively implemented this can lead to a substantial inflation of the type-I error rate. To address this, we embed the two-stage test in a permutation test framework to keep the nominal level alpha. We compare the operating characteristics of the two-stage test with the log-rank test and other tests by clinical trial simulations.|http://arxiv.org/abs/2402.08336v1|Jonas Brugger,Tim Friede,Florian Klinglmller,Martin Posch,Robin Ristl,Franz Knig
791|A unified Bayesian framework for interval hypothesis testing in clinical trials|The American Statistical Association (ASA) statement on statistical significance and P-values \cite{wasserstein2016asa} cautioned statisticians against making scientific decisions solely on the basis of traditional P-values. The statement delineated key issues with P-values, including a lack of transparency, an inability to quantify evidence in support of the null hypothesis, and an inability to measure the size of an effect or the importance of a result. In this article, we demonstrate that the interval null hypothesis framework (instead of the point null hypothesis framework), when used in tandem with Bayes factor-based tests, is instrumental in circumnavigating the key issues of P-values. Further, we note that specifying prior densities for Bayes factors is challenging and has been a reason for criticism of Bayesian hypothesis testing in existing literature. We address this by adapting Bayes factors directly based on common test statistics. We demonstrate, through numerical experiments and real data examples, that the proposed Bayesian interval hypothesis testing procedures can be calibrated to ensure frequentist error control while retaining their inherent interpretability. Finally, we illustrate the improved flexibility and applicability of the proposed methods by providing coherent frameworks for competitive landscape analysis and end-to-end Bayesian hypothesis tests in the context of reporting clinical trial outcomes.|http://arxiv.org/abs/2402.13890v1|Abhisek Chakraborty,Megan H. Murray,Ilya Lipkovich,Yu Du
792|Multilevel functional distributional models with application to continuous glucose monitoring in diabetes clinical trials|Continuous glucose monitoring (CGM) is a minimally invasive technology that allows continuous monitoring of an individual's blood glucose. We focus on a large clinical trial that collected CGM data every few minutes for 26 weeks and assumes that the basic observation unit is the distribution of CGM observations in a four-week interval. The resulting data structure is multilevel (because each individual has multiple months of data) and distributional (because the data for each four-week interval is represented as a distribution). The scientific goals are to: (1) identify and quantify the effects of factors that affect glycemic control in type 1 diabetes (T1D) patients; and (2) identify and characterize the patients who respond to treatment. To address these goals, we propose a new multilevel functional model that treats the CGM distributions as a response. Methods are motivated by and applied to data collected by The Juvenile Diabetes Research Foundation Continuous Glucose Monitoring Group. Reproducible code for the methods introduced here is available on GitHub.|http://arxiv.org/abs/2403.10514v1|Marcos Matabuena,Ciprian M. Crainiceanu
793|Sample size planning for estimating the global win probability with assurance and precision|Most clinical trials conducted in drug development contain multiple endpoints in order to collectively assess the intended effects of the drug on various disease characteristics. Focusing on the estimation of the global win probability, defined as the average win probability (WinP) across endpoints that a treated participant would have a better outcome than a control participant, we propose a closed-form sample size formula incorporating pre-specified precision and assurance, with precision denoted by the lower limit of confidence interval and assurance denoted by the probability of achieving that lower limit. We make use of the equivalence of the WinP and the area under the receiver operating characteristic curve (AUC) and adapt a formula originally developed for the difference between two AUCs to handle the global WinP. Unequal variance is allowed. Simulation results suggest that the method performs very well. We illustrate the proposed formula using a Parkinson's disease clinical trial design example.|http://arxiv.org/abs/2404.04415v1|Di Shu,Guangyong Zou
794|Inference of treatment effect and its regional modifiers using restricted mean survival time in multi-regional clinical trials|Multi-regional clinical trials (MRCTs) play an increasingly crucial role in global pharmaceutical development by expediting data gathering and regulatory approval across diverse patient populations. However, differences in recruitment practices and regional demographics often lead to variations in study participant characteristics, potentially biasing treatment effect estimates and undermining treatment effect consistency assessment across regions. To address this challenge, we propose novel estimators and inference methods utilizing inverse probability of sampling and calibration weighting. Our approaches aim to eliminate exogenous regional imbalance while preserving intrinsic differences across regions, such as race and genetic variants. Moreover, time-to-event outcomes in MRCT studies receive limited attention, with existing methodologies primarily focusing on hazard ratios. In this paper, we adopt restricted mean survival time to characterize the treatment effect, offering more straightforward interpretations of treatment effects with fewer assumptions than hazard ratios. Theoretical results are established for the proposed estimators, supported by extensive simulation studies. We illustrate the effectiveness of our methods through a real MRCT case study on acute coronary syndromes.|http://arxiv.org/abs/2404.08128v1|Kaiyuan Hua,Hwanhee Hong,Xiaofei Wang
795|DKE-Research at SemEval-2024 Task 2: Incorporating Data Augmentation with Generative Models and Biomedical Knowledge to Enhance Inference Robustness|Safe and reliable natural language inference is critical for extracting insights from clinical trial reports but poses challenges due to biases in large pre-trained language models. This paper presents a novel data augmentation technique to improve model robustness for biomedical natural language inference in clinical trials. By generating synthetic examples through semantic perturbations and domain-specific vocabulary replacement and adding a new task for numerical and quantitative reasoning, we introduce greater diversity and reduce shortcut learning. Our approach, combined with multi-task learning and the DeBERTa architecture, achieved significant performance gains on the NLI4CT 2024 benchmark compared to the original language models. Ablation studies validate the contribution of each augmentation method in improving robustness. Our best-performing model ranked 12th in terms of faithfulness and 8th in terms of consistency, respectively, out of the 32 participants.|http://arxiv.org/abs/2404.09206v1|Yuqi Wang,Zeqiang Wang,Wei Wang,Qi Chen,Kaizhu Huang,Anh Nguyen,Suparna De
796|PWEXP: An R Package Using Piecewise Exponential Model for Study Design and Event/Timeline Prediction|Parametric assumptions such as exponential distribution are commonly used in clinical trial design and analysis. However, violation of distribution assumptions can introduce biases in sample size and power calculations. Piecewise exponential (PWE) hazard model partitions the hazard function into segments each with constant hazards and is easy for interpretation and computation. Due to its piecewise property, PWE can fit a wide range of survival curves and accurately predict the future number of events and analysis time in event-driven clinical trials, thus enabling more flexible and reliable study designs. Compared with other existing approaches, the PWE model provides a superior balance of flexibility and robustness in model fitting and prediction. The proposed PWEXP package is designed for estimating and predicting PWE hazard models for right-censored data. By utilizing well-established criteria such as AIC, BIC, and cross-validation log-likelihood, the PWEXP package chooses the optimal number of change-points and determines the optimal position of change-points. With its particular goodness-of-fit, the PWEXP provides accurate and robust hazard estimation, which can be used for reliable power calculation at study design and timeline prediction at study conduct. The package also offers visualization functions to facilitate the interpretation of survival curve fitting results.|http://arxiv.org/abs/2404.17772v1|Tianchen Xu,Rachael Wen
797|Inference under covariate-adaptive randomization with many strata|Covariate-adaptive randomization is widely employed to balance baseline covariates in interventional studies such as clinical trials and experiments in development economics. Recent years have witnessed substantial progress in inference under covariate-adaptive randomization with a fixed number of strata. However, concerns have been raised about the impact of a large number of strata on its design and analysis, which is a common scenario in practice, such as in multicenter randomized clinical trials. In this paper, we propose a general framework for inference under covariate-adaptive randomization, which extends the seminal works of Bugni et al. (2018, 2019) by allowing for a diverging number of strata. Furthermore, we introduce a novel weighted regression adjustment that ensures efficiency improvement. On top of establishing the asymptotic theory, practical algorithms for handling situations involving an extremely large number of strata are also developed. Moreover, by linking design balance and inference robustness, we highlight the advantages of stratified block randomization, which enforces better covariate balance within strata compared to simple randomization. This paper offers a comprehensive landscape of inference under covariate-adaptive randomization, spanning from fixed to diverging to extremely large numbers of strata.|http://arxiv.org/abs/2405.18856v1|Jiahui Xin,Hanzhong Liu,Wei Ma
798|Comparative Effectiveness Research with Average Hazard for Censored Time-to-Event Outcomes: A Numerical Study|The average hazard (AH), recently introduced by Uno and Horiguchi, represents a novel summary metric of event time distributions, conceptualized as the general censoring-free average person-time incidence rate on a given time window, $[0,\tau].$ This metric is calculated as the ratio of the cumulative incidence probability at $\tau$ to the restricted mean survival time at $\tau$ and can be estimated through non-parametric methods. The AH's difference and ratio present viable alternatives to the traditional Cox's hazard ratio for quantifying the treatment effect on time-to-event outcomes in comparative clinical studies. While the methodology for evaluating the difference and ratio of AH in randomized clinical trials has been previously proposed, the application of the AH-based approach in general comparative effectiveness research (CER), where interventions are not randomly allocated, remains underdiscussed. This paper aims to introduce several approaches for applying the AH in general CER, thereby extending its utility beyond randomized trial settings to observational studies where treatment assignment is non-random.|http://arxiv.org/abs/2407.00709v1|Hong Xiong,Jean Connors,Deb Schrag,Hajime Uno
799|Win Ratio with Multiple Thresholds for Composite Endpoints|Composite endpoints consisting of both terminal and non-terminal events, such as death and hospitalization, are frequently used as primary endpoints in cardiovascular clinical trials. The Win Ratio method (WR) proposed by Pocock et al. (2012)1 employs a hierarchical structure to combine fatal and non-fatal events by giving death information an absolute priority, which adversely affects power if the treatment effect is mainly on the non-fatal outcomes. We hereby propose the Win Ratio with Multiple Thresholds (WR-MT) that releases the strict hierarchical structure of the standard WR by adding stages with non-zero thresholds. A weighted adaptive approach is developed to determine the thresholds in WR-MT. This method preserves the good statistical properties of the standard WR and has a greater capacity to detect treatment effects on non-fatal events. We show that WR-MT has an overall more favorable performance than WR in our simulation that addresses the influence of follow-up time, the association between events, and the treatment effect levels, as well as a case study based on the Digitalis Investigation Group clinical trial data.|http://arxiv.org/abs/2407.18341v2|Yunhan Mou,Tassos Kyriakides,Scott Hummel,Fan Li,Yuan Huang
800|Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral 7B Model and Data Augmentation|This paper describes our approach to the SemEval-2024 safe biomedical Natural Language Inference for Clinical Trials (NLI4CT) task, which concerns classifying statements about Clinical Trial Reports (CTRs). We explored the capabilities of Mistral-7B, a generalist open-source Large Language Model (LLM). We developed a prompt for the NLI4CT task, and fine-tuned a quantized version of the model using an augmented version of the training dataset. The experimental results show that this approach can produce notable results in terms of the macro F1-score, while having limitations in terms of faithfulness and consistency. All the developed code is publicly available on a GitHub repository|http://arxiv.org/abs/2408.03127v1|Artur Guimares,Bruno Martins,Joo Magalhes
801|Retrieved dropout imputation considering administrative study withdrawal|The International Council for Harmonisation of Technical Requirements for Pharmaceuticals for Human Use (ICH) E9 (R1) Addendum provides a framework for defining estimands in clinical trials. Treatment policy strategy is the mostly used approach to handle intercurrent events in defining estimands. Imputing missing values for potential outcomes under the treatment policy strategy has been discussed in the literature. Missing values as a result of administrative study withdrawals (such as site closures due to business reasons, COVID-19 control measures, and geopolitical conflicts, etc.) are often imputed in the same way as other missing values occurring after intercurrent events related to safety or efficacy. Some research suggests using a hypothetical strategy to handle the treatment discontinuations due to administrative study withdrawal in defining the estimands and imputing the missing values based on completer data assuming missing at random, but this approach ignores the fact that subjects might experience other intercurrent events had they not had the administrative study withdrawal. In this article, we consider the administrative study withdrawal censors the normal real-world like intercurrent events and propose two methods for handling the corresponding missing values under the retrieved dropout imputation framework. Simulation shows the two methods perform well. We also applied the methods to actual clinical trial data evaluating an anti-diabetes treatment.|http://arxiv.org/abs/2410.06774v1|Rong Liu,Yongming Qu
802|Network reinforcement driven drug repurposing for COVID-19 by exploiting disease-gene-drug associations|Currently, the number of patients with COVID-19 has significantly increased. Thus, there is an urgent need for developing treatments for COVID-19. Drug repurposing, which is the process of reusing already-approved drugs for new medical conditions, can be a good way to solve this problem quickly and broadly. Many clinical trials for COVID-19 patients using treatments for other diseases have already been in place or will be performed at clinical sites in the near future. Additionally, patients with comorbidities such as diabetes mellitus, obesity, liver cirrhosis, kidney diseases, hypertension, and asthma are at higher risk for severe illness from COVID-19. Thus, the relationship of comorbidity disease with COVID-19 may help to find repurposable drugs. To reduce trial and error in finding treatments for COVID-19, we propose building a network-based drug repurposing framework to prioritize repurposable drugs. First, we utilized knowledge of COVID-19 to construct a disease-gene-drug network (DGDr-Net) representing a COVID-19-centric interactome with components for diseases, genes, and drugs. DGDr-Net consisted of 592 diseases, 26,681 human genes and 2,173 drugs, and medical information for 18 common comorbidities. The DGDr-Net recommended candidate repurposable drugs for COVID-19 through network reinforcement driven scoring algorithms. The scoring algorithms determined the priority of recommendations by utilizing graph-based semi-supervised learning. From the predicted scores, we recommended 30 drugs, including dexamethasone, resveratrol, methotrexate, indomethacin, quercetin, etc., as repurposable drugs for COVID-19, and the results were verified with drugs that have been under clinical trials. The list of drugs via a data-driven computational approach could help reduce trial-and-error in finding treatment for COVID-19.|http://arxiv.org/abs/2008.05377v1|Yonghyun Nam,Jae-Seung Yun,Seung Mi Lee,Ji Won Park,Ziqi Chen,Brian Lee,Anurag Verma,Xia Ning,Li Shen,Dokyoon Kim
803|Efficiency of Multivariate Tests in Trials in Progressive Supranuclear Palsy|Measuring disease progression in clinical trials for testing novel treatments for multifaceted diseases as Progressive Supranuclear Palsy (PSP), remains challenging. In this study we assess a range of statistical approaches to compare outcomes measured by the items of the Progressive Supranuclear Palsy Rating Scale (PSPRS). We consider several statistical approaches, including sum scores, as an FDA-recommended version of the PSPRS, multivariate tests, and analysis approaches based on multiple comparisons of the individual items. We propose two novel approaches which measure disease status based on Item Response Theory models. We assess the performance of these tests in an extensive simulation study and illustrate their use with a re-analysis of the ABBV-8E12 clinical trial. Furthermore, we discuss the impact of the FDA-recommended scoring of item scores on the power of the statistical tests. We find that classical approaches as the PSPRS sum score demonstrate moderate to high power when treatment effects are consistent across the individual items. The tests based on Item Response Theory models yield the highest power when the simulated data are generated from an IRT model. The multiple testing based approaches have a higher power in settings where the treatment effect is limited to certain domains or items. The FDA-recommended item rescoring tends to decrease the simulated power. The study shows that there is no one-size-fits-all testing procedure for evaluating treatment effects using PSPRS items; the optimal method varies based on the specific effect size patterns. The efficiency of the PSPRS sum score, while generally robust and straightforward to apply, varies depending on the effect sizes' patterns encountered and more powerful alternatives are available in specific settings. These findings can have important implications for the design of future clinical trials in PSP.|http://arxiv.org/abs/2312.08169v1|Elham Yousefi,Mohamed Gewily,Franz Knig,Gnter Hglinger,Franziska Hopfner,Mats O. Karlsson,Robin Ristl,Sonja Zehetmayer,Martin Posch
804|Estimation methods for estimands using the treatment policy strategy; a simulation study based on the PIONEER 1 Trial|Estimands using the treatment policy strategy for addressing intercurrent events are common in Phase III clinical trials. One estimation approach for this strategy is retrieved dropout whereby observed data following an intercurrent event are used to multiply impute missing data. However, such methods have had issues with variance inflation and model fitting due to data sparsity. This paper introduces likelihood-based versions of these approaches, investigating and comparing their statistical properties to the existing retrieved dropout approaches, simpler analysis models and reference-based multiple imputation. We use a simulation based upon the data from the PIONEER 1 Phase III clinical trial in Type II diabetics to present complex and relevant estimation challenges. The likelihood-based methods display similar statistical properties to their multiple imputation equivalents, but all retrieved dropout approaches suffer from high variance. Retrieved dropout approaches appear less biased than reference-based approaches, resulting in a bias-variance trade-off, but we conclude that the large degree of variance inflation is often more problematic than the bias. Therefore, only the simpler retrieved dropout models appear appropriate as a primary analysis in a clinical trial, and only where it is believed most data following intercurrent events will be observed. The jump-to-reference approach may represent a more promising estimation approach for symptomatic treatments due to its relatively high power and ability to fit in the presence of much missing data, despite its strong assumptions and tendency towards conservative bias. More research is needed to further develop how to estimate the treatment effect for a treatment policy strategy.|http://arxiv.org/abs/2402.12850v1|James Bell,Thomas Drury,Tobias Mtze,Christian Bressen Pipper,Lorenzo Guizzaro,Marian Mitroiu,Khadija Rerhou Rantell,Marcel Wolbers,David Wright
805|Generative AI for Health Technology Assessment: Opportunities, Challenges, and Policy Considerations|This review introduces the transformative potential of generative Artificial Intelligence (AI) and foundation models, including large language models (LLMs), for health technology assessment (HTA). We explore their applications in four critical areas, evidence synthesis, evidence generation, clinical trials and economic modeling: (1) Evidence synthesis: Generative AI has the potential to assist in automating literature reviews and meta-analyses by proposing search terms, screening abstracts, and extracting data with notable accuracy; (2) Evidence generation: These models can potentially facilitate automating the process and analyze the increasingly available large collections of real-world data (RWD), including unstructured clinical notes and imaging, enhancing the speed and quality of real-world evidence (RWE) generation; (3) Clinical trials: Generative AI can be used to optimize trial design, improve patient matching, and manage trial data more efficiently; and (4) Economic modeling: Generative AI can also aid in the development of health economic models, from conceptualization to validation, thus streamlining the overall HTA process. Despite their promise, these technologies, while rapidly improving, are still nascent and continued careful evaluation in their applications to HTA is required. To ensure their responsible use and implementation, both developers and users of research incorporating these tools, should familiarize themselves with their current limitations, including the issues related to scientific validity, risk of bias, and consider equity and ethical implications. We also surveyed the current policy landscape and provide suggestions for HTA agencies on responsibly integrating generative AI into their workflows, emphasizing the importance of human oversight and the fast-evolving nature of these tools.|http://arxiv.org/abs/2407.11054v3|Rachael Fleurence,Jiang Bian,Xiaoyan Wang,Hua Xu,Dalia Dawoud,Mitch Higashi,Jagpreet Chhatwal
806|Asymptotics in randomized urn models|This paper studies a very general urn model stimulated by designs in clinical trials, where the number of balls of different types added to the urn at trial n depends on a random outcome directed by the composition at trials 1,2,...,n-1. Patient treatments are allocated according to types of balls. We establish the strong consistency and asymptotic normality for both the urn composition and the patient allocation under general assumptions on random generating matrices which determine how balls are added to the urn. Also we obtain explicit forms of the asymptotic variance-covariance matrices of both the urn composition and the patient allocation. The conditions on the nonhomogeneity of generating matrices are mild and widely satisfied in applications. Several applications are also discussed.|http://arxiv.org/abs/math/0503521v1|Zhi-Dong Bai,Feifang Hu
807|The assessment and planning of non-inferiority trials for retention of effect hypotheses - towards a general approach|The objective of this paper is to develop statistical methodology for planning and evaluating three-armed non-inferiority trials for general retention of effect hypotheses, where the endpoint of interest may follow any (regular) parametric distribution family. This generalizes and unifies specific results for binary, normally and exponentially distributed endpoints. We propose a Wald-type test procedure for the retention of effect hypothesis (RET), which assures that the test treatment maintains at least a proportion $\Delta$ of reference treatment effect compared to placebo. At this, we distinguish the cases where the variance of the test statistic is estimated unrestrictedly and restrictedly to the null hypothesis, to improve accuracy of the nominal level. We present a general valid sample size allocation rule to achieve optimal power and sample size formulas, which significantly improve existing ones. Moreover, we propose a general applicable rule of thumb for sample allocation and give conditions where this rule is theoretically justified. The presented methodologies are discussed in detail for binary and for Poisson distributed endpoints by means of two clinical trials in the treatment of depression and in the treatment of epilepsy, respectively. $R$-software for implementation of the proposed tests and for sample size planning accompanies this paper.|http://arxiv.org/abs/0912.4169v1|M. Mielke,A. Munk
808|Approximate Dynamic Programming and Its Applications to the Design of Phase I Cancer Trials|Optimal design of a Phase I cancer trial can be formulated as a stochastic optimization problem. By making use of recent advances in approximate dynamic programming to tackle the problem, we develop an approximation of the Bayesian optimal design. The resulting design is a convex combination of a "treatment" design, such as Babb et al.'s (1998) escalation with overdose control, and a "learning" design, such as Haines et al.'s (2003) $c$-optimal design, thus directly addressing the treatment versus experimentation dilemma inherent in Phase I trials and providing a simple and intuitive design for clinical use. Computational details are given and the proposed design is compared to existing designs in a simulation study. The design can also be readily modified to include a first stage that cautiously escalates doses similarly to traditional nonparametric step-up/down schemes, while validating the Bayesian parametric model for the efficient model-based design in the second stage.|http://arxiv.org/abs/1011.6509v1|Jay Bartroff,Tze Leung Lai
809|Bayesian Hypothesis Assessment in Two-arm Trials Using Relative Belief Ratios|This paper develops a Bayesian approach for assessing equivalence and non-inferiority hypotheses in two-arm trials using relative belief ratios. A relative belief ratio is a measure of statistical evidence and can indicate evidence either for or against a hypothesis. In addition to the relative belief ratio, we also compute a measure of the strength of this evidence as a calibration of the relative belief ratio. Furthermore, we make use of the relative belief ratio as a measure of evidence, to assess whether a given prior induces bias either for or against a hypothesis. Prior elicitation, model checking and checking for prior-data conflict procedures are developed to ensure that the choices of model and prior made are relevant to the specific application. We highlight the applicability of the approach and illustrate the proposed method by applying it to a data set obtained from a two-arm clinical trial.|http://arxiv.org/abs/1401.4215v1|Saman Muthukumarana,Michael Evans
810|Prediction of heart rate response to conclusion of spontaneous breathing trial by fluctuation dissipation theory|The non-equilibrium fluctuation dissipation theorem is applied to predict how critically ill patients respond to treatment, based upon data currently collected by standard hospital monitoring devices. This framework is demonstrated on a common procedure in critical care: the spontaneous breathing trial. It is shown that the responses of groups of similar patients to the spontaneous breathing trial can be predicted by the non-equilibrium fluctuation dissipation approach. This mathematical framework, when fully formed and applied to other clinical interventions, may serve as part of the basis for personalized critical care.|http://arxiv.org/abs/1401.7724v1|Man Chen,Liang Ren Niestemski,Robert Prevost,Michael McRae,Sharath Cholleti,Gabriel Najarro,Timothy G. Buchman,Michael W. Deem
811|Inconsistent treatment estimates from mis-specified logistic regression analyses of randomized trials|When the difference between treatments in a clinical trial is estimated by a difference in means, then it is well known that randomization ensures unbiassed estimation, even if no account is taken of important baseline covariates. However, when the treatment effect is assessed by other summaries, e.g. by an odds ratio if the outcome is binary, then bias can arise if some covariates are omitted, regardless of the use of randomization for treatment allocation or the size of the trial. We present accurate closed-form approximations for this asymptotic bias when important Normally distributed covariates are omitted from a logistic regression. We compare this approximation with ones in the literature and derive more convenient forms for some of these existing results. The expressions give insight into the form of the bias, which simulations show is usable for distributions other than the Normal. The key result applies even when there are additional binary covariates in the model.|http://arxiv.org/abs/1407.5509v1|J. N. S. Matthews,Nuri H. Badi
812|A Nonparametric Bayesian Basket Trial Design|Targeted therapies on the basis of genomic aberrations analysis of the tumor have shown promising results in cancer prognosis and treatment. Regardless of tumor type, trials that match patients to targeted therapies for their particular genomic aberrations have become a mainstream direction of therapeutic management of patients with cancer. Therefore, finding the subpopulation of patients who can most benefit from an aberration-specific targeted therapy across multiple cancer types is important. We propose an adaptive Bayesian clinical trial design for patient allocation and subpopulation identification. We start with a decision theoretic approach, including a utility function and a probability model across all possible subpopulation models. The main features of the proposed design and population finding methods are that we allow for variable sets of covariates to be recorded by different patients, adjust for missing data, allow high order interactions of covariates, and the adaptive allocation of each patient to treatment arms using the posterior predictive probability of which arm is best for each patient. The new method is demonstrated via extensive simulation studies.|http://arxiv.org/abs/1612.02705v3|Yanxun Xu,Peter Mueller,Apostolia M Tsimberidou,Donald Berry
813|On the Interval-Based Dose-Finding Designs|The landscape of dose-finding designs for phase I clinical trials is rapidly shifting in the recent years, noticeably marked by the emergence of interval-based designs. We categorize them as the iDesigns and the IB-Designs. The iDesigns are originated by the toxicity probability inter- val (TPI) designs and its two modifications, the mTPI and mTPI-2 designs. The IB-Designs started as the cumulative cohort design (CCD) and is recently extended by the BOIN design. We discuss the differences and similarities between these two classes of interval-based designs, and compare their simulation performance with popular non-interval designs, such as the CRM and 3+3 designs. We also show that in addition to the population-level operating characteristics from simulated trials, investigators should also assess the dose-finding decision tables from the implemented designs to better understand the per-trial and per-patient behavior. This is particularly important for nonstatisticians to assess the designs with transparency. We pro- vide, to our knowledge, the most comprehensive simulation-based comparative study on various interval-based dose-finding designs.|http://arxiv.org/abs/1706.03277v2|Yuan Ji,Shengjie Yang
814|An optimised multi-arm multi-stage clinical trial design for unknown variance|Multi-arm multi-stage trial designs can bring notable gains in efficiency to the drug development process. However, for normally distributed endpoints, the determination of a design typically depends on the assumption that the patient variance in response is known. In practice, this will not usually be the case. To allow for unknown variance, previous research explored the performance of t-test statistics, coupled with a quantile substitution procedure for modifying the stopping boundaries, at controlling the familywise error-rate to the nominal level. Here, we discuss an alternative method based on Monte Carlo simulation that allows the group size and stopping boundaries of a multi-arm multi-stage t-test to be optimised according to some nominated optimality criteria. We consider several examples, provide R code for general implementation, and show that our designs confer a familywise error-rate and power close to the desired level. Consequently, this methodology will provide utility in future multi-arm multi-stage trials.|http://arxiv.org/abs/1710.03490v1|Michael Grayling,James Wason,Adrian Mander
815|A noniterative sample size procedure for tests based on t distributions|A noniterative sample size procedure is proposed for a general hypothesis test based on the t distribution by modifying and extending Guenther's (1981) approach for the one sample and two sample t tests. The generalized procedure is employed to determine the sample size for treatment comparisons using the analysis of covariance (ANCOVA) and the mixed effects model for repeated measures (MMRM) in randomized clinical trials. The sample size is calculated by adding a few simple correction terms to the sample size from the normal approximation to account for the nonnormality of the t statistic and lower order variance terms, which are functions of the covariates in the model. But it does not require specifying the covariate distribution. The noniterative procedure is suitable for superiority tests, noninferiority tests and a special case of the tests for equivalence or bioequivalence, and generally yields the exact or nearly exact sample size estimate after rounding to an integer. The method for calculating the exact power of the two sample t test with unequal variance in superiority trials is extended to equivalence trials. We also derive accurate power formulae for ANCOVA and MMRM, and the formula for ANCOVA is exact for normally distributed covariates. Numerical examples demonstrate the accuracy of the proposed methods particularly in small samples.|http://arxiv.org/abs/1804.04557v1|Yongqiang Tang
816|Sample size determination in superiority or non-inferiority clinical trials with time-to-event data under exponential, Weibull and Gompertz distributions|To examine the effect of exponential, Weibull and Gompertz distributions on sample size determination for superiority trials (STs) or non-inferiority trials (NTs) with time-to-event data, we present two sample size formulas for STs or NTs based on Weibull and Gompertz distributions, respectively. The formulas are compared with the current exponential formula to examine their performance. The simulation results show that the sample size formula based on the Weibull distribution is the most robust among the three formulas in STs or NTs. We suggest that recognizing the appropriate distribution in advance is beneficial for proper project planning and that assuming a Weibull distributed survival time is most advantageous in STs or NTs.|http://arxiv.org/abs/1808.03831v1|Dong Han,Yawen Hou,Zheng Chen
817|A novel approach for identifying and addressing case-mix heterogeneity in individual participant data meta-analysis|Case-mix heterogeneity across studies complicates meta-analyses. As a result of this, treatments that are equally effective on patient subgroups may appear to have different effectiveness on patient populations with different case mix. It is therefore important that meta-analyses be explicit for what patient population they describe the treatment effect. To achieve this, we develop a new approach for meta-analysis of randomized clinical trials, which use individual patient data (IPD) from all trials to infer the treatment effect for the patient population in a given trial, based on direct standardization using either outcome regression (OCR) or inverse probability weighting (IPW). Accompanying random-effect meta-analysis models are developed. The new approach enables disentangling heterogeneity due to case mix from that due to beyond case-mix reasons.|http://arxiv.org/abs/1908.10613v3|Tat-Thang Vo,Raphael Porcher,Anna Chaimani,Stijn Vansteelandt
818|A simulation study of methods for handling disease progression in dose-finding clinical trials|In traditional dose-finding studies, dose-limiting toxicity (DLT) is determined within a fixed time observation window where DLT is often defined as a binary outcome. In the setting of oncology dose-finding trials, often patients in advanced stage of diseases are enrolled. Therefore, disease progression may occur within the DLT observation window leading to treatment discontinuation and rendering the patient unevaluable for DLT assessment. As a result, additional patients have to be enrolled, increasing the sample size. We propose and compare several practical methods for handling disease progression which occurs within the DLT observation window, in the context of the time-to-event continual reassessment method (TITE-CRM) which allows using partial observations. The methods differ on the way they define an evaluable patient and in the way incomplete observations are included. The methods are illustrated and contrasted in the context of a single simulated trial, and compared via simulations under various scenarios of dose-progression relationship, in the setting of advanced soft-tissue sarcoma.|http://arxiv.org/abs/1909.02913v1|Lucie Biard,Bin Cheng,Gulam A. Manji,Shing M. Lee
819|Interim Monitoring in Sequential Multiple Assignment Randomized Trials|A sequential multiple assignment randomized trial (SMART) facilitates comparison of multiple adaptive treatment strategies (ATSs) simultaneously. Previous studies have established a framework to test the homogeneity of multiple ATSs by a global Wald test through inverse probability weighting. SMARTs are generally lengthier than classical clinical trials due to the sequential nature of treatment randomization in multiple stages. Thus, it would be beneficial to add interim analyses allowing for early stop if overwhelming efficacy is observed. We introduce group sequential methods to SMARTs to facilitate interim monitoring based on multivariate chi-square distribution. Simulation study demonstrates that the proposed interim monitoring in SMART (IM-SMART) maintains the desired type I error and power with reduced expected sample size compared to the classical SMART. Lastly, we illustrate our method by re-analyzing a SMART assessing the effects of cognitive behavioral and physical therapies in patients with knee osteoarthritis and comorbid subsyndromal depressive symptoms.|http://arxiv.org/abs/2108.01155v1|Liwen Wu,Junyao Wang,Abdus S. Wahed
820|Optimal Design in Hierarchical Models with application in Multi-center Trials|Hierarchical random effect models are used for different purposes in clinical research and other areas. In general, the main focus is on population parameters related to the expected treatment effects or group differences among all units of an upper level (e.g. subjects in many settings). Optimal design for estimation of population parameters are well established for many models. However, optimal designs for the prediction for the individual units may be different. Several settings are identiffed in which individual prediction may be of interest. In this paper we determine optimal designs for the individual predictions, e.g. in multi-center trials, and compare them to a conventional balanced design with respect to treatment allocation. Our investigations show, that balanced designs are far from optimal if the treatment effects vary strongly as compared to the residual error and more subjects should be recruited to the active (new) treatment in multi-center trials. Nevertheless, effciency loss may be limited resulting in a moderate sample size increase when individual predictions are foreseen with a balanced allocation.|http://arxiv.org/abs/1807.10083v1|Maryna Prus,Norbert Benda,Rainer Schwabe
821|Design aspects of COVID-19 treatment trials: Improving probability and time of favourable events|As a reaction to the pandemic of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), a multitude of clinical trials for the treatment of SARS-CoV-2 or the resulting corona disease (COVID-19) are globally at various stages from planning to completion. Although some attempts were made to standardize study designs, this was hindered by the ferocity of the pandemic and the need to set up trials quickly. We take the view that a successful treatment of COVID-19 patients (i) increases the probability of a recovery or improvement within a certain time interval, say 28 days; (ii) aims to expedite favourable events within this time frame; and (iii) does not increase mortality over this time period. On this background we discuss the choice of endpoint and its analysis. Furthermore, we consider consequences of this choice for other design aspects including sample size and power and provide some guidance on the application of adaptive designs in this particular context.|http://arxiv.org/abs/2012.02103v1|Jan Beyersmann,Tim Friede,Claudia Schmoor
822|Bayesian model for early dose-finding in phase I trials with multiple treatment courses|Dose-finding clinical trials in oncology aim to determine the maximum tolerated dose (MTD) of a new drug, generally defined by the proportion of patients with short-term dose-limiting toxicities (DLTs). Model-based approaches for such phase I oncology trials have been widely designed and are mostly restricted to the DLTs occurring during the first cycle of treatment, although patients continue to receive treatment for multiple cycles. We aim to estimate the probability of DLTs over sequences of treatment cycles via a Bayesian cumulative modeling approach, where the probability of DLT is modeled taking into account the cumulative effect of the administered drug and the DLT cycle of occurrence. We propose a design, called DICE (Dose-fInding CumulativE), for dose escalation and de-escalation according to previously observed toxicities, which aims at finding the MTD sequence (MTS). We performed an extensive simulation study comparing this approach to the time-to-event continual reassessment method (TITE-CRM) and to a benchmark. In general, our approach achieved a better or comparable percentage of correct MTS selection. Moreover, we investigated the DICE prediction ability.|http://arxiv.org/abs/2012.03700v1|Moreno Ursino,Lucie Biard,Sylvie Chevret
823|The Ci3+3 Design for Dual-Agent Combination Dose-Finding Clinical Trials|We propose a rule-based statistical design for combination dose-finding trials with two agents. The Ci3+3 design is an extension of the i3+3 design with simple decision rules comparing the observed toxicity rates and equivalence intervals that define the maximum tolerated dose combination. Ci3+3 consists of two stages to allow fast and efficient exploration of the dose-combination space. Statistical inference is restricted to a beta-binomial model for dose evaluation, and the entire design is built upon a set of fixed rules. We show via simulation studies that the Ci3+3 design exhibits similar and comparable operating characteristics to more complex designs utilizing model-based inferences. We believe that the Ci3+3 design may provide an alternative choice to help simplify the design and conduct of combination dose-finding trials in practice.|http://arxiv.org/abs/2103.13693v2|Shijie Yuan,Tianjian Zhou,Yawen Lin,Yuan Ji
824|Estimating the Distribution of Ratio of Paired Event Times in Phase II Oncology Trials|With the rapid development of new anti-cancer agents which are cytostatic, new endpoints are needed to better measure treatment efficacy in phase II trials. For this purpose, Von Hoff (1998) proposed the growth modulation index (GMI), i.e. the ratio between times to progression or progression-free survival times in two successive treatment lines. An essential task in studies using GMI as an endpoint is to estimate the distribution of GMI. Traditional methods for survival data have been used for estimating the GMI distribution because censoring is common for GMI data. However, we point out that the independent censoring assumption required by traditional survival methods is always violated for GMI, which may lead to severely biased results. In this paper, we construct nonparametric estimators for the distribution of GMI, accounting for the dependent censoring of GMI. We prove that the proposed estimators are consistent and converge weakly to zero-mean Gaussian processes upon proper normalization. Extensive simulation studies show that our estimators perform well in practical situations and outperform traditional methods. A phase II clinical trial using GMI as the primary endpoint is provided for illustration.|http://arxiv.org/abs/2110.15846v1|Li Chen,Mark Burkard,Jianrong Wu,Jill M. Kolesar,Chi Wang
825|Cross-validated risk scores adaptive enrichment (CADEN) design|We propose a Cross-validated ADaptive ENrichment design (CADEN) in which a trial population is enriched with a subpopulation of patients who are predicted to benefit from the treatment more than an average patient (the sensitive group). This subpopulation is found using a risk score constructed from the baseline (potentially high-dimensional) information about patients. The design incorporates an early stopping rule for futility. Simulation studies are used to assess the properties of CADEN against the original (non-enrichment) cross-validated risk scores (CVRS) design that constructs a risk score at the end of the trial. We show that when there exists a sensitive group of patients, CADEN achieves a higher power and a reduction in the expected sample size, in comparison to the CVRS design. We illustrate the application of the design in two real clinical trials. We conclude that the new design offers improved statistical efficiency in comparison to the existing non-enrichment method, as well as increased benefit to patients. The method has been implemented in an R package caden.|http://arxiv.org/abs/2111.02299v2|Svetlana Cherlin,James M S Wason
826|Efficient algorithms for building representative matched pairs with enhanced generalizability|Many recent efforts center on assessing the ability of real-world evidence (RWE) generated from non-randomized, observational data to produce results compatible with those from randomized controlled trials (RCTs). One noticeable endeavor is the RCT DUPLICATE initiative (Franklin et al., 2020, 2021). To better reconcile findings from an observational study and an RCT, or two observational studies based on different databases, it is desirable to eliminate differences between study populations. We outline an efficient, network-flow-based statistical matching algorithm that designs well-matched pairs from observational data that resemble the covariate distributions of a target population, for instance, the target-RCT-eligible population in the RCT DUPLICATE initiative studies or a generic population of scientific interest. We demonstrate the usefulness of the method by revisiting the inconsistency regarding a cardioprotective effect of the hormone replacement therapy (HRT) in the Women's Health Initiative (WHI) clinical trial and corresponding observational study. We found that the discrepancy between the trial and observational study persisted in a design that adjusted for study populations' cardiovascular risk profile, but seemed to disappear in a study design that further adjusted for the HRT initiation age and previous estrogen-plus-progestin use. The proposed method is integrated into the R package match2C.|http://arxiv.org/abs/2205.04539v2|Bo Zhang
827|Simulating and reporting frequentist operating characteristics of clinical trials that borrow external information|Borrowing of information from historical or external data to inform inference in a current trial is an expanding field in the era of precision medicine, where trials are often performed in small patient cohorts for practical or ethical reasons. Many approaches for borrowing from external data have been proposed. Even though these methods are mainly based on Bayesian approaches by incorporating external information into the prior for the current analysis, frequentist operating characteristics of the analysis strategy are of interest. In particular, type I error and power at a prespecified point alternative are in the focus. It is well-known that borrowing from external information may lead to the alteration of type I error rate. We propose a procedure to investigate and report the frequentist operating characteristics in this context. The approach evaluates type I error rate of the test with borrowing from external data and calibrates the test without borrowing to this type I error rate. On this basis, a fair comparison of power between the test with and without borrowing is achieved.|http://arxiv.org/abs/2302.12651v1|Annette Kopp-Schneider,Manuel Wiesenfarth,Leonhard Held,Silvia Calderazzo
828|A basket trial design based on power priors|In basket trials a treatment is investigated in several subgroups. They are primarily used in oncology in early clinical phases as single-arm trials with a binary endpoint. For their analysis primarily Bayesian methods have been suggested, as they allow partial sharing of information based on the observed similarity between subgroups. Fujikawa et al. (2020) suggested an approach using empirical Bayes methods that allows flexible sharing based on easily interpretable weights derived from the Jensen-Shannon divergence between the subgroup-wise posterior distributions. We show that this design is closely related to the method of power priors and investigate several modifications of Fujikawa's design using methods from the power prior literature. While in Fujikawa's design, the amount of information that is shared between two baskets is only determined by their pairwise similarity, we also discuss extensions where the outcomes of all baskets are considered in the computation of the sharing weights. The results of our comparison study show that the power prior design has comparable performance to fully Bayesian designs in a range of different scenarios. At the same time, the power prior design is computationally cheap and even allows analytical computation of operating characteristics in some settings.|http://arxiv.org/abs/2309.06988v2|Lukas Baumann,Lukas Sauer,Meinhard Kieser
829|The Modified Combo i3+3 Design for Novel-Novel Combination Dose-Finding Trials in Oncology|We consider a modified Ci3+3 (MCi3+3) design for dual-agent dose-finding trials in which both agents are tested on multiple doses. This usually happens when the agents are novel therapies. The MCi3+3 design offers a two-stage or three-stage version, depending on the practical need. The first stage begins with single-agent dose escalation, the second stage launches a model-free combination dose finding for both agents, and optionally, the third stage follows with a model-based design. MCi3+3 aims to maintain a relatively simple framework to facilitate practical application, while also address challenges that are unique to novel-novel combination dose finding. Through simulations, we demonstrate that the MCi3+3 design adeptly manages various toxicity scenarios. It exhibits operational characteristics on par with other combination designs, while offering an enhanced safety profile. The design is motivated and tested for a real-life clinical trial.|http://arxiv.org/abs/2406.12666v2|Jiaxin Liu,Shijie Yuan,Qiqi Deng,Yuan Ji
830|Assessing mediation in cross-sectional stepped wedge cluster randomized trials|Mediation analysis has been comprehensively studied for independent data but relatively little work has been done for correlated data, especially for the increasingly adopted stepped wedge cluster randomized trials (SW-CRTs). Motivated by challenges in underlying the effect mechanisms in pragmatic and implementation science clinical trials, we develop new methods for mediation analysis in SW-CRTs. Specifically, based on a linear and generalized linear mixed models, we demonstrate how to estimate the natural indirect effect and mediation proportion in typical SW-CRTs with four data types, including both continuous and binary mediators and outcomes. Furthermore, to address the emerging challenges in exposure-time treatment effect heterogeneity, we derive the mediation expressions in SW-CRTs when the total effect varies as a function of the exposure time. The cluster jackknife approach is considered for inference across all data types and treatment effect structures. We conduct extensive simulations to evaluate the finite-sample performances of proposed mediation estimators and demonstrate the proposed approach in a real data example. A user-friendly R package mediateSWCRT has been developed to facilitate the practical implementation of the estimators.|http://arxiv.org/abs/2410.15596v2|Zhiqiang Cao,Fan Li
831|TW-BAG: Tensor-wise Brain-aware Gate Network for Inpainting Disrupted Diffusion Tensor Imaging|Diffusion Weighted Imaging (DWI) is an advanced imaging technique commonly used in neuroscience and neurological clinical research through a Diffusion Tensor Imaging (DTI) model. Volumetric scalar metrics including fractional anisotropy, mean diffusivity, and axial diffusivity can be derived from the DTI model to summarise water diffusivity and other quantitative microstructural information for clinical studies. However, clinical practice constraints can lead to sub-optimal DWI acquisitions with missing slices (either due to a limited field of view or the acquisition of disrupted slices). To avoid discarding valuable subjects for group-wise studies, we propose a novel 3D Tensor-Wise Brain-Aware Gate network (TW-BAG) for inpainting disrupted DTIs. The proposed method is tailored to the problem with a dynamic gate mechanism and independent tensor-wise decoders. We evaluated the proposed method on the publicly available Human Connectome Project (HCP) dataset using common image similarity metrics derived from the predicted tensors and scalar DTI metrics. Our experimental results show that the proposed approach can reconstruct the original brain DTI volume and recover relevant clinical imaging information.|http://arxiv.org/abs/2210.17076v1|Zihao Tang,Xinyi Wang,Lihaowen Zhu,Mariano Cabezas,Dongnan Liu,Michael Barnett,Weidong Cai,Chengyu Wang
832|Blended Survival Curves: A New Approach to Extrapolation for Time-to-Event Outcomes from Clinical Trial in Health Technology Assessment|Background Survival extrapolation is essential in the cost-effectiveness analysis to quantify the lifetime survival benefit associated with a new intervention, due to the restricted duration of randomized controlled trials (RCTs). Current approaches of extrapolation often assume that the treatment effect observed in the trial can continue indefinitely, which is unrealistic and may have a huge impact on decisions for resource allocation. Objective We introduce a novel methodology as a possible solution to alleviate the problem of performing survival extrapolation with heavily censored data from clinical trials. Method The main idea is to mix a flexible model (e.g., Cox semi-parametric) to fit as well as possible the observed data and a parametric model encoding assumptions on the expected behaviour of underlying long-term survival. The two are "blended" into a single survival curve that is identical with the Cox model over the range of observed times and gradually approaching the parametric model over the extrapolation period based on a weight function. The weight function regulates the way two survival curves are blended, determining how the internal and external sources contribute to the estimated survival over time. Results A 4-year follow-up RCT of rituximab in combination with fludarabine and cyclophosphamide v. fludarabine and cyclophosphamide alone for the first-line treatment of chronic lymphocytic leukemia is used to illustrate the method. Conclusion Long-term extrapolation from immature trial data may lead to significantly different estimates with various modelling assumptions. The blending approach provides sufficient flexibility, allowing a wide range of plausible scenarios to be considered as well as the inclusion of genuine external information, based e.g. on hard data or expert opinion. Both internal and external validity can be carefully examined.|http://arxiv.org/abs/2206.00154v1|Zhaojing Che,Nathan Green,Gianluca Baio
833|Bayesian sample sizes for exploratory clinical trials comparing multiple experimental treatments with a control|In this paper, a Bayesian approach is developed for simultaneously comparing multiple experimental treatments with a common control treatment in an exploratory clinical trial. The sample size is set to ensure that, at the end of the study, there will be at least one treatment for which the investigators have a strong belief that it is better than control, or else they have a strong belief that none of the experimental treatments are substantially better than control. This criterion bears a direct relationship with conventional frequentist power requirements, while allowing prior opinion to feature in the analysis with a consequent reduction in sample size. If it is concluded that at least one of the experimental treatments shows promise, then it is envisaged that one or more of these promising treatments will be developed further in a definitive phase III trial. The approach is developed in the context of normally distributed responses sharing a common standard deviation regardless of treatment. To begin with, the standard deviation will be assumed known when the sample size is calculated. The final analysis will not rely upon this assumption, although the intended properties of the design may not be achieved if the anticipated standard deviation turns out to be inappropriate. Methods that formally allow for uncertainty about the standard deviation, expressed in the form of a Bayesian prior, are then explored. Illustrations of the sample sizes computed from the new method are presented, and comparisons are made with frequentist methods devised for the same situation.|http://arxiv.org/abs/1408.6211v1|John Whitehead,Faye Cleary,Amanda Turner
834|Bayesian Meta-Analysis of Multiple Continuous Treatments: An Application to Antipsychotic Drugs|Modeling dose-response relationships of drugs is essential to understanding their effect on patient outcomes under realistic circumstances. While intention-to-treat analyses of clinical trials provide the effect of assignment to a particular drug and dose, they do not capture observed exposure after factoring in non-adherence and dropout. We develop Bayesian methods to flexibly model dose-response relationships of binary outcomes with continuous treatment, allowing for treatment effect heterogeneity and a non-linear response surface. We use a hierarchical framework for meta-analysis with the explicit goal of combining information from multiple trials while accounting for heterogeneity. In an application, we examine the risk of excessive weight gain for patients with schizophrenia treated with the second generation antipsychotics paliperidone, risperidone, or olanzapine in 14 clinical trials. Averaging over the sample population, we found that olanzapine contributed to a 15.6% (95% CrI: 6.7, 27.1) excess risk of weight gain at a 500mg cumulative dose. Paliperidone conferred a 3.2% (95% CrI: 1.5, 5.2) and risperidone a 14.9% (95% CrI: 0.0, 38.7) excess risk at 500mg olanzapine equivalent cumulative doses. Blacks had an additional 6.8% (95% CrI: 1.0, 12.4) risk of weight gain over non-blacks at 1000mg olanzapine equivalent cumulative doses of paliperidone.|http://arxiv.org/abs/1802.05186v1|Jacob Spertus,Marcela Horvitz-Lennon,Sharon-Lise Normand
835|Robust Optimal Design of Two-Armed Trials with Side Information|Significant evidence has become available that emphasizes the importance of personalization in medicine. In fact, it has become a common belief that personalized medicine is the future of medicine. The core of personalized medicine is the ability to design clinical trials that investigate the role of patient covariates on treatment effects. In this work, we study the optimal design of two-armed clinical trials to maximize accuracy of statistical models where the interaction between patient covariates and treatment effect are incorporated to enable precision medication. Such a modeling extension leads to significant complexities for the produced optimization problems because they include optimization over design and covariates concurrently. We take a robust optimization approach and minimize (over design) the maximum (over population) variance of interaction effect between treatment and patient covariates. This results in a min-max bi-level mixed integer nonlinear programming problem, which is notably challenging to solve. To address this challenge, we introduce a surrogate model by approximating the objective function for which we propose two solution approaches. The first approach provides an exact solution based on reformulation and decomposition techniques. In the second approach, we provide a lower bound for the inner optimization problem and solve the outer optimization problem over the lower bound. We test our proposed algorithms with synthetic and real-world data sets and compare it with standard (re-)randomization methods. Our numerical analysis suggests that the lower bounding approach provides high-quality solutions across a variety of settings.|http://arxiv.org/abs/2002.01095v2|Qiong Zhang,Amin Khademi,Yongjia Song
836|Propensity Score Weighting for Covariate Adjustment in Randomized Clinical Trials|Chance imbalance in baseline characteristics is common in randomized clinical trials. Regression adjustment such as the analysis of covariance (ANCOVA) is often used to account for imbalance and increase precision of the treatment effect estimate. An objective alternative is through inverse probability weighting (IPW) of the propensity scores. Although IPW and ANCOVA are asymptotically equivalent, the former may demonstrate inferior performance in finite samples. In this article, we point out that IPW is a special case of the general class of balancing weights, and advocate to use overlap weighting (OW) for covariate adjustment. The OW method has a unique advantage of completely removing chance imbalance when the propensity score is estimated by logistic regression. We show that the OW estimator attains the same semiparametric variance lower bound as the most efficient ANCOVA estimator and the IPW estimator for a continuous outcome, and derive closed-form variance estimators for OW when estimating additive and ratio estimands. Through extensive simulations, we demonstrate OW consistently outperforms IPW in finite samples and improves the efficiency over ANCOVA and augmented IPW when the degree of treatment effect heterogeneity is moderate or when the outcome model is incorrectly specified. We apply the proposed OW estimator to the Best Apnea Interventions for Research (BestAIR) randomized trial to evaluate the effect of continuous positive airway pressure on patient health outcomes. All the discussed propensity score weighting methods are implemented in the R package PSweight.|http://arxiv.org/abs/2004.10075v2|Shuxi Zeng,Fan Li,Rui Wang,Fan Li
837|Incorporating historical information to improve phase I clinical trial designs|Incorporating historical data or real-world evidence has a great potential to improve the efficiency of phase I clinical trials and to accelerate drug development. For model-based designs, such as the continuous reassessment method (CRM), this can be conveniently carried out by specifying a "skeleton," i.e., the prior estimate of dose limiting toxicity (DLT) probability at each dose. In contrast, little work has been done to incorporate historical data or real-world evidence into model-assisted designs, such as the Bayesian optimal interval (BOIN), keyboard, and modified toxicity probability interval (mTPI) designs. This has led to the misconception that model-assisted designs cannot incorporate prior information. In this paper, we propose a unified framework that allows for incorporating historical data or real-world evidence into model-assisted designs. The proposed approach uses the well-established "skeleton" approach, combined with the concept of prior effective sample size, thus it is easy to understand and use. More importantly, our approach maintains the hallmark of model-assisted designs: simplicity---the dose escalation/de-escalation rule can be tabulated prior to the trial conduct. Extensive simulation studies show that the proposed method can effectively incorporate prior information to improve the operating characteristics of model-assisted designs, similarly to model-based designs.|http://arxiv.org/abs/2004.12972v3|Yanhong Zhou,J. Jack Lee,Shunguang Wang,Stuart Bailey,Ying Yuan
838|Survival Analysis Using a 5-Step Stratified Testing and Amalgamation Routine in Randomized Clinical Trials|Randomized clinical trials are often designed to assess whether a test treatment prolongs survival relative to a control treatment. Increased patient heterogeneity, while desirable for generalizability of results, can weaken the ability of common statistical approaches to detect treatment differences, potentially hampering the regulatory approval of safe and efficacious therapies. A novel solution to this problem is proposed. A list of baseline covariates that have the potential to be prognostic for survival under either treatment is pre-specified in the analysis plan. At the analysis stage, using all observed survival times but blinded to patient-level treatment assignment, 'noise' covariates are removed with elastic net Cox regression. The shortened covariate list is used by a conditional inference tree algorithm to segment the heterogeneous trial population into subpopulations of prognostically homogeneous patients (risk strata). After patient-level treatment unblinding, a treatment comparison is done within each formed risk stratum and stratum-level results are combined for overall statistical inference. The impressive power-boosting performance of our proposed 5-step stratified testing and amalgamation routine (5-STAR), relative to that of the logrank test and other common approaches that do not leverage inherently structured patient heterogeneity, is illustrated using a hypothetical and two real datasets along with simulation results. Furthermore, the importance of reporting stratum-level comparative treatment effects (time ratios from accelerated failure time model fits in conjunction with model averaging and, as needed, hazard ratios from Cox proportional hazard model fits) is highlighted as a potential enabler of personalized medicine. A fiveSTAR R package is available at https://github.com/rmarceauwest/fiveSTAR.|http://arxiv.org/abs/2004.13611v1|Devan V. Mehrotra,Rachel Marceau West
839|Defining Estimands Using a Mix of Strategies to Handle Intercurrent Events in Clinical Trials|Randomized controlled trials (RCT) are the gold standard for evaluation of the efficacy and safety of investigational interventions. If every patient in an RCT were to adhere to the randomized treatment, one could simply analyze the complete data to infer the treatment effect. However, intercurrent events (ICEs) including the use of concomitant medication for unsatisfactory efficacy, treatment discontinuation due to adverse events, or lack of efficacy, may lead to interventions that deviate from the original treatment assignment. Therefore, defining the appropriate estimand (the appropriate parameter to be estimated) based on the primary objective of the study is critical prior to determining the statistical analysis method and analyzing the data. The International Council for Harmonisation (ICH) E9 (R1), published on November 20, 2019, provided 5 strategies to define the estimand: treatment policy, hypothetical, composite variable, while on treatment and principal stratum. In this article, we propose an estimand using a mix of strategies in handling ICEs. This estimand is an average of the null treatment difference for those with ICEs potentially related to safety and the treatment difference for the other patients if they would complete the assigned treatments. Two examples from clinical trials evaluating anti-diabetes treatments are provided to illustrate the estimation of this proposed estimand and to compare it with the estimates for estimands using hypothetical and treatment policy strategies in handling ICEs.|http://arxiv.org/abs/2006.03105v1|Yongming Qu,Linda Shurzinske,Shanthi Sethuraman
840|Statistical design considerations for trials that study multiple indications|Breakthroughs in cancer biology have defined new research programs emphasizing the development of therapies that target specific pathways in tumor cells. Innovations in clinical trial design have followed with master protocols defined by inclusive eligibility criteria and evaluations of multiple therapies and/or histologies. Consequently, characterization of subpopulation heterogeneity has become central to the formulation and selection of a study design. However, this transition to master protocols has led to challenges in identifying the optimal trial design and proper calibration of hyperparameters. We often evaluate a range of null and alternative scenarios, however there has been little guidance on how to synthesize the potentially disparate recommendations for what may be optimal. This may lead to the selection of suboptimal designs and statistical methods that do not fully accommodate the subpopulation heterogeneity. This article proposes novel optimization criteria for calibrating and evaluating candidate statistical designs of master protocols in the presence of the potential for treatment effect heterogeneity among enrolled patient subpopulations. The framework is applied to demonstrate the statistical properties of conventional study designs when treatments offer heterogeneous benefit as well as identify optimal designs devised to monitor the potential for heterogeneity among patients with differing clinical indications using Bayesian modeling.|http://arxiv.org/abs/2007.03792v1|Alexander M. Kaizer,Joseph S. Koopmeiners,Nan Chen,Brian P. Hobbs
841|Toward Better Practice of Covariate Adjustment in Analyzing Randomized Clinical Trials|In randomized clinical trials, adjustments for baseline covariates at both design and analysis stages are highly encouraged by regulatory agencies. A recent trend is to use a model-assisted approach for covariate adjustment to gain credibility and efficiency while producing asymptotically valid inference even when the model is incorrect. In this article we present three considerations for better practice when model-assisted inference is applied to adjust for covariates under simple or covariate-adaptive randomized trials: (1) guaranteed efficiency gain: a model-assisted method should often gain but never hurt efficiency; (2) wide applicability: a valid procedure should be applicable, and preferably universally applicable, to all commonly used randomization schemes; (3) robust standard error: variance estimation should be robust to model misspecification and heteroscedasticity. To achieve these, we recommend a model-assisted estimator under an analysis of heterogeneous covariance working model including all covariates utilized in randomization. Our conclusions are based on an asymptotic theory that provides a clear picture of how covariate-adaptive randomization and regression adjustment alter statistical efficiency. Our theory is more general than the existing ones in terms of studying arbitrary functions of response means (including linear contrasts, ratios, and odds ratios), multiple arms, guaranteed efficiency gain, optimality, and universal applicability.|http://arxiv.org/abs/2009.11828v2|Ting Ye,Jun Shao,Yanyao Yi,Qingyuan Zhao
842|A Bayesian framework for patient-level partitioned survival cost-utility analysis|Patient-level health economic data collected alongside clinical trials are an important component of the process of technology appraisal, with a view to informing resource allocation decisions. For end of life treatments, such as cancer treatments, modelling of cost-effectiveness/utility data may involve some form of partitioned survival analysis, where measures of health-related quality of life and survival time for both pre- and post-progression periods are combined to generate some aggregate measure of clinical benefits (e.g. quality-adjusted survival). In addition, resource use data are often collected from health records on different services from which different cost components are obtained (e.g. treatment, hospital or adverse events costs). A critical problem in these analyses is that both effectiveness and cost data present some complexities, including non-normality, spikes, and missingness, that should be addressed using appropriate methods. Bayesian modelling provides a powerful tool which has become more and more popular in the recent health economics and statistical literature to jointly handle these issues in a relatively easy way. This paper presents a general Bayesian framework that takes into account the complex relationships of trial-based partitioned survival cost-utility data, potentially providing a more adequate evidence for policymakers to inform the decision-making process. Our approach is motivated by, and applied to, a working example based on data from a trial assessing the cost-effectiveness of a new treatment for patients with advanced non-small-cell lung cancer.|http://arxiv.org/abs/2011.10732v1|Andrea Gabrio
843|Improving the assessment of the probability of success in late stage drug development|There are several steps to confirming the safety and efficacy of a new medicine. A sequence of trials, each with its own objectives, is usually required. Quantitative risk metrics can be useful for informing decisions about whether a medicine should transition from one stage of development to the next. To obtain an estimate of the probability of regulatory approval, pharmaceutical companies may start with industry-wide success rates and then apply to these subjective adjustments to reflect program-specific information. However, this approach lacks transparency and fails to make full use of data from previous clinical trials. We describe a quantitative Bayesian approach for calculating the probability of success (PoS) at the end of phase II which incorporates internal clinical data from one or more phase IIb studies, industry-wide success rates, and expert opinion or external data if needed. Using an example, we illustrate how PoS can be calculated accounting for differences between the phase IIb data and future phase III trials, and discuss how the methods can be extended to accommodate accelerated drug development pathways.|http://arxiv.org/abs/2102.02752v2|Lisa V Hampson,Bjrn Bornkamp,Bjrn Holzhauer,Joseph Kahn,Markus R Lange,Wen-Lin Luo,Giovanni Della Cioppa,Kelvin Stott,Steffen Ballerstedt
844|Studentized Permutation Method for Comparing Restricted Mean Survival Times with Small Sample from Randomized Trials|Recent observations, especially in cancer immunotherapy clinical trials with time-to-event outcomes, show that the commonly used proportial hazard assumption is often not justifiable, hampering an appropriate analyse of the data by hazard ratios. An attractive alternative advocated is given by the restricted mean survival time (RMST), which does not rely on any model assumption and can always be interpreted intuitively. As pointed out recently by Horiguchi and Uno (2020), methods for the RMST based on asymptotic theory suffer from inflated type-I error under small sample sizes. To overcome this problem, they suggested a permutation strategy leading to more convincing results in simulations. However, their proposal requires an exchangeable data set-up between comparison groups which may be limiting in practice. In addition, it is not possible to invert their testing procedure to obtain valid confidence intervals, which can provide more in-depth information. In this paper, we address these limitations by proposing a studentized permutation test as well as the corresponding permutation-based confidence intervals. In our extensive simulation study, we demonstrate the advantage of our new method, especially in situations with relative small sample sizes and unbalanced groups. Finally we illustrate the application of the proposed method by re-analysing data from a recent lung cancer clinical trial.|http://arxiv.org/abs/2102.10186v1|Marc Ditzhaus,Menggang Yu,Jin Xu
845|An introduction to the determination of the probability of a successful trial: Frequentist and Bayesian approaches|Determination of posterior probability for go-no-go decision and predictive power are becoming increasingly common for resource optimization in clinical investigation. There are vast published literature on these topics; however, the terminologies are not consistently used across the literature. Further, there is a lack of consolidated presentation of various concepts of the probability of success. We attempted to fill this gap. This paper first provides a detailed derivation of these probability of success measures under the frequentist and Bayesian paradigms in a general setting. Subsequently, we have presented the analytical formula for these probability of success measures for continuous, binary, and time-to-event endpoints separately. This paper can be used as a single point reference to determine the following measures: (a) the conditional power (CP) based on interim results, (b) the predictive power of success (PPoS) based on interim results with or without prior distribution, and (d) the probability of success (PoS) for a prospective trial at the design stage. We have discussed both clinical success and trial success. This paper's discussion is mostly based on the normal approximation for prior distribution and the estimate of the parameter of interest. Besides, predictive power using the beta prior for the binomial case is also presented. Some examples are given for illustration. R functions to calculate CP and PPoS are available through the LongCART package. An R shiny app is also available at https://ppos.herokuapp.com/.|http://arxiv.org/abs/2102.13550v2|Madan G. Kundu,Sandipan Samanta,Shoubhik Mondal
846|Non-constant hazard ratios in randomized controlled trials with composite endpoints|The hazard ratio is routinely used as a summary measure to assess the treatment effect in clinical trials with time-to-event endpoints. It is frequently assumed as constant over time although this assumption often does not hold. When the hazard ratio deviates considerably from being constant, the average of its plausible values is not a valid measure of the treatment effect, can be clinically misleading and common sample size formulas are not appropriate.   In this paper, we study the hazard ratio along time of a two-component composite endpoint under the assumption that the hazard ratio for each component is constant.   This work considers two measures for quantifying the non-proportionality of the hazard ratio: the difference $D$ between the maximum and minimum values of hazard ratio over time and the relative measure $R$ representing the ratio between the sample sizes for the minimum detectable and the average effects. We illustrate $D$ and $R$ by means of the ZODIAC trial where the primary endpoint was progression-free survival.   We have run a simulation study deriving scenarios for different values of the hazard ratios, different event rates and different degrees of association between the components. We illustrate situations that yield non-constant hazard ratios for the composite endpoints and consider the likely impact on sample size.   Results show that the distance between the two component hazard ratios plays an important role, especially when they are close to 1. Furthermore, even when the treatment effects for each component are similar, if the two-component hazards are markedly different, hazard ratio of the composite is often non-constant.|http://arxiv.org/abs/1907.10976v1|Jordi Corts Martnez,Moiss Gmez Mateu,KyungMann Kim,Guadalupe Gmez Melis
847|Sample size calculations for single-arm survival studies using transformations of the Kaplan-Meier estimator|In single-arm clinical trials with survival outcomes, the Kaplan-Meier estimator and its confidence interval are widely used to assess survival probability and median survival time. Since the asymptotic normality of the Kaplan-Meier estimator is a common result, the sample size calculation methods have not been studied in depth. An existing sample size calculation method is founded on the asymptotic normality of the Kaplan-Meier estimator using the log transformation. However, the small sample properties of the log transformed estimator are quite poor in small sample sizes (which are typical situations in single-arm trials), and the existing method uses an inappropriate standard normal approximation to calculate sample sizes. These issues can seriously influence the accuracy of results. In this paper, we propose alternative methods to determine sample sizes based on a valid standard normal approximation with several transformations that may give an accurate normal approximation even with small sample sizes. In numerical evaluations via simulations, some of the proposed methods provided more accurate results, and the empirical power of the proposed method with the arcsine square-root transformation tended to be closer to a prescribed power than the other transformations. These results were supported when methods were applied to data from three clinical trials.|http://arxiv.org/abs/2012.03355v2|Kengo Nagashima,Hisashi Noma,Yasunori Sato,Masahiko Gosho
848|Connecting Instrumental Variable methods for causal inference to the Estimand Framework|Causal inference methods are gaining increasing prominence in pharmaceutical drug development in light of the recently published addendum on estimands and sensitivity analysis in clinical trials to the E9 guideline of the International Council for Harmonisation. The E9 addendum emphasises the need to account for post-randomization or `intercurrent' events that can potentially influence the interpretation of a treatment effect estimate at a trial's conclusion. Instrumental Variables (IV) methods have been used extensively in economics, epidemiology and academic clinical studies for `causal inference', but less so in the pharmaceutical industry setting until now. In this tutorial paper we review the basic tools for causal inference, including graphical diagrams and potential outcomes, as well as several conceptual frameworks that an IV analysis can sit within. We discuss in detail how to map these approaches to the Treatment Policy, Principal Stratum and Hypothetical `estimand strategies' introduced in the E9 addendum, and provide details of their implementation using standard regression models. Specific attention is given to discussing the assumptions each estimation strategy relies on in order to be consistent, the extent to which they can be empirically tested and sensitivity analyses in which specific assumptions can be relaxed. We finish by applying the methods described to simulated data closely matching two recent pharmaceutical trials to further motivate and clarify the ideas|http://arxiv.org/abs/2012.03786v2|Jack Bowden,Bjoern Bornkamp,Ekkehard Glimm,Frank Bretz
849|SDF-Bayes: Cautious Optimism in Safe Dose-Finding Clinical Trials with Drug Combinations and Heterogeneous Patient Groups|Phase I clinical trials are designed to test the safety (non-toxicity) of drugs and find the maximum tolerated dose (MTD). This task becomes significantly more challenging when multiple-drug dose-combinations (DC) are involved, due to the inherent conflict between the exponentially increasing DC candidates and the limited patient budget. This paper proposes a novel Bayesian design, SDF-Bayes, for finding the MTD for drug combinations in the presence of safety constraints. Rather than the conventional principle of escalating or de-escalating the current dose of one drug (perhaps alternating between drugs), SDF-Bayes proceeds by cautious optimism: it chooses the next DC that, on the basis of current information, is most likely to be the MTD (optimism), subject to the constraint that it only chooses DCs that have a high probability of being safe (caution). We also propose an extension, SDF-Bayes-AR, that accounts for patient heterogeneity and enables heterogeneous patient recruitment. Extensive experiments based on both synthetic and real-world datasets demonstrate the advantages of SDF-Bayes over state of the art DC trial designs in terms of accuracy and safety.|http://arxiv.org/abs/2101.10998v1|Hyun-Suk Lee,Cong Shen,William Zame,Jang-Won Lee,Mihaela van der Schaar
850|Efficient nonparametric estimation of the covariate-adjusted threshold-response function, a support-restricted stochastic intervention|Identifying a biomarker or treatment-dose threshold that marks a specified level of risk is an important problem, especially in clinical trials. This risk, viewed as a function of thresholds and possibly adjusted for covariates, we call the threshold-response function. Extending the work of Donovan, Hudgens and Gilbert (2019), we propose a nonparametric efficient estimator for the covariate-adjusted threshold-response function, which utilizes machine learning and Targeted Minimum-Loss Estimation (TMLE). We additionally propose a more general estimator, based on sequential regression, that also applies when there is outcome missingness. We show that the threshold-response for a given threshold may be viewed as the expected outcome under a stochastic intervention where all participants are given a treatment dose above the threshold. We prove the estimator is efficient and characterize its asymptotic distribution. A method to construct simultaneous 95% confidence bands for the threshold-response function and its inverse is given. Furthermore, we discuss how to adjust our estimator when the treatment or biomarker is missing-at-random, as is the case in clinical trials with biased sampling designs, using inverse-probability-weighting. The methods are assessed in a diverse set of simulation settings with rare outcomes and cumulative case-control sampling. The methods are employed to estimate neutralizing antibody thresholds for virologically confirmed dengue risk in the CYD14 and CYD15 dengue vaccine trials.|http://arxiv.org/abs/2107.11459v2|Lars van der Laan,Wenbo Zhang,Peter B. Gilbert
851|Adjusting for publication bias in meta-analysis via inverse probability weighting using clinical trial registries|Publication bias is a major concern in conducting systematic reviews and meta-analyses. Various sensitivity analysis or bias-correction methods have been developed based on selection models and they have some advantages over the widely used bias-correction method of the trim-and-fill method. However, likelihood methods based on selection models may have difficulty in obtaining precise estimates and reasonable confidence intervals or require a complicated sensitivity analysis process. In this paper, we develop a simple publication bias adjustment method utilizing information on conducted but still unpublished trials from clinical trial registries. We introduce an estimating equation for parameter estimation in the selection function by regarding the publication bias issue as a missing data problem under missing not at random. With the estimated selection function, we introduce the inverse probability weighting (IPW) method to estimate the overall mean across studies. Furthermore, the IPW versions of heterogeneity measures such as the between-study variance and the I2 measure are proposed. We propose methods to construct asymptotic confidence intervals and suggest intervals based on parametric bootstrapping as an alternative. Through numerical experiments, we observed that the estimators successfully eliminate biases and the confidence intervals had empirical coverage probabilities close to the nominal level. On the other hand, the asymptotic confidence interval is much wider in some scenarios than the bootstrap confidence interval. Therefore, the latter is recommended for practical use.|http://arxiv.org/abs/2109.12526v1|Ao Huang,Kosuke Morikawa,Tim Friede,Satoshi Hattori
852|Translating questions to estimands in randomized clinical trials with intercurrent events|Intercurrent (post-treatment) events occur frequently in randomized trials, and investigators often express interest in treatment effects that suitably take account of these events. A naive conditioning on intercurrent events does not have a straight-forward causal interpretation, and the practical relevance of other commonly used approaches is debated. In this work, we discuss how to formulate and choose an estimand, beyond the marginal intention to treat effect, from the point of view of a decision maker and drug developer. In particular, we argue that careful articulation of a practically useful research question should either reflect decision making at this point in time or future drug development. Indeed, a substantially interesting estimand is a formalization of the (plain English) description of a research question. A common feature of estimands that are practically useful is that they correspond to possibly hypothetical but well-defined interventions in identifiable (sub)populations. To illustrate our points, we consider five examples that were recently used to motivate consideration of principal stratum estimands in clinical trials. In all of these examples, we propose alternative causal estimands, such as conditional effects, sequential regime effects and separable effects, that correspond to explicit research questions of substantial interest. Certain questions require stronger assumptions for identification. However, we highlight that our proposed estimands require less stringent assumptions than estimands commonly targeted in these settings, including principal stratum effects.|http://arxiv.org/abs/2111.08509v1|Mats J. Stensrud,Oliver Dukes
853|Assessing the commonly used assumptions in estimating the principal causal effect in clinical trials|In clinical trials, it is often of interest to understand the principal causal effect (PCE), the average treatment effect for a principal stratum (a subset of patients defined by the potential outcomes of one or more post-baseline variables). Commonly used assumptions include monotonicity, principal ignorability, and cross-world assumptions of principal ignorability and principal strata independence. In this article, we evaluate these assumptions through a 2$\times$2 cross-over study in which the potential outcomes under both treatments can be observed, provided there are no carry-over and study period effects. From this example, it seemed the monotonicity assumption and the within-treatment principal ignorability assumptions did not hold well. On the other hand, the assumptions of cross-world principal ignorability and cross-world principal stratum independence conditional on baseline covariates seemed reasonable. With the latter assumptions, we estimated the PCEs, defined by whether the blood glucose standard deviation increased in each treatment period, without relying on the cross-over feature, producing estimates close to the results when exploiting the cross-over feature. To the best of our knowledge, this article is the first attempt to evaluate the plausibility of commonly used assumptions for estimating PCEs using a cross-over trial.|http://arxiv.org/abs/2111.10938v3|Yongming Qu,Ilya Lipkovich,Stephen J. Ruberg
854|A Flexible Approach for Predictive Biomarker Discovery|An endeavor central to precision medicine is predictive biomarker discovery; they define patient subpopulations which stand to benefit most, or least, from a given treatment. The identification of these biomarkers is often the byproduct of the related but fundamentally different task of treatment rule estimation. Using treatment rule estimation methods to identify predictive biomarkers in clinical trials where the number of covariates exceeds the number of participants often results in high false discovery rates. The higher than expected number of false positives translates to wasted resources when conducting follow-up experiments for drug target identification and diagnostic assay development. Patient outcomes are in turn negatively affected. We propose a variable importance parameter for directly assessing the importance of potentially predictive biomarkers, and develop a flexible nonparametric inference procedure for this estimand. We prove that our estimator is double-robust and asymptotically linear under loose conditions on the data-generating process, permitting valid inference about the importance metric. The statistical guarantees of the method are verified in a thorough simulation study representative of randomized control trials with moderate and high-dimensional covariate vectors. Our procedure is then used to discover predictive biomarkers from among the tumor gene expression data of metastatic renal cell carcinoma patients enrolled in recently completed clinical trials. We find that our approach more readily discerns predictive from non-predictive biomarkers than procedures whose primary purpose is treatment rule estimation. An open-source software implementation of the methodology, the uniCATE R package, is briefly introduced.|http://arxiv.org/abs/2205.01285v2|Philippe Boileau,Nina Ting Qi,Mark J. van der Laan,Sandrine Dudoit,Ning Leng
855|A whole-body multi-scale mathematical model for dynamic simulation of the metabolism in man|We propose a whole-body model of the metabolism in man as well as a generalized approach for modeling metabolic networks. Using this approach, we are able to write a large metabolic network in a systematic and compact way. We demonstrate the approach using a whole-body model of the metabolism of the three macronutrients, carbohydrates, proteins and lipids. The model contains 7 organs, 16 metabolites and 31 enzymatic reactions. All reaction rates are described by Michaelis-Menten kinetics with an addition of a hormonal regulator based on the two hormones insulin and glucagon. We incorporate ingestion of food in order to simulate metabolite concentrations during the feed-fast cycle. The model can simulate several days due to the inclusion of storage forms (glycogen, muscle protein and lipid droplets), that can be depleted if food is not ingested regularly. A physiological model incorporating complex cellular metabolism and whole-body mass dynamics can be used in virtual clinical trials. Such trials can be used to improve the development of medicine, treatment strategies such as control algorithms, and increase the likelihood of a successful clinical trial.|http://arxiv.org/abs/2205.01473v1|Peter Emil Carstensen,Jacob Bendsen,Asbjrn Thode Reenberg,Tobias K. S. Ritschel,John Bagterp Jrgensen
856|Current status of antihistamines repurposing for infectious diseases|Objectives. This review gathers information on the potential role of antihistamines as anti-infective agents and identifies gaps in research that have impaired its applicability in human health. Methods. The literature search encompassed MEDLINE, PubMed and Google Scholar from 1990 to 2022. Results. The literature search identified 12 antihistamines with activity against different pathogens. Eight molecules were second-generation antihistamines with intrinsically lower tendency to cross the blood brain barrier thereby with reduced side effects. Only five antihistamines had in vivo evaluations in rodents while one study utilized a wax moth model to determine astemizole anti-Cryptococcus sp. activity combined with fluconazole. In vitro studies showed that clemastine was active against Plasmodium, Leishmania, and Trypanosoma, while terfenadine suppressed Candida spp. and Staphylococcus aureus growth. In vitro assays found that SARS-coV-2 was inhibited by doxepin, azelastine, desloratadine, and clemastine. Different antihistamines inhibited Ebola virus (diphenhydramine, chlorcyclizine), Hepatitis C virus (chlorcyclizine), and Influenza virus (carbinoxamine, chlorpheniramine). Generally, in vitro activity (IC50) of antihistamines was in the low to sub-microM range, except for Staphylococcus epidermidis (loratadine MIC=50 microM) and SARS-coV-2 (desloratadine 70% inhibition at 20 microM). Conclusion. Many antihistamine drugs showed potential to progress to clinical trials based on in vitro data and availability of toxicological and pharmacological data. However, the overall lack of systematic preclinical trials has hampered the advance of repurposed antihistamines for off label evaluation. The low interest of pharmaceutical companies has to be counterbalanced through collaborations between research groups, granting agencies and government to support the needed clinical trials.|http://arxiv.org/abs/2206.03454v1|Bruno L. Travi
857|Quantification of follow-up time in oncology clinical trials with a time-to-event endpoint: Asking the right questions|For the analysis of a time-to-event endpoint in a single-arm or randomized clinical trial it is generally perceived that interpretation of a given estimate of the survival function, or the comparison between two groups, hinges on some quantification of the amount of follow-up. Typically, a median of some loosely defined quantity is reported. However, whatever median is reported, is typically not answering the question(s) trialists actually have in terms of follow-up quantification. In this paper, inspired by the estimand framework, we formulate a comprehensive list of relevant scientific questions that trialists have when reporting time-to-event data. We illustrate how these questions should be answered, and that reference to an unclearly defined follow-up quantity is not needed at all. In drug development, key decisions are made based on randomized controlled trials, and we therefore also discuss relevant scientific questions not only when looking at a time-to-event endpoint in one group, but also for comparisons. We find that different thinking about some of the relevant scientific questions around follow-up is required depending on whether a proportional hazards assumption can be made or other patterns of survival functions are anticipated, e.g. delayed separation, crossing survival functions, or the potential for cure. We conclude the paper with practical recommendations.|http://arxiv.org/abs/2206.05216v3|Kaspar Rufibach,Lynda Grinsted,Jiang Li,Hans-Jochen Weber,Cheng Zheng,Jiangxiu Zhou
858|Towards Optimal Use of Surrogate Markers to Improve Power|Motivated by increasing pressure for decision makers to shorten the time required to evaluate the efficacy of a treatment such that treatments deemed safe and effective can be made publicly available, there has been substantial recent interest in using an earlier or easier to measure surrogate marker, $S$, in place of the primary outcome, $Y$. To validate the utility of a surrogate marker in these settings, a commonly advocated measure is the proportion of treatment effect on the primary outcome that is explained by the treatment effect on the surrogate marker (PTE). Model based and model free estimators for PTE have also been developed. While this measure is very intuitive, it does not directly address the important questions of how $S$ can be used to make inference of the unavailable $Y$ in the next phase clinical trials. In this paper, to optimally use the information of surrogate S, we provide a framework for deriving an optimal transformation of $S$, $g_{opt}(S)$, such that the treatment effect on $g_{opt}(S)$ maximally approximates the treatment effect on $Y$ in a certain sense. Based on the optimally transformed surrogate, $g_{opt}(S)$, we propose a new measure to quantify surrogacy, the relative power (RP), and demonstrate how RP can be used to make decisions with $S$ instead of $Y$ for next phase trials. We propose nonparametric estimation procedures, derive asymptotic properties, and compare the RP measure with the PTE measure. Finite sample performance of our estimators is assessed via a simulation study. We illustrate our proposed procedures using an application to the Diabetes Prevention Program (DPP) clinical trial to evaluate the utility of hemoglobin A1c and fasting plasma glucose as surrogate markers for diabetes.|http://arxiv.org/abs/2209.08414v1|Xuan Wang,Layla Parast,Lu Tian,Tianxi Cai
859|Confirmatory adaptive group sequential designs for clinical trials with multiple time-to-event outcomes in Markov models|The analysis of multiple time-to-event outcomes in a randomised controlled clinical trial can be accomplished with exisiting methods. However, depending on the characteristics of the disease under investigation and the circumstances in which the study is planned, it may be of interest to conduct interim analyses and adapt the study design if necessary. Due to the expected dependency of the endpoints, the full available information on the involved endpoints may not be used for this purpose. We suggest a solution to this problem by embedding the endpoints in a multi-state model. If this model is Markovian, it is possible to take the disease history of the patients into account and allow for data-dependent design adaptiations. To this end, we introduce a flexible test procedure for a variety of applications, but are particularly concerned with the simultaneous consideration of progression-free survival (PFS) and overall survival (OS). This setting is of key interest in oncological trials. We conduct simulation studies to determine the properties for small sample sizes and demonstrate an application based on data from the NB2004-HR study.|http://arxiv.org/abs/2306.16056v1|Moritz Fabian Danzer,Andreas Faldum,Thorsten Simon,Barbara Hero,Rene Schmidt
860|A Bayesian hierarchical mixture cure modelling framework to utilize multiple survival datasets for long-term survivorship estimates: A case study from previously untreated metastatic melanoma|Time to an event of interest over a lifetime is a central measure of the clinical benefit of an intervention used in a health technology assessment (HTA). Within the same trial, multiple end-points may also be considered. For example, overall and progression-free survival time for different drugs in oncology studies. A common challenge is when an intervention is only effective for some proportion of the population who are not clinically identifiable. Therefore, latent group membership as well as separate survival models for identified groups need to be estimated. However, follow-up in trials may be relatively short leading to substantial censoring. We present a general Bayesian hierarchical framework that can handle this complexity by exploiting the similarity of cure fractions between end-points; accounting for the correlation between them and improving the extrapolation beyond the observed data. Assuming exchangeability between cure fractions facilitates the borrowing of information between end-points. We undertake a comprehensive simulation study to evaluate the model performance under different scenarios. We also show the benefits of using our approach with a motivating example, the CheckMate 067 phase 3 trial consisting of patients with metastatic melanoma treated with first line therapy.|http://arxiv.org/abs/2401.13820v2|Nathan Green,Murat Kurt,Andriy Moshyk,James Larkin,Gianluca Baio
861|A Bayesian Approach for Selecting Relevant External Data (BASE): Application to a study of Long-Term Outcomes in a Hemophilia Gene Therapy Trial|Gene therapies aim to address the root causes of diseases, particularly those stemming from rare genetic defects that can be life-threatening or severely debilitating. While there has been notable progress in the development of gene therapies in recent years, understanding their long-term effectiveness remains challenging due to a lack of data on long-term outcomes, especially during the early stages of their introduction to the market. To address the critical question of estimating long-term efficacy without waiting for the completion of lengthy clinical trials, we propose a novel Bayesian framework. This framework selects pertinent data from external sources, often early-phase clinical trials with more comprehensive longitudinal efficacy data that could lead to an improved inference of the long-term efficacy outcome. We apply this methodology to predict the long-term factor IX (FIX) levels of HEMGENIX (etranacogene dezaparvovec), the first FDA-approved gene therapy to treat adults with severe Hemophilia B, in a phase 3 study. Our application showcases the capability of the framework to estimate the 5-year FIX levels following HEMGENIX therapy, demonstrating sustained FIX levels induced by HEMGENIX infusion. Additionally, we provide theoretical insights into the methodology by establishing its posterior convergence properties.|http://arxiv.org/abs/2403.13260v2|Tianyu Pan,Xiang Zhang,Weining Shen,Ting Ye
862|Finite-sample adjustments for comparing clustered adaptive interventions using data from a clustered SMART|Adaptive interventions, aka dynamic treatment regimens, are sequences of pre-specified decision rules that guide the provision of treatment for an individual given information about their baseline and evolving needs, including in response to prior intervention. Clustered adaptive interventions (cAIs) extend this idea by guiding the provision of intervention at the level of clusters (e.g., clinics), but with the goal of improving outcomes at the level of individuals within the cluster (e.g., clinicians or patients within clinics). A clustered, sequential multiple-assignment randomized trials (cSMARTs) is a multistage, multilevel randomized trial design used to construct high-quality cAIs. In a cSMART, clusters are randomized at multiple intervention decision points; at each decision point, the randomization probability can depend on response to prior data. A challenge in cluster-randomized trials, including cSMARTs, is the deleterious effect of small samples of clusters on statistical inference, particularly via estimation of standard errors. \par This manuscript develops finite-sample adjustment (FSA) methods for making improved statistical inference about the causal effects of cAIs in a cSMART. The paper develops FSA methods that (i) scale variance estimators using a degree-of-freedom adjustment, (ii) reference a t distribution (instead of a normal), and (iii) employ a ``bias corrected" variance estimator. Method (iii) requires extensions that are unique to the analysis of cSMARTs. Extensive simulation experiments are used to test the performance of the methods. The methods are illustrated using the Adaptive School-based Implementation of CBT (ASIC) study, a cSMART designed to construct a cAI for improving the delivery of cognitive behavioral therapy (CBT) by school mental health professionals within high schools in Michigan.|http://arxiv.org/abs/2405.00185v1|Wenchu Pan,Daniel Almirall,Amy M. Kilbourne,Andrew Quanbeck,Lu Wang
863|Radiogenomic biomarkers for immunotherapy in glioblastoma: A systematic review of magnetic resonance imaging studies|Immunotherapy is an effective precision medicine treatment for several cancers. Imaging signatures of the underlying genome (radiogenomics) in glioblastoma patients may serve as preoperative biomarkers of the tumor-host immune apparatus. Validated biomarkers would have the potential to stratify patients during immunotherapy clinical trials, and if trials are beneficial, facilitate personalized neo-adjuvant treatment. The increased use of whole genome sequencing data, and the advances in bioinformatics and machine learning make such developments plausible. We performed a systematic review to determine the extent of development and validation of immune-related radiogenomic biomarkers for glioblastoma. A systematic review was performed following PRISMA guidelines using the PubMed, Medline, and Embase databases. Qualitative analysis was performed by incorporating the QUADAS 2 tool and CLAIM checklist. PROSPERO registered CRD42022340968. Extracted data were insufficiently homogenous to perform a meta-analysis. Results Nine studies, all retrospective, were included. Biomarkers extracted from magnetic resonance imaging volumes of interest included apparent diffusion coefficient values, relative cerebral blood volume values, and image-derived features. These biomarkers correlated with genomic markers from tumor cells or immune cells or with patient survival. The majority of studies had a high risk of bias and applicability concerns regarding the index test performed. Radiogenomic immune biomarkers have the potential to provide early treatment options to patients with glioblastoma. Targeted immunotherapy, stratified by these biomarkers, has the potential to allow individualized neo-adjuvant precision treatment options in clinical trials. However, there are no prospective studies validating these biomarkers, and interpretation is limited due to study bias with little evidence of generalizability.|http://arxiv.org/abs/2405.07858v1|Prajwal Ghimire,Ben Kinnersley,Golestan Karami,Prabhu Arumugam,Richard Houlston,Keyoumars Ashkan,Marc Modat,Thomas C Booth
864|Replicable Bandits for Digital Health Interventions|Adaptive treatment assignment algorithms, such as bandit and reinforcement learning algorithms, are increasingly used in digital health intervention clinical trials. Causal inference and related data analyses are critical for evaluating digital health interventions, deciding how to refine the intervention, and deciding whether to roll-out the intervention more broadly. However the replicability of these analyses has received relatively little attention. This work investigates the replicability of statistical analyses from trials deploying adaptive treatment assignment algorithms. We demonstrate that many standard statistical estimators can be inconsistent and fail to be replicable across repetitions of the clinical trial, even as the sample size grows large. We show that this non-replicability is intimately related to properties of the adaptive algorithm itself. We introduce a formal definition of a "replicable bandit algorithm" and prove that under such algorithms, a wide variety of common statistical analyses are guaranteed to be consistent. We present both theoretical results and simulation studies based on a mobile health oral health self-care intervention. Our findings underscore the importance of designing adaptive algorithms with replicability in mind, especially for settings like digital health where deployment decisions rely heavily on replicated evidence. We conclude by discussing open questions on the connections between algorithm design, statistical inference, and experimental replicability.|http://arxiv.org/abs/2407.15377v2|Kelly W. Zhang,Nowell Closser,Anna L. Trella,Susan A. Murphy
865|Methodological Explainability Evaluation of an Interpretable Deep Learning Model for Post-Hepatectomy Liver Failure Prediction Incorporating Counterfactual Explanations and Layerwise Relevance Propagation: A Prospective In Silico Trial|Artificial intelligence (AI)-based decision support systems have demonstrated value in predicting post-hepatectomy liver failure (PHLF) in hepatocellular carcinoma (HCC). However, they often lack transparency, and the impact of model explanations on clinicians' decisions has not been thoroughly evaluated. Building on prior research, we developed a variational autoencoder-multilayer perceptron (VAE-MLP) model for preoperative PHLF prediction. This model integrated counterfactuals and layerwise relevance propagation (LRP) to provide insights into its decision-making mechanism. Additionally, we proposed a methodological framework for evaluating the explainability of AI systems. This framework includes qualitative and quantitative assessments of explanations against recognized biomarkers, usability evaluations, and an in silico clinical trial. Our evaluations demonstrated that the model's explanation correlated with established biomarkers and exhibited high usability at both the case and system levels. Furthermore, results from the three-track in silico clinical trial showed that clinicians' prediction accuracy and confidence increased when AI explanations were provided.|http://arxiv.org/abs/2408.03771v1|Xian Zhong,Zohaib Salahuddin,Yi Chen,Henry C Woodruff,Haiyi Long,Jianyun Peng,Nuwan Udawatte,Roberto Casale,Ayoub Mokhtari,Xiaoer Zhang,Jiayao Huang,Qingyu Wu,Li Tan,Lili Chen,Dongming Li,Xiaoyan Xie,Manxia Lin,Philippe Lambin
866|A response-adaptive multi-arm design for continuous endpoints based on a weighted information measure|Multi-arm trials are gaining interest in practice given the statistical and logistical advantages that they can offer. The standard approach is to use a fixed (throughout the trial) allocation ratio, but there is a call for making it adaptive and skewing the allocation of patients towards better performing arms. However, among other challenges, it is well-known that these approaches might suffer from lower statistical power. We present a response-adaptive design for continuous endpoints which explicitly allows to control the trade-off between the number of patients allocated to the 'optimal' arm and the statistical power. Such a balance is achieved through the calibration of a tuning parameter, and we explore various strategies to effectively select it. The proposed criterion is based on a context-dependent information measure which gives a greater weight to those treatment arms which have characteristics close to a pre-specified clinical target. We also introduce a simulation-based hypothesis testing procedure which focuses on selecting the target arm, discussing strategies to effectively control the type-I error rate. The potential advantage of the proposed criterion over currently used alternatives is evaluated in simulations, and its practical implementation is illustrated in the context of early Phase IIa proof-of-concept oncology clinical trials.|http://arxiv.org/abs/2409.04970v1|Gianmarco Caruso,Pavel Mozgunov
867|RoBIn: A Transformer-Based Model For Risk Of Bias Inference With Machine Reading Comprehension|Objective: Scientific publications play a crucial role in uncovering insights, testing novel drugs, and shaping healthcare policies. Accessing the quality of publications requires evaluating their Risk of Bias (RoB), a process typically conducted by human reviewers. In this study, we introduce a new dataset for machine reading comprehension and RoB assessment and present RoBIn (Risk of Bias Inference), an innovative model crafted to automate such evaluation. The model employs a dual-task approach, extracting evidence from a given context and assessing the RoB based on the gathered evidence. Methods: We use data from the Cochrane Database of Systematic Reviews (CDSR) as ground truth to label open-access clinical trial publications from PubMed. This process enabled us to develop training and test datasets specifically for machine reading comprehension and RoB inference. Additionally, we created extractive (RoBInExt) and generative (RoBInGen) Transformer-based approaches to extract relevant evidence and classify the RoB effectively. Results: RoBIn is evaluated across various settings and benchmarked against state-of-the-art methods for RoB inference, including large language models in multiple scenarios. In most cases, the best-performing RoBIn variant surpasses traditional machine learning and LLM-based approaches, achieving an ROC AUC of 0.83. Conclusion: Based on the evidence extracted from clinical trial reports, RoBIn performs a binary classification to decide whether the trial is at a low RoB or a high/unclear RoB. We found that both RoBInGen and RoBInExt are robust and have the best results in many settings.|http://arxiv.org/abs/2410.21495v1|Abel Corra Dias,Viviane Pereira Moreira,Joo Luiz Dihl Comba
868|Information borrowing in Bayesian clinical trials: choice of tuning parameters for the robust mixture prior|Borrowing historical data for use in clinical trials has increased in recent years. This is accomplished in the Bayesian framework by specification of informative prior distributions. One such approach is the robust mixture prior arising as a weighted mixture of an informative prior and a robust prior inducing dynamic borrowing that allows to borrow most when the current and external data are observed to be similar. The robust mixture prior requires the choice of three additional quantities: the mixture weight, and the mean and dispersion of the robust component. Some general guidance is available, but a case-by-case study of the impact of these quantities on specific operating characteristics seems lacking. We focus on evaluating the impact of parameter choices for the robust component of the mixture prior in one-arm and hybrid-control trials. The results show that all three quantities can strongly impact the operating characteristics. In particular, as already known, variance of the robust component is linked to robustness. Less known, however, is that its location can have a strong impact on Type I error rate and MSE which can even become unbounded. Further, the impact of the weight choice is strongly linked with the robust component's location and variance. Recommendations are provided for the choice of the robust component parameters, prior weight, alternative functional form for this component as well as considerations to keep in mind when evaluating operating characteristics.|http://arxiv.org/abs/2412.03185v1|Vivienn Weru,Annette Kopp-Schneider,Manuel Wiesenfarth,Sebastian Weber,Silvia Calderazzo
869|Bayesian Clustering Prior with Overlapping Indices for Effective Use of Multisource External Data|The use of external data in clinical trials offers numerous advantages, such as reducing the number of patients, increasing study power, and shortening trial durations. In Bayesian inference, information in external data can be transferred into an informative prior for future borrowing (i.e., prior synthesis). However, multisource external data often exhibits heterogeneity, which can lead to information distortion during the prior synthesis. Clustering helps identifying the heterogeneity, enhancing the congruence between synthesized prior and external data, thereby preventing information distortion. Obtaining optimal clustering is challenging due to the trade-off between congruence with external data and robustness to future data. We introduce two overlapping indices: the overlapping clustering index (OCI) and the overlapping evidence index (OEI). Using these indices alongside a K-Means algorithm, the optimal clustering of external data can be identified by balancing the trade-off. Based on the clustering result, we propose a prior synthesis framework to effectively borrow information from multisource external data. By incorporating the (robust) meta-analytic predictive prior into this framework, we develop (robust) Bayesian clustering MAP priors. Simulation studies and real-data analysis demonstrate their superiority over commonly used priors in the presence of heterogeneity. Since the Bayesian clustering priors are constructed without needing data from the prospective study to be conducted, they can be applied to both study design and data analysis in clinical trials or experiments.|http://arxiv.org/abs/2412.06098v1|Xuetao Lu,J. Jack Lee
870|The Estimand Framework and Causal Inference: Complementary not Competing Paradigms|The creation of the ICH E9 (R1) estimands framework has led to more precise specification of the treatment effects of interest in the design and statistical analysis of clinical trials. However, it is unclear how the new framework relates to causal inference, as both approaches appear to define what is being estimated and have a quantity labelled an estimand. Using illustrative examples, we show that both approaches can be used to define a population-based summary of an effect on an outcome for a specified population and highlight the similarities and differences between these approaches. We demonstrate that the ICH E9 (R1) estimand framework offers a descriptive, structured approach that is more accessible to non-mathematicians, facilitating clearer communication of trial objectives and results. We then contrast this with the causal inference framework, which provides a mathematically precise definition of an estimand, and allows the explicit articulation of assumptions through tools such as causal graphs. Despite these differences, the two paradigms should be viewed as complementary rather than competing. The combined use of both approaches enhances the ability to communicate what is being estimated. We encourage those familiar with one framework to appreciate the concepts of the other to strengthen the robustness and clarity of clinical trial design, analysis, and interpretation.|http://arxiv.org/abs/2412.12380v1|Thomas Drury,Jonathan W. Bartlett,David Wright,Oliver N. Keene
871|A mathematical model of CAR-T cell therapy in combination with chemotherapy for malignant gliomas|We study the dynamics and interactions between combined chemotherapy and chimeric antigen receptor (CAR-T) cells therapy and malignant gliomas (MG). MG is one of the most common primary brain tumor, with high resistance to therapy and unfavorable prognosis. Here, we develop a mathematical model that describes the application of chemo- and CAR-T cell therapies and the dynamics of sensitive and resistant populations of tumor cells. This model is a five-dimensional dynamical system with impulsive inputs corresponding to clinical administration of chemo- and immunotherapy. We provide a proof of non-negativeness of solutions of the proposed model for non-negative initial data. We demonstrate that if we apply both therapies only once, the trajectories will be attracted to an invariant surface that corresponds to the tumor carrying capacity. On the other hand, if we apply both treatments constantly, we find regions of the parameter where the tumor is eradicated. Moreover, we study applications of different combinations of the above treatments in order to find an optimal combination at the population level. To this aim, we generate a population of $10^{4}$ virtual patients with the model parameters uniformly distributed in the medically relevant ranges and perform \emph{in silico} trials with different combinations of treatments. We obtain optimal protocols for several different relations of tumor growth rates between sensitive and drug resistant cells. We demonstrate that the tumor growth rate, efficacy of chemotherapy, and tumor immunosuppression are the parameters that mostly impact survival time in \emph{in silico} trials. We believe that our results provide new theoretical insights to guide the design of clinical trials for MG therapies.|http://arxiv.org/abs/2501.13774v1|Dmitry Sinelshchikov,Juan Belmonte-Beitia,Matteo Italia
872|Automatic Estimation of Ulcerative Colitis Severity from Endoscopy Videos using Ordinal Multi-Instance Learning|Ulcerative colitis (UC) is a chronic inflammatory bowel disease characterized by relapsing inflammation of the large intestine. The severity of UC is often represented by the Mayo Endoscopic Subscore (MES) which quantifies mucosal disease activity from endoscopy videos. In clinical trials, an endoscopy video is assigned an MES based upon the most severe disease activity observed in the video. For this reason, severe inflammation spread throughout the colon will receive the same MES as an otherwise healthy colon with severe inflammation restricted to a small, localized segment. Therefore, the extent of disease activity throughout the large intestine, and overall response to treatment, may not be completely captured by the MES. In this work, we aim to automatically estimate UC severity for each frame in an endoscopy video to provide a higher resolution assessment of disease activity throughout the colon. Because annotating severity at the frame-level is expensive, labor-intensive, and highly subjective, we propose a novel weakly supervised, ordinal classification method to estimate frame severity from video MES labels alone. Using clinical trial data, we first achieved 0.92 and 0.90 AUC for predicting mucosal healing and remission of UC, respectively. Then, for severity estimation, we demonstrate that our models achieve substantial Cohen's Kappa agreement with ground truth MES labels, comparable to the inter-rater agreement of expert clinicians. These findings indicate that our framework could serve as a foundation for novel clinical endpoints, based on a more localized scoring system, to better evaluate UC drug efficacy in clinical trials.|http://arxiv.org/abs/2109.14685v1|Evan Schwab,Gabriela Oana Cula,Kristopher Standish,Stephen S. F. Yip,Aleksandar Stojmirovic,Louis Ghanem,Christel Chehoud
873|Histopathology Based AI Model Predicts Anti-Angiogenic Therapy Response in Renal Cancer Clinical Trial|Predictive biomarkers of treatment response are lacking for metastatic clear cell renal cell carcinoma (ccRCC), a tumor type that is treated with angiogenesis inhibitors, immune checkpoint inhibitors, mTOR inhibitors and a HIF2 inhibitor. The Angioscore, an RNA-based quantification of angiogenesis, is arguably the best candidate to predict anti-angiogenic (AA) response. However, the clinical adoption of transcriptomic assays faces several challenges including standardization, time delay, and high cost. Further, ccRCC tumors are highly heterogenous, and sampling multiple areas for sequencing is impractical. Here we present a novel deep learning (DL) approach to predict the Angioscore from ubiquitous histopathology slides. To overcome the lack of interpretability, one of the biggest limitations of typical DL models, our model produces a visual vascular network which is the basis of the model's prediction. To test its reliability, we applied this model to multiple cohorts including a clinical trial dataset. Our model accurately predicts the RNA-based Angioscore on multiple independent cohorts (spearman correlations of 0.77 and 0.73). Further, the predictions help unravel meaningful biology such as association of angiogenesis with grade, stage, and driver mutation status. Finally, we find our model can predict response to AA therapy, in both a real-world cohort and the IMmotion150 clinical trial. The predictive power of our model vastly exceeds that of CD31, a marker of vasculature, and nearly rivals the performance (c-index 0.66 vs 0.67) of the ground truth RNA-based Angioscore at a fraction of the cost. By providing a robust yet interpretable prediction of the Angioscore from histopathology slides alone, our approach offers insights into angiogenesis biology and AA treatment response.|http://arxiv.org/abs/2405.18327v1|Jay Jasti,Hua Zhong,Vandana Panwar,Vipul Jarmale,Jeffrey Miyata,Deyssy Carrillo,Alana Christie,Dinesh Rakheja,Zora Modrusan,Edward Ernest Kadel III,Niha Beig,Mahrukh Huseni,James Brugarolas,Payal Kapur,Satwik Rajaram
874|Development and Validation of a Deep-Learning Model for Differential Treatment Benefit Prediction for Adults with Major Depressive Disorder Deployed in the Artificial Intelligence in Depression Medication Enhancement (AIDME) Study|INTRODUCTION: The pharmacological treatment of Major Depressive Disorder (MDD) relies on a trial-and-error approach. We introduce an artificial intelligence (AI) model aiming to personalize treatment and improve outcomes, which was deployed in the Artificial Intelligence in Depression Medication Enhancement (AIDME) Study. OBJECTIVES: 1) Develop a model capable of predicting probabilities of remission across multiple pharmacological treatments for adults with at least moderate major depression. 2) Validate model predictions and examine them for amplification of harmful biases. METHODS: Data from previous clinical trials of antidepressant medications were standardized into a common framework and included 9,042 adults with moderate to severe major depression. Feature selection retained 25 clinical and demographic variables. Using Bayesian optimization, a deep learning model was trained on the training set, refined using the validation set, and tested once on the held-out test set. RESULTS: In the evaluation on the held-out test set, the model demonstrated achieved an AUC of 0.65. The model outperformed a null model on the test set (p = 0.01). The model demonstrated clinical utility, achieving an absolute improvement in population remission rate in hypothetical and actual improvement testing. While the model did identify one drug (escitalopram) as generally outperforming the other drugs (consistent with the input data), there was otherwise significant variation in drug rankings. On bias testing, the model did not amplify potentially harmful biases. CONCLUSIONS: We demonstrate the first model capable of predicting outcomes for 10 different treatment options for patients with MDD, intended to be used at or near the start of treatment to personalize treatment. The model was put into clinical practice during the AIDME randomized controlled trial whose results are reported separately.|http://arxiv.org/abs/2406.04993v1|David Benrimoh,Caitrin Armstrong,Joseph Mehltretter,Robert Fratila,Kelly Perlman,Sonia Israel,Adam Kapelner,Sagar V. Parikh,Jordan F. Karp,Katherine Heller,Gustavo Turecki
875|An introduction to the immune network|In this paper, after a telegraphic introduction to modern immunology, we present a simple model for the idiotypic network among antibodies and we study its relevance for the maintenance of immunological memory. We also consider the problem of computing the memory capacity of such a model.|http://arxiv.org/abs/cond-mat/9501046v1|Giorgio Parisi
876|HIV dynamics and natural history studies: Joint modeling with doubly interval-censored event time and infrequent longitudinal data|Hepatitis C virus (HCV) coinfection has become one of the most challenging clinical situations to manage in HIV-infected patients. Recently the effect of HCV coinfection on HIV dynamics following initiation of highly active antiretroviral therapy (HAART) has drawn considerable attention. Post-HAART HIV dynamics are commonly studied in short-term clinical trials with frequent data collection design. For example, the elimination process of plasma virus during treatment is closely monitored with daily assessments in viral dynamics studies of AIDS clinical trials. In this article instead we use infrequent cohort data from long-term natural history studies and develop a model for characterizing post-HAART HIV dynamics and their associations with HCV coinfection. Specifically, we propose a joint model for doubly interval-censored data for the time between HAART initiation and viral suppression, and the longitudinal CD4 count measurements relative to the viral suppression. Inference is accomplished using a fully Bayesian approach. Doubly interval-censored data are modeled semiparametrically by Dirichlet process priors and Bayesian penalized splines are used for modeling population-level and individual-level mean CD4 count profiles. We use the proposed methods and data from the HIV Epidemiology Research Study (HERS) to investigate the effect of HCV coinfection on the response to HAART.|http://arxiv.org/abs/1105.0543v1|Li Su,Joseph W. Hogan
877|The potential for bias in principal causal effect estimation when treatment received depends on a key covariate|Motivated by a potential-outcomes perspective, the idea of principal stratification has been widely recognized for its relevance in settings susceptible to posttreatment selection bias such as randomized clinical trials where treatment received can differ from treatment assigned. In one such setting, we address subtleties involved in inference for causal effects when using a key covariate to predict membership in latent principal strata. We show that when treatment received can differ from treatment assigned in both study arms, incorporating a stratum-predictive covariate can make estimates of the "complier average causal effect" (CACE) derive from observations in the two treatment arms with different covariate distributions. Adopting a Bayesian perspective and using Markov chain Monte Carlo for computation, we develop posterior checks that characterize the extent to which incorporating the pretreatment covariate endangers estimation of the CACE. We apply the method to analyze a clinical trial comparing two treatments for jaw fractures in which the study protocol allowed surgeons to overrule both possible randomized treatment assignments based on their clinical judgment and the data contained a key covariate (injury severity) predictive of treatment received.|http://arxiv.org/abs/1111.1509v1|Corwin M. Zigler,Thomas R. Belin
878|Actigraphy-based Sleep/Wake Pattern Detection using Convolutional Neural Networks|Common medical conditions are often associated with sleep abnormalities. Patients with medical disorders often suffer from poor sleep quality compared to healthy individuals, which in turn may worsen the symptoms of the disorder. Accurate detection of sleep/wake patterns is important in developing personalized digital markers, which can be used for objective measurements and efficient disease management. Big Data technologies and advanced analytics methods hold the promise to revolutionize clinical research processes, enabling the effective blending of digital data into clinical trials. Actigraphy, a non-invasive activity monitoring method is heavily used to detect and evaluate activities and movement disorders, and assess sleep/wake behavior. In order to study the connection between sleep/wake patterns and a cluster headache disorder, activity data was collected using a wearable device in the course of a clinical trial. This study presents two novel modeling schemes that utilize Deep Convolutional Neural Networks (CNN) to identify sleep/wake states. The proposed methods are a sequential CNN, reminiscent of the bi-directional CNN for slot filling, and a Multi-Task Learning (MTL) based model. Furthermore, we expand standard "Sleep" and "Wake" activity states space by adding the "Falling asleep" and "Siesta" states. We show that the proposed methods provide promising results in accurate detection of the expanded sleep/wake states. Finally, we explore the relations between the detected sleep/wake patterns and onset of cluster headache attacks, and present preliminary observations.|http://arxiv.org/abs/1802.07945v1|Lena Granovsky,Gabi Shalev,Nancy Yacovzada,Yotam Frank,Shai Fine
879|Nonparametric Bayesian Instrumental Variable Analysis: Evaluating Heterogeneous Effects of Coronary Arterial Access Site Strategies|Percutaneous coronary interventions (PCIs) are nonsurgical procedures to open blocked blood vessels to the heart, frequently using a catheter to place a stent. The catheter can be inserted into the blood vessels using an artery in the groin or an artery in the wrist. Because clinical trials have indicated that access via the wrist may result in fewer post procedure complications, shortening the length of stay, and ultimately cost less than groin access, adoption of access via the wrist has been encouraged. However, patients treated in usual care are likely to differ from those participating in clinical trials, and there is reason to believe that the effectiveness of wrist access may differ between males and females. Moreover, the choice of artery access strategy is likely to be influenced by patient or physician unmeasured factors. To study the effectiveness of the two artery access site strategies on hospitalization charges, we use data from a state-mandated clinical registry including 7,963 patients undergoing PCI. A hierarchical Bayesian likelihood-based instrumental variable analysis under a latent index modeling framework is introduced to jointly model outcomes and treatment status. Our approach accounts for unobserved heterogeneity via a latent factor structure, and permits nonparametric error distributions with Dirichlet process mixture models. Our results demonstrate that artery access in the wrist reduces hospitalization charges compared to access in the groin, with higher mean reduction for male patients.|http://arxiv.org/abs/1804.08055v4|Samrachana Adhikari,Sherri Rose,Sharon-Lise Normand
880|A Progressively-trained Scale-invariant and Boundary-aware Deep Neural Network for the Automatic 3D Segmentation of Lung Lesions|Volumetric segmentation of lesions on CT scans is important for many types of analysis, including lesion growth kinetic modeling in clinical trials and machine learning of radiomic features. Manual segmentation is laborious, and impractical for large-scale use. For routine clinical use, and in clinical trials that apply the Response Evaluation Criteria In Solid Tumors (RECIST), clinicians typically outline the boundaries of a lesion on a single slice to extract diameter measurements. In this work, we have collected a large-scale database, named LesionVis, with pixel-wise manual 2D lesion delineations on the RECIST-slices. To extend the 2D segmentations to 3D, we propose a volumetric progressive lesion segmentation (PLS) algorithm to automatically segment the 3D lesion volume from 2D delineations using a scale-invariant and boundary-aware deep convolutional network (SIBA-Net). The SIBA-Net copes with the size transition of a lesion when the PLS progresses from the RECIST-slice to the edge-slices, as well as when performing longitudinal assessment of lesions whose size change over multiple time points. The proposed PLS-SiBA-Net (P-SiBA) approach is assessed on the lung lesion cases from LesionVis. Our experimental results demonstrate that the P-SiBA approach achieves mean Dice similarity coefficients (DSC) of 0.81, which significantly improves 3D segmentation accuracy compared with the approaches proposed previously (highest mean DSC at 0.78 on LesionVis). In summary, by leveraging the limited 2D delineations on the RECIST-slices, P-SiBA is an effective semi-supervised approach to produce accurate lesion segmentations in 3D.|http://arxiv.org/abs/1811.04437v1|Bo Zhou,Randolph Crawford,Belma Dogdas,Gregory Goldmacher,Antong Chen
881|Current Landscape of Mesenchymal Stem Cell Therapy in COVID Induced Acute Respiratory Distress Syndrome|The severe acute respiratory syndrome coronavirus 2 outbreak in Chinas Hubei area in late 2019 has now created a global pandemic that has spread to over 150 countries. In most people, COVID 19 is a respiratory infection that produces fever, cough, and shortness of breath. Patients with severe COVID 19 may develop ARDS. MSCs can come from a number of places, such as bone marrow, umbilical cord, and adipose tissue. Because of their easy accessibility and low immunogenicity, MSCs were often used in animal and clinical research. In recent studies, MSCs have been shown to decrease inflammation, enhance lung permeability, improve microbial and alveolar fluid clearance, and accelerate lung epithelial and endothelial repair. Furthermore, MSC-based therapy has shown promising outcomes in preclinical studies and phase 1 clinical trials in sepsis and ARDS. In this paper, we posit the therapeutic strategies using MSC and dissect how and why MSC therapy is a potential treatment option for COVID 19 induced ARDS. We cite numerous promising clinical trials, elucidate the potential advantages of MSC therapy for COVID 19 ARDS patients, examine the detriments of this therapeutic strategy and suggest possibilities of subsequent research.|http://arxiv.org/abs/2211.02829v1|Adrita Chanda,Adrija Aich,Arka Sanyal,Anantika Chandra,Saumyadeep Goswami
882|Progression models for repeated measures: Estimating novel treatment effects in progressive diseases|Mixed Models for Repeated Measures (MMRMs) are ubiquitous when analyzing outcomes of clinical trials. However, the linearity of the fixed-effect structure in these models largely restrict their use to estimating treatment effects that are defined as linear combinations of effects on the outcome scale. In some situations, alternative quantifications of treatment effects may be more appropriate. In progressive diseases, for example, one may want to estimate if a drug has cumulative effects resulting in increasing efficacy over time or whether it slows the time progression of disease. This paper introduces a class of nonlinear mixed-effects models called Progression Models for Repeated Measures (PMRMs) that, based on a continuous-time extension of the categorical-time parametrization of MMRMs, enables estimation of novel types of treatment effects, including measures of slowing or delay of the time progression of disease. Compared to conventional estimates of treatment effects where the unit matches that of the outcome scale (e.g. 2 points benefit on a cognitive scale), the time-based treatment effects can offer better interpretability and clinical meaningfulness (e.g. 6 months delay in progression of cognitive decline). The PMRM class includes conventionally used MMRMs and related models for longitudinal data analysis, as well as variants of previously proposed disease progression models as special cases. The potential of the PMRM framework is illustrated using both simulated and historical data from clinical trials in Alzheimer's disease with different types of artificially simulated treatment effects. Compared to conventional models it is shown that PMRMs can offer substantially increased power to detect disease-modifying treatment effects where the benefit is increasing with treatment duration.|http://arxiv.org/abs/2203.15555v2|Lars Lau Raket
883|Mission Imputable: Correcting for Berkson Error When Imputing a Censored Covariate|To select outcomes for clinical trials testing experimental therapies for Huntington disease, a fatal neurodegenerative disorder, analysts model how potential outcomes change over time. Yet, subjects with Huntington disease are often observed at different levels of disease progression. To account for these differences, analysts include time to clinical diagnosis as a covariate when modeling potential outcomes, but this covariate is often censored. One popular solution is imputation, whereby we impute censored values using predictions from a model of the censored covariate given other data, then analyze the imputed dataset. However, when this imputation model is misspecified, our outcome model estimates can be biased. To address this problem, we developed a novel method, dubbed "ACE imputation." First, we model imputed values as error-prone versions of the true covariate values. Then, we correct for these errors using semiparametric theory. Specifically, we derive an outcome model estimator that is consistent, even when the censored covariate is imputed using a misspecified imputation model. Simulation results show that ACE imputation remains empirically unbiased even if the imputation model is misspecified, unlike multiple imputation which yields >100% bias. Applying our method to a Huntington disease study pinpoints outcomes for clinical trials aimed at slowing disease progression.|http://arxiv.org/abs/2303.01602v1|Kyle F. Grosser,Sarah C. Lotspeich,Tanya P. Garcia
884|Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models|Image-based precision medicine aims to personalize treatment decisions based on an individual's unique imaging features so as to improve their clinical outcome. Machine learning frameworks that integrate uncertainty estimation as part of their treatment recommendations would be safer and more reliable. However, little work has been done in adapting uncertainty estimation techniques and validation metrics for precision medicine. In this paper, we use Bayesian deep learning for estimating the posterior distribution over factual and counterfactual outcomes on several treatments. This allows for estimating the uncertainty for each treatment option and for the individual treatment effects (ITE) between any two treatments. We train and evaluate this model to predict future new and enlarging T2 lesion counts on a large, multi-center dataset of MR brain images of patients with multiple sclerosis, exposed to several treatments during randomized controlled trials. We evaluate the correlation of the uncertainty estimate with the factual error, and, given the lack of ground truth counterfactual outcomes, demonstrate how uncertainty for the ITE prediction relates to bounds on the ITE error. Lastly, we demonstrate how knowledge of uncertainty could modify clinical decision-making to improve individual patient and clinical trial outcomes.|http://arxiv.org/abs/2305.03829v4|Joshua Durso-Finley,Jean-Pierre Falet,Raghav Mehta,Douglas L. Arnold,Nick Pawlowski,Tal Arbel
885|Introduction of accelerated BOIN design and facilitation of its application|Purpose: During discussions at the Data Science Roundtable meeting in Japan, there were instances where the adoption of the BOIN design was declined, attributed to the extension of study duration and increased sample size in comparison to the 3+3 design. We introduce an accelerated BOIN design aimed at completing a clinical phase I trial at a pace comparable to the 3+3 design. Additionally, we introduce how we could have applied the BOIN design within our company, which predominantly utilized the 3+3 design for most of its clinical oncology dose escalation trials. Methods: The accelerated BOIN design is adaptable by using efficiently designated stopping criterion for the existing BOIN framework. Our approach is to terminate the dose escalation study if the number of evaluable patients treated at the current dose reaches 6 and the decision is to stay at the current dose for the next cohort of patients. In addition, for lower dosage levels, considering a cohort size smaller than 3 may be feasible when there are no safety concerns from non-clinical studies. We demonstrate the accelerated BOIN design using a case study and subsequently evaluate the performance of our proposed design through a simulation study. Results: In the simulation study, the average difference in the percentage of correct MTD selection between the accelerated BOIN design and the standard BOIN design was -2.43%, the average study duration and the average sample size of the accelerated BOIN design was reduced by 14.8 months and 9.22 months, respectively, compared with the standard BOIN design. Conclusion: We conclude that our proposed accelerated BOIN design not only provides superior operating characteristics but also enables the study to be completed as fast as the 3+3 design.|http://arxiv.org/abs/2309.08616v1|Masahiro Kojima,Wu Wende,Henry Zhao
886|From Diagnostic CT to DTI Tractography labels: Using Deep Learning for Corticospinal Tract Injury Assessment and Outcome Prediction in Intracerebral Haemorrhage|The preservation of the corticospinal tract (CST) is key to good motor recovery after stroke. The gold standard method of assessing the CST with imaging is diffusion tensor tractography. However, this is not available for most intracerebral haemorrhage (ICH) patients. Non-contrast CT scans are routinely available in most ICH diagnostic pipelines, but delineating white matter from a CT scan is challenging. We utilise nnU-Net, trained on paired diagnostic CT scans and high-directional diffusion tractography maps, to segment the CST from diagnostic CT scans alone, and we show our model reproduces diffusion based tractography maps of the CST with a Dice similarity coefficient of 57%.   Surgical haematoma evacuation is sometimes performed after ICH, but published clinical trials to date show that whilst surgery reduces mortality, there is no evidence of improved functional recovery. Restricting surgery to patients with an intact CST may reveal a subset of patients for whom haematoma evacuation improves functional outcome. We investigated the clinical utility of our model in the MISTIE III clinical trial dataset. We found that our model's CST integrity measure significantly predicted outcome after ICH in the acute and chronic time frames, therefore providing a prognostic marker for patients to whom advanced diffusion tensor imaging is unavailable. This will allow for future probing of subgroups who may benefit from surgery.|http://arxiv.org/abs/2408.06403v1|Olivia N Murray,Hamied Haroon,Paul Ryu,Hiren Patel,George Harston,Marieke Wermer,Wilmar Jolink,Daniel Hanley,Catharina Klijn,Ulrike Hammerbeck,Adrian Parry-Jones,Timothy Cootes
887|Estimands and Their Implications for Evidence Synthesis for Oncology: A Simulation Study of Treatment Switching in Meta-Analysis|The ICH E9(R1) addendum provides guidelines on accounting for intercurrent events in clinical trials using the estimands framework. However, there has been limited attention on the estimands framework for meta-analysis. Using treatment switching, a well-known intercurrent event that occurs frequently in oncology, we conducted a simulation study to explore the bias introduced by pooling together estimates targeting different estimands in a meta-analysis of randomized clinical trials (RCTs) that allowed for treatment switching. We simulated overall survival data of a collection of RCTs that allowed patients in the control group to switch to the intervention treatment after disease progression under fixed-effects and random-effects models. For each RCT, we calculated effect estimates for a treatment policy estimand that ignored treatment switching, and a hypothetical estimand that accounted for treatment switching by censoring switchers at the time of switching. Then, we performed random-effects and fixed-effects meta-analyses to pool together RCT effect estimates while varying the proportions of treatment policy and hypothetical effect estimates. We compared the results of meta-analyses that pooled different types of effect estimates with those that pooled only treatment policy or hypothetical estimates. We found that pooling estimates targeting different estimands results in pooled estimators that reflect neither the treatment policy estimand nor the hypothetical estimand. This finding shows that pooling estimates of varying target estimands can generate misleading results, even under a random-effects model. Adopting the estimands framework for meta-analysis may improve alignment between meta-analytic results and the clinical research question of interest.|http://arxiv.org/abs/2411.14323v1|Quang Vuong,Rebecca K. Metcalfe,Antonio Remiro-Azcar,Anders Gorst-Rasmussen,Oliver Keene,Jay J. H. Park
888|On a penalised likelihood approach for joint modelling of longitudinal covariates and partly interval-censored data -- an application to the Anti-PD1 brain collaboration trial|This article considers the joint modeling of longitudinal covariates and partly-interval censored time-to-event data. Longitudinal time-varying covariates play a crucial role in obtaining accurate clinically relevant predictions using a survival regression model. However, these covariates are often measured at limited time points and may be subject to measurement error. Further methodological challenges arise from the fact that, in many clinical studies, the event times of interest are interval-censored. A model that simultaneously accounts for all these factors is expected to improve the accuracy of survival model estimations and predictions. In this article, we consider joint models that combine longitudinal time-varying covariates with the Cox model for time-to-event data which is subject to interval censoring. The proposed model employs a novel penalised likelihood approach for estimating all parameters, including the random effects. The covariance matrix of the estimated parameters can be obtained from the penalised log-likelihood. The performance of the model is compared to an existing method under various scenarios. The simulation results demonstrated that our new method can provide reliable inferences when dealing with interval-censored data. Data from the Anti-PD1 brain collaboration clinical trial in advanced melanoma is used to illustrate the application of the new method.|http://arxiv.org/abs/2412.03042v1|Annabel Webb,Nan Zou,Serigne Lo,Jun Ma
889|Beyond Fixed Restriction Time: Adaptive Restricted Mean Survival Time Methods in Clinical Trials|Restricted mean survival time (RMST) offers a compelling nonparametric alternative to hazard ratios for right-censored time-to-event data, particularly when the proportional hazards assumption is violated. By capturing the total event-free time over a specified horizon, RMST provides an intuitive and clinically meaningful measure of absolute treatment benefit. Nonetheless, selecting the restriction time $L$ poses challenges: choosing a small $L$ can overlook late-emerging benefits, whereas a large $L$, often underestimated in its impact, may inflate variance and undermine power. We propose a novel data-driven, adaptive procedure that identifies the optimal restriction time $L^*$ from a continuous range by maximizing a criterion balancing effect size and estimation precision. Consequently, our procedure is particularly useful when the pattern of the treatment effect is unknown at the design stage. We provide a rigorous theoretical foundation that accounts for variability introduced by adaptively choosing $L^*$. To address nonregular estimation under the null, we develop two complementary strategies: a convex-hull-based estimator, and a penalized approach that further enhances power. Additionally, when restriction time candidates are defined on a discrete grid, we propose a procedure that surprisingly incurs no asymptotic penalty for selection, thus achieving oracle performance. Extensive simulations across realistic survival scenarios demonstrate that our method outperforms traditional RMST analyses and the log-rank test, achieving superior power while maintaining nominal Type I error rates. In a phase III pancreatic cancer trial with transient treatment effects, our procedure uncovers clinically meaningful benefits that standard methods overlook. Our methods are implemented in the R package AdaRMST.|http://arxiv.org/abs/2501.15284v1|Jinghao Sun,Douglas E. Schaubel,Eric J. Tchetgen Tchetgen
890|Extracting synaptic conductances from single membrane potential traces|In awake animals, the activity of the cerebral cortex is highly complex, with neurons firing irregularly with apparent Poisson statistics. One way to characterize this complexity is to take advantage of the high interconnectivity of cerebral cortex and use intracellular recordings of cortical neurons, which contain information about the activity of thousands of other cortical neurons. Identifying the membrane potential (Vm) to a stochastic process enables the extraction of important statistical signatures of this complex synaptic activity. Typically, one estimates the total synaptic conductances (excitatory and inhibitory) but this type of estimation requires at least two Vm levels and therefore cannot be applied to single Vm traces. We propose here a method to extract excitatory and inhibitory conductances (mean and variance) from single Vm traces. This "VmT method" estimates conductance parameters using maximum likelihood criteria, under the assumption are that synaptic conductances are described by Gaussian stochastic processes and are integrated by a passive leaky membrane. The method is illustrated using models and is tested on guinea-pig visual cortex neurons in vitro using dynamic-clamp experiments. The VmT method holds promises for extracting conductances from single-trial measurements, which has a high potential for in vivo applications.|http://arxiv.org/abs/0807.3238v2|Martin Pospischil,Zuzanna Piwkowska,Thierry Bal,Alain Destexhe
891|Decoding dynamic brain patterns from evoked responses: A tutorial on multivariate pattern analysis applied to time-series neuroimaging data|Multivariate pattern analysis (MVPA) or brain decoding methods have become standard practice in analysing fMRI data. Although decoding methods have been extensively applied in Brain Computing Interfaces (BCI), these methods have only recently been applied to time-series neuroimaging data such as MEG and EEG to address experimental questions in Cognitive Neuroscience. In a tutorial-style review, we describe a broad set of options to inform future time-series decoding studies from a Cognitive Neuroscience perspective. Using example MEG data, we illustrate the effects that different options in the decoding analysis pipeline can have on experimental results where the aim is to 'decode' different perceptual stimuli or cognitive states over time from dynamic brain activation patterns. We show that decisions made at both preprocessing (e.g., dimensionality reduction, subsampling, trial averaging) and decoding (e.g., classifier selection, cross-validation design) stages of the analysis can significantly affect the results. In addition to standard decoding, we describe extensions to MVPA for time-varying neuroimaging data including representational similarity analysis, temporal generalisation, and the interpretation of classifier weight maps. Finally, we outline important caveats in the design and interpretation of time-series decoding experiments.|http://arxiv.org/abs/1606.02840v2|Tijl Grootswagers,Susan G. Wardle,Thomas A. Carlson
892|A Goal-Based Movement Model for Continuous Multi-Agent Tasks|Despite increasing attention paid to the need for fast, scalable methods to analyze next-generation neuroscience data, comparatively little attention has been paid to the development of similar methods for behavioral analysis. Just as the volume and complexity of brain data have grown, behavioral paradigms in systems neuroscience have likewise become more naturalistic and less constrained, necessitating an increase in the flexibility and scalability of the models used to study them. In particular, key assumptions made in the analysis of typical decision paradigms --- optimality; analytic tractability; discrete, low-dimensional action spaces --- may be untenable in richer tasks. Here, using the case of a two-player, real-time, continuous strategic game as an example, we show how the use of modern machine learning methods allows us to relax each of these assumptions. Following an inverse reinforcement learning approach, we are able to succinctly characterize the joint distribution over players' actions via a generative model that allows us to simulate realistic game play. We compare simulated play from a number of generative time series models and show that ours successfully resists mode collapse while generating trajectories with the rich variability of real behavior. Together, these methods offer a rich class of models for the analysis of continuous action tasks at the single-trial level.|http://arxiv.org/abs/1702.07319v2|Shariq Iqbal,John Pearson
893|Deep learning as a tool for neural data analysis: speech classification and cross-frequency coupling in human sensorimotor cortex|A fundamental challenge in neuroscience is to understand what structure in the world is represented in spatially distributed patterns of neural activity from multiple single-trial measurements. This is often accomplished by learning a simple, linear transformations between neural features and features of the sensory stimuli or motor task. While successful in some early sensory processing areas, linear mappings are unlikely to be ideal tools for elucidating nonlinear, hierarchical representations of higher-order brain areas during complex tasks, such as the production of speech by humans. Here, we apply deep networks to predict produced speech syllables from cortical surface electric potentials recorded from human sensorimotor cortex. We found that deep networks had higher decoding prediction accuracy compared to baseline models, and also exhibited greater improvements in accuracy with increasing dataset size. We further demonstrate that deep network's confusions revealed hierarchical latent structure in the neural data, which recapitulated the underlying articulatory nature of speech motor control. Finally, we used deep networks to compare task-relevant information in different neural frequency bands, and found that the high-gamma band contains the vast majority of information relevant for the speech prediction task, with little-to-no additional contribution from lower-frequencies. Together, these results demonstrate the utility of deep networks as a data analysis tool for neuroscience.|http://arxiv.org/abs/1803.09807v1|Jesse A. Livezey,Kristofer E. Bouchard,Edward F. Chang
894|From internal models toward metacognitive AI|In several papers published in Biological Cybernetics in the 1980s and 1990s, Kawato and colleagues proposed computational models explaining how internal models are acquired in the cerebellum. These models were later supported by neurophysiological experiments using monkeys and neuroimaging experiments involving humans. These early studies influenced neuroscience from basic, sensory-motor control to higher cognitive functions. One of the most perplexing enigmas related to internal models is to understand the neural mechanisms that enable animals to learn large-dimensional problems with so few trials. Consciousness and metacognition -- the ability to monitor one's own thoughts, may be part of the solution to this enigma. Based on literature reviews of the past 20 years, here we propose a computational neuroscience model of metacognition. The model comprises a modular hierarchical reinforcement-learning architecture of parallel and layered, generative-inverse model pairs. In the prefrontal cortex, a distributed executive network called the "cognitive reality monitoring network" (CRMN) orchestrates conscious involvement of generative-inverse model pairs in perception and action. Based on mismatches between computations by generative and inverse models, as well as reward prediction errors, CRMN computes a "responsibility signal" that gates selection and learning of pairs in perception, action, and reinforcement learning. A high responsibility signal is given to the pairs that best capture the external world, that are competent in movements (small mismatch), and that are capable of reinforcement learning (small reward prediction error). CRMN selects pairs with higher responsibility signals as objects of metacognition, and consciousness is determined by the entropy of responsibility signals across all pairs.|http://arxiv.org/abs/2109.12798v2|Mitsuo Kawato,Aurelio Cortese
895|The Randomized CRM: An Approach to Overcoming the Long-Memory Property of the CRM|The primary object of a phase I clinical trial is to determine the maximum tolerated dose (MTD). Typically, the MTD is identified using a dose-escalation study, where initial subjects are treated at the lowest dose level and subsequent subjects are treated at progressively higher dose levels until the MTD is identified. The continual reassessment method (CRM) is a popular model-based dose-escalation design, which utilizes a formal model for the relationship between dose and toxicity to guide dose-finding. Recently, it was shown that the CRM has a tendency to get "stuck" on a dose-level, with little escalation or de-escalation in the late stages of the trial, due to the long-memory property of the CRM. We propose the randomized CRM (rCRM), which introduces random escalation and de-escalation into the standard CRM dose-finding algorithm, as an approach to overcoming the long-memory property of the CRM. We discuss two approaches to random escalation and de-escalation and compare the operating characteristics of the rCRM to the standard CRM by simulation. Our simulation results show that the rCRM identifies the true MTD at a similar rate and results in a similar number of DLTs compared to the standard CRM, while reducing the trial-to-trial variability in the number of cohorts treated at the true MTD.|http://arxiv.org/abs/1405.1275v1|Joseph S. Koopmeiners,Andrew Wey
896|Estimating the sample mean and standard deviation from the sample size, median, range and/or interquartile range|In systematic reviews and meta-analysis, researchers often pool the results of the sample mean and standard deviation from a set of similar clinical trials. A number of the trials, however, reported the study using the median, the minimum and maximum values, and/or the first and third quartiles. Hence, in order to combine results, one may have to estimate the sample mean and standard deviation for such trials. In this paper, we propose to improve the existing literature in several directions. First, we show that the sample standard deviation estimation in Hozo et al. (2005) has some serious limitations and is always less satisfactory in practice. Inspired by this, we propose a new estimation method by incorporating the sample size. Second, we systematically study the sample mean and standard deviation estimation problem under more general settings where the first and third quartiles are also available for the trials. Through simulation studies, we demonstrate that the proposed methods greatly improve the existing methods and enrich the literature. We conclude our work with a summary table that serves as a comprehensive guidance for performing meta-analysis in different situations.|http://arxiv.org/abs/1407.8038v4|Xiang Wan,Wenqian Wang,Jiming Liu,Tiejun Tong
897|Improved Precision in the Analysis of Randomized Trials with Survival Outcomes, without Assuming Proportional Hazards|We present a new estimator of the restricted mean survival time in randomized trials where there is right censoring that may depend on treatment and baseline variables. The proposed estimator leverages prognostic baseline variables to obtain equal or better asymptotic precision compared to traditional estimators. Under regularity conditions and random censoring within strata of treatment and baseline variables, the proposed estimator has the following features: (i) it is interpretable under violations of the proportional hazards assumption; (ii) it is consistent and at least as precise as the Kaplan-Meier estimator under independent censoring; (iii) it remains consistent under violations of independent censoring (unlike the Kaplan-Meier estimator) when either the censoring or survival distributions are estimated consistently; and (iv) it achieves the nonparametric efficiency bound when both of these distributions are consistently estimated. We illustrate the performance of our method using simulations based on resampling data from a completed, phase 3 randomized clinical trial of a new surgical treatment for stroke; the proposed estimator achieves a 12% gain in relative efficiency compared to the Kaplan-Meier estimator. The proposed estimator has potential advantages over existing approaches for randomized trials with time-to-event outcomes, since existing methods either rely on model assumptions that are untenable in many applications, or lack some of the efficiency and consistency properties (i)-(iv). We focus on estimation of the restricted mean survival time, but our methods may be adapted to estimate any treatment effect measure defined as a smooth contrast between the survival curves for each study arm. We provide R code to implement the estimator.|http://arxiv.org/abs/1511.08404v2|Ivn Daz,Elizabeth Colantuoni,Daniel F. Hanley,Michael Rosenblum
898|Unreported links between trial registrations and published articles were identified using document similarity measures in a cross-sectional analysis of ClinicalTrials.gov|Objectives: Trial registries can be used to measure reporting biases and support systematic reviews but 45% of registrations do not provide a link to the article reporting on the trial. We evaluated the use of document similarity methods to identify unreported links between ClinicalTrials.gov and PubMed. Study Design and Setting: We extracted terms and concepts from a dataset of 72,469 ClinicalTrials.gov registrations and 276,307 PubMed articles, and tested methods for ranking articles across 16,005 reported links and 90 manually-identified unreported links. Performance was measured by the median rank of matching articles, and the proportion of unreported links that could be found by screening ranked candidate articles in order. Results: The best performing concept-based representation produced a median rank of 3 (IQR 1-21) for reported links and 3 (IQR 1-19) for the manually-identified unreported links, and term-based representations produced a median rank of 2 (1-20) for reported links and 2 (IQR 1-12) in unreported links. The matching article was ranked first for 40% of registrations, and screening 50 candidate articles per registration identified 86% of the unreported links. Conclusions: Leveraging the growth in the corpus of reported links between ClinicalTrials.gov and PubMed, we found that document similarity methods can assist in the identification of unreported links between trial registrations and corresponding articles.|http://arxiv.org/abs/1709.02116v3|Adam G. Dunn,Enrico Coiera,Florence Bourgeois
899|Bayesian inference for a principal stratum estimand to assess the treatment effect in a subgroup characterized by post-randomization events|The treatment effect in a specific subgroup is often of interest in randomized clinical trials. When the subgroup is characterized by the absence of certain post-randomization events, a naive analysis on the subset of patients without these events may be misleading. The principal stratification framework allows one to define an appropriate causal estimand in such settings. Statistical inference for the principal stratum estimand hinges on scientifically justified assumptions, which can be included with Bayesian methods through prior distributions. Our motivating example is a large randomized placebo-controlled trial of siponimod in patients with secondary progressive multiple sclerosis. The primary objective of this trial was to demonstrate the efficacy of siponimod relative to placebo in delaying disability progression for the whole study population. However, the treatment effect in the subgroup of patients who would not relapse during the trial is relevant from both a scientific and regulatory perspective. Assessing this subgroup treatment effect is challenging as there is strong evidence that siponimod reduces relapses. Aligned with the draft regulatory guidance ICH E9(R1), we describe in detail the scientific question of interest, the principal stratum estimand, the corresponding analysis method for binary endpoints and sensitivity analyses.|http://arxiv.org/abs/1809.03741v1|Baldur P. Magnusson,Heinz Schmidli,Nicolas Rouyrre,Daniel O. Scharfstein
900|Identifying treatment effect heterogeneity in dose-finding trials using Bayesian hierarchical models|An important task in drug development is to identify patients, which respond better or worse to an experimental treatment. Identifying predictive covariates, which influence the treatment effect and can be used to define subgroups of patients, is a key aspect of this task. Analyses of treatment effect heterogeneity are however known to be challenging, since the number of possible covariates or subgroups is often large, while samples sizes in earlier phases of drug development are often small. In addition, distinguishing predictive covariates from prognostic covariates, which influence the response independent of the given treatment, can often be difficult. While many approaches for these types of problems have been proposed, most of them focus on the two-arm clinical trial setting, where patients are given either the treatment or a control. In this paper we consider parallel groups dose-finding trials, in which patients are administered different doses of the same treatment. To investigate treatment effect heterogeneity in this setting we propose a Bayesian hierarchical dose-response model with covariate effects on dose-response parameters. We make use of shrinkage priors to prevent overfitting, which can easily occur, when the number of considered covariates is large and sample sizes are small. We compare several such priors in simulations and also investigate dependent modeling of prognostic and predictive effects to better distinguish these two types of effects. We illustrate the use of our proposed approach using a Phase II dose-finding trial and show how it can be used to identify predictive covariates and subgroups of patients with increased treatment effects.|http://arxiv.org/abs/1811.10488v1|Marius Thomas,Bjrn Bornkamp,Katja Ickstadt
901|Causal Inference for Comprehensive Cohort Studies|In a comprehensive cohort study of two competing treatments (say, A and B), clinically eligible individuals are first asked to enroll in a randomized trial and, if they refuse, are then asked to enroll in a parallel observational study in which they can choose treatment according to their own preference. We consider estimation of two estimands: (1) comprehensive cohort causal effect -- the difference in mean potential outcomes had all patients in the comprehensive cohort received treatment A vs. treatment B and (2) randomized trial causal effect -- the difference in mean potential outcomes had all patients enrolled in the randomized trial received treatment A vs. treatment B. For each estimand, we consider inference under various sets of unconfoundedness assumptions and construct semiparametric efficient and robust estimators. These estimators depend on nuisance functions, which we estimate, for illustrative purposes, using generalized additive models. Using the theory of sample splitting, we establish the asymptotic properties of our proposed estimators. We also illustrate our methodology using data from the Bypass Angioplasty Revascularization Investigation (BARI) randomized trial and observational registry to evaluate the effect of percutaneous transluminal coronary balloon angioplasty versus coronary artery bypass grafting on 5-year mortality. To evaluate the finite sample performance of our estimators, we use the BARI dataset as the basis of a realistic simulation study.|http://arxiv.org/abs/1910.03531v1|Yi Lu,Daniel O. Scharfstein,Maria M. Brooks,Kevin Quach,Edward H. Kennedy
902|A Transformation Perspective on Marginal and Conditional Models|Clustered observations are ubiquitous in controlled and observational studies and arise naturally in multi-centre trials or longitudinal surveys. We present a novel model for the analysis of clustered observations where the marginal distributions are described by a linear transformation model and the correlations by a joint multivariate normal distribution. The joint model provides an analytic formula for the marginal distribution. Owing to the richness of transformation models, the techniques are applicable to any type of response variable, including bounded, skewed, binary, ordinal, or survival responses. We demonstrate how the common normal assumption for reaction times can be relaxed in the sleep deprivation benchmark dataset and report marginal odds ratios for the notoriously difficult toe nail data. We furthermore discuss the analysis of two clinical trials aiming at the estimation of marginal treatment effects. In the first trial, pain was repeatedly assessed on a bounded visual analog scale and marginal proportional-odds models are presented. The second trial reported disease-free survival in rectal cancer patients, where the marginal hazard ratio from Weibull and Cox models is of special interest. An empirical evaluation compares the performance of the novel approach to general estimation equations for binary responses and to conditional mixed-effects models for continuous responses. An implementation is available in the "tram" add-on package to the R system and was benchmarked against established models in the literature.|http://arxiv.org/abs/1910.09219v3|Luisa Barbanti,Torsten Hothorn
903|A modified weighted log-rank test for confirmatory trials with a high proportion of treatment switching|In confirmatory cancer clinical trials, overall survival (OS) is normally a primary endpoint in the intention-to-treat (ITT) analysis under regulatory standards. After the tumor progresses, it is common that patients allocated to the control group switch to the experimental treatment, or another drug in the same class. Such treatment switching may dilute the relative efficacy of the new drug compared to the control group, leading to lower statistical power. It would be possible to decrease the estimation bias by shortening the follow-up period but this may lead to a loss of information and power. Instead we propose a modified weighted log-rank test (mWLR) that aims at balancing these factors by down-weighting events occurring when many patients have switched treatment.   As the weighting should be pre-specified and the impact of treatment switching is unknown, we predict the hazard ratio function and use it to compute the weights of the mWLR. The method may incorporate information from previous trials regarding the potential hazard ratio function over time.   We are motivated by the RECORD-1 trial of everolimus against placebo in patients with metastatic renal-cell carcinoma where almost 80\% of the patients in the placebo group received everolimus after disease progression. Extensive simulations show that the new test gives considerably higher efficiency than the standard log-rank test in realistic scenarios.|http://arxiv.org/abs/2005.09213v1|Jos L. Jimnez,Julia Niewczas,Alexander Bore,Carl-Fredrik Burman
904|A review of Bayesian perspectives on sample size derivation for confirmatory trials|Sample size derivation is a crucial element of the planning phase of any confirmatory trial. A sample size is typically derived based on constraints on the maximal acceptable type I error rate and a minimal desired power. Here, power depends on the unknown true effect size. In practice, power is typically calculated either for the smallest relevant effect size or a likely point alternative. The former might be problematic if the minimal relevant effect is close to the null, thus requiring an excessively large sample size. The latter is dubious since it does not account for the a priori uncertainty about the likely alternative effect size. A Bayesian perspective on the sample size derivation for a frequentist trial naturally emerges as a way of reconciling arguments about the relative a priori plausibility of alternative effect sizes with ideas based on the relevance of effect sizes. Many suggestions as to how such `hybrid' approaches could be implemented in practice have been put forward in the literature. However, key quantities such as assurance, probability of success, or expected power are often defined in subtly different ways in the literature. Starting from the traditional and entirely frequentist approach to sample size derivation, we derive consistent definitions for the most commonly used `hybrid' quantities and highlight connections, before discussing and demonstrating their use in the context of sample size derivation for clinical trials.|http://arxiv.org/abs/2006.15715v1|Kevin Kunzmann,Michael J. Grayling,Kim May Lee,David S. Robertson,Kaspar Rufibach,James M. S. Wason
905|A Bayesian Machine Learning Approach for Estimating Heterogeneous Survivor Causal Effects: Applications to a Critical Care Trial|Motivated by the Acute Respiratory Distress Syndrome Network (ARDSNetwork) ARDS respiratory management (ARMA) trial, we developed a flexible Bayesian machine learning approach to estimate the average causal effect and heterogeneous causal effects among the always-survivors stratum when clinical outcomes are subject to truncation. We adopted Bayesian additive regression trees (BART) to flexibly specify separate models for the potential outcomes and latent strata membership. In the analysis of the ARMA trial, we found that the low tidal volume treatment had an overall benefit for participants sustaining acute lung injuries on the outcome of time to returning home, but substantial heterogeneity in treatment effects among the always-survivors, driven most strongly by sex and the alveolar-arterial oxygen gradient at baseline (a physiologic measure of lung function and source of hypoxemia). These findings illustrate how the proposed methodology could guide the prognostic enrichment of future trials in the field. We also demonstrated through a simulation study that our proposed Bayesian machine learning approach outperforms other parametric methods in reducing the estimation bias in both the average causal effect and heterogeneous causal effects for always-survivors.|http://arxiv.org/abs/2204.06657v3|Xinyuan Chen,Michael O. Harhay,Guangyu Tong,Fan Li
906|Estimation of treatment effects following a sequential trial of multiple treatments|When a clinical trial is subject to a series of interim analyses as a result of which the study may be terminated or modified, final frequentist analyses need to take account of the design used. Failure to do so may result in overstated levels of significance, biased effect estimates and confidence intervals with inadequate coverage probabilities. A wide variety of valid methods of frequentist analysis have been devised for sequential designs comparing a single experimental treatment with a single control treatment. It is less clear how to perform the final analysis of a sequential or adaptive design applied in a more complex setting, for example to determine which treatment or set of treatments amongst several candidates should be recommended.   This paper has been motivated by consideration of a trial in which four treatments for sepsis are to be compared, with interim analyses allowing the dropping of treatments or termination of the trial to declare a single winner or to conclude that there is little difference between the treatments that remain. The approach taken is based on the method of Rao-Blackwellisation which enhances the accuracy of unbiased estimates available from the first interim analysis by taking their conditional expectations given final sufficient statistics. Analytic approaches to determine such expectations are difficult and specific to the details of the design, and instead "reverse simulations" are conducted to construct replicate realisations of the first interim analysis from the final test statistics. The method also provides approximate confidence intervals for the differences between treatments.|http://arxiv.org/abs/1906.11324v1|John Whitehead,Yasin Desai,Thomas Jaki
907|Evaluating the Reproducibility of Research in Obstetrics and Gynecology|Objective: Reproducibility is a core tenet of scientific research. A reproducible study is one where the results can be recreated by different investigators in different circumstances using the same methodology and materials. Unfortunately, reproducibility is not a standard to which the majority of research is currently adherent. Methods: We objectively evaluated 300 trials in the field of Obstetrics and Gynecology for fourteen indicators of reproducibility. These indicators include availability of data, analysis scripts, pre-registration information, study protocols and whether or not the study was available via Open Access. We also assessed the trials for financial conflict of interest statements and source of funding. Results: Of the 300 trials in our sample, 208 contained empirical data that could be assessed for reproducibility. None of the trials in our sample provided a link to their protocols or provided a statement on availability of materials. None were replication studies. Just 10.58% provided a statement regarding their data availability, while only 5.82% provided a statement on preregistration. 25.85% failed to report the presence or absence of conflicts of interest and 54.08% did not state the origin of their funding. Conclusion: Research in the field of Obstetrics and Gynecology is not consistently reproducible and frequently lacks conflict of interest disclosure. Consequences of this could be far-reaching and include increased research waste, widespread acceptance of misleading results and erroneous conclusions guiding clinical decision-making.|http://arxiv.org/abs/1907.06999v1|Aaron Bowers,Shelby Rauh,Drayton Rorah,Daniel Tritz,Lance Frye,Matt Vassar
908|Optimal Bayesian hierarchical model to accelerate the development of tissue-agnostic drugs and basket trials|Tissue-agnostic trials enroll patients based on their genetic biomarkers, not tumor type, in an attempt to determine if a new drug can successfully treat disease conditions based on biomarkers. The Bayesian hierarchical model (BHM) provides an attractive approach to design phase II tissue-agnostic trials by allowing information borrowing across multiple disease types. In this article, we elucidate two intrinsic and inevitable issues that may limit the use of BHM to tissue-agnostic trials: sensitivity to the prior specification of the shrinkage parameter and the competing "interest" among disease types in increasing power and controlling type I error. To address these issues, we propose the optimal BHM (OBHM) approach. With OBHM, we first specify a flexible utility function to quantify the tradeoff between type I error and power across disease type based on the study objectives, and then we select the prior of the shrinkage parameter to optimize the utility function of clinical and regulatory interest. OBMH effectively balances type I and II errors, addresses the sensitivity of the prior selection, and reduces the "unwarranted" subjectivity in the prior selection. Simulation study shows that the resulting OBHM and its extensions, clustered OBHM (COBHM) and adaptive OBHM (AOBHM), have desirable operating characteristics, outperforming some existing methods with better balanced power and type I error control. Our method provides a systematic, rigorous way to apply BHM and solve the common problem of blindingly using a non-informative inverse-gamma prior (with a large variance) or priors arbitrarily chosen that may lead to pathological statistical properties.|http://arxiv.org/abs/2012.02378v1|Liyun Jiang,Lei Nie,Fangrong Yan,Ying Yuan
909|Informed Bayesian survival analysis|We overview Bayesian estimation, hypothesis testing, and model-averaging and illustrate how they benefit parametric survival analysis. We contrast the Bayesian framework to the currently dominant frequentist approach and highlight advantages, such as seamless incorporation of historical data, continuous monitoring of evidence, and incorporating uncertainty about the true data generating process. We illustrate the application of the Bayesian approaches on an example data set from a colon cancer trial. We compare the Bayesian parametric survival analysis and frequentist models with AIC/BIC model selection in fixed-n and sequential designs with a simulation study. In the example data set, the Bayesian framework provided evidence for the absence of a positive treatment effect on disease-free survival in patients with resected colon cancer. Furthermore, the Bayesian sequential analysis would have terminated the trial 10.3 months earlier than the standard frequentist analysis. In a simulation study with sequential designs, the Bayesian framework on average reached a decision in almost half the time required by the frequentist counterparts, while maintaining the same power, and an appropriate false-positive rate. Under model misspecification, the Bayesian framework resulted in higher false-negative rate compared to the frequentist counterparts, which resulted in a higher proportion of undecided trials. In fixed-n designs, the Bayesian framework showed slightly higher power, slightly elevated error rates, and lower bias and RMSE when estimating treatment effects in small samples. We have made the analytic approach readily available in RoBSA R package. The outlined Bayesian framework provides several benefits when applied to parametric survival analyses. It uses data more efficiently, is capable of greatly shortening the length of clinical trials, and provides a richer set of inferences.|http://arxiv.org/abs/2112.08311v2|Frantiek Barto,Frederik Aust,Julia M. Haaf
910|Estimation and Hypothesis Testing of Strain-Specific Vaccine Efficacy with Missing Strain Types, with Applications to a COVID-19 Vaccine Trial|Statistical methods are developed for analysis of clinical and virus genetics data from phase 3 randomized, placebo-controlled trials of vaccines against novel coronavirus COVID-19. Vaccine efficacy (VE) of a vaccine to prevent COVID-19 caused by one of finitely many genetic strains of SARS-CoV-2 may vary by strain. The problem of assessing differential VE by viral genetics can be formulated under a competing risks model where the endpoint is virologically confirmed COVID-19 and the cause-of-failure is the infecting SARS-CoV-2 genotype. Strain-specific VE is defined as one minus the cause-specific hazard ratio (vaccine/placebo). For the COVID-19 VE trials, the time to COVID-19 is right-censored, and a substantial percentage of failure cases are missing the infecting virus genotype. We develop estimation and hypothesis testing procedures for strain-specific VE when the failure time is subject to right censoring and the cause-of-failure is subject to missingness, focusing on $J \ge 2$ discrete categorical unordered or ordered virus genotypes. The stratified Cox proportional hazards model is used to relate the cause-specific outcomes to explanatory variables. The inverse probability weighted complete-case (IPW) estimator and the augmented inverse probability weighted complete-case (AIPW) estimator are investigated. Hypothesis tests are developed to assess whether the vaccine provides at least a specified level of efficacy against some viral genotypes and whether VE varies across genotypes, adjusting for covariates. The finite-sample properties of the proposed tests are studied through simulations and are shown to have good performances. In preparation for the real data analyses, the developed methods are applied to a pseudo dataset mimicking the Moderna COVE trial.|http://arxiv.org/abs/2201.08946v1|Fei Heng,Yanqing Sun,Peter B. Gilbert
911|Assessing treatment effect heterogeneity in the presence of missing effect modifier data in cluster-randomized trials|Understanding whether and how treatment effects vary across subgroups is crucial to inform clinical practice and recommendations. Accordingly, the assessment of heterogeneous treatment effects (HTE) based on pre-specified potential effect modifiers has become a common goal in modern randomized trials. However, when one or more potential effect modifiers are missing, complete-case analysis may lead to bias and under-coverage. While statistical methods for handling missing data have been proposed and compared for individually randomized trials with missing effect modifier data, few guidelines exist for the cluster-randomized setting, where intracluster correlations in the effect modifiers, outcomes, or even missingness mechanisms may introduce further threats to accurate assessment of HTE. In this article, the performance of several missing data methods are compared through a simulation study of cluster-randomized trials with continuous outcome and missing binary effect modifier data, and further illustrated using real data from the Work, Family, and Health Study. Our results suggest that multilevel multiple imputation (MMI) and Bayesian MMI have better performance than other available methods, and that Bayesian MMI has lower bias and closer to nominal coverage than standard MMI when there are model specification or compatibility issues.|http://arxiv.org/abs/2209.01297v2|Bryan S. Blette,Scott D. Halpern,Fan Li,Michael O. Harhay
912|Interim Monitoring of Sequential Multiple Assignment Randomized Trials Using Partial Information|The sequential multiple assignment randomized trial (SMART) is the gold standard trial design to generate data for the evaluation of multi-stage treatment regimes. As with conventional (single-stage) randomized clinical trials, interim monitoring allows early stopping; however, there are few methods for principled interim analysis in SMARTs. Because SMARTs involve multiple stages of treatment, a key challenge is that not all enrolled participants will have progressed through all treatment stages at the time of an interim analysis. Wu et al. (2021) propose basing interim analyses on an estimator for the mean outcome under a given regime that uses data only from participants who have completed all treatment stages. We propose an estimator for the mean outcome under a given regime that gains efficiency by using partial information from enrolled participants regardless of their progression through treatment stages. Using the asymptotic distribution of this estimator, we derive associated Pocock and O'Brien-Fleming testing procedures for early stopping. In simulation experiments, the estimator controls type I error and achieves nominal power while reducing expected sample size relative to the method of Wu et al. (2021). We present an illustrative application of the proposed estimator based on a recent SMART evaluating behavioral pain interventions for breast cancer patients.|http://arxiv.org/abs/2209.06306v2|Cole Manschot,Eric Laber,Marie Davidian
913|Incorporating Participants' Welfare into Sequential Multiple Assignment Randomized Trials|Dynamic treatment regimes (DTRs) are sequences of decision rules that recommend treatments based on patients' time-varying clinical conditions. The sequential multiple assignment randomized trial (SMART) is an experimental design that can provide high-quality evidence for constructing optimal DTRs. In a conventional SMART, participants are randomized to available treatments at multiple stages with balanced randomization probabilities. Despite its relative simplicity of implementation and desirable performance in comparing embedded DTRs, the conventional SMART faces inevitable ethical issues including assigning many participants to the empirically inferior treatment or the treatment they dislike, which might slow down the recruitment procedure and lead to higher attrition rates, ultimately leading to poor internal and external validities of the trial results. In this context, we propose a SMART under the Experiment-as-Market framework (SMART-EXAM), a novel SMART design that holds the potential to improve participants' welfare by incorporating their preferences and predicted treatment effects into the randomization procedure. We describe the steps of conducting a SMART-EXAM and evaluate its performance compared to the conventional SMART. The results indicate that the SMART-EXAM can improve the welfare of the participants enrolled in the trial, while also achieving a desirable ability to construct an optimal DTR when the experimental parameters are suitably specified. We finally illustrate the practical potential of the SMART-EXAM design using data from a SMART for children with attention-deficit/hyperactivity disorder (ADHD).|http://arxiv.org/abs/2210.16255v2|Xinru Wang,Nina Deliu,Yusuke Narita,Bibhas Chakraborty
914|Prognostic Covariate Adjustment for Binary Outcomes Using Stratification|Covariate adjustment and methods of incorporating historical data in randomized clinical trials (RCTs) each provide opportunities to increase trial power. We unite these approaches for the analysis of RCTs with binary outcomes based on the Cochran-Mantel-Haenszel (CMH) test for marginal risk ratio (RR). In PROCOVA-CMH, subjects are stratified on a single prognostic covariate reflective of their predicted outcome on the control treatment (e.g. placebo). This prognostic score is generated based on baseline covariates through a model trained on historical data. We propose two closed-form prospective estimators for the asymptotic sampling variance of the log RR that rely only on values obtainable from observed historical outcomes and the prognostic model. Importantly, these estimators can be used to inform sample size during trial planning. PROCOVA-CMH demonstrates type I error control and appropriate asymptotic coverage for valid inference. Like other covariate adjustment methods, PROCOVA-CMH can reduce the variance of the treatment effect estimate when compared to an unadjusted (unstratified) CMH analysis. In addition to statistical methods, simulations and a case study in Alzheimer's Disease are given to demonstrate performance. Results show that PROCOVA-CMH can provide a gain in power, which can be used to conduct smaller trials.|http://arxiv.org/abs/2212.09903v1|Alyssa M. Vanderbeek,Jessica L. Ross,David P. Miller,Alejandro Schuler
915|A Brief Wellbeing Training Session Delivered by a Humanoid Social Robot: A Pilot Randomized Controlled Trial|Mental health and psychological distress are rising in adults, showing the importance of wellbeing promotion, support, and technique practice that is effective and accessible. Interactive social robots have been tested to deliver health programs but have not been explored to deliver wellbeing technique training in detail. A pilot randomised controlled trial was conducted to explore the feasibility of an autonomous humanoid social robot to deliver a brief mindful breathing technique to promote information around wellbeing. It contained two conditions: brief technique training (Technique) and control designed to represent a simple wait-list activity to represent a relationship-building discussion (Simple Rapport). This trial also explored willingness to discuss health-related topics with a robot. Recruitment uptake rate through convenience sampling was high (53%). A total of 230 participants took part (mean age = 29 years) with 71% being higher education students. There were moderate ratings of technique enjoyment, perceived usefulness, and likelihood to repeat the technique again. Interaction effects were found across measures with scores varying across gender and distress levels. Males with high distress and females with low distress who received the simple rapport activity reported greater comfort to discuss non-health topics than males with low distress and females with high distress. This trial marks a notable step towards the design and deployment of an autonomous wellbeing intervention to investigate the impact of a brief robot-delivered mindfulness training program for a sub-clinical population.|http://arxiv.org/abs/2308.06435v1|Nicole Robinson,Jennifer Connolly,Gavin Suddrey,David J. Kavanagh
916|Power calculation for cross-sectional stepped wedge cluster randomized trials with a time-to-event endpoint|Stepped wedge cluster randomized trials (SW-CRTs) are a form of randomized trial whereby clusters are progressively transitioned from control to intervention, with the timing of transition randomized for each cluster. An important task at the design stage is to ensure that the planned trial has sufficient power to observe a clinically meaningful effect size. While methods for determining study power have been well-developed for SW-CRTs with continuous and binary outcomes, limited methods for power calculation are available for SW-CRTs with censored time-to-event outcomes. In this article, we propose a stratified marginal Cox model to account for secular trend in cross-sectional SW-CRTs, and derive an explicit expression of the robust sandwich variance to facilitate power calculations without the need for computationally intensive simulations. Power formulas based on both the Wald and robust score tests are developed and validated via simulation under different finite-sample scenarios. Finally, we illustrate our methods in the context of a SW-CRT testing the effect of a new electronic reminder system on time to catheter removal in hospital settings. We also offer an R Shiny application to facilitate sample size and power calculations using our proposed methods.|http://arxiv.org/abs/2312.13097v2|Mary M. Ryan,Denise Esserman,Monica Taljaard,Fan Li
917|Bayesian Model Averaging for Partial Ordering Continual Reassessment Methods|Phase I clinical trials are essential to bringing novel therapies from chemical development to widespread use. Traditional approaches to dose-finding in Phase I trials, such as the '3+3' method and the Continual Reassessment Method (CRM), provide a principled approach for escalating across dose levels. However, these methods lack the ability to incorporate uncertainty regarding the dose-toxicity ordering as found in combination drug trials. Under this setting, dose-levels vary across multiple drugs simultaneously, leading to multiple possible dose-toxicity orderings. The Partial Ordering CRM (POCRM) extends to these settings by allowing for multiple dose-toxicity orderings. In this work, it is shown that the POCRM is vulnerable to 'estimation incoherency' whereby toxicity estimates shift in an illogical way, threatening patient safety and undermining clinician trust in dose-finding models. To this end, the Bayesian model averaged POCRM (BMA-POCRM) is proposed. BMA-POCRM uses Bayesian model averaging to take into account all possible orderings simultaneously, reducing the frequency of estimation incoherencies. The effectiveness of BMA-POCRM in drug combination settings is demonstrated through a specific instance of estimate incoherency of POCRM and simulation studies. The results highlight the improved safety, accuracy and reduced occurrence of estimate incoherency in trials applying the BMA-POCRM relative to the POCRM model.|http://arxiv.org/abs/2403.00701v1|Luka Kovacevic,Thomas Jaki,Helen Barnett,Pavel Mozgunov
918|Weighting methods for truncation by death in cluster-randomized trials|Patient-centered outcomes, such as quality of life and length of hospital stay, are the focus in a wide array of clinical studies. However, participants in randomized trials for elderly or critically and severely ill patient populations may have truncated or undefined non-mortality outcomes if they do not survive through the measurement time point. To address truncation by death, the survivor average causal effect (SACE) has been proposed as a causally interpretable subgroup treatment effect defined under the principal stratification framework. However, the majority of methods for estimating SACE have been developed in the context of individually-randomized trials. Only limited discussions have been centered around cluster-randomized trials (CRTs), where methods typically involve strong distributional assumptions for outcome modeling. In this paper, we propose two weighting methods to estimate SACE in CRTs that obviate the need for potentially complicated outcome distribution modeling. We establish the requisite assumptions that address latent clustering effects to enable point identification of SACE, and we provide computationally-efficient asymptotic variance estimators for each weighting estimator. In simulations, we evaluate our weighting estimators, demonstrating their finite-sample operating characteristics and robustness to certain departures from the identification assumptions. We illustrate our methods using data from a CRT to assess the impact of a sedation protocol on mechanical ventilation among children with acute respiratory failure.|http://arxiv.org/abs/2404.10629v1|Dane Isenberg,Michael Harhay,Nandita Mitra,Fan Li
919|Effective Monitoring of Online Decision-Making Algorithms in Digital Intervention Implementation|Online AI decision-making algorithms are increasingly used by digital interventions to dynamically personalize treatment to individuals. These algorithms determine, in real-time, the delivery of treatment based on accruing data. The objective of this paper is to provide guidelines for enabling effective monitoring of online decision-making algorithms with the goal of (1) safeguarding individuals and (2) ensuring data quality. We elucidate guidelines and discuss our experience in monitoring online decision-making algorithms in two digital intervention clinical trials (Oralytics and MiWaves). Our guidelines include (1) developing fallback methods, pre-specified procedures executed when an issue occurs, and (2) identifying potential issues categorizing them by severity (red, yellow, and green). Across both trials, the monitoring systems detected real-time issues such as out-of-memory issues, database timeout, and failed communication with an external source. Fallback methods prevented participants from not receiving any treatment during the trial and also prevented the use of incorrect data in statistical analyses. These trials provide case studies for how health scientists can build monitoring systems for their digital intervention. Without these algorithm monitoring systems, critical issues would have gone undetected and unresolved. Instead, these monitoring systems safeguarded participants and ensured the quality of the resulting data for updating the intervention and facilitating scientific discovery. These monitoring guidelines and findings give digital intervention teams the confidence to include online decision-making algorithms in digital interventions.|http://arxiv.org/abs/2409.10526v1|Anna L. Trella,Susobhan Ghosh,Erin E. Bonar,Lara Coughlin,Finale Doshi-Velez,Yongyi Guo,Pei-Yao Hung,Inbal Nahum-Shani,Vivek Shetty,Maureen Walton,Iris Yan,Kelly W. Zhang,Susan A. Murphy
920|On the Consistency of Partial Ordering Continual Reassessment Method with Model and Ordering Misspecification|One of the aims of Phase I clinical trial designs in oncology is typically to find the maximum tolerated doses. A number of innovative dose-escalation designs were proposed in the literature to achieve this goal efficiently. Although the sample size of Phase I trials is usually small, the asymptotic properties (e.g. consistency) of dose-escalation designs can provide useful guidance on the design parameters and improve fundamental understanding of these designs. For the first proposed model-based monotherapy dose-escalation design, the Continual Reassessment Method (CRM), sufficient consistency conditions have been previously derived and then greatly influenced on how these studies are run in practice. At the same time, there is an increasing interest in Phase I combination-escalation trial in which two or more drugs are combined. The monotherapy dose-escalation design cannot be generally applied in this case due to uncertainty between monotonic ordering between some of the combinations, and, as a result, specialised designs were proposed. However, there were no theoretical or asymptotic properties evaluation of these proposals. In this paper, we derive the consistency conditions of the partial Ordering CRM (POCRM) design when there exists uncertainty in the monotonic ordering with a focus on dual-agent combination-escalation trials. Based on the derived consistency condition, we provide guidance on how the design parameters and ordering of the POCRM should be defined.|http://arxiv.org/abs/2410.21989v1|Weishi Chen,Pavel Mozgunov
921|Restricted Win Probability with Bayesian Estimation for Implementing the Estimand Framework in Clinical Trials With a Time-to-Event Outcome|We propose a restricted win probability estimand for comparing treatments in a randomized trial with a time-to-event outcome. We also propose Bayesian estimators for this summary measure as well as the unrestricted win probability. Bayesian estimation is scalable and facilitates seamless handling of censoring mechanisms as compared to related non-parametric pairwise approaches like win ratios. Unlike the log-rank test, these measures effectuate the estimand framework as they reflect a clearly defined population quantity related to the probability of a later event time with the potential restriction that event times exceeding a pre-specified time are deemed equivalent. We compare efficacy with established methods using computer simulation and apply the proposed approach to 304 reconstructed datasets from oncology trials. We show that the proposed approach has more power than the log-rank test in early treatment difference scenarios, and at least as much power as the win ratio in all scenarios considered. We also find that the proposed approach's statistical significance is concordant with the log-rank test for the vast majority of the oncology datasets examined. The proposed approach offers an interpretable, efficient alternative for trials with time-to-event outcomes that aligns with the estimand framework.|http://arxiv.org/abs/2411.02755v1|Michelle Leeberg,Xianghua Luo,Thomas A. Murray
922|Straightforward Phase I Dose-Finding Design for Healthy Volunteers Accounting for Surrogate Activity Biomarkers|Conventionally, a first-in-human phase I trial in healthy volunteers aims to confirm the safety of a drug in humans. In such situations, volunteers should not suffer from any safety issues and simple algorithm-based dose-escalation schemes are often used. However, to avoid too many clinical trials in the future, it might be appealing to design these trials to accumulate information on the link between dose and efficacy/activity under strict safety constraints. Furthermore, an increasing number of molecules for which the increasing dose-activity curve reaches a plateau are emerging.In a phase I dose-finding trial context, our objective is to determine, under safety constraints, among a set of doses, the lowest dose whose probability of activity is closest to a given target. For this purpose, we propose a two-stage dose-finding design. The first stage is a typical algorithm dose escalation phase that can both check the safety of the doses and accumulate activity information. The second stage is a model-based dose-finding phase that involves selecting the best dose-activity model according to the plateau location.Our simulation study shows that our proposed method performs better than the common Bayesian logistic regression model in selecting the optimal dose.|http://arxiv.org/abs/2412.03298v1|Sandrine Boulet,Emmanuelle Comets,Antoine Guillon,Linda B. S. Aulin,Robin Michelet,Charlotte Kloft,Sarah Zohar,Moreno Ursino
923|BUPD: A Bayesian under-parameterized basket design with the unit information prior in oncology trials|Basket trials in oncology enroll multiple patients with cancer harboring identical gene alterations and evaluate their response to targeted therapies across cancer types. Several existing methods have extended a Bayesian hierarchical model borrowing information on the response rates in different cancer types to account for the heterogeneity of drug effects. However, these methods rely on several pre-specified parameters to account for the heterogeneity of response rates among different cancer types. Here, we propose a novel Bayesian under-parameterized basket design with a unit information prior (BUPD) that uses only one (or two) pre-specified parameters to control the amount of information borrowed among cancer types, considering the heterogeneity of response rates. BUPD adapts the unit information prior approach, originally developed for borrowing information from historical clinical trial data, to enable mutual information borrowing between two cancer types. BUPD enables flexible controls of the type 1 error rate and power by explicitly specifying the strength of borrowing while providing interpretable estimations of response rates. Simulation studies revealed that BUPD reduced the type 1 error rate in scenarios with few ineffective cancer types and improved the power in scenarios with few effective cancer types better than five existing methods. This study also illustrated the efficiency of BUPD using response rates from a real basket trial.|http://arxiv.org/abs/2412.11140v1|Ryo Kitabayashi,Hiroyuki Sato,Akihiro Hirakawa
924|Residual Weighted Learning for Estimating Individualized Treatment Rules|Personalized medicine has received increasing attention among statisticians, computer scientists, and clinical practitioners. A major component of personalized medicine is the estimation of individualized treatment rules (ITRs). Recently, Zhao et al. (2012) proposed outcome weighted learning (OWL) to construct ITRs that directly optimize the clinical outcome. Although OWL opens the door to introducing machine learning techniques to optimal treatment regimes, it still has some problems in performance. In this article, we propose a general framework, called Residual Weighted Learning (RWL), to improve finite sample performance. Unlike OWL which weights misclassification errors by clinical outcomes, RWL weights these errors by residuals of the outcome from a regression fit on clinical covariates excluding treatment assignment. We utilize the smoothed ramp loss function in RWL, and provide a difference of convex (d.c.) algorithm to solve the corresponding non-convex optimization problem. By estimating residuals with linear models or generalized linear models, RWL can effectively deal with different types of outcomes, such as continuous, binary and count outcomes. We also propose variable selection methods for linear and nonlinear rules, respectively, to further improve the performance. We show that the resulting estimator of the treatment rule is consistent. We further obtain a rate of convergence for the difference between the expected outcome using the estimated ITR and that of the optimal treatment rule. The performance of the proposed RWL methods is illustrated in simulation studies and in an analysis of cystic fibrosis clinical trial data.|http://arxiv.org/abs/1508.03179v1|Xin Zhou,Nicole Mayer-Hamblett,Umer Khan,Michael R. Kosorok
925|DeepRadiologyNet: Radiologist Level Pathology Detection in CT Head Images|We describe a system to automatically filter clinically significant findings from computerized tomography (CT) head scans, operating at performance levels exceeding that of practicing radiologists. Our system, named DeepRadiologyNet, builds on top of deep convolutional neural networks (CNNs) trained using approximately 3.5 million CT head images gathered from over 24,000 studies taken from January 1, 2015 to August 31, 2015 and January 1, 2016 to April 30 2016 in over 80 clinical sites. For our initial system, we identified 30 phenomenological traits to be recognized in the CT scans. To test the system, we designed a clinical trial using over 4.8 million CT head images (29,925 studies), completely disjoint from the training and validation set, interpreted by 35 US Board Certified radiologists with specialized CT head experience. We measured clinically significant error rates to ascertain whether the performance of DeepRadiologyNet was comparable to or better than that of US Board Certified radiologists. DeepRadiologyNet achieved a clinically significant miss rate of 0.0367% on automatically selected high-confidence studies. Thus, DeepRadiologyNet enables significant reduction in the workload of human radiologists by automatically filtering studies and reporting on the high-confidence ones at an operating point well below the literal error rate for US Board Certified radiologists, estimated at 0.82%.|http://arxiv.org/abs/1711.09313v3|Jameson Merkow,Robert Lufkin,Kim Nguyen,Stefano Soatto,Zhuowen Tu,Andrea Vedaldi
926|Estimating the effect of PEG in ALS patients using observational data subject to censoring by death and missing outcomes|Though they may offer valuable patient and disease information that is impossible to study in a randomized trial, clinical disease registries also require special care and attention in causal inference. Registry data may be incomplete, inconsistent, and subject to confounding. In this paper we aim to address several analytical issues in estimating treatment effects that plague clinical registries such as the Emory amyotrophic lateral sclerosis (ALS) Clinic Registry. When attempting to assess the effect of a surgical insertion of a percutaneous endoscopic gastrostomy (PEG) tube on body mass index (BMI) using the data from the ALS Clinic Registry, one must combat issues of confounding, censoring by death, and missing outcome data that have not been addressed in previous studies of PEG. We propose a causal inference framework for estimating the survivor average causal effect (SACE) of PEG, which incorporates a model for generalized propensity scores to correct for confounding by pre-treatment variables, a model for principal stratification to account for censoring by death, and a model for the missing data mechanism. Applying the proposed framework to the ALS Clinic Registry Data, our analysis shows that PEG has a positive SACE on BMI at month 18 post-baseline; our results likely offer more definitive answers regarding the effect of PEG than previous studies of PEG.|http://arxiv.org/abs/1905.02062v1|Pallavi Mishra-Kalyani,Brent A. Johnson,Jonathan D. Glass,Qi Long
927|Real-World Multi-Domain Data Applications for Generalizations to Clinical Settings|With promising results of machine learning based models in computer vision, applications on medical imaging data have been increasing exponentially. However, generalizations to complex real-world clinical data is a persistent problem. Deep learning models perform well when trained on standardized datasets from artificial settings, such as clinical trials. However, real-world data is different and translations are yielding varying results. The complexity of real-world applications in healthcare could emanate from a mixture of different data distributions across multiple device domains alongside the inevitable noise sourced from varying image resolutions, human errors, and the lack of manual gradings. In addition, healthcare applications not only suffer from the scarcity of labeled data, but also face limited access to unlabeled data due to HIPAA regulations, patient privacy, ambiguity in data ownership, and challenges in collecting data from different sources. These limitations pose additional challenges to applying deep learning algorithms in healthcare and clinical translations. In this paper, we utilize self-supervised representation learning methods, formulated effectively in transfer learning settings, to address limited data availability. Our experiments verify the importance of diverse real-world data for generalization to clinical settings. We show that by employing a self-supervised approach with transfer learning on a multi-domain real-world dataset, we can achieve 16% relative improvement on a standardized dataset over supervised baselines.|http://arxiv.org/abs/2007.12672v1|Nooshin Mojab,Vahid Noroozi,Darvin Yi,Manoj Prabhakar Nallabothula,Abdullah Aleem,Phillip S. Yu,Joelle A. Hallak
928|Test of Significance for High-dimensional Thresholds with Application to Individualized Minimal Clinically Important Difference|This work is motivated by learning the individualized minimal clinically important difference, a vital concept to assess clinical importance in various biomedical studies. We formulate the scientific question into a high-dimensional statistical problem where the parameter of interest lies in an individualized linear threshold. The goal is to develop a hypothesis testing procedure for the significance of a single element in this parameter as well as of a linear combination of this parameter. The difficulty dues to the high-dimensional nuisance in developing such a testing procedure, and also stems from the fact that this high-dimensional threshold model is nonregular and the limiting distribution of the corresponding estimator is nonstandard. To deal with these challenges, we construct a test statistic via a new bias-corrected smoothed decorrelated score approach, and establish its asymptotic distributions under both null and local alternative hypotheses. We propose a double-smoothing approach to select the optimal bandwidth in our test statistic and provide theoretical guarantees for the selected bandwidth. We conduct simulation studies to demonstrate how our proposed procedure can be applied in empirical studies. We apply the proposed method to a clinical trial where the scientific goal is to assess the clinical importance of a surgery procedure.|http://arxiv.org/abs/2108.04306v2|Huijie Feng,Jingyi Duan,Yang Ning,Jiwei Zhao
929|Improved clinical data imputation via classical and quantum determinantal point processes|Imputing data is a critical issue for machine learning practitioners, including in the life sciences domain, where missing clinical data is a typical situation and the reliability of the imputation is of great importance. Currently, there is no canonical approach for imputation of clinical data and widely used algorithms introduce variance in the downstream classification. Here we propose novel imputation methods based on determinantal point processes that enhance popular techniques such as the Multivariate Imputation by Chained Equations (MICE) and MissForest. Their advantages are two-fold: improving the quality of the imputed data demonstrated by increased accuracy of the downstream classification; and providing deterministic and reliable imputations that remove the variance from the classification results. We experimentally demonstrate the advantages of our methods by performing extensive imputations on synthetic and real clinical data. We also perform quantum hardware experiments by applying the quantum circuits for DPP sampling, since such quantum algorithms provide a computational advantage with respect to classical ones. We demonstrate competitive results with up to ten qubits for small-scale imputation tasks on a state-of-the-art IBM quantum processor. Our classical and quantum methods improve the effectiveness and robustness of clinical data prediction modeling by providing better and more reliable data imputations. These improvements can add significant value in settings demanding high precision, such as in pharmaceutical drug trials where our approach can provide higher confidence in the predictions made.|http://arxiv.org/abs/2303.17893v2|Skander Kazdaghli,Iordanis Kerenidis,Jens Kieckbusch,Philip Teare
930|A wearable anti-gravity supplement to therapy does not improve arm function in chronic stroke: a randomized trial|Background: Gravity confounds arm movement ability in post-stroke hemiparesis. Reducing its influence allows effective practice leading to recovery. Yet, there is a scarcity of wearable devices suitable for personalized use across diverse therapeutic activities in the clinic. Objective: In this study, we investigated the safety, feasibility, and efficacy of anti-gravity therapy using the ExoNET device in post-stroke participants. Methods: Twenty chronic stroke survivors underwent six, 45-minute occupational therapy sessions while wearing the ExoNET, randomized into either the treatment (ExoNET tuned to gravity-support) or control group (ExoNET tuned to slack condition). Clinical outcomes were evaluated by a blinded-rater at baseline, post, and six-week follow-up sessions. Kinetic, kinematic, and patient experience outcomes were also assessed. Results: Mixed-effect models showed a significant improvement in Box and Blocks scores in the post-intervention session for the treatment group (effect size: 2.1, p = .04). No significant effects were found between the treatment and control groups for ARAT scores and other clinical metrics. Direct kinetic effects revealed a significant reduction in muscle activity during free exploration with an effect size of (-7.12%, p< 005). There were no significant longitudinal kinetic or kinematic trends. Subject feedback suggested a generally positive perception of the anti-gravity therapy. Conclusions: Anti-gravity therapy with the ExoNET is a safe and feasible treatment for post-stroke rehabilitation. The device provided anti-gravity forces, did not encumber range of motion, and clinical metrics of anti-gravity therapy demonstrated improvements in gross manual dexterity. Further research is required to explore potential benefits in broader clinical metrics.|http://arxiv.org/abs/2405.04707v1|Courtney Celian,Partha Ryali,Valentino Wilson,Adith Srivatsaa,James L. Patton
931|Decoding Patterns of Data Generation Teams for Clinical and Scientific Success: Insights from the Bridge2AI Talent Knowledge Graph|High-quality biomedical datasets are essential for medical research and disease treatment innovation. The NIH-funded Bridge2AI project strives to facilitate such innovations by uniting top-tier, diverse teams to curate datasets designed for AI-driven biomedical research. We examined 1,699 dataset papers from the Nucleic Acids Research (NAR) database issues and the Bridge2AI Talent Knowledge Graph. By treating each paper's authors as a team, we explored the relationship between team attributes (team power and fairness) and dataset paper quality, measured by scientific impact (Relative Citation Ratio percentile) and clinical translation power (APT, likelihood of citation by clinical trials and guidelines). Utilizing the SHAP explainable AI framework, we identified correlations between team attributes and the success of dataset papers in both citation impact and clinical translation. Key findings reveal that (1) PI (Principal Investigator) leadership and team academic prowess are strong predictors of dataset success; (2) team size and career age are positively correlated with scientific impact but show inverse patterns for clinical translation; and (3) higher female representation correlates with greater dataset success. Although our results are correlational, they offer valuable insights into forming high-performing data generation teams. Future research should incorporate causal frameworks to deepen understanding of these relationships.|http://arxiv.org/abs/2501.09897v1|Jiawei Xu,Qingnan Xie,Meijun Liu,Zhandos Sembay,Swathi Thaker,Pamela Payne-Foster,Jake Chen,Ying Ding
932|fMRI: preprocessing, classification and pattern recognition|As machine learning continues to gain momentum in the neuroscience community, we witness the emergence of novel applications such as diagnostics, characterization, and treatment outcome prediction for psychiatric and neurological disorders, for instance, epilepsy and depression. Systematic research into these mental disorders increasingly involves drawing clinical conclusions on the basis of data-driven approaches; to this end, structural and functional neuroimaging serve as key source modalities. Identification of informative neuroimaging markers requires establishing a comprehensive preparation pipeline for data which may be severely corrupted by artifactual signal fluctuations. In this work, we review a large body of literature to provide ample evidence for the advantages of pattern recognition approaches in clinical applications, overview advanced graph-based pattern recognition approaches, and propose a noise-aware neuroimaging data processing pipeline. To demonstrate the effectiveness of our approach, we provide results from a pilot study, which show a significant improvement in classification accuracy, indicating a promising research direction.|http://arxiv.org/abs/1804.10167v1|Maxim Sharaev,Alexander Andreev,Alexey Artemov,Alexander Bernstein,Evgeny Burnaev,Ekaterina Kondratyeva,Svetlana Sushchinskaya,Renat Akzhigitov
933|Challenges for machine learning in clinical translation of big data imaging studies|The combination of deep learning image analysis methods and large-scale imaging datasets offers many opportunities to imaging neuroscience and epidemiology. However, despite the success of deep learning when applied to many neuroimaging tasks, there remain barriers to the clinical translation of large-scale datasets and processing tools. Here, we explore the main challenges and the approaches that have been explored to overcome them. We focus on issues relating to data availability, interpretability, evaluation and logistical challenges, and discuss the challenges we believe are still to be overcome to enable the full success of big data deep learning approaches to be experienced outside of the research field.|http://arxiv.org/abs/2107.05630v1|Nicola K Dinsdale,Emma Bluemke,Vaanathi Sundaresan,Mark Jenkinson,Stephen Smith,Ana IL Namburete
934|A streamable large-scale clinical EEG dataset for Deep Learning|Deep Learning has revolutionized various fields, including Computer Vision, Natural Language Processing, as well as Biomedical research. Within the field of neuroscience, specifically in electrophysiological neuroimaging, researchers are starting to explore leveraging deep learning to make predictions on their data without extensive feature engineering. The availability of large-scale datasets is a crucial aspect of allowing the experimentation of Deep Learning models. We are publishing the first large-scale clinical EEG dataset that simplifies data access and management for Deep Learning. This dataset contains eyes-closed EEG data prepared from a collection of 1,574 juvenile participants from the Healthy Brain Network. We demonstrate a use case integrating this framework, and discuss why providing such neuroinformatics infrastructure to the community is critical for future scientific discoveries.|http://arxiv.org/abs/2203.02552v2|Dung Truong,Manisha Sinha,Kannan Umadevi Venkataraju,Michael Milham,Arnaud Delorme
935|A labeled Clinical-MRI dataset of Nigerian brains|We describe a Magnetic Resonance Imaging (MRI) dataset from individuals from the African nation of Nigeria. The dataset contains pseudonymized structural MRI (T1w, T2w, FLAIR) data of clinical quality. The dataset contains data from 36 images from healthy control subjects, 32 images from individuals diagnosed with age-related dementia and 20 from individuals with Parkinson's disease. There is currently a paucity of data from the African continent. Given the potential for Africa to contribute to the global neuroscience community, this first MRI dataset represents both an opportunity and benchmark for future studies to share data from the African continent.|http://arxiv.org/abs/2311.04425v1|Eberechi Wogu,Patrick Filima,Bradley Caron,Daniel Levitas,Peer Herholz,Catherine Leal,Mohammed F. Mehboob,Soichi Hayashi,Simisola Akintoye,George Ogoh,Tawe Godwin,Damian Eke,Franco Pestilli
936|Coordinate-Based Neural Representation Enabling Zero-Shot Learning for 3D Multiparametric Quantitative MRI|Quantitative magnetic resonance imaging (qMRI) offers tissue-specific physical parameters with significant potential for neuroscience research and clinical practice. However, lengthy scan times for 3D multiparametric qMRI acquisition limit its clinical utility. Here, we propose SUMMIT, an innovative imaging methodology that includes data acquisition and an unsupervised reconstruction for simultaneous multiparametric qMRI. SUMMIT first encodes multiple important quantitative properties into highly undersampled k-space. It further leverages implicit neural representation incorporated with a dedicated physics model to reconstruct the desired multiparametric maps without needing external training datasets. SUMMIT delivers co-registered T1, T2, T2*, and quantitative susceptibility mapping. Extensive simulations and phantom imaging demonstrate SUMMIT's high accuracy. Additionally, the proposed unsupervised approach for qMRI reconstruction also introduces a novel zero-shot learning paradigm for multiparametric imaging applicable to various medical imaging modalities.|http://arxiv.org/abs/2410.01577v1|Guoyan Lao,Ruimin Feng,Haikun Qi,Zhenfeng Lv,Qiangqiang Liu,Chunlei Liu,Yuyao Zhang,Hongjiang Wei
937|EEG-GMACN: Interpretable EEG Graph Mutual Attention Convolutional Network|Electroencephalogram (EEG) is a valuable technique to record brain electrical activity through electrodes placed on the scalp. Analyzing EEG signals contributes to the understanding of neurological conditions and developing brain-computer interface. Graph Signal Processing (GSP) has emerged as a promising method for EEG spatial-temporal analysis, by further considering the topological relationships between electrodes. However, existing GSP studies lack interpretability of electrode importance and the credibility of prediction confidence. This work proposes an EEG Graph Mutual Attention Convolutional Network (EEG-GMACN), by introducing an 'Inverse Graph Weight Module' to output interpretable electrode graph weights, enhancing the clinical credibility and interpretability of EEG classification results. Additionally, we incorporate a mutual attention mechanism module into the model to improve its capability to distinguish critical electrodes and introduce credibility calibration to assess the uncertainty of prediction results. This study enhances the transparency and effectiveness of EEG analysis, paving the way for its widespread use in clinical and neuroscience research.|http://arxiv.org/abs/2412.17834v1|Haili Ye,Stephan Goerttler,Fei He
938|Standardizing Paediatric Clinical Data: The Development of the conect4children (c4c) Cross Cutting Paediatric Data Dictionary|Standardization of data items collected in paediatric clinical trials is an important but challenging issue. The Clinical Data Interchange Standards Consortium (CDISC) data standards are well understood by the pharmaceutical industry but lack the implementation of some paediatric specific concepts. When a paediatric concept is absent within CDISC standards, companies and research institutions take multiple approaches in the collection of paediatric data, leading to different implementations of standards and potentially limited utility for reuse. To overcome these challenges, the conect4children consortium has developed a cross-cutting paediatric data dictionary (CCPDD). The dictionary was built over three phases - scoping (including a survey sent out to ten industrial and 34 academic partners to gauge interest), creation of a longlist and consensus building for the final set of terms. The dictionary was finalized during a workshop with attendees from academia, hospitals, industry and CDISC. The attendees held detailed discussions on each data item and participated in the final vote on the inclusion of the item in the CCPDD. Nine industrial and 34 academic partners responded to the survey, which showed overall interest in the development of the CCPDD. Following the final vote on 27 data items, three were rejected, six were deferred to the next version and a final opinion was sought from CDISC. The first version of the CCPDD with 25 data items was released in August 2019. The continued use of the dictionary has the potential to ensure the collection of standardized data that is interoperable and can later be pooled and reused for other applications. The dictionary is already being used for case report form creation in three clinical trials. The CCPDD will also serve as one of the inputs to the Paediatric User Guide, which is being developed by CDISC.|http://arxiv.org/abs/2302.13340v1|Anando Sen,Victoria Hedley,John Owen,Ronald Cornet,Dipak Kalra,Corinna Engel,Avril Palmeri,Joanne Lee,Jean-Christophe Roze,Joseph F Standing,Adilia Warris,Claudia Pansieri,Rebecca Leary,Mark Turner,Volker Straub
939|Impact on clinical guideline adherence of Orient-COVID, a CDSS based on dynamic medical decision trees for COVID19 management: a randomized simulation trial|Background: The adherence of clinicians to clinical practice guidelines is known to be low, including for the management of COVID-19, due to their difficult use at the point of care and their complexity. Clinical decision support systems have been proposed to implement guidelines and improve adherence. One approach is to permit the navigation inside the recommendations, presented as a decision tree, but the size of the tree often limits this approach and may cause erroneous navigation, especially when it does not fit in a single screen. Methods: We proposed an innovative visual interface to allow clinicians easily navigating inside decision trees for the management of COVID-19 patients. It associates a multi-path tree model with the use of the fisheye visual technique, allowing the visualization of large decision trees in a single screen. To evaluate the impact of this tool on guideline adherence, we conducted a randomized controlled trial in a near-real simulation setting, comparing the decisions taken by medical students using Orient-COVID with those taken with paper guidelines or without guidance, when performing on six realistic clinical cases. Results: The results show that paper guidelines had no impact (p=0.97), while Orient-COVID significantly improved the guideline adherence compared to both other groups (p<0.0003). A significant impact of Orient-COVID was identified on several key points during the management of COVID-19: ordering troponin lab tests, prescribing anticoagulant and oxygen therapy. A multifactor analysis showed no difference between male and female participants. Conclusions: The use of an interactive decision tree for the management of COVID-19 significantly improved the clinician adherence to guidelines. Future works will focus on the integration of the system to electronic health records and on the adaptation of the system to other clinical conditions.|http://arxiv.org/abs/2407.11205v1|Mouin Jammal,Antoine Saab,Cynthia Abi Khalil,Charbel Mourad,Rosy Tsopra,Melody Saikali,Jean-Baptiste Lamy
940|A randomized simulation trial evaluating ABiMed, a clinical decision support system for medication reviews and polypharmacy management|Background: Medication review is a structured interview of the patient, performed by the pharmacist and aimed at optimizing drug treatments. In practice, medication review is a long and cognitively-demanding task that requires specific knowledge. Clinical practice guidelines have been proposed, but their application is tedious. Methods: We designed ABiMed, a clinical decision support system for medication reviews, based on the implementation of the STOPP/START v2 guidelines and on the visual presentation of aggregated drug knowledge using tables, graphs and flower glyphs. We evaluated ABiMed with 39 community pharmacists during a randomized simulation trial, each pharmacist performing a medication review for two fictitious patients without ABiMed, and two others with ABiMed. We recorded the problems identified by the pharmacists, the interventions proposed, the response time, the perceived usability and the comments. Pharmacists' medication reviews were compared to an expert-designed gold standard. Results: With ABiMed, pharmacists found 1.6 times more relevant drug-related problems during the medication review (p=1.1e-12) and proposed better interventions (p=9.8e-9), without needing more time (p=0.56). The System Usability Scale score is 82.7, which is ranked "excellent". In their comments, pharmacists appreciated the visual aspect of ABiMed and its ability to compare the current treatment with the proposed one. A multifactor analysis showed no difference in the support offered by ABiMed according to the pharmacist's age or sex, in terms of percentage of problems identified or quality of the proposed interventions. Conclusions: The use of an intelligent and visual clinical decision support system can help pharmacists when they perform medication reviews. Our main perspective is the validation of the system in clinical conditions.|http://arxiv.org/abs/2409.01903v1|Abdelmalek Mouazer,Sophie Dubois,Romain Lguillon,Nada Boudegzdame,Thibaud Levrard,Yoann Le Bars,Christian Simon,Brigitte Sroussi,Julien Grosjean,Romain Lelong,Catherine Letord,Stfan Darmoni,Karima Sedki,Pierre Meneton,Rosy Tsopra,Hector Falcoff,Jean-Baptiste Lamy
941|Efficient transfer entropy analysis of non-stationary neural time series|Information theory allows us to investigate information processing in neural systems in terms of information transfer, storage and modification. Especially the measure of information transfer, transfer entropy, has seen a dramatic surge of interest in neuroscience. Estimating transfer entropy from two processes requires the observation of multiple realizations of these processes to estimate associated probability density functions. To obtain these observations, available estimators assume stationarity of processes to allow pooling of observations over time. This assumption however, is a major obstacle to the application of these estimators in neuroscience as observed processes are often non-stationary. As a solution, Gomez-Herrero and colleagues theoretically showed that the stationarity assumption may be avoided by estimating transfer entropy from an ensemble of realizations. Such an ensemble is often readily available in neuroscience experiments in the form of experimental trials. Thus, in this work we combine the ensemble method with a recently proposed transfer entropy estimator to make transfer entropy estimation applicable to non-stationary time series. We present an efficient implementation of the approach that deals with the increased computational demand of the ensemble method's practical application. In particular, we use a massively parallel implementation for a graphics processing unit to handle the computationally most heavy aspects of the ensemble method. We test the performance and robustness of our implementation on data from simulated stochastic processes and demonstrate the method's applicability to magnetoencephalographic data. While we mainly evaluate the proposed method for neuroscientific data, we expect it to be applicable in a variety of fields that are concerned with the analysis of information transfer in complex biological, social, and artificial systems.|http://arxiv.org/abs/1401.4068v2|Patricia Wollstadt,Mario Martnez-Zarzuela,Raul Vicente,Francisco J. Daz-Pernas,Michael Wibral
942|Using survival curves for comparison of ordinal qualitative data in clinical studies|Background and Objective: The survival-agreement plot was proposed and improved to assess the reliability of a quantitative measure. We propose the use of survival analysis as an alternative non-parametric approach for comparison of ordinal qualitative data.   Study Design and Setting: Two case studies were presented. The first one is related to a randomized, double blind, placebo-controlled clinical trial to investigate the safety and efficacy of silymarin/metionin for chronic hepatitis C. The second one is a prospective study to identify gustatory alterations due to chorda tympani nerve involvement in patients with chronic otitis media without prior surgery.   Results: No significant difference was detected between the two treatments related to the chronic hepatitis C (p > 0.5). On the other hand, a significant association was observed between the healthy side and the affected side of the face of patients with chronic otitis media related to gustatory alterations (p < 0.05).   Conclusion: The proposed method can serve as an alternative procedure to statistical test for comparison of samples from ordinal qualitative variables. This approach has the advantage of being more familiar to clinical researchers.|http://arxiv.org/abs/0904.3915v1|B. de B. Pereira,E. M. Nascimento,F. Felix,G. F. M. Rezende
943|On Estimation of Optimal Treatment Regimes For Maximizing t-Year Survival Probability|A treatment regime is a deterministic function that dictates personalized treatment based on patients' individual prognostic information. There is a fast-growing interest in finding optimal treatment regimes to maximize expected long-term clinical outcomes of patients for complex diseases, such as cancer and AIDS. For many clinical studies with survival time as a primary endpoint, a main goal is to maximize patients' survival probabilities given treatments. In this article, we first propose two nonparametric estimators for survival function of patients following a given treatment regime. Then, we derive the estimation of the optimal treatment regime based on a value-based searching algorithm within a set of treatment regimes indexed by parameters. The asymptotic properties of the proposed estimators for survival probabilities under derived optimal treatment regimes are established under suitable regularity conditions. Simulations are conducted to evaluate the numerical performance of the proposed estimators under various scenarios. An application to an AIDS clinical trial data is also given to illustrate the methods.|http://arxiv.org/abs/1407.7820v1|Runchao Jiang,Wenbin Lu,Rui Song,Marie Davidian
944|A Multi-Smartwatch System for Assessing Speech Characteristics of People with Dysarthria in Group Settings|Speech-language pathologists (SLPs) frequently use vocal exercises in the treatment of patients with speech disorders. Patients receive treatment in a clinical setting and need to practice outside of the clinical setting to generalize speech goals to functional communication. In this paper, we describe the development of technology that captures mixed speech signals in a group setting and allows the SLP to analyze the speech signals relative to treatment goals. The mixed speech signals are blindly separated into individual signals that are preprocessed before computation of loudness, pitch, shimmer, jitter, semitone standard deviation and sharpness. The proposed method has been previously validated on data obtained from clinical trials of people with Parkinson disease and healthy controls.|http://arxiv.org/abs/1605.06238v1|Harishchandra Dubey,J. Cody Goldberg,Kunal Mankodiya,Leslie Mahler
945|Augmented Outcome-weighted Learning for Optimal Treatment Regimes|Precision medicine is of considerable interest in clinical, academic and regulatory parties. The key to precision medicine is the optimal treatment regime. Recently, Zhou et al. (2017) developed residual weighted learning (RWL) to construct the optimal regime that directly optimize the clinical outcome. However, this method involves computationally intensive non-convex optimization, which cannot guarantee a global solution. Furthermore, this method does not possess fully semiparametrical efficiency. In this article, we propose augmented outcome-weighted learning (AOL). The method is built on a doubly robust augmented inverse probability weighted estimator, and hence constructs semiparametrically efficient regimes. Our proposed AOL is closely related to RWL. The weights are obtained from counterfactual residuals, where negative residuals are reflected to positive and accordingly their treatment assignments are switched to opposites. Convex loss functions are thus applied to guarantee a global solution and to reduce computations. We show that AOL is universally consistent, i.e., the estimated regime of AOL converges the Bayes regime when the sample size approaches infinity, without knowing any specifics of the distribution of the data. We also propose variable selection methods for linear and nonlinear regimes, respectively, to further improve performance. The performance of the proposed AOL methods is illustrated in simulation studies and in an analysis of the Nefazodone-CBASP clinical trial data.|http://arxiv.org/abs/1711.10654v1|Xin Zhou,Michael R. Kosorok
946|A Likelihood-based Alternative to Null Hypothesis Significance Testing|The logical and practical difficulties associated with research interpretation using P values and null hypothesis significance testing have been extensively documented. This paper describes an alternative, likelihood-based approach to P-value interpretation. The P-value and sample size of a research study are used to derive a likelihood function with a single parameter, the estimated population effect size, and the method of maximum likelihood estimation is used to calculate the most likely effect size. Comparison of the likelihood of the most likely effect size and the likelihood of the minimum clinically significant effect size using the likelihood ratio test yields the clinical significance support level (or S-value), a logical and easily understood metric of research evidence. This clinical significance likelihood approach has distinct advantages over null hypothesis significance testing. As motivating examples we demonstrate the calculation and interpretation of S-values applied to two recent widely publicised trials, WOMAN from the Lancet and RELIEF from the New England Journal of Medicine.|http://arxiv.org/abs/1806.02419v4|Nicholas Adams,Gerard O'Reilly
947|A Deep-learning Approach for Prognosis of Age-Related Macular Degeneration Disease using SD-OCT Imaging Biomarkers|We propose a hybrid sequential deep learning model to predict the risk of AMD progression in non-exudative AMD eyes at multiple timepoints, starting from short-term progression (3-months) up to long-term progression (21-months). Proposed model combines radiomics and deep learning to handle challenges related to imperfect ratio of OCT scan dimension and training cohort size. We considered a retrospective clinical trial dataset that includes 671 fellow eyes with 13,954 dry AMD observations for training and validating the machine learning models on a 10-fold cross validation setting. The proposed RNN model achieved high accuracy (0.96 AUCROC) for the prediction of both short term and long-term AMD progression, and outperformed the traditional random forest model trained. High accuracy achieved by the RNN establishes the ability to identify AMD patients at risk of progressing to advanced AMD at an early stage which could have a high clinical impact as it allows for optimal clinical follow-up, with more frequent screening and potential earlier treatment for those patients at high risk.|http://arxiv.org/abs/1902.10700v1|Imon Banerjee,Luis de Sisternes,Joelle Hallak,Theodore Leng,Aaron Osborne,Mary Durbin,Daniel Rubin
948|PRISM: Patient Response Identifiers for Stratified Medicine|Pharmaceutical companies continue to seek innovative ways to explore whether a drug under development is likely to be suitable for all or only an identifiable stratum of patients in the target population. The sooner this can be done during the clinical development process, the better it is for the company, and downstream for prescribers, payers, and most importantly, for patients. To help enable this vision of stratified medicine, we describe a powerful statistical framework, Patient Response Identifiers for Stratified Medicine (PRISM), for the discovery of potential predictors of drug response and associated subgroups using machine learning tools. PRISM is highly flexible and can have many "configurations", allowing the incorporation of complementary models or tools for a variety of outcomes and settings. One promising PRISM configuration is to use the observed outcomes for subgroup identification, while using counterfactual within-patient predicted treatment differences for subgroup-specific treatment estimates and associated interpretation. This separates the "subgroup-identification" from the "decision-making" and, to facilitate clinical design planning, is a simple way to obtain unbiased treatment effect sizes in the discovered subgroups. Simulation results, along with data from a real clinical trial are used to illustrate the utility of the proposed PRISM framework.|http://arxiv.org/abs/1912.03337v1|Thomas O. Jemielita,Devan V. Mehrotra
949|Prediction meets causal inference: the role of treatment in clinical prediction models|In this paper we study approaches for dealing with treatment when developing a clinical prediction model. Analogous to the estimand framework recently proposed by the European Medicines Agency for clinical trials, we propose a `predictimand' framework of different questions that may be of interest when predicting risk in relation to treatment started after baseline. We provide a formal definition of the estimands matching these questions, give examples of settings in which each is useful and discuss appropriate estimators including their assumptions. We illustrate the impact of the predictimand choice in a dataset of patients with end-stage kidney disease. We argue that clearly defining the estimand is equally important in prediction research as in causal inference.|http://arxiv.org/abs/2004.06998v1|Nan van Geloven,Sonja Swanson,Chava Ramspek,Kim Luijken,Merel van Diepen,Tim Morris,Rolf Groenwold,Hans van Houwelingen,Hein Putter,Saskia le Cessie
950|Labeling of Multilingual Breast MRI Reports|Medical reports are an essential medium in recording a patient's condition throughout a clinical trial. They contain valuable information that can be extracted to generate a large labeled dataset needed for the development of clinical tools. However, the majority of medical reports are stored in an unregularized format, and a trained human annotator (typically a doctor) must manually assess and label each case, resulting in an expensive and time consuming procedure. In this work, we present a framework for developing a multilingual breast MRI report classifier using a custom-built language representation called LAMBR. Our proposed method overcomes practical challenges faced in clinical settings, and we demonstrate improved performance in extracting labels from medical reports when compared with conventional approaches.|http://arxiv.org/abs/2007.03028v3|Chen-Han Tsai,Nahum Kiryati,Eli Konen,Miri Sklair-Levy,Arnaldo Mayer
951|Identifying and Analyzing Sepsis States: A Retrospective Study on Patients with Sepsis in ICUs|Sepsis accounts for more than 50% of hospital deaths, and the associated cost ranks the highest among hospital admissions in the US. Improved understanding of disease states, severity, and clinical markers has the potential to significantly improve patient outcomes and reduce cost. We develop a computational framework that identifies disease states in sepsis using clinical variables and samples in the MIMIC-III database. We identify six distinct patient states in sepsis, each associated with different manifestations of organ dysfunction. We find that patients in different sepsis states are statistically significantly composed of distinct populations with disparate demographic and comorbidity profiles. Collectively, our framework provides a holistic view of sepsis, and our findings provide the basis for future development of clinical trials and therapeutic strategies for sepsis.|http://arxiv.org/abs/2009.10820v2|Chih-Hao Fang,Vikram Ravindra,Salma Akhter,Mohammad Adibuzzaman,Paul Griffin,Shankar Subramaniam,Ananth Grama
952|A Decision-Theoretic Comparison of Treatments to Resolve Air Leaks After Lung Surgery Based on Nonparametric Modeling|We propose a Bayesian nonparametric utility-based group sequential design for a randomized clinical trial to compare a gel sealant to standard care for resolving air leaks after pulmonary resection. Clinically, resolving air leaks in the days soon after surgery is highly important, since longer resolution time produces undesirable complications that require extended hospitalization. The problem of comparing treatments is complicated by the fact that the resolution time distributions are skewed and multi-modal, so using means is misleading. We address these challenges by assuming Bayesian nonparametric probability models for the resolution time distributions and basing the comparative test on weighted means. The weights are elicited as clinical utilities of the resolution times. The proposed design uses posterior expected utilities as group sequential test criteria. The procedure's frequentist properties are studied by extensive simulations.|http://arxiv.org/abs/1506.07687v2|Yanxun Xu,Peter F. Thall,Peter Mueller,Mehran J. Reza
953|ESD CYCLOPS: A new robotic surgical system for GI surgery|Gastrointestinal (GI) cancers account for 1.5 million deaths worldwide. Endoscopic Submucosal Dissection (ESD) is an advanced therapeutic endoscopy technique with superior clinical outcome due to the minimally invasive and en bloc removal of tumours. In the western world, ESD is seldom carried out, due to its complex and challenging nature. Various surgical systems are being developed to make this therapy accessible, however, these solutions have shown limited operational workspace, dexterity, or low force exertion capabilities. The current paper shows the ESD CYCLOPS system, a bimanual surgical robotic attachment that can be mounted at the end of any flexible endoscope. The system is able to achieve forces of up to 46N, and showed a mean error of 0.217mm during an elliptical tracing task. The workspace and instrument dexterity is shown by pre-clinical ex vivo trials, in which ESD is succesfully performed by a GI surgeon. The system is currently undergoing pre-clinical in vivo validation.|http://arxiv.org/abs/1712.03388v2|Timo J. C. Oude Vrielink,Ming Zhao,Ara Darzi,George P. Mylonas
954|Generating Radiology Reports via Memory-driven Transformer|Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain. In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer. Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings.|http://arxiv.org/abs/2010.16056v2|Zhihong Chen,Yan Song,Tsung-Hui Chang,Xiang Wan
955|Neural Entity Recognition with Gazetteer based Fusion|Incorporating external knowledge into Named Entity Recognition (NER) systems has been widely studied in the generic domain. In this paper, we focus on clinical domain where only limited data is accessible and interpretability is important. Recent advancement in technology and the acceleration of clinical trials has resulted in the discovery of new drugs, procedures as well as medical conditions. These factors motivate towards building robust zero-shot NER systems which can quickly adapt to new medical terminology. We propose an auxiliary gazetteer model and fuse it with an NER system, which results in better robustness and interpretability across different clinical datasets. Our gazetteer based fusion model is data efficient, achieving +1.7 micro-F1 gains on the i2b2 dataset using 20% training data, and brings + 4.7 micro-F1 gains on novel entity mentions never presented during training. Moreover, our fusion model is able to quickly adapt to new mentions in gazetteers without re-training and the gains from the proposed fusion model are transferable to related datasets.|http://arxiv.org/abs/2105.13225v1|Qing Sun,Parminder Bhatia
956|Automatic Detection of COVID-19 and Pneumonia from Chest X-Ray using Deep Learning|In this study, a dataset of X-ray images from patients with common viral pneumonia, bacterial pneumonia, confirmed Covid-19 disease was utilized for the automatic detection of the Coronavirus disease. The point of the investigation is to assess the exhibition of cutting edge convolutional neural system structures proposed over the ongoing years for clinical picture order. In particular, the system called Transfer Learning was received. With transfer learning, the location of different variations from the norm in little clinical picture datasets is a reachable objective, regularly yielding amazing outcomes. The datasets used in this trial. Firstly, a collection of 24000 X-ray images includes 6000 images for confirmed Covid-19 disease,6000 confirmed common bacterial pneumonia and 6000 images of normal conditions. The information was gathered and expanded from the accessible X-Ray pictures on open clinical stores. The outcomes recommend that Deep Learning with X-Ray imaging may separate noteworthy biological markers identified with the Covid-19 sickness, while the best precision, affectability, and particularity acquired is 97.83%, 96.81%, and 98.56% individually.|http://arxiv.org/abs/2110.09384v1|Sarath Pathari
957|Longitudinal patient stratification of electronic health records with flexible adjustment for clinical outcomes|The increase in availability of longitudinal electronic health record (EHR) data is leading to improved understanding of diseases and discovery of novel phenotypes. The majority of clustering algorithms focus only on patient trajectories, yet patients with similar trajectories may have different outcomes. Finding subgroups of patients with different trajectories and outcomes can guide future drug development and improve recruitment to clinical trials. We develop a recurrent neural network autoencoder to cluster EHR data using reconstruction, outcome, and clustering losses which can be weighted to find different types of patient clusters. We show our model is able to discover known clusters from both data biases and outcome differences, outperforming baseline models. We demonstrate the model performance on $29,229$ diabetes patients, showing it finds clusters of patients with both different trajectories and different outcomes which can be utilized to aid clinical decision making.|http://arxiv.org/abs/2111.06152v1|Oliver Carr,Avelino Javer,Patrick Rockenschaub,Owen Parsons,Robert Drichen
958|Flexible Inference of Optimal Individualized Treatment Strategy in Covariate Adjusted Randomization with Multiple Covariates|To maximize clinical benefit, clinicians routinely tailor treatment to the individual characteristics of each patient, where individualized treatment rules are needed and are of significant research interest to statisticians. In the covariate-adjusted randomization clinical trial with many covariates, we model the treatment effect with an unspecified function of a single index of the covariates and leave the baseline response completely arbitrary. We devise a class of estimators to consistently estimate the treatment effect function and its associated index while bypassing the estimation of the baseline response, which is subject to the curse of dimensionality. We further develop inference tools to identify predictive covariates and isolate effective treatment region. The usefulness of the methods is demonstrated in both simulations and a clinical data example.|http://arxiv.org/abs/2111.10425v1|Trinetri Ghosh,Yanyuan Ma,Rui Song,Pingshou Zhong
959|Digital Therapeutics for Mental Health: Is Attrition the Achilles Heel?|Digit therapeutics are novel software devices that clinicians may utilize in delivering quality mental health care and ensuring positive outcomes. However, uptake of digital therapeutics and clinically tested software-based programs remains low. This article presents possible reasons for attrition and low engagement in clinical studies investigating digital therapeutics, analyses of studies in which engagement was high, and design constructs that may encourage user engagement. The aim is to shed light on the importance of real-world attrition data of digital therapeutics, and important characteristics of medical devices that have positively influenced user engagement. The findings presented in this article will be useful to relevant stakeholders and medical device experts tasked with addressing the gap between software medical design and user engagement present in digital therapeutic clinical trials.|http://arxiv.org/abs/2207.05179v2|Adaora Nwosu,Samantha Boardman,Mustafa M. Husain,P. Murali Doraiswamy
960|Duration of and time to response in oncology clinical trials from the perspective of the estimand framework|Duration of response (DOR) and time to response (TTR) are typically evaluated as secondary endpoints in early-stage clinical studies in oncology when efficacy is assessed by the best overall response (BOR) and presented as the overall response rate (ORR). Despite common use of DOR and TTR in particular in single-arm studies, the definition of these endpoints and the questions they are intended to answer remain unclear. Motivated by the estimand framework, we present relevant scientific questions of interest for DOR and TTR and propose corresponding estimand definitions. We elaborate on how to deal with relevant intercurrent events which should follow the same considerations as implemented for the primary response estimand. A case study in mantle cell lymphoma illustrates the implementation of relevant estimands of DOR and TTR. We close the paper with practical recommendations to implement DOR and TTR in clinical study protocols.|http://arxiv.org/abs/2212.10911v1|Hans-Jochen Weber,Stephen Corson,Jiang Li,Francois Mercier,Satrajit Roychoudhury,Martin Oliver Sailer,Stephen Sun,Alexander Todd,Godwin Yung
961|The Smart Data Extractor, a Clinician Friendly Solution to Accelerate and Improve the Data Collection During Clinical Trials|In medical research, the traditional way to collect data, i.e. browsing patient files, has been proven to induce bias, errors, human labor and costs. We propose a semi-automated system able to extract every type of data, including notes. The Smart Data Extractor pre-populates clinic research forms by following rules. We performed a cross-testing experiment to compare semi-automated to manual data collection. 20 target items had to be collected for 79 patients. The average time to complete one form was 6'81'' for manual data collection and 3'22'' with the Smart Data Extractor. There were also more mistakes during manual data collection (163 for the whole cohort) than with the Smart Data Extractor (46 for the whole cohort). We present an easy to use, understandable and agile solution to fill out clinical research forms. It reduces human effort and provides higher quality data, avoiding data re-entry and fatigue induced errors.|http://arxiv.org/abs/2308.16537v1|Sophie Quennelle,Maxime Douillet,Lisa Friedlander,Olivia Boyer,Anita Burgun,Antoine Neuraz,Nicolas Garcelon
962|Principles from Clinical Research for NLP Model Generalization|The NLP community typically relies on performance of a model on a held-out test set to assess generalization. Performance drops observed in datasets outside of official test sets are generally attributed to "out-of-distribution" effects. Here, we explore the foundations of generalizability and study the factors that affect it, articulating lessons from clinical studies. In clinical research, generalizability is an act of reasoning that depends on (a) internal validity of experiments to ensure controlled measurement of cause and effect, and (b) external validity or transportability of the results to the wider population. We demonstrate how learning spurious correlations, such as the distance between entities in relation extraction tasks, can affect a model's internal validity and in turn adversely impact generalization. We, therefore, present the need to ensure internal validity when building machine learning models in NLP. Our recommendations also apply to generative large language models, as they are known to be sensitive to even minor semantic preserving alterations. We also propose adapting the idea of matching in randomized controlled trials and observational studies to NLP evaluation to measure causation.|http://arxiv.org/abs/2311.03663v3|Aparna Elangovan,Jiayuan He,Yuan Li,Karin Verspoor
963|CARE: Extracting Experimental Findings From Clinical Literature|Extracting fine-grained experimental findings from literature can provide dramatic utility for scientific applications. Prior work has developed annotation schemas and datasets for limited aspects of this problem, failing to capture the real-world complexity and nuance required. Focusing on biomedicine, this work presents CARE -- a new IE dataset for the task of extracting clinical findings. We develop a new annotation schema capturing fine-grained findings as n-ary relations between entities and attributes, which unifies phenomena challenging for current IE systems such as discontinuous entity spans, nested relations, variable arity n-ary relations and numeric results in a single schema. We collect extensive annotations for 700 abstracts from two sources: clinical trials and case reports. We also demonstrate the generalizability of our schema to the computer science and materials science domains. We benchmark state-of-the-art IE systems on CARE, showing that even models such as GPT4 struggle. We release our resources to advance research on extracting and aggregating literature findings.|http://arxiv.org/abs/2311.09736v2|Aakanksha Naik,Bailey Kuehl,Erin Bransom,Doug Downey,Tom Hope
964|Comprehensive Joint Modeling of First-Line Therapeutics in Non-Small Cell Lung Cancer|First-line antiproliferatives for non-small cell lung cancer (NSCLC) have a relatively high failure rate due to high intrinsic resistance rates and acquired resistance rates to therapy. 57% patients are diagnosed in late-stage disease due to the tendency of early-stage NSCLC to be asymptomatic. For patients first diagnosed with metastatic disease the 5-year survival rate is approximately 5%. To help accelerate the development of novel therapeutics and computer-based tools for optimizing individual therapy, we have collated data from 11 different clinical trials in NSCLC and developed a semi-mechanistic, clinical model of NSCLC growth and pharmacodynamics relative to the various therapeutics represented in the study. In this study, we have produced extremely precise estimates of clinical parameters fundamental to cancer modeling such as the rate of acquired resistance to various pharmaceuticals, the relationship between drug concentration and rate of cancer cell death, as well as the fine temporal dynamics of anti-VEGF therapy. In the simulation sets documented in this study, we have used the model to make meaningful descriptions of efficacy gain in making bevacizumab-antiproliferative combination therapy sequential, over a series of days, rather than concurrent.|http://arxiv.org/abs/2401.07719v1|Benjamin Schneider,Sbastien Benzekry,Jonathan Mochel
965|A Large Language Model Pipeline for Breast Cancer Oncology|Large language models (LLMs) have demonstrated potential in the innovation of many disciplines. However, how they can best be developed for oncology remains underdeveloped. State-of-the-art OpenAI models were fine-tuned on a clinical dataset and clinical guidelines text corpus for two important cancer treatment factors, adjuvant radiation therapy and chemotherapy, using a novel Langchain prompt engineering pipeline. A high accuracy (0.85+) was achieved in the classification of adjuvant radiation therapy and chemotherapy for breast cancer patients. Furthermore, a confidence interval was formed from observational data on the quality of treatment from human oncologists to estimate the proportion of scenarios in which the model must outperform the original oncologist in its treatment prediction to be a better solution overall as 8.2% to 13.3%. Due to indeterminacy in the outcomes of cancer treatment decisions, future investigation, potentially a clinical trial, would be required to determine if this threshold was met by the models. Nevertheless, with 85% of U.S. cancer patients receiving treatment at local community facilities, these kinds of models could play an important part in expanding access to quality care with outcomes that lie, at minimum, close to a human oncologist.|http://arxiv.org/abs/2406.06455v2|Tristen Pool,Dennis Trujillo
966|Estimating causal effects of time-dependent exposures on a binary endpoint in a high-dimensional setting|Recently, the intervention calculus when the DAG is absent (IDA) method was developed to estimate lower bounds of causal effects from observational high-dimensional data. Originally it was introduced to assess the effect of baseline biomarkers which do not vary over time. However, in many clinical settings, measurements of biomarkers are repeated at fixed time points during treatment exposure and, therefore, this method need to be extended. The purpose of this paper is then to extend the first step of the IDA, the Peter Clarks (PC)-algorithm, to a time-dependent exposure in the context of a binary outcome. We generalised the PC-algorithm for taking into account the chronological order of repeated measurements of the exposure and propose to apply the IDA with our new version, the chronologically ordered PC-algorithm (COPC-algorithm). A simulation study has been performed before applying the method for estimating causal effects of time-dependent immunological biomarkers on toxicity, death and progression in patients with metastatic melanoma. The simulation study showed that the completed partially directed acyclic graphs (CPDAGs) obtained using COPC-algorithm were structurally closer to the true CPDAG than CPDAGs obtained using PC-algorithm. Also, causal effects were more accurate when they were estimated based on CPDAGs obtained using COPC-algorithm. Moreover, CPDAGs obtained by COPC-algorithm allowed removing non-chronologic arrows with a variable measured at a time t pointing to a variable measured at a time t' where t'< t. Bidirected edges were less present in CPDAGs obtained with the COPC-algorithm, supporting the fact that there was less variability in causal effects estimated from these CPDAGs. The COPC-algorithm provided CPDAGs that keep the chronological structure present in the data, thus allowed to estimate lower bounds of the causal effect of time-dependent biomarkers.|http://arxiv.org/abs/1803.10535v2|Vah Asvatourian,Cllia Coutzac,Nathalie Chaput,Caroline Robert,Stefan Michiels,Emilie Lanoy
967|Neural Network Segmentation of Interstitial Fibrosis, Tubular Atrophy, and Glomerulosclerosis in Renal Biopsies|Glomerulosclerosis, interstitial fibrosis, and tubular atrophy (IFTA) are histologic indicators of irrecoverable kidney injury. In standard clinical practice, the renal pathologist visually assesses, under the microscope, the percentage of sclerotic glomeruli and the percentage of renal cortical involvement by IFTA. Estimation of IFTA is a subjective process due to a varied spectrum and definition of morphological manifestations. Modern artificial intelligence and computer vision algorithms have the ability to reduce inter-observer variability through rigorous quantitation. In this work, we apply convolutional neural networks for the segmentation of glomerulosclerosis and IFTA in periodic acid-Schiff stained renal biopsies. The convolutional network approach achieves high performance in intra-institutional holdout data, and achieves moderate performance in inter-intuitional holdout data, which the network had never seen in training. The convolutional approach demonstrated interesting properties, such as learning to predict regions better than the provided ground truth as well as developing its own conceptualization of segmental sclerosis. Subsequent estimations of IFTA and glomerulosclerosis percentages showed high correlation with ground truth.|http://arxiv.org/abs/2002.12868v1|Brandon Ginley,Kuang-Yu Jen,Avi Rosenberg,Felicia Yen,Sanjay Jain,Agnes Fogo,Pinaki Sarder
968|Family based HLA imputation and optimization of haplo-identical transplants|Recently, haplo-identical transplantation with multiple HLA mismatches has become a viable option for system cell transplants. Haplotype sharing detection requires imputation of donor and recipient. We show that even in high-resolution typing when all alleles are known, there is a 15% error rate in haplotype phasing, and even more in low resolution typings. Similarly, in related donors, parents haplotypes should be imputed to determine what haplotype each child inherited. We propose GRAMM (GRaph bAsed FaMilly iMputation) to phase alleles in family pedigree HLA typing data, and in mother-cord blood unit pairs. We show that GRAMM has practically no phasing errors when pedigree data are available. We apply GRAMM to simulations with different typing resolutions as well as paired cord-mother typings, and show very high phasing accuracy, and improved alleles imputation accuracy. We use GRAMM to detect recombination events and show that the rate of falsely detected recombination events (False Positive Rate) in simulations is very low. We then apply recombination detection to typed families to estimate the recombination rate in Israeli and Australian population datasets. The estimated recombination rate has an upper bound of 10-20% per family (1-4% per individual). GRAMM is available at: https://gramm.math.biu.ac.il/.|http://arxiv.org/abs/2208.05882v1|Zuriya Ansbacher-Feldman,Sapir Israeli,Martin Maiers,Loren Gragert,Dianne De Santis,Moshe Israeli,Yoram Louzoun
969|Efficient Similarity-Preserving Unsupervised Learning using Modular Sparse Distributed Codes and Novelty-Contingent Noise|There is increasing realization in neuroscience that information is represented in the brain, e.g., neocortex, hippocampus, in the form sparse distributed codes (SDCs), a kind of cell assembly. Two essential questions are: a) how are such codes formed on the basis of single trials, and how is similarity preserved during learning, i.e., how do more similar inputs get mapped to more similar SDCs. I describe a novel Modular Sparse Distributed Code (MSDC) that provides simple, neurally plausible answers to both questions. An MSDC coding field (CF) consists of Q WTA competitive modules (CMs), each comprised of K binary units (analogs of principal cells). The modular nature of the CF makes possible a single-trial, unsupervised learning algorithm that approximately preserves similarity and crucially, runs in fixed time, i.e., the number of steps needed to store an item remains constant as the number of stored items grows. Further, once items are stored as MSDCs in superposition and such that their intersection structure reflects input similarity, both fixed time best-match retrieval and fixed time belief update (updating the probabilities of all stored items) also become possible. The algorithm's core principle is simply to add noise into the process of choosing a code, i.e., choosing a winner in each CM, which is proportional to the novelty of the input. This causes the expected intersection of the code for an input, X, with the code of each previously stored input, Y, to be proportional to the similarity of X and Y. Results demonstrating these capabilities for spatial patterns are given in the appendix.|http://arxiv.org/abs/2010.10926v1|Rod Rinkus
970|Design and Implementation of DC-to-5~MHz Wide-Bandwidth High-Power High-Fidelity Converter|Advances in power electronics have made it possible to achieve high power levels, e.g., reaching GW in grids, or alternatively high output bandwidths, e.g., beyond MHz in communication. Achieving both simultaneously, however, remains challenging. Various applications, ranging from efficient multichannel wireless power transfer to cutting-edge medical and neuroscience applications, are demanding both high power and wide bandwidth. Conventional inverters can achieve high power and high quality at grid or specific frequency ranges but lose their fidelity when reaching higher output frequencies. Resonant circuits can promise a high output frequency but only a narrow bandwidth. We overcome the hardware challenges by combining gallium-nitride (GaN) transistors with modular cascaded double-H bridge circuits and control that can manage typical timing and balancing issues. We developed a lightweight embedded control solution that includes an improved look-up-table digital synthesizer and a novel adaptive-bias-elimination nearest-level modulation. This solution effectively solves the conflict between a high power level and high output bandwidth and can--in contrast to previous approaches--in principle be scaled in both dimensions. Our prototype exhibits a frequency range from DC to 5 MHz with <18% total voltage distortion across the entire frequency spectrum, while achieving a power level of >5 kW. We conducted tests by sweeping the output frequency and two channel-mixing trials, which included a practical magnetogenetics-oriented stimulation pulse and an entertaining trial to reproduce the famous Arecibo message with the current spectrum.|http://arxiv.org/abs/2309.06409v1|Jinshui Zhang,Boshuo Wang,Xiaoyang Tian,Angel Peterchev,Stefan Goetz
971|Neuro-Vision to Language: Enhancing Brain Recording-based Visual Reconstruction and Language Interaction|Decoding non-invasive brain recordings is pivotal for advancing our understanding of human cognition but faces challenges due to individual differences and complex neural signal representations. Traditional methods often require customized models and extensive trials, lacking interpretability in visual reconstruction tasks. Our framework integrates 3D brain structures with visual semantics using a Vision Transformer 3D. This unified feature extractor efficiently aligns fMRI features with multiple levels of visual embeddings, eliminating the need for subject-specific models and allowing extraction from single-trial data. The extractor consolidates multi-level visual features into one network, simplifying integration with Large Language Models (LLMs). Additionally, we have enhanced the fMRI dataset with diverse fMRI-image-related textual data to support multimodal large model development. Integrating with LLMs enhances decoding capabilities, enabling tasks such as brain captioning, complex reasoning, concept localization, and visual reconstruction. Our approach demonstrates superior performance across these tasks, precisely identifying language-based concepts within brain signals, enhancing interpretability, and providing deeper insights into neural processes. These advances significantly broaden the applicability of non-invasive brain decoding in neuroscience and human-computer interaction, setting the stage for advanced brain-computer interfaces and cognitive models.|http://arxiv.org/abs/2404.19438v4|Guobin Shen,Dongcheng Zhao,Xiang He,Linghao Feng,Yiting Dong,Jihang Wang,Qian Zhang,Yi Zeng
972|Fast Multi-Group Gaussian Process Factor Models|Gaussian processes are now commonly used in dimensionality reduction approaches tailored to neuroscience, especially to describe changes in high-dimensional neural activity over time. As recording capabilities expand to include neuronal populations across multiple brain areas, cortical layers, and cell types, interest in extending Gaussian process factor models to characterize multi-population interactions has grown. However, the cubic runtime scaling of current methods with the length of experimental trials and the number of recorded populations (groups) precludes their application to large-scale multi-population recordings. Here, we improve this scaling from cubic to linear in both trial length and group number. We present two approximate approaches to fitting multi-group Gaussian process factor models based on (1) inducing variables and (2) the frequency domain. Empirically, both methods achieved orders of magnitude speed-up with minimal impact on statistical performance, in simulation and on neural recordings of hundreds of neurons across three brain areas. The frequency domain approach, in particular, consistently provided the greatest runtime benefits with the fewest trade-offs in statistical performance. We further characterize the estimation biases introduced by the frequency domain approach and demonstrate effective strategies to mitigate them. This work enables a powerful class of analysis techniques to keep pace with the growing scale of multi-population recordings, opening new avenues for exploring brain function.|http://arxiv.org/abs/2412.16773v1|Evren Gokcen,Anna I. Jasper,Adam Kohn,Christian K. Machens,Byron M. Yu
973|Survival analysis for AdVerse events with VarYing follow-up times (SAVVY) -- estimation of adverse event risks|The SAVVY project aims to improve the analyses of adverse event (AE) data in clinical trials through the use of survival techniques appropriately dealing with varying follow-up times and competing events (CEs). Although statistical methodologies have advanced, in AE analyses often the incidence proportion, the incidence density, or a non-parametric Kaplan-Meier estimator (KME) are used, which either ignore censoring or CEs. In an empirical study including randomized clinical trials from several sponsor organisations, these potential sources of bias are investigated. The main aim is to compare the estimators that are typically used in AE analysis to the Aalen-Johansen estimator (AJE) as the gold-standard. Here, one-sample findings are reported, while a companion paper considers consequences when comparing treatment groups. Estimators are compared with descriptive statistics, graphical displays and with a random effects meta-analysis. The influence of different factors on the size of the bias is investigated in a meta-regression. Comparisons are conducted at the maximum follow-up time and at earlier evaluation time points. CEs definition does not only include death before AE but also end of follow-up for AEs due to events possibly related to the disease course or the treatment. Ten sponsor organisations provided 17 trials including 186 types of AEs. The one minus KME was on average about 1.2-fold larger than the AJE. Leading forces influencing bias were the amount of censoring and of CEs. As a consequence, the average bias using the incidence proportion was less than 5%. Assuming constant hazards using incidence densities was hardly an issue provided that CEs were accounted for. There is a need to improve the guidelines of reporting risks of AEs so that the KME and the incidence proportion are replaced by the AJE with an appropriate definition of CEs.|http://arxiv.org/abs/2008.07883v1|Regina Stegherr,Claudia Schmoor,Jan Beyersmann,Kaspar Rufibach,Valentine Jehl,Andreas Brckner,Lewin Eisele,Thomas Knzel,Katrin Kupas,Frank Langer,Friedhelm Leverkus,Anja Loos,Christiane Norenberg,Florian Voss,Tim Friede
974|Estimation of treatment policy estimands for continuous outcomes using off treatment sequential multiple imputation|The estimands framework outlined in ICH E9 (R1) describes the components needed to precisely define the effects to be estimated in clinical trials, which includes how post-baseline "intercurrent" events (IEs) are to be handled. In late-stage clinical trials, it is common to handle intercurrent events like "treatment discontinuation" using the treatment policy strategy and target the treatment effect on all outcomes regardless of treatment discontinuation. For continuous repeated measures, this type of effect is often estimated using all observed data before and after discontinuation using either a mixed model for repeated measures (MMRM) or multiple imputation (MI) to handle any missing data. In basic form, both of these estimation methods ignore treatment discontinuation in the analysis and therefore may be biased if there are differences in patient outcomes after treatment discontinuation compared to patients still assigned to treatment, and missing data being more common for patients who have discontinued treatment. We therefore propose and evaluate a set of MI models that can accommodate differences between outcomes before and after treatment discontinuation. The models are evaluated in the context of planning a phase 3 trial for a respiratory disease. We show that analyses ignoring treatment discontinuation can introduce substantial bias and can sometimes underestimate variability. We also show that some of the MI models proposed can successfully correct the bias but inevitably lead to increases in variance. We conclude that some of the proposed MI models are preferable to the traditional analysis ignoring treatment discontinuation, but the precise choice of MI model will likely depend on the trial design, disease of interest and amount of observed and missing data following treatment discontinuation.|http://arxiv.org/abs/2308.10857v2|Thomas Drury,Juan J Abellan,Nicky Best,Ian R. White
975|Development, validation and clinical usefulness of a prognostic model for relapse in relapsing-remitting multiple sclerosis|Prognosis on the occurrence of relapses in individuals with Relapsing-Remitting Multiple Sclerosis (RRMS), the most common subtype of Multiple Sclerosis (MS), could support individualized decisions and disease management and could be helpful for efficiently selecting patients in future randomized clinical trials. There are only three previously published prognostic models on this, all of them with important methodological shortcomings.   We aim to present the development, internal validation, and evaluation of the potential clinical benefit of a prognostic model for relapses for individuals with RRMS using real world data. We followed seven steps to develop and validate the prognostic model. Finally, we evaluated the potential clinical benefit of the developed prognostic model using decision curve analysis.   We selected eight baseline prognostic factors: age, sex, prior MS treatment, months since last relapse, disease duration, number of prior relapses, expanded disability status scale (EDSS), and gadolinium enhanced lesions. We also developed a web application where the personalized probabilities to relapse within two years are calculated automatically. The optimism-corrected c-statistic is 0.65 and the optimism-corrected calibration slope was 0.92. The model appears to be clinically useful between the range 15% and 30% of the threshold probability to relapse.   The prognostic model we developed offers several advantages in comparison to previously published prognostic models on RRMS. Importantly, we assessed the potential clinical benefit to better quantify the clinical impact of the model. Our web application, once externally validated in the future, could be used by patients and doctors to calculate the individualized probability to relapse within two years and to inform the management of their disease.|http://arxiv.org/abs/2105.06941v2|Konstantina Chalkou,Ewout Steyerberg,Patrick Bossuyt,Suvitha Subramanian,Pascal Benkert,Jens Kuhle,Giulio Disanto,Ludwig Kappos,Matthias Egger,Georgia Salanti
976|Ten years of image analysis and machine learning competitions in dementia|Machine learning methods exploiting multi-parametric biomarkers, especially based on neuroimaging, have huge potential to improve early diagnosis of dementia and to predict which individuals are at-risk of developing dementia. To benchmark algorithms in the field of machine learning and neuroimaging in dementia and assess their potential for use in clinical practice and clinical trials, seven grand challenges have been organized in the last decade.   The seven grand challenges addressed questions related to screening, clinical status estimation, prediction and monitoring in (pre-clinical) dementia. There was little overlap in clinical questions, tasks and performance metrics. Whereas this aids providing insight on a broad range of questions, it also limits the validation of results across challenges. The validation process itself was mostly comparable between challenges, using similar methods for ensuring objective comparison, uncertainty estimation and statistical testing. In general, winning algorithms performed rigorous data preprocessing and combined a wide range of input features.   Despite high state-of-the-art performances, most of the methods evaluated by the challenges are not clinically used. To increase impact, future challenges could pay more attention to statistical analysis of which factors relate to higher performance, to clinical questions beyond Alzheimer's disease, and to using testing data beyond the Alzheimer's Disease Neuroimaging Initiative. Grand challenges would be an ideal venue for assessing the generalizability of algorithm performance to unseen data of other cohorts. Key for increasing impact in this way are larger testing data sizes, which could be reached by sharing algorithms rather than data to exploit data that cannot be shared.|http://arxiv.org/abs/2112.07922v2|Esther E. Bron,Stefan Klein,Annika Reinke,Janne M. Papma,Lena Maier-Hein,Daniel C. Alexander,Neil P. Oxtoby
977|Sensing Danger: Innate Immunology for Intrusion Detection|The immune system provides an ideal metaphor for anomaly detection in general and computer security in particular. Based on this idea, artificial immune systems have been used for a number of years for intrusion detection, unfortunately so far with little success. However, these previous systems were largely based on immunological theory from the 1970s and 1980s and over the last decade our understanding of immunological processes has vastly improved. In this paper we present two new immune inspired algorithms based on the latest immunological discoveries, such as the behaviour of Dendritic Cells. The resultant algorithms are applied to real world intrusion problems and show encouraging results. Overall, we believe there is a bright future for these next generation artificial immune algorithms.|http://arxiv.org/abs/0802.4002v3|Uwe Aickelin,Julie Greensmith
978|A statistical mechanics approach to autopoietic immune networks|The aim of this work is to try to bridge over theoretical immunology and disordered statistical mechanics. Our long term hope is to contribute to the development of a quantitative theoretical immunology from which practical applications may stem. In order to make theoretical immunology appealing to the statistical physicist audience we are going to work out a research article which, from one side, may hopefully act as a benchmark for future improvements and developments, from the other side, it is written in a very pedagogical way both from a theoretical physics viewpoint as well as from the theoretical immunology one.   Furthermore, we have chosen to test our model describing a wide range of features of the adaptive immune response in only a paper: this has been necessary in order to emphasize the benefit available when using disordered statistical mechanics as a tool for the investigation. However, as a consequence, each section is not at all exhaustive and would deserve deep investigation: for the sake of completeness, we restricted details in the analysis of each feature with the aim of introducing a self-consistent model.|http://arxiv.org/abs/1001.3857v1|Adriano Barra,Elena Agliari
979|A Beginners Guide to Systems Simulation in Immunology|Some common systems modelling and simulation approaches for immune problems are Monte Carlo simulations, system dynamics, discrete-event simulation and agent-based simulation. These methods, however, are still not widely adopted in immunology research. In addition, to our knowledge, there is few research on the processes for the development of simulation models for the immune system. Hence, for this work, we have two contributions to knowledge. The first one is to show the importance of systems simulation to help immunological research and to draw the attention of simulation developers to this research field. The second contribution is the introduction of a quick guide containing the main steps for modelling and simulation in immunology, together with challenges that occur during the model development. Further, this paper introduces an example of a simulation problem, where we test our guidelines.|http://arxiv.org/abs/1307.1597v2|Grazziela P. Figueredo,Peer-Olaf Siebers,Uwe Aickelin,Stephanie Foan
980|Immunological recognition by artificial neural networks|The binding affinity between the T-cell receptors (TCRs) and antigenic peptides mainly determines immunological recognition. It is not a trivial task that T cells identify the digital sequences of peptide amino acids by simply relying on the integrated binding affinity between TCRs and antigenic peptides. To address this problem, we examine whether the affinity-based discrimination of peptide sequences is learnable and generalizable by artificial neural networks (ANNs) that process the digital experimental amino acid sequence information of receptors and peptides. A pair of TCR and peptide sequences correspond to the input for ANNs, while the success or failure of the immunological recognition correspond to the output. The output is obtained by both theoretical model and experimental data. In either case, we confirmed that ANNs could learn the immunological recognition. We also found that a homogenized encoding of amino acid sequence was more effective for the supervised learning task.|http://arxiv.org/abs/1808.03386v2|Jin Xu,Junghyo Jo
981|Modeling COVID-19 vaccine-induced immunological memory development and its links to antibody level and infectiousness|COVID-19 vaccines have proven to be effective against SARS-CoV-2 infection. However, the dynamics of vaccine-induced immunological memory development and neutralizing antibodies generation are not fully understood, limiting vaccine development and vaccination regimen determination. Herein, we constructed a mathematical model to characterize the vaccine-induced immune response based on fitting the viral infection and vaccination datasets. With the example of CoronaVac, we revealed the association between vaccine-induced immunological memory development and neutralizing antibody levels. The establishment of the intact immunological memory requires more than 6 months after the first and second doses, after that a booster shot can induce high levels neutralizing antibodies. By introducing the maximum viral load and recovery time after viral infection, we quantitatively studied the protective effect of vaccines against viral infection. Accordingly, we optimized the vaccination regimen, including dose and vaccination timing, and predicted the effect of the fourth dose. Last, by combining the viral transmission model, we showed the suppression of virus transmission by vaccination, which may be instructive for the development of public health policies.|http://arxiv.org/abs/2204.01700v1|Xin Gao,Jianwei Li,Dianjie Li
982|Hybrid Approaches for our Participation to the n2c2 Challenge on Cohort Selection for Clinical Trials|Objective: Natural language processing can help minimize human intervention in identifying patients meeting eligibility criteria for clinical trials, but there is still a long way to go to obtain a general and systematic approach that is useful for researchers. We describe two methods taking a step in this direction and present their results obtained during the n2c2 challenge on cohort selection for clinical trials. Materials and Methods: The first method is a weakly supervised method using an unlabeled corpus (MIMIC) to build a silver standard, by producing semi-automatically a small and very precise set of rules to detect some samples of positive and negative patients. This silver standard is then used to train a traditional supervised model. The second method is a terminology-based approach where a medical expert selects the appropriate concepts, and a procedure is defined to search the terms and check the structural or temporal constraints. Results: On the n2c2 dataset containing annotated data about 13 selection criteria on 288 patients, we obtained an overall F1-measure of 0.8969, which is the third best result out of 45 participant teams, with no statistically significant difference with the best-ranked team. Discussion: Both approaches obtained very encouraging results and apply to different types of criteria. The weakly supervised method requires explicit descriptions of positive and negative examples in some reports. The terminology-based method is very efficient when medical concepts carry most of the relevant information. Conclusion: It is unlikely that much more annotated data will be soon available for the task of identifying a wide range of patient phenotypes. One must focus on weakly or non-supervised learning methods using both structured and unstructured data and relying on a comprehensive representation of the patients.|http://arxiv.org/abs/1903.07879v2|Xavier Tannier,Nicolas Paris,Hugo Cisneros,Christel Daniel,Matthieu Doutreligne,Catherine Duclos,Nicolas Griffon,Claire Hassen-Khodja,Ivan Lerner,Adrien Parrot,ric Sadou,Cyrina Saussol,Pascal Vaillant
983|Requirement for preclinical prioritization of neuroprotective strategies in stroke: Incorporation of preconditioning|Acute neuroprotection in numerous human clinical trials has been an abject failure. Major systemic-and procedural-based issues have subsequently been identified in both clinical trials and preclinical animal model experimentation. As well, issues related to the neuroprotective moiety itself have contributed to clinical trial failures, including late delivery, mono-targeting, low potency and poor tolerability. Conditioning (pre-or post-) strategies can potentially address these issues and are therefore gaining increasing attention as approaches to protect the brain from cerebral ischemia. In principle, conditioning can address concerns of timing (preconditioning could be pre-emptively applied in high-risk patients, and post-conditioning after patients experience an unannounced brain infarction) and signaling (multi-modal). However, acute neuroprotection and conditioning strategies face a common translational issue: a myriad of possibilities exist, but with no strategy to select optimal candidates. In this review, we argue that what is required is a neuroprotective framework to identify the "best" agent(s), at the earliest investigational stage possible. This may require switching mindsets from identifying how neuroprotection can be achieved to determining how neuroprotection can fail, for the vast majority of candidates. Understanding the basis for failure can in turn guide supplementary treatment, thereby forming an evidence-based rationale for selecting combinations of therapies. An appropriately designed in vitro (neuron culture, brain slices) approach, based on increasing the harshness of the ischemic-like insult, can be useful in identifying the "best" conditioner or acute neuroprotective therapy, as well as how the two modalities can be combined to overcome individual limitations. This would serve as a base from which to launch further investigation into therapies required to protect the neurovascular unit in in vivo animal models of cerebral ischemia. Based on these respective approaches, our laboratories suggest that there is merit in examining synaptic activity-and nutraceutical-based preconditioning / acute neuroprotection.|http://arxiv.org/abs/1908.03332v1|Tauskela Joseph S.,Blondeau Nicolas
984|New drugs and stock market: how to predict pharma market reaction to clinical trial announcements|Pharmaceutical companies operate in a strictly regulated and highly risky environment in which a single slip can lead to serious financial implications. Accordingly, the announcements of clinical trial results tend to determine the future course of events, hence being closely monitored by the public. In this work, we provide statistical evidence for the result promulgation influence on the public pharma market value. Whereas most works focus on retrospective impact analysis, the present research aims to predict the numerical values of announcement-induced changes in stock prices. For this purpose, we develop a pipeline that includes a BERT-based model for extracting sentiment polarity of announcements, a Temporal Fusion Transformer for forecasting the expected return, a graph convolution network for capturing event relationships, and gradient boosting for predicting the price change. The challenge of the problem lies in inherently different patterns of responses to positive and negative announcements, reflected in a stronger and more pronounced reaction to the negative news. Moreover, such phenomenon as the drop in stocks after the positive announcements affirms the counterintuitiveness of the price behavior. Importantly, we discover two crucial factors that should be considered while working within a predictive framework. The first factor is the drug portfolio size of the company, indicating the greater susceptibility to an announcement in the case of small drug diversification. The second one is the network effect of the events related to the same company or nosology. All findings and insights are gained on the basis of one of the biggest FDA (the Food and Drug Administration) announcement datasets, consisting of 5436 clinical trial announcements from 681 companies over the last five years.|http://arxiv.org/abs/2208.07248v2|Semen Budennyy,Alexey Kazakov,Elizaveta Kovtun,Leonid Zhukov
985|Accelerating Complex Disease Treatment through Network Medicine and GenAI: A Case Study on Drug Repurposing for Breast Cancer|The objective of this research is to introduce a network specialized in predicting drugs that can be repurposed by investigating real-world evidence sources, such as clinical trials and biomedical literature. Specifically, it aims to generate drug combination therapies for complex diseases (e.g., cancer, Alzheimer's). We present a multilayered network medicine approach, empowered by a highly configured ChatGPT prompt engineering system, which is constructed on the fly to extract drug mentions in clinical trials. Additionally, we introduce a novel algorithm that connects real-world evidence with disease-specific signaling pathways (e.g., KEGG database). This sheds light on the repurposability of drugs if they are found to bind with one or more protein constituents of a signaling pathway. To demonstrate, we instantiated the framework for breast cancer and found that, out of 46 breast cancer signaling pathways, the framework identified 38 pathways that were covered by at least two drugs. This evidence signals the potential for combining those drugs. Specifically, the most covered signaling pathway, ID hsa:2064, was covered by 108 drugs, some of which can be combined. Conversely, the signaling pathway ID hsa:1499 was covered by only two drugs, indicating a significant gap for further research. Our network medicine framework, empowered by GenAI, shows promise in identifying drug combinations with a high degree of specificity, knowing the exact signaling pathways and proteins that serve as targets. It is noteworthy that ChatGPT successfully accelerated the process of identifying drug mentions in clinical trials, though further investigations are required to determine the relationships among the drug mentions.|http://arxiv.org/abs/2406.13106v3|Ahmed Abdeen Hamed,Tamer E. Fandy
986|A foundation model for human-AI collaboration in medical literature mining|Systematic literature review is essential for evidence-based medicine, requiring comprehensive analysis of clinical trial publications. However, the application of artificial intelligence (AI) models for medical literature mining has been limited by insufficient training and evaluation across broad therapeutic areas and diverse tasks. Here, we present LEADS, an AI foundation model for study search, screening, and data extraction from medical literature. The model is trained on 633,759 instruction data points in LEADSInstruct, curated from 21,335 systematic reviews, 453,625 clinical trial publications, and 27,015 clinical trial registries. We showed that LEADS demonstrates consistent improvements over four cutting-edge generic large language models (LLMs) on six tasks. Furthermore, LEADS enhances expert workflows by providing supportive references following expert requests, streamlining processes while maintaining high-quality results. A study with 16 clinicians and medical researchers from 14 different institutions revealed that experts collaborating with LEADS achieved a recall of 0.81 compared to 0.77 experts working alone in study selection, with a time savings of 22.6%. In data extraction tasks, experts using LEADS achieved an accuracy of 0.85 versus 0.80 without using LEADS, alongside a 26.9% time savings. These findings highlight the potential of specialized medical literature foundation models to outperform generic models, delivering significant quality and efficiency benefits when integrated into expert workflows for medical literature mining.|http://arxiv.org/abs/2501.16255v1|Zifeng Wang,Lang Cao,Qiao Jin,Joey Chan,Nicholas Wan,Behdad Afzali,Hyun-Jin Cho,Chang-In Choi,Mehdi Emamverdi,Manjot K. Gill,Sun-Hyung Kim,Yijia Li,Yi Liu,Hanley Ong,Justin Rousseau,Irfan Sheikh,Jenny J. Wei,Ziyang Xu,Christopher M. Zallek,Kyungsang Kim,Yifan Peng,Zhiyong Lu,Jimeng Sun
987|Blockchain in Healthcare and Medicine: A Contemporary Research of Applications, Challenges, and Future Perspectives|Blockchain technology is one of the most contemporary and disruptive technologies in the world. It has gained considerable attention in numerous applications such as financial services, cybersecurity applications, Internet of Things (IoT), network data management. Now its range of applications is beyond the financial services as the healthcare industry has also adopted blockchain technology in its various subdomains such as Electronic Health Records (EHR), medical supply chain management system, genomic market, neuroscience technology, clinical research, and pharmaceutical medicine. Blockchain is considered a secure and viable solution for storing and accessing patients medical records and the patients can diagnosed and treated with safe and secure data sharing. Blockchain technology will revolutionize the healthcare systems with personalized, authentic, and secure access to the clinical data of patients and that data can be used for further health improvements and clinical researches. In this paper, we conduct a contemporary research on existing applications and developments in healthcare industry with the use of blockchain technology. We also discuss some robust applications and various existing companies that are using blockchain solutions for securing their data along with some current challenges and future perspectives.|http://arxiv.org/abs/2004.06795v3|H. Sami Ullah,S. Aslam
988|Reproducibility of the Standard Model of diffusion in white matter on clinical MRI systems|Estimating intra- and extra-axonal microstructure parameters, such as volume fractions and diffusivities, has been one of the major efforts in brain microstructure imaging with MRI. The Standard Model (SM) of diffusion in white matter has unified various modeling approaches based on impermeable narrow cylinders embedded in locally anisotropic extra-axonal space. However, estimating the SM parameters from a set of conventional diffusion MRI (dMRI) measurements is ill-conditioned. Multidimensional dMRI helps resolve the estimation degeneracies, but there remains a need for clinically feasible acquisitions that yield robust parameter maps. Here we find optimal multidimensional protocols by minimizing the mean-squared error of machine learning-based SM parameter estimates for two 3T scanners with corresponding gradient strengths of $40$ and $80\,\unit{mT/m}$. We assess intra-scanner and inter-scanner repeatability for 15-minute optimal protocols by scanning 20 healthy volunteers twice on both scanners. The coefficients of variation all SM parameters except free water fraction are $\lesssim 10\%$ voxelwise and $1-4 \%$ for their region-averaged values. As the achieved SM reproducibility outcomes are similar to those of conventional diffusion tensor imaging, our results enable robust in vivo mapping of white matter microstructure in neuroscience research and in the clinic.|http://arxiv.org/abs/2202.02399v1|Santiago Coelho,Steven H. Baete,Gregory Lemberskiy,Benjamin Ades-Aaron,Genevieve Barrol,Jelle Veraart,Dmitry S. Novikov,Els Fieremans
989|Applications of Generative Adversarial Networks in Neuroimaging and Clinical Neuroscience|Generative adversarial networks (GANs) are one powerful type of deep learning models that have been successfully utilized in numerous fields. They belong to a broader family called generative methods, which generate new data with a probabilistic model by learning sample distribution from real examples. In the clinical context, GANs have shown enhanced capabilities in capturing spatially complex, nonlinear, and potentially subtle disease effects compared to traditional generative methods. This review appraises the existing literature on the applications of GANs in imaging studies of various neurological conditions, including Alzheimer's disease, brain tumors, brain aging, and multiple sclerosis. We provide an intuitive explanation of various GAN methods for each application and further discuss the main challenges, open questions, and promising future directions of leveraging GANs in neuroimaging. We aim to bridge the gap between advanced deep learning methods and neurology research by highlighting how GANs can be leveraged to support clinical decision making and contribute to a better understanding of the structural and functional patterns of brain diseases.|http://arxiv.org/abs/2206.07081v2|Rongguang Wang,Vishnu Bashyam,Zhijian Yang,Fanyang Yu,Vasiliki Tassopoulou,Sai Spandana Chintapalli,Ioanna Skampardoni,Lasya P. Sreepada,Dushyant Sahoo,Konstantina Nikita,Ahmed Abdulkadir,Junhao Wen,Christos Davatzikos
990|BrainWave: A Brain Signal Foundation Model for Clinical Applications|Neural electrical activity is fundamental to brain function, underlying a range of cognitive and behavioral processes, including movement, perception, decision-making, and consciousness. Abnormal patterns of neural signaling often indicate the presence of underlying brain diseases. The variability among individuals, the diverse array of clinical symptoms from various brain disorders, and the limited availability of diagnostic classifications, have posed significant barriers to formulating reliable model of neural signals for diverse application contexts. Here, we present BrainWave, the first foundation model for both invasive and non-invasive neural recordings, pretrained on more than 40,000 hours of electrical brain recordings (13.79 TB of data) from approximately 16,000 individuals. Our analysis show that BrainWave outperforms all other competing models and consistently achieves state-of-the-art performance in the diagnosis and identification of neurological disorders. We also demonstrate robust capabilities of BrainWave in enabling zero-shot transfer learning across varying recording conditions and brain diseases, as well as few-shot classification without fine-tuning, suggesting that BrainWave learns highly generalizable representations of neural signals. We hence believe that open-sourcing BrainWave will facilitate a wide range of clinical applications in medicine, paving the way for AI-driven approaches to investigate brain disorders and advance neuroscience research.|http://arxiv.org/abs/2402.10251v6|Zhizhang Yuan,Fanqi Shen,Meng Li,Yuguo Yu,Chenhao Tan,Yang Yang
991|Proceedings of the 5th Workshop on Machine Learning and Interpretation in Neuroimaging (MLINI) at NIPS 2015|This volume is a collection of contributions from the 5th Workshop on Machine Learning and Interpretation in Neuroimaging (MLINI) at the Neural Information Processing Systems (NIPS 2015) conference. Modern multivariate statistical methods developed in the rapidly growing field of machine learning are being increasingly applied to various problems in neuroimaging, from cognitive state detection to clinical diagnosis and prognosis. Multivariate pattern analysis methods are designed to examine complex relationships between high-dimensional signals, such as brain images, and outcomes of interest, such as the category of a stimulus, a type of a mental state of a subject, or a specific mental disorder. Such techniques are in contrast with the traditional mass-univariate approaches that dominated neuroimaging in the past and treated each individual imaging measurement in isolation.   We believe that machine learning has a prominent role in shaping how questions in neuroscience are framed, and that the machine-learning mind set is now entering modern psychology and behavioral studies. It is also equally important that practical applications in these fields motivate a rapidly evolving line or research in the machine learning community. In parallel, there is an intense interest in learning more about brain function in the context of rich naturalistic environments and scenes. Efforts to go beyond highly specific paradigms that pinpoint a single function, towards schemes for measuring the interaction with natural and more varied scene are made. The goal of the workshop is to pinpoint the most pressing issues and common challenges across the neuroscience, neuroimaging, psychology and machine learning fields, and to sketch future directions and open questions in the light of novel methodology.|http://arxiv.org/abs/1605.04435v1|I. Rish,L. Wehbe,G. Langs,M. Grosse-Wentrup,B. Murphy,G. Cecchi
992|Computational Logic for Biomedicine and Neurosciences|We advocate here the use of computational logic for systems biology, as a \emph{unified and safe} framework well suited for both modeling the dynamic behaviour of biological systems, expressing properties of them, and verifying these properties. The potential candidate logics should have a traditional proof theoretic pedigree (including either induction, or a sequent calculus presentation enjoying cut-elimination and focusing), and should come with certified proof tools. Beyond providing a reliable framework, this allows the correct encodings of our biological systems. % For systems biology in general and biomedicine in particular, we have so far, for the modeling part, three candidate logics: all based on linear logic. The studied properties and their proofs are formalized in a very expressive (non linear) inductive logic: the Calculus of Inductive Constructions (CIC). The examples we have considered so far are relatively simple ones; however, all coming with formal semi-automatic proofs in the Coq system, which implements CIC. In neuroscience, we are directly using CIC and Coq, to model neurons and some simple neuronal circuits and prove some of their dynamic properties. % In biomedicine, the study of multi omic pathway interactions, together with clinical and electronic health record data should help in drug discovery and disease diagnosis. Future work includes using more automatic provers. This should enable us to specify and study more realistic examples, and in the long term to provide a system for disease diagnosis and therapy prognosis.|http://arxiv.org/abs/2007.07571v2|Elisabetta de Maria,Joelle Despeyroux,Amy Felty,Pietro Li,Carlos Olarte,Abdorrahim Bahrami
993|Simple RGC: ImageJ plugins for counting retinal ganglion cells and determining the transduction efficiency of viral vectors in retinal wholemounts|Simple RGC consists of a collection of ImageJ plugins to assist researchers investigating retinal ganglion cell (RGC) injury models in addition to helping assess the effectiveness of treatments. The first plugin named RGC Counter accurately calculates the total number of RGCs from retinal wholemount images. The second plugin named RGC Transduction measures the co-localisation between two channels making it possible to determine the transduction efficiencies of viral vectors and transgene expression levels. The third plugin named RGC Batch is a batch image processor to deliver fast analysis of large groups of microscope images. These ImageJ plugins make analysis of RGCs in retinal wholemounts easy, quick, consistent, and less prone to unconscious bias by the investigator. The plugins are freely available from the ImageJ update site https://sites.imagej.net/Sonjoonho/.|http://arxiv.org/abs/2008.06276v2|Tiger Cross,Rasika Navarange,Joon-Ho Son,William Burr,Arjun Singh,Kelvin Zhang,Miruna Rusu,Konstantinos Gkoutzis,Andrew Osborne,Bart Nieuwenhuis
994|The Portiloop: a deep learning-based open science tool for closed-loop brain stimulation|Closed-loop brain stimulation refers to capturing neurophysiological measures such as electroencephalography (EEG), quickly identifying neural events of interest, and producing auditory, magnetic or electrical stimulation so as to interact with brain processes precisely. It is a promising new method for fundamental neuroscience and perhaps for clinical applications such as restoring degraded memory function; however, existing tools are expensive, cumbersome, and offer limited experimental flexibility. In this article, we propose the Portiloop, a deep learning-based, portable and low-cost closed-loop stimulation system able to target specific brain oscillations. We first document open-hardware implementations that can be constructed from commercially available components. We also provide a fast, lightweight neural network model and an exploration algorithm that automatically optimizes the model hyperparameters to the desired brain oscillation. Finally, we validate the technology on a challenging test case of real-time sleep spindle detection, with results comparable to off-line expert performance on the Massive Online Data Annotation spindle dataset (MODA; group consensus). Software and plans are available to the community as an open science initiative to encourage further development and advance closed-loop neuroscience research.|http://arxiv.org/abs/2107.13473v3|Nicolas Valenchon,Yann Bouteiller,Hugo R. Jourde,Xavier L'Heureux,Milo Sobral,Emily B. J. Coffey,Giovanni Beltrame
995|Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis|Human brains lie at the core of complex neurobiological systems, where the neurons, circuits, and subsystems interact in enigmatic ways. Understanding the structural and functional mechanisms of the brain has long been an intriguing pursuit for neuroscience research and clinical disorder therapy. Mapping the connections of the human brain as a network is one of the most pervasive paradigms in neuroscience. Graph Neural Networks (GNNs) have recently emerged as a potential method for modeling complex network data. Deep models, on the other hand, have low interpretability, which prevents their usage in decision-critical contexts like healthcare. To bridge this gap, we propose an interpretable framework to analyze disorder-specific Regions of Interest (ROIs) and prominent connections. The proposed framework consists of two modules: a brain-network-oriented backbone model for disease prediction and a globally shared explanation generator that highlights disorder-specific biomarkers including salient ROIs and important connections. We conduct experiments on three real-world datasets of brain disorders. The results verify that our framework can obtain outstanding performance and also identify meaningful biomarkers. All code for this work is available at https://github.com/HennyJie/IBGNN.git.|http://arxiv.org/abs/2207.00813v2|Hejie Cui,Wei Dai,Yanqiao Zhu,Xiaoxiao Li,Lifang He,Carl Yang
996|Reliability and validity of TMS-EEG biomarkers|Noninvasive brain stimulation and neuroimaging have revolutionized human neuroscience, with a multitude of applications including diagnostic subtyping, treatment optimization, and relapse prediction. It is therefore particularly relevant to identify robust and clinically valuable brain biomarkers linking symptoms to their underlying neural mechanisms. Brain biomarkers must be reproducible (i.e., have internal reliability) across similar experiments within a laboratory and be generalizable (i.e., have external reliability) across experimental setups, laboratories, brain regions, and disease states. However, reliability (internal and external) is not alone sufficient; biomarkers also must have validity. Validity describes closeness to a true measure of the underlying neural signal or disease state. We propose that these two metrics, reliability and validity, should be evaluated and optimized before any biomarker is used to inform treatment decisions. Here, we discuss these metrics with respect to causal brain connectivity biomarkers from coupling transcranial magnetic stimulation (TMS) with electroencephalography (EEG). We discuss controversies around TMS-EEG stemming from the multiple large off-target components (noise) and relatively weak genuine brain responses (signal), as is unfortunately often the case with human neuroscience. We review the current state of TMS-EEG recordings, which consist of a mix of reliable noise and unreliable signal. We describe methods for evaluating TMS-EEG biomarkers, including how to assess internal and external reliability across facilities, cognitive states, brain networks, and disorders, and how to validate these biomarkers using invasive neural recordings or treatment response. We provide recommendations to increase reliability and validity, discuss lessons learned, and suggest future directions for the field.|http://arxiv.org/abs/2207.08456v1|Sara Parmigiani,Jessica M. Ross,Christopher Cline,Christopher B. Minasi,Juha Gogulski,Corey J Keller
997|Neuralizer: General Neuroimage Analysis without Re-Training|Neuroimage processing tasks like segmentation, reconstruction, and registration are central to the study of neuroscience. Robust deep learning strategies and architectures used to solve these tasks are often similar. Yet, when presented with a new task or a dataset with different visual characteristics, practitioners most often need to train a new model, or fine-tune an existing one. This is a time-consuming process that poses a substantial barrier for the thousands of neuroscientists and clinical researchers who often lack the resources or machine-learning expertise to train deep learning models. In practice, this leads to a lack of adoption of deep learning, and neuroscience tools being dominated by classical frameworks.   We introduce Neuralizer, a single model that generalizes to previously unseen neuroimaging tasks and modalities without the need for re-training or fine-tuning. Tasks do not have to be known a priori, and generalization happens in a single forward pass during inference. The model can solve processing tasks across multiple image modalities, acquisition methods, and datasets, and generalize to tasks and modalities it has not been trained on. Our experiments on coronal slices show that when few annotated subjects are available, our multi-task network outperforms task-specific baselines without training on the task.|http://arxiv.org/abs/2305.02644v2|Steffen Czolbe,Adrian V. Dalca
998|BrainMass: Advancing Brain Network Analysis for Diagnosis with Large-scale Self-Supervised Learning|Foundation models pretrained on large-scale datasets via self-supervised learning demonstrate exceptional versatility across various tasks. Due to the heterogeneity and hard-to-collect medical data, this approach is especially beneficial for medical image analysis and neuroscience research, as it streamlines broad downstream tasks without the need for numerous costly annotations. However, there has been limited investigation into brain network foundation models, limiting their adaptability and generalizability for broad neuroscience studies. In this study, we aim to bridge this gap. In particular, (1) we curated a comprehensive dataset by collating images from 30 datasets, which comprises 70,781 samples of 46,686 participants. Moreover, we introduce pseudo-functional connectivity (pFC) to further generates millions of augmented brain networks by randomly dropping certain timepoints of the BOLD signal. (2) We propose the BrainMass framework for brain network self-supervised learning via mask modeling and feature alignment. BrainMass employs Mask-ROI Modeling (MRM) to bolster intra-network dependencies and regional specificity. Furthermore, Latent Representation Alignment (LRA) module is utilized to regularize augmented brain networks of the same participant with similar topological properties to yield similar latent representations by aligning their latent embeddings. Extensive experiments on eight internal tasks and seven external brain disorder diagnosis tasks show BrainMass's superior performance, highlighting its significant generalizability and adaptability. Nonetheless, BrainMass demonstrates powerful few/zero-shot learning abilities and exhibits meaningful interpretation to various diseases, showcasing its potential use for clinical applications.|http://arxiv.org/abs/2403.01433v1|Yanwu Yang,Chenfei Ye,Guinan Su,Ziyao Zhang,Zhikai Chang,Hairui Chen,Piu Chan,Yue Yu,Ting Ma
999|Interpretable Dynamic Treatment Regimes|Precision medicine is currently a topic of great interest in clinical and intervention science. One way to formalize precision medicine is through a treatment regime, which is a sequence of decision rules, one per stage of clinical intervention, that map up-to-date patient information to a recommended treatment. An optimal treatment regime is defined as maximizing the mean of some cumulative clinical outcome if applied to a population of interest. It is well-known that even under simple generative models an optimal treatment regime can be a highly nonlinear function of patient information. Consequently, a focal point of recent methodological research has been the development of flexible models for estimating optimal treatment regimes. However, in many settings, estimation of an optimal treatment regime is an exploratory analysis intended to generate new hypotheses for subsequent research and not to directly dictate treatment to new patients. In such settings, an estimated treatment regime that is interpretable in a domain context may be of greater value than an unintelligible treatment regime built using "black-box" estimation methods. We propose an estimator of an optimal treatment regime composed of a sequence of decision rules, each expressible as a list of "if-then" statements that can be presented as either a paragraph or as a simple flowchart that is immediately interpretable to domain experts. The discreteness of these lists precludes smooth, i.e., gradient-based, methods of estimation and leads to non-standard asymptotics. Nevertheless, we provide a computationally efficient estimation algorithm, prove consistency of the proposed estimator, and derive rates of convergence. We illustrate the proposed methods using a series of simulation examples and application to data from a sequential clinical trial on bipolar disorder.|http://arxiv.org/abs/1606.01472v1|Yichi Zhang,Eric B. Laber,Anastasios Tsiatis,Marie Davidian
1000|CheXpedition: Investigating Generalization Challenges for Translation of Chest X-Ray Algorithms to the Clinical Setting|Although there have been several recent advances in the application of deep learning algorithms to chest x-ray interpretation, we identify three major challenges for the translation of chest x-ray algorithms to the clinical setting. We examine the performance of the top 10 performing models on the CheXpert challenge leaderboard on three tasks: (1) TB detection, (2) pathology detection on photos of chest x-rays, and (3) pathology detection on data from an external institution. First, we find that the top 10 chest x-ray models on the CheXpert competition achieve an average AUC of 0.851 on the task of detecting TB on two public TB datasets without fine-tuning or including the TB labels in training data. Second, we find that the average performance of the models on photos of x-rays (AUC = 0.916) is similar to their performance on the original chest x-ray images (AUC = 0.924). Third, we find that the models tested on an external dataset either perform comparably to or exceed the average performance of radiologists. We believe that our investigation will inform rapid translation of deep learning algorithms to safe and effective clinical decision support tools that can be validated prospectively with large impact studies and clinical trials.|http://arxiv.org/abs/2002.11379v2|Pranav Rajpurkar,Anirudh Joshi,Anuj Pareek,Phil Chen,Amirhossein Kiani,Jeremy Irvin,Andrew Y. Ng,Matthew P. Lungren
1001|Optimizing Medical Treatment for Sepsis in Intensive Care: from Reinforcement Learning to Pre-Trial Evaluation|Our aim is to establish a framework where reinforcement learning (RL) of optimizing interventions retrospectively allows us a regulatory compliant pathway to prospective clinical testing of the learned policies in a clinical deployment. We focus on infections in intensive care units which are one of the major causes of death and difficult to treat because of the complex and opaque patient dynamics, and the clinically debated, highly-divergent set of intervention policies required by each individual patient, yet intensive care units are naturally data rich. In our work, we build on RL approaches in healthcare ("AI Clinicians"), and learn off-policy continuous dosing policy of pharmaceuticals for sepsis treatment using historical intensive care data under partially observable MDPs (POMDPs). POMPDs capture uncertainty in patient state better by taking in all historical information, yielding an efficient representation, which we investigate through ablations. We compensate for the lack of exploration in our retrospective data by evaluating each encountered state with a best-first tree search. We mitigate state distributional shift by optimizing our policy in the vicinity of the clinicians' compound policy. Crucially, we evaluate our model recommendations using not only conventional policy evaluations but a novel framework that incorporates human experts: a model-agnostic pre-clinical evaluation method to estimate the accuracy and uncertainty of clinician's decisions versus our system recommendations when confronted with the same individual patient history ("shadow mode").|http://arxiv.org/abs/2003.06474v2|Luchen Li,Ignacio Albert-Smet,Aldo A. Faisal
1002|Causal inference with multiple versions of treatment and application to personalized medicine|The development of high-throughput sequencing and targeted therapies has led to the emergence of personalized medicine: a patient's molecular profile or the presence of a specific biomarker of drug response will correspond to a treatment recommendation made either by a physician or by a treatment assignment algorithm. The growing number of such algorithms raises the question of how to quantify their clinical impact knowing that a personalized medicine strategy will inherently include different versions of treatment.   We thus specify an appropriate causal framework with multiple versions of treatment to define the causal effects of interest for precision medicine strategies and estimate them emulating clinical trials with observational data. Therefore, we determine whether the treatment assignment algorithm is more efficient than different control arms: gold standard treatment, observed treatments or random assignment of targeted treatments.   Causal estimates of the precision medicine effects are first evaluated on simulated data and they demonstrate a lower biases and variances compared with naive estimation of the difference in expected outcome between treatment arms. The various simulations scenarios also point out the different bias sources depending on the clinical situation (heterogeneity of response, assignment of observed treatments etc.). A RShiny interactive application is also provided to further explore other user-defined scenarios. The method is then applied to data from patient-derived xenografts (PDX): each patient tumour is implanted in several immunodeficient cloned mice later treated with different drugs, thus providing access to all corresponding drug sensitivities for all patients. Access to these unique pre-clinical data emulating counterfactual outcomes allows to validate the reliability of causal estimates obtained with the proposed method.|http://arxiv.org/abs/2005.12427v1|Jonas Bal,Aurlien Latouche
1003|VIRDOCD: a VIRtual DOCtor to Predict Dengue Fatality|Clinicians make routine diagnosis by scrutinizing patients' medical signs and symptoms, a skill popularly referred to as "Clinical Eye". This skill evolves through trial-and-error and improves with time. The success of the therapeutic regime relies largely on the accuracy of interpretation of such sign-symptoms, analyzing which a clinician assesses the severity of the illness. The present study is an attempt to propose a complementary medical front by mathematically modeling the "Clinical Eye" of a VIRtual DOCtor, using Statistical and Machine Intelligence tools (SMI), to analyze Dengue epidemic infected patients (100 case studies with 11 weighted sign-symptoms). The SMI in VIRDOCD reads medical data and translates these into a vector comprising Multiple Linear Regression (MLR) coefficients to predict infection severity grades of dengue patients that clone the clinician's experience-based assessment. Risk managed through ANOVA, the dengue severity grade prediction accuracy from VIRDOCD is found higher (ca 75%) than conventional clinical practice (ca 71.4%, mean accuracy profile assessed by a team of 10 senior consultants). Free of human errors and capable of deciphering even minute differences from almost identical symptoms (to the Clinical Eye), VIRDOCD is uniquely individualized in its decision-making ability. The algorithm has been validated against Random Forest classification (RF, ca 63%), another regression-based classifier similar to MLR that can be trained through supervised learning. We find that MLR-based VIRDOCD is superior to RF in predicting the grade of Dengue morbidity. VIRDOCD can be further extended to analyze other epidemic infections, such as COVID-19.|http://arxiv.org/abs/2104.14282v2|Amit K Chattopadhyay,Subhagata Chattopadhyay
1004|Predicting Clinical Outcome of Stroke Patients with Tractographic Feature|The volume of stroke lesion is the gold standard for predicting the clinical outcome of stroke patients. However, the presence of stroke lesion may cause neural disruptions to other brain regions, and these potentially damaged regions may affect the clinical outcome of stroke patients. In this paper, we introduce the tractographic feature to capture these potentially damaged regions and predict the modified Rankin Scale (mRS), which is a widely used outcome measure in stroke clinical trials. The tractographic feature is built from the stroke lesion and average connectome information from a group of normal subjects. The tractographic feature takes into account different functional regions that may be affected by the stroke, thus complementing the commonly used stroke volume features. The proposed tractographic feature is tested on a public stroke benchmark Ischemic Stroke Lesion Segmentation 2017 and achieves higher accuracy than the stroke volume and the state-of-the-art feature on predicting the mRS grades of stroke patients. In addition, the tractographic feature also yields a lower average absolute error than the commonly used stroke volume feature.|http://arxiv.org/abs/1907.10419v3|Po-Yu Kao,Jefferson W. Chen,B. S. Manjunath
1005|Technical Report: Quality Assessment Tool for Machine Learning with Clinical CT|Image Quality Assessment (IQA) is important for scientific inquiry, especially in medical imaging and machine learning. Potential data quality issues can be exacerbated when human-based workflows use limited views of the data that may obscure digital artifacts. In practice, multiple factors such as network issues, accelerated acquisitions, motion artifacts, and imaging protocol design can impede the interpretation of image collections. The medical image processing community has developed a wide variety of tools for the inspection and validation of imaging data. Yet, IQA of computed tomography (CT) remains an under-recognized challenge, and no user-friendly tool is commonly available to address these potential issues. Here, we create and illustrate a pipeline specifically designed to identify and resolve issues encountered with large-scale data mining of clinically acquired CT data. Using the widely studied National Lung Screening Trial (NLST), we have identified approximately 4% of image volumes with quality concerns out of 17,392 scans. To assess robustness, we applied the proposed pipeline to our internal datasets where we find our tool is generalizable to clinically acquired medical images. In conclusion, the tool has been useful and time-saving for research study of clinical data, and the code and tutorials are publicly available at https://github.com/MASILab/QA_tool.|http://arxiv.org/abs/2107.12842v1|Riqiang Gao,Mirza S. Khan,Yucheng Tang,Kaiwen Xu,Steve Deppen,Yuankai Huo,Kim L. Sandler,Pierre P. Massion,Bennett A. Landman
1006|Stroke recovery phenotyping through network trajectory approaches and graph neural networks|Stroke is a leading cause of neurological injury characterized by impairments in multiple neurological domains including cognition, language, sensory and motor functions. Clinical recovery in these domains is tracked using a wide range of measures that may be continuous, ordinal, interval or categorical in nature, which presents challenges for standard multivariate regression approaches. This has hindered stroke researchers' ability to achieve an integrated picture of the complex time-evolving interactions amongst symptoms. Here we use tools from network science and machine learning that are particularly well-suited to extracting underlying patterns in such data, and may assist in prediction of recovery patterns. To demonstrate the utility of this approach, we analyzed data from the NINDS tPA trial using the Trajectory Profile Clustering (TPC) method to identify distinct stroke recovery patterns for 11 different neurological domains at 5 discrete time points. Our analysis identified 3 distinct stroke trajectory profiles that align with clinically relevant stroke syndromes, characterized both by distinct clusters of symptoms, as well as differing degrees of symptom severity. We then validated our approach using graph neural networks to determine how well our model performed predictively for stratifying patients into these trajectory profiles at early vs. later time points post-stroke. We demonstrate that trajectory profile clustering is an effective method for identifying clinically relevant recovery subtypes in multidimensional longitudinal datasets, and for early prediction of symptom progression subtypes in individual patients. This paper is the first work introducing network trajectory approaches for stroke recovery phenotyping, and is aimed at enhancing the translation of such novel computational approaches for practical clinical application.|http://arxiv.org/abs/2109.14659v1|Sanjukta Krishnagopal,Keith Lohse,Robynne Braun
1007|Need for objective task-based evaluation of AI-based segmentation methods for quantitative PET|Artificial intelligence (AI)-based methods are showing substantial promise in segmenting oncologic positron emission tomography (PET) images. For clinical translation of these methods, assessing their performance on clinically relevant tasks is important. However, these methods are typically evaluated using metrics that may not correlate with the task performance. One such widely used metric is the Dice score, a figure of merit that measures the spatial overlap between the estimated segmentation and a reference standard (e.g., manual segmentation). In this work, we investigated whether evaluating AI-based segmentation methods using Dice scores yields a similar interpretation as evaluation on the clinical tasks of quantifying metabolic tumor volume (MTV) and total lesion glycolysis (TLG) of primary tumor from PET images of patients with non-small cell lung cancer. The investigation was conducted via a retrospective analysis with the ECOG-ACRIN 6668/RTOG 0235 multi-center clinical trial data. Specifically, we evaluated different structures of a commonly used AI-based segmentation method using both Dice scores and the accuracy in quantifying MTV/TLG. Our results show that evaluation using Dice scores can lead to findings that are inconsistent with evaluation using the task-based figure of merit. Thus, our study motivates the need for objective task-based evaluation of AI-based segmentation methods for quantitative PET.|http://arxiv.org/abs/2303.00640v1|Ziping Liu,Joyce C. Mhlanga,Barry A. Siegel,Abhinav K. Jha
1008|Artificial Intelligence for Dementia Research Methods Optimization|Introduction: Machine learning (ML) has been extremely successful in identifying key features from high-dimensional datasets and executing complicated tasks with human expert levels of accuracy or greater. Methods: We summarize and critically evaluate current applications of ML in dementia research and highlight directions for future research. Results: We present an overview of ML algorithms most frequently used in dementia research and highlight future opportunities for the use of ML in clinical practice, experimental medicine, and clinical trials. We discuss issues of reproducibility, replicability and interpretability and how these impact the clinical applicability of dementia research. Finally, we give examples of how state-of-the-art methods, such as transfer learning, multi-task learning, and reinforcement learning, may be applied to overcome these issues and aid the translation of research to clinical practice in the future. Discussion: ML-based models hold great promise to advance our understanding of the underlying causes and pathological mechanisms of dementia.|http://arxiv.org/abs/2303.01949v1|Magda Bucholc,Charlotte James,Ahmad Al Khleifat,AmanPreet Badhwar,Natasha Clarke,Amir Dehsarvi,Christopher R. Madan,Sarah J. Marzi,Cameron Shand,Brian M. Schilder,Stefano Tamburin,Hanz M. Tantiangco,Ilianna Lourida,David J. Llewellyn,Janice M. Ranson
1009|Harmonization Across Imaging Locations(HAIL): One-Shot Learning for Brain MRI|For machine learning-based prognosis and diagnosis of rare diseases, such as pediatric brain tumors, it is necessary to gather medical imaging data from multiple clinical sites that may use different devices and protocols. Deep learning-driven harmonization of radiologic images relies on generative adversarial networks (GANs). However, GANs notoriously generate pseudo structures that do not exist in the original training data, a phenomenon known as "hallucination". To prevent hallucination in medical imaging, such as magnetic resonance images (MRI) of the brain, we propose a one-shot learning method where we utilize neural style transfer for harmonization. At test time, the method uses one image from a clinical site to generate an image that matches the intensity scale of the collaborating sites. Our approach combines learning a feature extractor, neural style transfer, and adaptive instance normalization. We further propose a novel strategy to evaluate the effectiveness of image harmonization approaches with evaluation metrics that both measure image style harmonization and assess the preservation of anatomical structures. Experimental results demonstrate the effectiveness of our method in preserving patient anatomy while adjusting the image intensities to a new clinical site. Our general harmonization model can be used on unseen data from new sites, making it a valuable tool for real-world medical applications and clinical trials.|http://arxiv.org/abs/2308.11047v1|Abhijeet Parida,Zhifan Jiang,Syed Muhammad Anwar,Nicholas Foreman,Nicholas Stence,Michael J. Fisher,Roger J. Packer,Robert A. Avery,Marius George Linguraru
1010|Large Language Models Streamline Automated Machine Learning for Clinical Studies|A knowledge gap persists between machine learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the ChatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to ChatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Following the re-implementation and optimization of the published models, the head-to-head comparison of the ChatGPT ADA-crafted ML models and their respective manually crafted counterparts revealed no significant differences in traditional performance metrics (P>0.071). Strikingly, the ChatGPT ADA-crafted ML models often outperformed their counterparts. In conclusion, ChatGPT ADA offers a promising avenue to democratize ML in medicine by simplifying complex data analyses, yet should enhance, not replace, specialized training and resources, to promote broader applications in medical research and practice.|http://arxiv.org/abs/2308.14120v5|Soroosh Tayebi Arasteh,Tianyu Han,Mahshad Lotfinia,Christiane Kuhl,Jakob Nikolas Kather,Daniel Truhn,Sven Nebelung
1011|Quantifying Itch and its Impact on Sleep Using Machine Learning and Radio Signals|Chronic itch affects 13% of the US population, is highly debilitating, and underlies many medical conditions. A major challenge in clinical care and new therapeutics development is the lack of an objective measure for quantifying itch, leading to reliance on subjective measures like patients' self-assessment of itch severity. In this paper, we show that a home radio device paired with artificial intelligence (AI) can concurrently capture scratching and evaluate its impact on sleep quality by analyzing radio signals bouncing in the environment. The device eliminates the need for wearable sensors or skin contact, enabling monitoring of chronic itch over extended periods at home without burdening patients or interfering with their skin condition. To validate the technology, we conducted an observational clinical study of chronic pruritus patients, monitored at home for one month using both the radio device and an infrared camera. Comparing the output of the device to ground truth data from the camera demonstrates its feasibility and accuracy (ROC AUC = 0.997, sensitivity = 0.825, specificity = 0.997). The results reveal a significant correlation between scratching and low sleep quality, manifested as a reduction in sleep efficiency (R = 0.6, p < 0.001) and an increase in sleep latency (R = 0.68, p < 0.001). Our study underscores the potential of passive, long-term, at-home monitoring of chronic scratching and its sleep implications, offering a valuable tool for both clinical care of chronic itch patients and pharmaceutical clinical trials.|http://arxiv.org/abs/2501.04896v1|Michail Ouroutzoglou,Mingmin Zhao,Joshua Hellerstein,Hariharan Rahul,Asima Badic,Brian S. Kim,Dina Katabi
1012|Augmenting control arms with Real-World Data for cancer trials: Hybrid control arm methods and considerations|Randomized controlled trials (RCTs) are the gold standard for assessing drug safety and efficacy. However, RCTs have some drawbacks which have led to the use of single-arm studies to make certain internal drug development and regulatory decisions, particularly in oncology. Hybrid controlled trials with real-world data (RWD), in which the control arm is composed of both trial and real-world patients, have the potential to help address some of the shortcomings of both RCTs and single-arm studies in particular situations, such as when a disease has low prevalence or when the standard of care to be used in the control arm is ineffective or highly toxic and an experimental therapy shows early promise. This paper discusses why it may be beneficial to consider hybrid controlled trials with RWD, what such a design entails, when it may be appropriate, and how to conduct the analyses. We propose a novel two-step borrowing method for the construction of hybrid control arms. We use simulations to demonstrate the operating characteristics of dynamic and static borrowing methods, and highlight the trade-offs and analytic decisions that study teams will need to address when designing a hybrid study.|http://arxiv.org/abs/2108.07335v1|W. Katherine Tan,Brian D. Segal,Melissa D. Curtis,Shrujal S. Baxi,William B. Capra,Elizabeth Garrett-Mayer,Brian P. Hobbs,David S. Hong,Rebecca A. Hubbard,Jiawen Zhu,Somnath Sarkar,Meghna Samant
1013|Testing for Treatment Effect Twice Using Internal and External Controls in Clinical Trials|Leveraging external controls -- relevant individual patient data under control from external trials or real-world data -- has the potential to reduce the cost of randomized controlled trials (RCTs) while increasing the proportion of trial patients given access to novel treatments. However, due to lack of randomization, RCT patients and external controls may differ with respect to covariates that may or may not have been measured. Hence, after controlling for measured covariates, for instance by matching, testing for treatment effect using external controls may still be subject to unmeasured biases. In this paper, we propose a sensitivity analysis approach to quantify the magnitude of unmeasured bias that would be needed to alter the study conclusion that presumed no unmeasured biases are introduced by employing external controls. Whether leveraging external controls increases power or not depends on the interplay between sample sizes and the magnitude of treatment effect and unmeasured biases, which may be difficult to anticipate. This motivates a combined testing procedure that performs two highly correlated analyses, one with and one without external controls, with a small correction for multiple testing using the joint distribution of the two test statistics. The combined test provides a new method of sensitivity analysis designed for data fusion problems, which anchors at the unbiased analysis based on RCT only and spends a small proportion of the type I error to also test using the external controls. In this way, if leveraging external controls increases power, the power gain compared to the analysis based on RCT only can be substantial; if not, the power loss is small. The proposed method is evaluated in theory and power calculations, and applied to a real trial.|http://arxiv.org/abs/2203.04194v2|Yanyao Yi,Ying Zhang,Yu Du,Ting Ye
1014|A Wavelet Based Algorithm for the Identification of Oscillatory Event-Related Potential Components|Event Related Potentials (ERPs) are very feeble alterations in the ongoing Electroencephalogram (EEG) and their detection is a challenging problem. Based on the unique time-based parameters derived from wavelet coefficients and the asymmetry property of wavelets a novel algorithm to separate ERP components in single-trial EEG data is described. Though illustrated as a specific application to N170 ERP detection, the algorithm is a generalized approach that can be easily adapted to isolate different kinds of ERP components. The algorithm detected the N170 ERP component with a high level of accuracy. We demonstrate that the asymmetry method is more accurate than the matching wavelet algorithm and t-CWT method by 48.67 and 8.03 percent respectively. This paper provides an off-line demonstration of the algorithm and considers issues related to the extension of the algorithm to real-time applications.|http://arxiv.org/abs/1407.2227v1|Arun Kumar A,Ninan Sajeeth Philip,Vincent J Samar,James A Desjardins,Sidney J Segalowitz
1015|The right time to learn: mechanisms and optimization of spaced learning|For many types of learning, spaced training that involves repeated long inter-trial intervals (ITIs) leads to more robust memory formation than does massed training that involves short or no intervals. Several cognitive theories have been proposed to explain this superiority, but only recently has data begun to delineate the underlying cellular and molecular mechanisms of spaced training. We review these theories and data here. Computational models of the implicated signaling cascades have predicted that spaced training with irregular ITIs can enhance learning. This strategy of using models to predict optimal spaced training protocols, combined with pharmacotherapy, suggests novel ways to rescue impaired synaptic plasticity and learning.|http://arxiv.org/abs/1606.08370v1|Paul Smolen,Yili Zhang,John H. Byrne
1016|Unifying and generalizing models of neural dynamics during decision-making|An open question in systems and computational neuroscience is how neural circuits accumulate evidence towards a decision. Fitting models of decision-making theory to neural activity helps answer this question, but current approaches limit the number of these models that we can fit to neural data. Here we propose a unifying framework for modeling neural activity during decision-making tasks. The framework includes the canonical drift-diffusion model and enables extensions such as multi-dimensional accumulators, variable and collapsing boundaries, and discrete jumps. Our framework is based on constraining the parameters of recurrent state-space models, for which we introduce a scalable variational Laplace-EM inference algorithm. We applied the modeling approach to spiking responses recorded from monkey parietal cortex during two decision-making tasks. We found that a two-dimensional accumulator better captured the trial-averaged responses of a set of parietal neurons than a single accumulator model. Next, we identified a variable lower boundary in the responses of an LIP neuron during a random dot motion task.|http://arxiv.org/abs/2001.04571v1|David M. Zoltowski,Jonathan W. Pillow,Scott W. Linderman
1017|Reinforcement Learning Framework for Deep Brain Stimulation Study|Malfunctioning neurons in the brain sometimes operate synchronously, reportedly causing many neurological diseases, e.g. Parkinson's. Suppression and control of this collective synchronous activity are therefore of great importance for neuroscience, and can only rely on limited engineering trials due to the need to experiment with live human brains. We present the first Reinforcement Learning gym framework that emulates this collective behavior of neurons and allows us to find suppression parameters for the environment of synthetic degenerate models of neurons. We successfully suppress synchrony via RL for three pathological signaling regimes, characterize the framework's stability to noise, and further remove the unwanted oscillations by engaging multiple PPO agents.|http://arxiv.org/abs/2002.10948v1|Dmitrii Krylov,Remi Tachet,Romain Laroche,Michael Rosenblum,Dmitry V. Dylov
1018|Characterizing spreading dynamics of subsampled systems with non-stationary external input|Many systems with propagation dynamics, such as spike propagation in neural networks and spreading of infectious diseases, can be approximated by autoregressive models. The estimation of model parameters can be complicated by the experimental limitation that one observes only a fraction of the system (subsampling) and potentially time-dependent parameters, leading to incorrect estimates. We show analytically how to overcome the subsampling bias when estimating the propagation rate for systems with certain non-stationary external input. This approach is readily applicable to trial-based experimental setups and seasonal fluctuations, as demonstrated on spike recordings from monkey prefrontal cortex and spreading of norovirus and measles.|http://arxiv.org/abs/2005.00608v1|Jorge de Heuvel,Jens Wilting,Moritz Becker,Viola Priesemann,Johannes Zierenberg
1019|Representational dissimilarity metric spaces for stochastic neural networks|Quantifying similarity between neural representations -- e.g. hidden layer activation vectors -- is a perennial problem in deep learning and neuroscience research. Existing methods compare deterministic responses (e.g. artificial networks that lack stochastic layers) or averaged responses (e.g., trial-averaged firing rates in biological data). However, these measures of _deterministic_ representational similarity ignore the scale and geometric structure of noise, both of which play important roles in neural computation. To rectify this, we generalize previously proposed shape metrics (Williams et al. 2021) to quantify differences in _stochastic_ representations. These new distances satisfy the triangle inequality, and thus can be used as a rigorous basis for many supervised and unsupervised analyses. Leveraging this novel framework, we find that the stochastic geometries of neurobiological representations of oriented visual gratings and naturalistic scenes respectively resemble untrained and trained deep network representations. Further, we are able to more accurately predict certain network attributes (e.g. training hyperparameters) from its position in stochastic (versus deterministic) shape space.|http://arxiv.org/abs/2211.11665v2|Lyndon R. Duong,Jingyang Zhou,Josue Nassar,Jules Berman,Jeroen Olieslagers,Alex H. Williams
1020|Application of the fluctuation theorem to motor proteins: from F1-ATPase to axonal cargo transport by kinesin and dynein|The fluctuation theorem is a representative theorem in non-equilibrium statistical physics actively studied in the 1990's. Relating to entropy production in non-equilibrium states, the theorem has been used to estimate the driving power of motor proteins from fluctuation in their motion. In this review, usage of the fluctuation theorem in experiments on motor proteins is illustrated for biologists, especially those who study mechanobiology, in which force measurement is a central issue. We first introduce the application of the fluctuation theorem in measuring the rotary torque of the rotary motor protein F1-ATPase. Next, as an extension of this application, a recent trial estimating the force generated during cargo transport in vivo by the microtubule motors kinesin and dynein is introduced. Elucidation of the physical mechanism of such transport is important, especially for neurons, in which deficits in cargo transport are deeply related to neuronal diseases. Finally, perspectives on the fluctuation theorem as a new technique in the field of neuroscience are discussed.|http://arxiv.org/abs/1807.01067v1|Kumiko Hayashi
1021|Decoding Spiking Mechanism with Dynamic Learning on Neuron Population|A main concern in cognitive neuroscience is to decode the overt neural spike train observations and infer latent representations under neural circuits. However, traditional methods entail strong prior on network structure and hardly meet the demand for real spike data. Here we propose a novel neural network approach called Neuron Activation Network that extracts neural information explicitly from single trial neuron population spike trains. Our proposed method consists of a spatiotemporal learning procedure on sensory environment and a message passing mechanism on population graph, followed by a neuron activation process in a recursive fashion. Our model is aimed to reconstruct neuron information while inferring representations of neuron spiking states. We apply our model to retinal ganglion cells and the experimental results suggest that our model holds a more potent capability in generating neural spike sequences with high fidelity than the state-of-the-art methods, as well as being more expressive and having potential to disclose latent spiking mechanism. The source code will be released with the final paper.|http://arxiv.org/abs/1911.09309v1|Zhijie Chen,Junchi Yan,Longyuan Li,Xiaokang Yang
1022|Simultaneous clustering and estimation of additive shape invariant models for recurrent event data|Technological advancements have enabled the recording of spiking activities from large neuron ensembles, presenting an exciting yet challenging opportunity for statistical analysis. This project considers the challenges from a common type of neuroscience experiments, where randomized interventions are applied over the course of each trial. The objective is to identify groups of neurons with unique stimulation responses and estimate these responses. The observed data, however, comprise superpositions of neural responses to all stimuli, which is further complicated by varying response latencies across neurons. We introduce a novel additive shape invariant model that is capable of simultaneously accommodating multiple clusters, additive components, and unknown time-shifts. We establish conditions for the identifiability of model parameters, offering guidance for the design of future experiments. We examine the properties of the proposed algorithm through simulation studies, and apply the proposed method on neural data collected in mice.|http://arxiv.org/abs/2404.03160v1|Zitong Zhang,Shizhe Chen
1023|TADPOLE Challenge: Accurate Alzheimer's disease prediction through crowdsourced forecasting of future data|The TADPOLE Challenge compares the performance of algorithms at predicting the future evolution of individuals at risk of Alzheimer's disease. TADPOLE Challenge participants train their models and algorithms on historical data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study. Participants are then required to make forecasts of three key outcomes for ADNI-3 rollover participants: clinical diagnosis, ADAS-Cog 13, and total volume of the ventricles -- which are then compared with future measurements. Strong points of the challenge are that the test data did not exist at the time of forecasting (it was acquired afterwards), and that it focuses on the challenging problem of cohort selection for clinical trials by identifying fast progressors. The submission phase of TADPOLE was open until 15 November 2017; since then data has been acquired until April 2019 from 219 subjects with 223 clinical visits and 150 Magnetic Resonance Imaging (MRI) scans, which was used for the evaluation of the participants' predictions. Thirty-three teams participated with a total of 92 submissions. No single submission was best at predicting all three outcomes. For diagnosis prediction, the best forecast (team Frog), which was based on gradient boosting, obtained a multiclass area under the receiver-operating curve (MAUC) of 0.931, while for ventricle prediction the best forecast (team EMC1), which was based on disease progression modelling and spline regression, obtained mean absolute error of 0.41% of total intracranial volume (ICV). For ADAS-Cog 13, no forecast was considerably better than the benchmark mixed effects model (BenchmarkME), provided to participants before the submission deadline. Further analysis can help understand which input features and algorithms are most suitable for Alzheimer's disease prediction and for aiding patient stratification in clinical trials.|http://arxiv.org/abs/2001.09016v1|Razvan V. Marinescu,Neil P. Oxtoby,Alexandra L. Young,Esther E. Bron,Arthur W. Toga,Michael W. Weiner,Frederik Barkhof,Nick C. Fox,Polina Golland,Stefan Klein,Daniel C. Alexander
1024|Intuitive and Ubiquitous Fever Monitoring Using Smartphones and Smartwatches|Inside all smart devices, such as smartphones or smartwatches, there are thermally sensitive resistors known as thermistors which are used to monitor the temperature of the device. These thermistors are sensitive to temperature changes near their location on-device. While they are designed to measure the temperature of the device components such as the battery, they can also sense changes in the temperature of the ambient environment or thermal entities in contact with the device. We have developed a model to estimate core body temperature from signals sensed by these thermistors during a user interaction in which the user places the capacitive touchscreen of a smart device against a thermal site on their body such as their forehead. During the interaction, the device logs the temperature sensed by the thermistors as well as the raw capacitance seen by the touch screen to capture features describing the rate of heat transfer from the body to the device and device-to-skin contact respectively. These temperature and contact features are then used to model the rate of heat transferred from the user's body to the device and thus core-body temperature of the user for ubiquitous and accessible fever monitoring using only a smart device. We validate this system in a lab environment on a simulated skin-like heat source with a temperature estimate mean absolute error of 0.743$^{\circ}$F (roughly 0.4$^{\circ}$C) and limit of agreement of $\pm2.374^{\circ}$F (roughly 1.3$^{\circ}$C) which is comparable to some off-the-shelf peripheral and tympanic thermometers. We found a Pearson's correlation $R^2$ of 0.837 between ground truth temperature and temperature estimated by our system. We also deploy this system in an ongoing clinical study on a population of 7 participants in a clinical environment to show the similarity between simulated and clinical trials.|http://arxiv.org/abs/2106.11855v1|Joseph Breda,Shwetak Patel
1025|Deep Learning (DL)-based Automatic Segmentation of the Internal Pudendal Artery (IPA) for Reduction of Erectile Dysfunction in Definitive Radiotherapy of Localized Prostate Cancer|Background and purpose: Radiation-induced erectile dysfunction (RiED) is commonly seen in prostate cancer patients. Clinical trials have been developed in multiple institutions to investigate whether dose-sparing to the internal-pudendal-arteries (IPA) will improve retention of sexual potency. The IPA is usually not considered a conventional organ-at-risk (OAR) due to segmentation difficulty. In this work, we propose a deep learning (DL)-based auto-segmentation model for the IPA that utilizes CT and MRI or CT alone as the input image modality to accommodate variation in clinical practice. Materials and methods: 86 patients with CT and MRI images and noisy IPA labels were recruited in this study. We split the data into 42/14/30 for model training, testing, and a clinical observer study, respectively. There were three major innovations in this model: 1) we designed an architecture with squeeze-and-excite blocks and modality attention for effective feature extraction and production of accurate segmentation, 2) a novel loss function was used for training the model effectively with noisy labels, and 3) modality dropout strategy was used for making the model capable of segmentation in the absence of MRI. Results: The DSC, ASD, and HD95 values for the test dataset were 62.2%, 2.54mm, and 7mm, respectively. AI segmented contours were dosimetrically equivalent to the expert physician's contours. The observer study showed that expert physicians' scored AI contours (mean=3.7) higher than inexperienced physicians' contours (mean=3.1). When inexperienced physicians started with AI contours, the score improved to 3.7. Conclusion: The proposed model achieved good quality IPA contours to improve uniformity of segmentation and to facilitate introduction of standardized IPA segmentation into clinical trials and practice.|http://arxiv.org/abs/2302.01493v1|Anjali Balagopal,Michael Dohopolski,Young Suk Kwon,Steven Montalvo,Howard Morgan,Ti Bai,Dan Nguyen,Xiao Liang,Xinran Zhong,Mu-Han Lin,Neil Desai,Steve Jiang
1026|Virtual imaging trials improved the transparency and reliability of AI systems in COVID-19 imaging|The credibility of Artificial Intelligence (AI) models in medical imaging, particularly during the COVID-19 pandemic, has been challenged by reproducibility issues and obscured clinical insights. To address these concerns, we propose a Virtual Imaging Trials (VIT) framework, utilizing both clinical and simulated datasets to evaluate AI systems. This study focuses on using convolutional neural networks (CNNs) for COVID-19 diagnosis using computed tomography (CT) and chest radiography (CXR). We developed and tested multiple AI models, 3D ResNet-like and 2D EfficientNetv2 architectures, across diverse datasets. Our evaluation metrics included the area under the curve (AUC). Statistical analyses, such as the DeLong method for AUC confidence intervals, were employed to assess performance differences. Our findings demonstrate that VIT provides a robust platform for objective assessment, revealing significant influences of dataset characteristics, patient factors, and imaging physics on AI efficacy. Notably, models trained on the most diverse datasets showed the highest external testing performance, with AUC values ranging from 0.73 to 0.76 for CT and 0.70 to 0.73 for CXR. Internal testing yielded higher AUC values (0.77 to 0.85 for CT and 0.77 to 1.0 for CXR), highlighting a substantial drop in performance during external validation, which underscores the importance of diverse and comprehensive training and testing data. This approach enhances model transparency and reliability, offering nuanced insights into the factors driving AI performance and bridging the gap between experimental and clinical settings. The study underscores the potential of VIT to improve the reproducibility and clinical relevance of AI systems in medical imaging.|http://arxiv.org/abs/2308.09730v3|Fakrul Islam Tushar,Lavsen Dahal,Saman Sotoudeh-Paima,Ehsan Abadi,W. Paul Segars,Ehsan Samei,Joseph Y. Lo
1027|Characterization of the Clinically Approved MRI Tracer Resotran for Magnetic Particle Imaging in a Comparison Study|Objective. The availability of magnetic nanoparticles with medical approval for human intervention is fundamental to the clinical translation of magnetic particle imaging (MPI). In this work, we thoroughly evaluate and compare the magnetic properties of an magnetic resonance imaging (MRI) approved tracer to validate its performance for MPI in future human trials. Approach. We analyze whether the recently approved MRI tracer Resotran is suitable for MPI. In addition, we compare Resotran with the previously approved and extensively studied tracer Resovist, with Ferrotran, which is currently in a clinical phase III study, and with the tailored MPI tracer Perimag. Main results. Initial magnetic particle spectroscopy measurements indicate that Resotran exhibits performance characteristics akin to Resovist, but below Perimag. We provide data on four different tracers using dynamic light scattering, transmission electron microscopy, vibrating sample magnetometry measurements, magnetic particle spectroscopy to derive hysteresis, point spread functions, and a serial dilution, as well as system matrix based MPI measurements on a preclinical scanner (Bruker 25/20 FF), including reconstructed images. Significance. Numerous approved magnetic nanoparticles used as tracers in MRI lack the necessary magnetic properties essential for robust signal generation in MPI. The process of obtaining medical approval for dedicated MPI tracers optimized for signal performance is an arduous and costly endeavor, often only justifiable for companies with a well-defined clinical business case. Resotran is an approved tracer that has become available in Europe for MRI. In this work, we study the eligibility of Resotran for MPI in an effort to pave the way for human MPI trials.|http://arxiv.org/abs/2402.06350v1|Fabian Mohn,Konrad Scheffler,Justin Ackers,Agnes Weimer,Franz Wegner,Florian Thieben,Mandy Ahlborg,Patrick Vogel,Matthias Graeser,Tobias Knopp
1028|The status and challenges for prostate SBRT treatments in United States proton therapy centers: An NRG Oncology practice survey|A survey was designed to inquire about the practice of proton SBRT treatment for prostate cancer. The survey was distributed to all 30 proton therapy centers in the United States that participate in the National Clinical Trial Network in Feb. 2023. The survey focused on usage, patient selection criteria, prescriptions, target contours, dose constraints, treatment plan optimization and evaluation methods, patient-specific QA, and IGRT methods. Results: We received responses from 25 centers (83% participation). Only 8 respondent proton centers (32%) reported performing SBRT of the prostate. The remaining 17 centers cited three primary reasons for not offering this treatment: no clinical need, lack of volumetric imaging, and/or lack of clinical evidence. Only 1 center cited the reduction in overall reimbursement as a concern for not offering prostate SBRT. Several common practices among the 8 centers offering SBRT for the prostate were noted, such as using Hydrogel spacers, fiducial markers, and MRI for target delineation. Most proton centers (87.5%) utilized pencil beam scanning (PBS) delivery and completed Imaging and Radiation Oncology Core (IROC) phantom credentialing. Treatment planning typically used parallel opposed lateral beams, and consistent parameters for setup and range uncertainties were used for plan optimization and robustness evaluation. Measurements-based patient-specific QA, beam delivery every other day, fiducial contours for IGRT, and total doses of 35-40 GyRBE were consistent across all centers. However, there was no consensus on the risk levels for patient selection. Conclusion: Prostate SBRT is used in about 1/3 of proton centers in the US. There was a significant consistency in practices among proton centers treating with proton SBRT. It is possible that the adoption of proton SBRT may become more common if proton SBRT is more commonly offered in clinical trials.|http://arxiv.org/abs/2402.17244v1|Jiajian Shen,Paige A. Taylor,Carlos E. Vargas,Minglei Kang,Jatinder Saini,Jun Zhou,Peilong Wang,Wei Liu,Charles B. Simone II,Ying Xiao,Liyong Lin
1029|Evaluating the Efficacy and Safety of Stereotactic Arrhythmia Radioablation in Ventricular Tachycardia: A Comprehensive Systematic Review and Meta-Analysis|Purpose: Stereotactic arrhythmia radioablation (STAR) has emerged as a promising non-invasive treatment for refractory ventricular tachycardia (VT), offering a novel alternative for patients who are poor candidates for catheter ablation. This systematic review and meta-analysis evaluates the safety, efficacy, and technical aspects of STAR across preclinical studies, case reports, case series, and clinical trials. Methods and Materials: A systematic review identified 80 studies published between 2015 and 2024, including 12 preclinical studies, 47 case reports, 15 case series, and 6 clinical trials. Data on patient demographics, treatment parameters, and clinical outcomes were extracted. Meta-analyses were performed for pooled mortality rates, VT burden reduction, and acute toxicities, with subgroup analyses exploring cardiomyopathy type, age, left ventricular ejection fraction (LVEF), and treatment modality. Results: The pooled 6- and 12-month mortality rates were 16% (95% CI: 11-21%) and 32% (95% CI: 26-39%), respectively. VT burden reduction at 6 months was 75% (95% CI: 73-77%), with significant heterogeneity (I^2 = 98.8%). Grade 3+ acute toxicities were observed in 7% (95% CI: 4-11%), with pneumonitis being the most common. Subgroup analyses showed comparable outcomes between LINAC- and CyberKnife-based treatments, with minor differences based on patient characteristics and cardiomyopathy type. Conclusions: STAR demonstrates significant potential in reducing VT burden and improving patient outcomes. While favorable acute safety profiles and efficacy support clinical adoption, variability in treatment protocols underscores the need for standardized practices. Future studies should aim to optimize patient selection, establish robust dosimetric standards, and evaluate long-term safety.|http://arxiv.org/abs/2501.18872v1|Keyur D. Shah,Chih-Wei Chang,Sibo Tian,Pretesh Patel,Richard Qiu,Justin Roper,Jun Zhou,Zhen Tian,Xiaofeng Yang
1030|Validation of the Virtual Reality Neuroscience Questionnaire: Maximum Duration of Immersive Virtual Reality Sessions Without the Presence of Pertinent Adverse Symptomatology|Research suggests that the duration of a VR session modulates the presence and intensity of VRISE, but there are no suggestions regarding the appropriate maximum duration of VR sessions. The implementation of high-end VR HMDs in conjunction with ergonomic VR software seems to mitigate the presence of VRISE substantially. However, a brief tool does not currently exist to appraise and report both the quality of software features and VRISE intensity quantitatively. The VRNQ was developed to assess the quality of VR software in terms of user experience, game mechanics, in-game assistance, and VRISE. Forty participants aged between 28 and 43 years were recruited (18 gamers and 22 non-gamers) for the study. They participated in 3 different VR sessions until they felt weary or discomfort and subsequently filled in the VRNQ. Our results demonstrated that VRNQ is a valid tool for assessing VR software as it has good convergent, discriminant, and construct validity. The maximum duration of VR sessions should be between 55-70 minutes when the VR software meets or exceeds the parsimonious cut-offs of the VRNQ and the users are familiarized with the VR system. Also. the gaming experience does not seem to affect how long VR sessions should last. Also, while the quality of VR software substantially modulates the maximum duration of VR sessions, age and education do not. Finally, deeper immersion, better quality of graphics and sound, and more helpful in-game instructions and prompts were found to reduce VRISE intensity. The VRNQ facilitates the brief assessment and reporting of the quality of VR software features and/or the intensity of VRISE, while its minimum and parsimonious cut-offs may appraise the suitability of VR software. The findings of this study contribute to the establishment of rigorous VR methods that are crucial for the viability of immersive VR as a research and clinical tool.|http://arxiv.org/abs/2101.08146v1|Panagiotis Kourtesis,Simona Collina,Leonidas A. A. Doumas,Sarah E. MacPherson
1031|Guidelines for the Development of Immersive Virtual Reality Software for Cognitive Neuroscience and Neuropsychology: The Development of Virtual Reality Everyday Assessment Lab (VR-EAL)|Virtual reality (VR) head-mounted displays (HMD) appear to be effective research tools, which may address the problem of ecological validity in neuropsychological testing. However, their widespread implementation is hindered by VR induced symptoms and effects (VRISE) and the lack of skills in VR software development. This study offers guidelines for the development of VR software in cognitive neuroscience and neuropsychology, by describing and discussing the stages of the development of Virtual Reality Everyday Assessment Lab (VR-EAL), the first neuropsychological battery in immersive VR. Techniques for evaluating cognitive functions within a realistic storyline are discussed. The utility of various assets in Unity, software development kits, and other software are described so that cognitive scientists can overcome challenges pertinent to VRISE and the quality of the VR software. In addition, this pilot study attempts to evaluate VR-EAL in accordance with the necessary criteria for VR software for research purposes. The VR neuroscience questionnaire (VRNQ; Kourtesis et al., 2019b) was implemented to appraise the quality of the three versions of VR-EAL in terms of user experience, game mechanics, in-game assistance, and VRISE. Twenty-five participants aged between 20 and 45 years with 12-16 years of full-time education evaluated various versions of VR-EAL. The final version of VR-EAL achieved high scores in every sub-score of the VRNQ and exceeded its parsimonious cut-offs. It also appeared to have better in-game assistance and game mechanics, while its improved graphics substantially increased the quality of the user experience and almost eradicated VRISE. The results substantially support the feasibility of the development of effective VR research and clinical software without the presence of VRISE during a 60-minute VR session.|http://arxiv.org/abs/2101.08166v1|Panagiotis Kourtesis,Danai Korre,Simona Collina,Leonidas A. A. Doumas,Sarah E. MacPherson
1032|A Mutagenetic Tree Hidden Markov Model for Longitudinal Clonal HIV Sequence Data|RNA viruses provide prominent examples of measurably evolving populations. In HIV infection, the development of drug resistance is of particular interest, because precise predictions of the outcome of this evolutionary process are a prerequisite for the rational design of antiretroviral treatment protocols. We present a mutagenetic tree hidden Markov model for the analysis of longitudinal clonal sequence data. Using HIV mutation data from clinical trials, we estimate the order and rate of occurrence of seven amino acid changes that are associated with resistance to the reverse transcriptase inhibitor efavirenz.|http://arxiv.org/abs/q-bio/0603031v1|Niko Beerenwinkel,Mathias Drton
1033|Survey of the didemnins: A class of depsipeptide natural products with promising biomedical applications|The didemnins represent a versatile class of depsipeptides of marine origin and hold a great deal of potential for biomedical application. The biological and geographical origins of the didemnins are reviewed in addition to the chemical structures of the major didemnins. The biological mechanisms behind the antiviral and anticancer effects of selected didemnins are summarized and the special case of dehydrodidemnin B (Aplidin) is expounded upon including structural characteristics, synthesis, pharmacological mechanism and a discussion of its current clinical trials as an anticancer agent.|http://arxiv.org/abs/q-bio/0612040v1|Jonathan L. Belof
1034|A New Family of Covariate-Adjusted Response Adaptive Designs and their Asymptotic Properties|It is often important to incorporating covariate information in the design of clinical trials. In literature, there are many designs of using stratification and covariate-adaptive randomization to balance on certain known covariate. Recently Zhang, Hu, Cheung and Chan (2007) have proposed a family of covariate-adjusted response-adaptive (CARA) designs and studied their asymptotic properties. However, these CARA designs often have high variabilities. In this paper, we propose a new family of covariate-adjusted response-adaptive (CARA) designs. We show that the new designs have smaller variabilities and therefore more efficient.|http://arxiv.org/abs/0812.3691v1|Li-Xin Zhang,Feifang Hu
1035|New developments of the odds theorem|The odds theorem and the corresponding solution algorithm (odds algorithm) are tools to solve a wide range of optimal stopping problems. Its generality and tractability have caught much attention. (Google for instance "Bruss odds" to obtain a quick overview.) Many extensions and modifications of the this result have appeared since publication in~2000. This article reviews the important gnew developments and applications in this field. The spectrum of application comprises as different fields as secretary problems, more general stopping problems, robotic maintenance problems, compassionate use clinical trials and others. This review also includes a new contribution of our own.|http://arxiv.org/abs/1212.1391v1|Rmi Dendievel
1036|A stochastic evolutionary model for survival dynamics|The recent interest in human dynamics has led researchers to investigate the stochastic processes that explain human behaviour in different contexts. Here we propose a generative model to capture the essential dynamics of survival analysis, traditionally employed in clinical trials and reliability analysis in engineering. In our model, the only implicit assumption made is that the longer an actor has been in the system, the more likely it is to have failed. We derive a power-law distribution for the process and provide preliminary empirical evidence for the validity of the model from two well-known survival analysis data sets.|http://arxiv.org/abs/1401.5957v2|Trevor Fenner,Mark Levene,George Loizou
1037|Online learning in MDPs with side information|We study online learning of finite Markov decision process (MDP) problems when a side information vector is available. The problem is motivated by applications such as clinical trials, recommendation systems, etc. Such applications have an episodic structure, where each episode corresponds to a patient/customer. Our objective is to compete with the optimal dynamic policy that can take side information into account.   We propose a computationally efficient algorithm and show that its regret is at most $O(\sqrt{T})$, where $T$ is the number of rounds. To best of our knowledge, this is the first regret bound for this setting.|http://arxiv.org/abs/1406.6812v1|Yasin Abbasi-Yadkori,Gergely Neu
1038|Partial stochastic dominance for the multivariate Gaussian distribution|Gaussian comparison inequalities provide a way of bounding probabilities relating to multivariate Gaussian random vectors in terms of probabilities of random variables with simpler correlation structures. In this paper, we establish the partial stochastic dominance result that the cumulative distribution function of the maximum of a multivariate normal random vector, with positive intraclass correlation coefficient, intersects the cumulative distribution function of a standard normal random variable at most once. This result can be applied to the Bayesian design of a clinical trial in which several experimental treatments are compared to a single control.|http://arxiv.org/abs/1407.0936v1|Amanda Turner,John Whitehead
1039|A Statistical Model for Stroke Outcome Prediction and Treatment Planning|Stroke is a major cause of mortality and long--term disability in the world. Predictive outcome models in stroke are valuable for personalized treatment, rehabilitation planning and in controlled clinical trials. In this paper we design a new model to predict outcome in the short-term, the putative therapeutic window for several treatments. Our regression-based model has a parametric form that is designed to address many challenges common in medical datasets like highly correlated variables and class imbalance. Empirically our model outperforms the best--known previous models in predicting short--term outcomes and in inferring the most effective treatments that improve outcome.|http://arxiv.org/abs/1602.07280v1|Abhishek Sengupta,Vaibhav Rajan,Sakyajit Bhattacharya,G R K Sarma
1040|Thresholding Bandit for Dose-ranging: The Impact of Monotonicity|We analyze the sample complexity of the thresholding bandit problem, with and without the assumption that the mean values of the arms are increasing. In each case, we provide a lower bound valid for any risk $\delta$ and any $\delta$-correct algorithm; in addition, we propose an algorithm whose sample complexity is of the same order of magnitude for small risks. This work is motivated by phase 1 clinical trials, a practically important setting where the arm means are increasing by nature, and where no satisfactory solution is available so far.|http://arxiv.org/abs/1711.04454v2|Aurlien Garivier,Pierre Mnard,Laurent Rossi,Pierre Menard
1041|Pharmacokinetics Simulations for Studying Correlates of Prevention Efficacy of Passive HIV-1 Antibody Prophylaxis in the Antibody Mediated Prevention (AMP) Study|A key objective in two phase 2b AMP clinical trials of VRC01 is to evaluate whether drug concentration over time, as estimated by non-linear mixed effects pharmacokinetics (PK) models, is associated with HIV infection rate. We conducted a simulation study of marker sampling designs, and evaluated the effect of study adherence and sub-cohort sample size on PK model estimates in multiple-dose studies. With m=120, even under low adherence (about half of study visits missing per participant), reasonably unbiased and consistent estimates of most fixed and random effect terms were obtained. Coarsened marker sampling schedules were also studied.|http://arxiv.org/abs/1801.08626v1|Lily Zhang,Peter B. Gilbert,Edmund Capparelli,Yunda Huang
1042|Learning Patient Representations from Text|Mining electronic health records for patients who satisfy a set of predefined criteria is known in medical informatics as phenotyping. Phenotyping has numerous applications such as outcome prediction, clinical trial recruitment, and retrospective studies. Supervised machine learning for phenotyping typically relies on sparse patient representations such as bag-of-words. We consider an alternative that involves learning patient representations. We develop a neural network model for learning patient representations and show that the learned representations are general enough to obtain state-of-the-art performance on a standard comorbidity detection task.|http://arxiv.org/abs/1805.02096v1|Dmitriy Dligach,Timothy Miller
1043|PHI Scrubber: A Deep Learning Approach|Confidentiality of patient information is an essential part of Electronic Health Record System. Patient information, if exposed, can cause a serious damage to the privacy of individuals receiving healthcare. Hence it is important to remove such details from physician notes. A system is proposed which consists of a deep learning model where a de-convolutional neural network and bi-directional LSTM-CNN is used along with regular expressions to recognize and eliminate the individually identifiable information. This information is then removed from a medical practitioner's data which further allows the fair usage of such information among researchers and in clinical trials.|http://arxiv.org/abs/1808.01128v1|Abhai Kollara Dilip,Kamal Raj K,Malaikannan Sankarasubbu
1044|Risk-Averse Stochastic Convex Bandit|Motivated by applications in clinical trials and finance, we study the problem of online convex optimization (with bandit feedback) where the decision maker is risk-averse. We provide two algorithms to solve this problem. The first one is a descent-type algorithm which is easy to implement. The second algorithm, which combines the ellipsoid method and a center point device, achieves (almost) optimal regret bounds with respect to the number of rounds. To the best of our knowledge this is the first attempt to address risk-aversion in the online convex bandit problem.|http://arxiv.org/abs/1810.00737v1|Adrian Rivera Cardoso,Huan Xu
1045|A Multi-parameter regression model for interval censored survival data|We develop flexible multi-parameter regression survival models for interval censored survival data arising in longitudinal prospective studies and longitudinal randomised controlled clinical trials. A multi-parameter Weibull regression survival model, which is wholly parametric, and has non-proportional hazards, is the main focus of the paper. We describe the basic model, develop the interval-censored likelihood and extend the model to include gamma frailty and a dispersion model. We evaluate the models by means of a simulation study and a detailed re-analysis of data from the Signal Tandmobiel$^{\circledR}$ study. The results demonstrate that the multi-parameter regression model with frailty is computationally efficient and provides an excellent fit to the data.|http://arxiv.org/abs/1901.09634v1|Defen Peng,Gilbert MacKenzie,Kevin Burke
1046|Energy distance and kernel mean embeddings for two-sample survival testing|We study the comparison problem of distribution equality between two random samples under a right censoring scheme. To address this problem, we design a series of tests based on energy distance and kernel mean embeddings. We calibrate our tests using permutation methods and prove that they are consistent against all fixed continuous alternatives. To evaluate our proposed tests, we simulate survival curves from previous clinical trials. Additionally, we provide practitioners with a set of recommendations on how to select parameters/distances for the delay effect problem. Based on the method for parameter tunning that we propose, we show that our tests demonstrate a considerable gain of statistical power against classical survival tests.|http://arxiv.org/abs/1912.04160v1|Marcos Matabuena,Oscar Hernan Madrid Padilla
1047|A Comparison of Prior Elicitation Aggregation using the Classical Method and SHELF|Subjective Bayesian prior distributions elicited from experts can be aggregated together to form group priors. This paper compares aggregated priors formed by Equal Weight Aggregation, the Classical Method, and the Sheffield Elicitation Framework to each other and individual expert priors, using an expert elicitation carried out for a clinical trial. Aggregation methods and individual expert prior distributions are compared using proper scoring rules to compare the informativeness and calibration of the distributions. The three aggregation methods outperform the individual experts, and the Sheffield Elicitation Framework performs best amongst them.|http://arxiv.org/abs/2001.11365v3|Cameron J. Williams,Kevin J. Wilson,Nina Wilson
1048|Online learning with Corrupted context: Corrupted Contextual Bandits|We consider a novel variant of the contextual bandit problem (i.e., the multi-armed bandit with side-information, or context, available to a decision-maker) where the context used at each decision may be corrupted ("useless context"). This new problem is motivated by certain on-line settings including clinical trial and ad recommendation applications. In order to address the corrupted-context setting,we propose to combine the standard contextual bandit approach with a classical multi-armed bandit mechanism. Unlike standard contextual bandit methods, we are able to learn from all iteration, even those with corrupted context, by improving the computing of the expectation for each arm. Promising empirical results are obtained on several real-life datasets.|http://arxiv.org/abs/2006.15194v1|Djallel Bouneffouf
1049|Sample size calculation for the Andersen-Gill model comparing rates of recurrent events|Recurrent events arise frequently in biomedical research, where the subject may experience the same type of events more than once. The Andersen-Gill (AG) model has become increasingly popular in the analysis of recurrent events particularly when the event rate is not constant over time. We propose a procedure for calculating the power and sample size for the robust Wald test from the AG model in superiority, noninferiority, and equivalence clinical trials. Its performance is demonstrated by numerical examples. Sample SAS code is provided in the Supplementary Material.|http://arxiv.org/abs/2011.11026v1|Yongqiang Tang,Ronan Fitzpatrick
1050|AI Ethics in Smart Healthcare|This article reviews the landscape of ethical challenges of integrating artificial intelligence (AI) into smart healthcare products, including medical electronic devices. Differences between traditional ethics in the medical domain and emerging ethical challenges with AI-driven healthcare are presented, particularly as they relate to transparency, bias, privacy, safety, responsibility, justice, and autonomy. Open challenges and recommendations are outlined to enable the integration of ethical principles into the design, validation, clinical trials, deployment, monitoring, repair, and retirement of AI-based smart healthcare products.|http://arxiv.org/abs/2211.06346v1|Sudeep Pasricha
1051|Blended Mastery Learning in Mathematics|In this paper we report a study in which we have developed a teaching cycle based closely on Bloom's Learning for Mastery (LFM). The teaching cycle ameliorates some of the practical problems with LFM by making use of the STACK online assessment system to provide automated assessment and feedback to students. We report a clinical trial of this teaching cycle with groups of university level engineering students. Our results are modest, but positive: performance on the exercises predicted mastery according to the formative tests to a small extent. Students also report being supportive of the use of the new teaching cycle.|http://arxiv.org/abs/1712.07848v1|Timo Pelkola,Antti Rasila,Christopher Sangwin
1052|Identifying mediating variables with graphical models: an application to the study of causal pathways in people living with HIV|We empirically demonstrate that graphical models can be a valuable tool in the identification of mediating variables in causal pathways. We make use of graphical models to elucidate the causal pathway through which the treatment influences the levels of fatigue and weakness in people living with HIV (PLHIV) based on a secondary analysis of a categorical dataset collected in a behavioral clinical trial: is weakness a mediator for the treatment and fatigue, or is fatigue a mediator for the treatment and weakness? Causal mediation analysis could not offer any definite answers to these questions.\\ KEYWORDS: Contingency tables; graphical models; loglinear models; HIV; mediation|http://arxiv.org/abs/1907.04838v1|Adrian Dobra,Katherine Buhikire,Joachim G. Voss
1053|Non-parametric estimation of Expectation and Variance of event count and of incidence rate in a recurrent process -- where intensity of event-occurrence changes with the occurrence of each higher order event|In this paper, a novel non-parametric method for estimation of expectation and maximum value of the variance function is proposed for recurrent events where intensity of event occurrence changes with the occurrence of each higher order event. These kinds of recurrent events are often observed in clinical trials for cardio-vascular events and also in many social experiments involving drug addiction, armed robberies, etc. Simulated data is used to demonstrate the novel approach for estimating the mean and variance of such recurrent events and the results are compared with the result of Nelson Aalen estimator.|http://arxiv.org/abs/2012.09746v1|Sudipta Bhattacharya
1054|Chemistry42: An AI-based platform for de novo molecular design|Chemistry42 is a software platform for de novo small molecule design that integrates Artificial Intelligence (AI) techniques with computational and medicinal chemistry methods. Chemistry42 is unique in its ability to generate novel molecular structures with predefined properties validated through in vitro and in vivo studies. Chemistry42 is a core component of Insilico Medicine Pharma.ai drug discovery suite that also includes target discovery and multi-omics data analysis (PandaOmics) and clinical trial outcomes predictions (InClinico).|http://arxiv.org/abs/2101.09050v1|Yan A. Ivanenkov,Alex Zhebrak,Dmitry Bezrukov,Bogdan Zagribelnyy,Vladimir Aladinskiy,Daniil Polykovskiy,Evgeny Putin,Petrina Kamya,Alexander Aliper,Alex Zhavoronkov
1055|What Clinical Trials Can Teach Us about the Development of More Resilient AI for Cybersecurity|Policy-mandated, rigorously administered scientific testing is needed to provide transparency into the efficacy of artificial intelligence-based (AI-based) cyber defense tools for consumers and to prioritize future research and development. In this article, we propose a model that is informed by our experience, urged forward by massive scale cyberattacks, and inspired by parallel developments in the biomedical field and the unprecedentedly fast development of new vaccines to combat global pathogens.|http://arxiv.org/abs/2105.06545v1|Edmon Begoli,Robert A. Bridges,Sean Oesch,Kathryn E. Knight
1056|Training like Playing: A Reinforcement Learning And Knowledge Graph-based framework for building Automatic Consultation System in Medical Field|We introduce a framework for AI-based medical consultation system with knowledge graph embedding and reinforcement learning components and its implement. Our implement of this framework leverages knowledge organized as a graph to have diagnosis according to evidence collected from patients recurrently and dynamically. According to experiment we designed for evaluating its performance, it archives a good result. More importantly, for getting better performance, researchers can implement it on this framework based on their innovative ideas, well designed experiments and even clinical trials.|http://arxiv.org/abs/2106.07502v1|Yining Huang,Meilian Chen,Keke Tang
1057|Data Integration in Causal Inference|Integrating data from multiple heterogeneous sources has become increasingly popular to achieve a large sample size and diverse study population. This paper reviews development in causal inference methods that combines multiple datasets collected by potentially different designs from potentially heterogeneous populations. We summarize recent advances on combining randomized clinical trial with external information from observational studies or historical controls, combining samples when no single sample has all relevant variables with application to two-sample Mendelian randomization, distributed data setting under privacy concerns for comparative effectiveness and safety research using real-world data, Bayesian causal inference, and causal discovery methods.|http://arxiv.org/abs/2110.01106v1|Xu Shi,Ziyang Pan,Wang Miao
1058|The Role of Pairwise Matching in Experimental Design for an Incidence Outcome|We consider the problem of evaluating designs for a two-arm randomized experiment with an incidence (binary) outcome under a nonparametric general response model. Our two main results are that the priori pair matching design of Greevy et al. (2004) is (1) the optimal design as measured by mean squared error among all block designs which includes complete randomization. And (2), this pair-matching design is minimax, i.e. it provides the lowest mean squared error under an adversarial response model. Theoretical results are supported by simulations and clinical trial data.|http://arxiv.org/abs/2209.00490v1|Adam Kapelner,Abba M. Krieger,David Azriel
1059|Testing Homogeneity of Proportion Ratios for Stratified Bilateral Correlated Data|Intraclass correlation in bilateral data has been investigated in recent decades with various statistical methods. In practice, stratifying bilateral data by some control variables will provide more sophisticated statistical results to satisfy different research proposed in random clinical trials. In this article, we propose three test statistics (likelihood ratio test, score test, and Wald-type test statistics) to evaluate the homogeneity of proportion ratios for stratified bilateral correlated data under an equal correlation assumption. Monte Carlo simulations of Type I error and power are performed, and the score test yields a robust outcome based on empirical Type I error and power. Lastly, a real data example is conducted to illustrate the proposed three tests.|http://arxiv.org/abs/2303.12943v1|Wanqing Tian,Chang-Xing Ma
1060|Confidence Intervals for Ratios of Proportions in Stratified Bilateral Correlated Data|Confidence interval (CI) methods for stratified bilateral studies use intraclass correlation to avoid misleading results. In this article, we propose four CI methods (sample-size weighted global MLE-based Wald-type CI, complete MLE-based Wald-type CI, profile likelihood CI, and complete MLE-based score CI) to investigate CIs of proportion ratios to clinical trial design with stratified bilateral data under Dallal's intraclass model. Monte Carlo simulations are performed, and the complete MLE-based score confidence interval (CS) method yields a robust outcome. Lastly, a real data example is conducted to illustrate the proposed four CIs.|http://arxiv.org/abs/2303.13557v1|Wanqing Tian,Chang-Xing Ma
1061|Analyzing Generalized Plya Urn Models using Martingales, with an Application to Viral Evolution|The randomized play-the-winner (RPW) model is a generalized P\'olya Urn process with broad applications ranging from clinical trials to molecular evolution. We derive an exact expression for the variance of the RPW model by transforming the P\'olya Urn process into a martingale, correcting an earlier result of Matthews and Rosenberger (1997). We then use this result to approximate the full probability mass function of the RPW model for certain parameter values relevant to genetic applications. Finally, we fit our model to genomic sequencing data of SARS-CoV-2, demonstrating a novel method of estimating the viral mutation rate that delivers comparable results to existing scientific literature.|http://arxiv.org/abs/2306.17375v2|Ivan Specht,Michael Mitzenmacher
1062|Causal rule ensemble method for estimating heterogeneous treatment effect with consideration of main effects|This study proposes a novel framework based on the RuleFit method to estimate Heterogeneous Treatment Effect (HTE) in a randomized clinical trial. To achieve this, we adopted S-learner of the metaalgorithm for our proposed framework. The proposed method incorporates a rule term for the main effect and treatment effect, which allows HTE to be interpretable form of rule. By including a main effect term in the proposed model, the selected rule is represented as an HTE that excludes other effects. We confirmed a performance equivalent to that of another ensemble learning methods through numerical simulation and demonstrated the interpretation of the proposed method from a real data application.|http://arxiv.org/abs/2307.14766v1|Mayu Hiraishi,Ke Wan,Kensuke Tanioka,Hiroshi Yadohisa,Toshio Shimokawa
1063|Extracting Scalar Measures from Curves|The ability to order outcomes is necessary to make comparisons which is complicated when there is no natural ordering on the space of outcomes, as in the case of functional outcomes. This paper examines methods for extracting a scalar summary from functional or longitudinal outcomes based on an average rate of change which can be used to compare curves. Common approaches used in practice use a change score or an analysis of covariance (ANCOVA) to make comparisons. However, these standard approaches only use a fraction of the available data and are inefficient. We derive measures of performance of an averaged rate of change of a functional outcome and compare this measure to standard measures. Simulations and data from a depression clinical trial are used to illustrate results.|http://arxiv.org/abs/2402.01827v1|Lanqiu Yao,Thaddeus Tarpey
1064|DFKI-NLP at SemEval-2024 Task 2: Towards Robust LLMs Using Data Perturbations and MinMax Training|The NLI4CT task at SemEval-2024 emphasizes the development of robust models for Natural Language Inference on Clinical Trial Reports (CTRs) using large language models (LLMs). This edition introduces interventions specifically targeting the numerical, vocabulary, and semantic aspects of CTRs. Our proposed system harnesses the capabilities of the state-of-the-art Mistral model, complemented by an auxiliary model, to focus on the intricate input space of the NLI4CT dataset. Through the incorporation of numerical and acronym-based perturbations to the data, we train a robust system capable of handling both semantic-altering and numerical contradiction interventions. Our analysis on the dataset sheds light on the challenging sections of the CTRs for reasoning.|http://arxiv.org/abs/2405.00321v1|Bhuvanesh Verma,Lisa Raithel
1065|FZI-WIM at SemEval-2024 Task 2: Self-Consistent CoT for Complex NLI in Biomedical Domain|This paper describes the inference system of FZI-WIM at the SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials. Our system utilizes the chain of thought (CoT) paradigm to tackle this complex reasoning problem and further improves the CoT performance with self-consistency. Instead of greedy decoding, we sample multiple reasoning chains with the same prompt and make the final verification with majority voting. The self-consistent CoT system achieves a baseline F1 score of 0.80 (1st), faithfulness score of 0.90 (3rd), and consistency score of 0.73 (12th). We release the code and data publicly https://github.com/jens5588/FZI-WIM-NLI4CT.|http://arxiv.org/abs/2406.10040v1|Jin Liu,Steffen Thoma
1066|Experimental Design For Causal Inference Through An Optimization Lens|The study of experimental design offers tremendous benefits for answering causal questions across a wide range of applications, including agricultural experiments, clinical trials, industrial experiments, social experiments, and digital experiments. Although valuable in such applications, the costs of experiments often drive experimenters to seek more efficient designs. Recently, experimenters have started to examine such efficiency questions from an optimization perspective, as experimental design problems are fundamentally decision-making problems. This perspective offers a lot of flexibility in leveraging various existing optimization tools to study experimental design problems. This manuscript thus aims to examine the foundations of experimental design problems in the context of causal inference as viewed through an optimization lens.|http://arxiv.org/abs/2408.09607v2|Jinglong Zhao
1067|Categorical Data Analysis|Categorical data are common in educational and social science research; however, methods for its analysis are generally not covered in introductory statistics courses. This chapter overviews fundamental concepts and methods in categorical data analysis. It describes and illustrates the analysis of contingency tables given different sampling processes and distributions, estimation of probabilities, hypothesis testing, measures of associations, and tests of no association with nominal variables, as well as the test of linear association with ordinal variables. Three data sets illustrate fatal police shootings in the United States, clinical trials of the Moderna vaccine, and responses to General Social Survey questions.|http://arxiv.org/abs/2409.02942v1|Dandan Chen,Carolyn Anderson
1068|Generative AI in Medicine|The increased capabilities of generative AI have dramatically expanded its possible use cases in medicine. We provide a comprehensive overview of generative AI use cases for clinicians, patients, clinical trial organizers, researchers, and trainees. We then discuss the many challenges -- including maintaining privacy and security, improving transparency and interpretability, upholding equity, and rigorously evaluating models -- which must be overcome to realize this potential, and the open research directions they give rise to.|http://arxiv.org/abs/2412.10337v2|Divya Shanmugam,Monica Agrawal,Rajiv Movva,Irene Y. Chen,Marzyeh Ghassemi,Maia Jacobs,Emma Pierson
1069|To Study Properties of a Known Procedure in Adaptive Sequential Sampling Design|We revisit the procedure proposed by Bhandari et al. (2009) in the context of two-treatment clinical trials, with the objective of minimizing the applications of a less effective drug to the least number of patients. Our focus is on an adaptive sequential procedure that is both simple and intuitive. Our findings show that the expected number of applications of the less effective drug remains finite. In contrast, Bhandari et al. (2009) observed that this number increases logarithmically with the total sample size. We attribute this discrepancy to differences in their choice of starting sample size and the method of analysis employed.|http://arxiv.org/abs/2412.17791v1|Sampurna Kundu,Jayant Jha,Subir Kumar Bhandari
1070|How to build an Open Science Monitor based on publications? A French perspective|Many countries and institutions are striving to develop tools to monitor their open science policies. Since 2018, with the launch of its National Plan for Open Science, France has been progressively implementing a monitoring framework for its public policy, relying exclusively on reliable, open, and controlled data. Currently, this monitoring focuses on research outputs, particularly publications, as well as theses and clinical trials. Publications serve as a basis for analyzing other dimensions, including research data, code, and software. The metadata associated with publications is therefore particularly valuable, but the methodology for leveraging it raises several challenges. Here, we briefly outline how we have used this metadata to construct the French Open Science Monitor.|http://arxiv.org/abs/2501.02856v1|Laetitia Bracco,Eric Jeangirard,Anne L'Hte,Laurent Romary
1071|A Bureaucratic Theory of Statistics|This commentary proposes a framework for understanding the role of statistics in policy-making, regulation, and bureaucratic systems. I introduce the concept of "ex ante policy," describing statistical rules and procedures designed before data collection to govern future actions. Through examining examples, particularly clinical trials, I explore how ex ante policy serves as a calculus of bureaucracy, providing numerical foundations for governance through clear, transparent rules. The ex ante frame obviates heated debates about inferential interpretations of probability and statistical tests, p-values, and rituals. I conclude by calling for a deeper appreciation of statistics' bureaucratic function and suggesting new directions for research in policy-oriented statistical methodology.|http://arxiv.org/abs/2501.03457v1|Benjamin Recht
1072|Dynamics of multi-stable states during ongoing and evoked cortical activity|Single trial analyses of ensemble activity in alert animals demonstrate that cortical circuits dynamics evolve through temporal sequences of metastable states. Metastability has been studied for its potential role in sensory coding, memory and decision-making. Yet, very little is known about the network mechanisms responsible for its genesis. It is often assumed that the onset of state sequences is triggered by an external stimulus. Here we show that state sequences can be observed also in the absence of overt sensory stimulation. Analysis of multielectrode recordings from the gustatory cortex of alert rats revealed ongoing sequences of states, where single neurons spontaneously attain several firing rates across different states. This single neuron multi-stability represents a challenge to existing spiking network models, where typically each neuron is at most bi-stable. We present a recurrent spiking network model that accounts for both the spontaneous generation of state sequences and the multi-stability in single neuron firing rates. Each state results from the activation of neural clusters with potentiated intra-cluster connections, with the firing rate in each cluster depending on the number of active clusters. Simulations show that the models ensemble activity hops among the different states, reproducing the ongoing dynamics observed in the data. When probed with external stimuli, the model predicts the quenching of single neuron multi-stability into bi-stability and the reduction of trial-by-trial variability. Both predictions were confirmed in the data. Altogether, these results provide a theoretical framework that captures both ongoing and evoked network dynamics in a single mechanistic model.|http://arxiv.org/abs/1508.00165v3|Luca Mazzucato,Alfredo Fontanini,Giancarlo La Camera
1073|A symbolic information approach applied to human intracranial data to characterize and distinguish different congnitive processes|How the human brain processes information during different cognitive tasks is one of the greatest questions in contemporary neuroscience. Understanding the statistical properties of brain signals during specific activities is one promising way to address this question. Here we analyze freely available data from implanted electrocorticography (ECoG) in five human subjects during two different cognitive tasks in the light of information theory quantifiers ideas. We employ a symbolic information approach to determine the probability distribution function associated with the time series from different cortical areas. Then we utilize these probabilities to calculate the associated Shannon entropy and a statistical complexity measure based on the disequilibrium between the actual time series and one with a uniform probability distribution function. We show that an Euclidian distance in the complexity-entropy plane and an asymmetry index for complexity are useful for comparing the two conditions. We show that our method can distinguish visual search epochs from blank screen intervals in different electrodes and patients. By using a multi-scale approach and embedding time delays to downsample the data, we find important time scales in which the relevant information is being processed. We also determine cortical regions and time intervals along the 2-second-long trials that present more pronounced differences between the two cognitive tasks. Finally, we show that the method is useful to distinguish cognitive processes using brain activity on a trial-by-trial basis.|http://arxiv.org/abs/2404.17981v1|caro Rodolfo Soares Coelho Da Paz,Pedro F. A. Silva,Helena Bordini de Lucas,Srgio H. A. Lira,Osvaldo A. Rosso,Fernanda Selingardi Matias
1074|Nonlinear Mixed-Effect Models for Prostate-Specific Antigen Kinetics and Link with Survival in the Context of Metastatic Prostate Cancer: a Comparison by Simulation of Two-Stage and Joint Approaches|In metastatic castration-resistant prostate cancer (mCRPC) clinical trials, the assessment of treatment efficacy essentially relies on the time-to-death and the kinetics of prostate-specific antigen (PSA). Joint modelling has been increasingly used to characterize the relationship between a time-to-event and a biomarker kinetics but numerical difficulties often limit this approach to linear models. Here we evaluated by simulation the capability of a new feature of the Stochastic Approximation Expectation-Maximization algorithm in Monolix to estimate the parameters of a joint model where PSA kinetics was defined by a mechanistic nonlinear mixed-effect model. The design of the study and the parameter values were inspired from one arm of a clinical trial. Increasingly high levels of association between PSA and survival were considered and results were compared with those found using two simplified alternatives to joint model, a two-stage and a joint sequential model. We found that joint model allowed for a precise estimation of all longitudinal and survival parameters. In particular the effect of PSA kinetics on survival could be precisely estimated, regardless of the strength of the association. In contrast, both simplified approaches led to bias on longitudinal parameters and two-stage model systematically underestimated the effect of PSA kinetics on survival. In summary we showed that joint model can be used to characterize the relationship between a nonlinear kinetics and survival. This opens the way for the use of more complex and physiological models to improve treatment evaluation and prediction in oncology.|http://arxiv.org/abs/1503.05103v1|Solne Desme,France Mentr,Christine Veyrat-Follet,Jrmie Guedj
1075|Dynamic path analysis - A useful tool to investigate mediation processes in clinical survival trials|When it comes to clinical survival trials, regulatory restrictions usually require the application of methods that solely utilize baseline covariates and the intention-to-treat principle. Thereby a lot of potentially useful information is lost, as collection of time-to-event data often goes hand in hand with collection of information on biomarkers and other internal time-dependent covariates. However, there are tools to incorporate information from repeated measurements in a useful manner that can help to shed more light on the underlying treatment mechanisms. We consider dynamic path analysis, a model for mediation analysis in the presence of a time-to-event outcome and time-dependent covariates to investigate direct and indirect effects in a study of different lipid lowering treatments in patients with previous myocardial infarctions. Further, we address the question whether survival in itself may produce associations between the treatment and the mediator in dynamic path analysis and give an argument that, due to linearity of the assumed additive hazard model, this is not the case. We further elaborate on our view that, when studying mediation, we are actually dealing with underlying processes rather than single variables measured only once during the study period. This becomes apparent in results from various models applied to the study of lipid lowering treatments as well as our additionally conducted simulation study, where we clearly observe, that discarding information on repeated measurements can lead to potentially erroneous conclusions.|http://arxiv.org/abs/1504.06506v1|Susanne Strohmaier,Kjetil Rysland,Rune Hoff,rnulf Borgan,Terje Pedersen,Odd O. Aalen
1076|Optimally estimating the sample mean from the sample size, median, mid-range and/or mid-quartile range|The era of big data is coming, and evidence-based medicine is attracting increasing attention to improve decision making in medical practice via integrating evidence from well designed and conducted clinical research. Meta-analysis is a statistical technique widely used in evidence-based medicine for analytically combining the findings from independent clinical trials to provide an overall estimation of a treatment effectiveness. The sample mean and standard deviation are two commonly used statistics in meta-analysis but some trials use the median, the minimum and maximum values, or sometimes the first and third quartiles to report the results. Thus, to pool results in a consistent format, researchers need to transform those information back to the sample mean and standard deviation. In this paper, we investigate the optimal estimation of the sample mean for meta-analysis from both theoretical and empirical perspectives. A major drawback in the literature is that the sample size, needless to say its importance, is either ignored or used in a stepwise but somewhat arbitrary manner, e.g., the famous method proposed by Hozo et al. We solve this issue by incorporating the sample size in a smoothly changing weight in the estimators to reach the optimal estimation. Our proposed estimators not only improve the existing ones significantly but also share the same virtue of the simplicity. The real data application indicates that our proposed estimators are capable to serve as "rules of thumb" and will be widely applied in evidence-based medicine.|http://arxiv.org/abs/1505.05687v4|Dehui Luo,Xiang Wan,Jiming Liu,Tiejun Tong
1077|Outcome Modeling Using Clinical DVH Data|Purpose: To quantify the ability of correlation and regression analysis to extract the normal lung dose-response function from dose volume histogram (DVH) data. Methods: A local injury model is adopted, in which radiation-induced damage (functional loss) G is the integral of the DVH with function R(D). RP risk is H(G) where H() is the sigmoid cumulative distribution of functional reserve. RP incidence is a Bernoulli function of risk. A homogeneous patient cohort is assumed, allowing non-dose-related factors to be ignored. Clinically realistic DVHs are combined with the injury model to simulate RP data. Results: Correlation analysis is often used to identify predictor variables that are correlated with outcome, for inclusion in a predictive model. In the local injury model, all DVH metrics VD contribute to damage. Correlation analysis therefore has limited value. The subset of VD significantly correlated with incidence varies randomly from trial to trial due to random variations in the DVH set, and does not necessarily reveal anything useful about the patient cohort or the underlying biological dose-response relationship. Regression or matrix analysis can extract R(D) from damage or risk data, provided smoothness regularization is employed. Extraction of R(D) from incidence data was not successful, due to its higher level of statistical variability. Conclusions: To the author's knowledge, smoothness regularization has not been applied to this problem, so represents a novel approach. Dose-response functions can be successfully extracted from measurements of integral (as opposed to regional) lung damage G, suggesting value in re-visiting available measurements of ventilation, perfusion and radiographic damage. The techniques developed here can potentially be used to extract the dose-response functions of different tissues from multiple types of quantitative volumetric imaging data.|http://arxiv.org/abs/1508.04507v2|J. J. Gordon
1078|Individual Treatment Effect Prediction for ALS Patients|A treatment for a complicated disease may be helpful for some but not all patients, which makes predicting the treatment effect for new patients important yet challenging. Here we develop a method for predicting the treatment effect based on patient char- acteristics and use it for predicting the effect of the only drug (Riluzole) approved for treating Amyotrophic Lateral Sclerosis (ALS). Our proposed method of model-based ran- dom forests detects similarities in the treatment effect among patients and on this basis computes personalised models for new patients. The entire procedure focuses on a base model, which usually contains the treatment indicator as a single covariate and takes the survival time or a health or treatment success measurement as primary outcome. This base model is used both to grow the model-based trees within the forest, in which the patient characteristics that interact with the treatment are split variables, and to com- pute the personalised models, in which the similarity measurements enter as weights. We applied the personalised models using data from several clinical trials for ALS from the PRO-ACT database. Our results indicate that some ALS patients benefit more from the drug Riluzole than others. Our method allows shifting from stratified medicine to person- alised medicine and can also be used in assessing the treatment effect for other diseases studied in a clinical trial.|http://arxiv.org/abs/1604.08720v1|Heidi Seibold,Achim Zeileis,Torsten Hothorn
1079|An Empirical Biomarker-based Calculator for Autosomal Recessive Polycystic Kidney Disease - The Nieto-Narayan Formula|Autosomal polycystic kidney disease (ARPKD) is associated with progressive enlargement of the kidneys fuelled by the formation and expansion of fluid-filled cysts. The disease is congenital and children that do not succumb to it during the neonatal period will, by age 10 years, more often than not, require nephrectomy+renal replacement therapy for management of both pain and renal insufficiency. Since increasing cystic index (CI; percent of kidney occupied by cysts) drives both renal expansion and organ dysfunction, management of these patients, including decisions such as elective nephrectomy and prioritization on the transplant waitlist, could clearly benefit from serial determination of CI. So also, clinical trials in ARPKD evaluating the efficacy of novel drug candidates could benefit from serial determination of CI. Although ultrasound is currently the imaging modality of choice for diagnosis of ARPKD, its utilization for assessing disease progression is highly limited. Magnetic resonance imaging or computed tomography, although more reliable for determination of CI, are expensive, time-consuming and somewhat impractical in the pediatric population. Using a well-established mammalian model of ARPKD, we undertook a big data-like analysis of minimally- or non-invasive serum and urine biomarkers of renal injury/dysfunction to derive a family of equations for estimating CI. We then applied a signal averaging protocol to distill these equations to a single empirical formula for calculation of CI. Such a formula will eventually find use in identifying and monitoring patients at high risk for progressing to end-stage renal disease and aid in the conduct of clinical trials.|http://arxiv.org/abs/1607.07359v2|Jake A. Nieto,Michael A. Yamin,Itzhak D. Goldberg,Prakash Narayan
1080|Better than Counting Seconds: Identifying Fallers among Healthy Elderly using Fusion of Accelerometer Features and Dual-Task Timed Up and Go|Devices and sensors for identification of fallers can be used to implement actions to prevent falls and to allow the elderly to live an independent life while reducing the long-term care costs. In this study we aimed to investigate the accuracy of Timed Up and Go test, for fallers' identification, using fusion of features extracted from accelerometer data. Single and dual tasks TUG (manual and cognitive) were performed by a final sample (94% power) of 36 community dwelling healthy older persons (18 fallers paired with 18 non-fallers) while they wear a single triaxial accelerometer at waist with sampling rate of 200Hz. The segmentation of the TUG different trials and its comparative analysis allows to better discriminate fallers from non-fallers, while conventional functional tests fail to do so. In addition, we show that the fusion of features improve the discrimination power, achieving AUC of 0.84 (Sensitivity=Specificity=0.83, 95% CI 0.62-0.91), and demonstrating the clinical relevance of the study. We concluded that features extracted from segmented TUG trials acquired with dual tasks has potential to improve performance when identifying fallers via accelerometer sensors, which can improve TUG accuracy for clinical and epidemiological applications.|http://arxiv.org/abs/1609.05339v4|Moacir Ponti,Patricia Bet,Caroline Oliveira,Paula C. Castro
1081|Optimal exact tests for multiple binary endpoints|In confirmatory clinical trials with small sample sizes, hypothesis tests based on asymptotic distributions are often not valid and exact non-parametric procedures are applied instead. However, the latter are based on discrete test statistics and can become very conservative, even more so, if adjustments for multiple testing as the Bonferroni correction are applied. We propose improved exact multiple testing procedures for the setting where two parallel groups are compared in multiple binary endpoints. Based on the joint conditional distribution of test statistics of Fisher's exact tests, optimal rejection regions for intersection hypotheses tests are constructed. To efficiently search the large space of possible rejection regions, we propose an optimization algorithm based on constrained optimization and integer linear programming. Depending on the optimization objective, the optimal test yields maximal power under a specific alternative, maximal exhaustion of the nominal type I error rate, or the largest possible rejection region controlling the type I error rate. Applying the closed testing principle, we construct optimized multiple testing procedures with strong familywise error rate control. Furthermore, we propose a greedy algorithm for nearly optimal tests, which is computationally more efficient. We numerically compare the unconditional power of the optimized procedure with alternative approaches and illustrate the optimal tests with a clinical trial example in a rare disease.|http://arxiv.org/abs/1612.07561v1|Robin Ristl,Dong Xi,Ekkehard Glimm,Martin Posch
1082|Controlling IL-7 injections in HIV-infected patients|Immune interventions consisting in repeated injection are broadly used as they are thought to improve the quantity and the quality of the immune response. However, they also raised several questions that remains unanswered, in particular the number of injections to make or the delay to respect between different injections to acheive this goal. Practical and financial considerations add constraints to these questions, especially in the framework of human studies. We specifically focus here on the use of interleukine-7 (IL-7) injections in HIV-infected patients under antiretroviral treatment, but still unable to restore normal levels of CD4+ T lymphocytes. Clinical trials have already shown that repeated cycles of injections of IL-7 could help maintaining CD4+ T lymphocytes levels over the limit of 500 cells per microL, by affecting proliferation and survival of CD4+ T cells. We then aim at answering the question : how to maintain a patient's level of CD4+ T lymphocytes by using a minimum number of injections (ie optimizing the strategy of injections) ? Based on mechanistic models that were previously developed for the dynamics of CD4+ T lymphocytes in this context, we model the process by a piecewise deterministic Markov model. We then address the question by using some recently established theory on impulse control problem in order to develop a numerical tool determining the optimal strategy. Results are obtained on a reduced model, as a proof of concept : the method allows to defined an optimal strategy for a given patient. This method could applied to optimize injections schedules in clinical trials.|http://arxiv.org/abs/1801.06227v1|Chlo Pasin,Franois Dufour,Laura Villain,Huilong Zhang,Rodolphe Thibaut
1083|Dynamically borrowing strength from another study through shrinkage estimation|Meta-analytic methods may be used to combine evidence from different sources of information. Quite commonly, the normal-normal hierarchical model (NNHM) including a random-effect to account for between-study heterogeneity is utilized for such analyses. The same modeling framework may also be used to not only derive a combined estimate, but also to borrow strength for a particular study from another by deriving a shrinkage estimate. For instance, a small-scale randomized controlled trial could be supported by a non-randomized study, e.g. a clinical registry. This would be particularly attractive in the context of rare diseases. We demonstrate that a meta-analysis still makes sense in this extreme two-study setup, as illustrated using a recent trial and a clinical registry in Creutzfeld-Jakob disease. Derivation of a shrinkage estimate within a Bayesian random-effects meta-analysis may substantially improve a given estimate even based on only a single additional estimate while accounting for potential effect heterogeneity between the studies. Alternatively, inference may equivalently be motivated via a model specification that does not require a common overall mean parameter but considers the treatment effect in one study, and the difference in effects between the studies. The proposed approach is quite generally applicable to combine different types of evidence originating e.g. from meta-analyses or individual studies. An application of this more general setup is provided in immunosuppression following liver transplantation in children.|http://arxiv.org/abs/1806.01015v2|Christian Rver,Tim Friede
1084|Optimizing adaptive cancer therapy: dynamic programming and evolutionary game theory|Recent clinical trials have shown that the adaptive drug therapy can be more efficient than a standard MTD-based policy in treatment of cancer patients. The adaptive therapy paradigm is not based on a preset schedule; instead, the doses are administered based on the current state of tumor. But the adaptive treatment policies examined so far have been largely ad hoc. In this paper we propose a method for systematically optimizing the rules of adaptive policies based on an Evolutionary Game Theory model of cancer dynamics. Given a set of treatment objectives, we use the framework of dynamic programming to find the optimal treatment strategies. In particular, we optimize the total drug usage and time to recovery by solving a Hamilton-Jacobi-Bellman equation based on a mathematical model of tumor evolution. We compare adaptive/optimal treatment strategy with MTD-based treatment policy. We show that optimal treatment strategies can dramatically decrease the total amount of drugs prescribed as well as increase the fraction of initial tumour states from which the recovery is possible. We also examine the optimization trade-offs between the total administered drugs and recovery time. The adaptive therapy combined with optimal control theory is a promising concept in the cancer treatment and should be integrated into clinical trial design.|http://arxiv.org/abs/1812.01805v2|Mark Gluzman,Jacob G. Scott,Alexander Vladimirsky
1085|Estimation of group means in generalized linear mixed models|In this manuscript, we investigate the concept of the mean response for a treatment group mean as well as its estimation and prediction for generalized linear models with a subject-wise random effect. Generalized linear models are commonly used to analyze categorical data. The model-based mean for a treatment group usually estimates the response at the mean covariate. However, the mean response for the treatment group for studied population is at least equally important in the context of clinical trials. New methods were proposed to estimate such a mean response in generalized linear models; however, this has only been done when there are no random effects in the model. We suggest that, in a generalized linear mixed model (GLMM), there are at least two possible definitions of a treatment group mean response that can serve as estimation/prediction targets. The estimation of these treatment group means is important for healthcare professionals to be able to understand the absolute benefit versus risk. For both of these treatment group means, we propose a new set of methods that suggests how to estimate/predict both of them in a GLMM models with a univariate subject-wise random effect. Our methods also suggest an easy way of constructing corresponding confidence and prediction intervals for both possible treatment group means. Simulations show that proposed confidence and prediction intervals provide correct empirical coverage probability under most circumstances. Proposed methods have also been applied to analyze hypoglycemia data from diabetes clinical trials.|http://arxiv.org/abs/1904.06384v2|Jiexin Duan,Michael Levine,Junxiang Luo,Yongming Qu
1086|Optimizing Graphical Procedures for Multiplicity Control in a Confirmatory Clinical Trial via Deep Learning|In confirmatory clinical trials, it has been proposed to use a simple iterative graphical approach to construct and perform intersection hypotheses tests with a weighted Bonferroni-type procedure to control type I errors in the strong sense. Given Phase II study results or other prior knowledge, it is usually of main interest to find the optimal graph that maximizes a certain objective function in a future Phase III study. In this article, we evaluate the performance of two existing derivative-free constrained methods, and further propose a deep learning enhanced optimization framework. Our method numerically approximates the objective function via feedforward neural networks (FNNs) and then performs optimization with available gradient information. It can be constrained so that some features of the testing procedure are held fixed while optimizing over other features. Simulation studies show that our FNN-based approach has a better balance between robustness and time efficiency than some existing derivative-free constrained optimization algorithms. Compared to the traditional stochastic search method, our optimizer has moderate multiplicity adjusted power gain when the number of hypotheses is relatively large. We further apply it to a case study to illustrate how to optimize a multiple testing procedure with respect to a specific study objective.|http://arxiv.org/abs/1908.10262v2|Tianyu Zhan,Alan H Hartford,Jian Kang,Walter W Offen
1087|Theory on Covariate-Adaptive Randomized Clinical Trials: Efficiency, Selection bias and Randomization Methods|The theocratical properties of the power of the conventional testing hypotheses and the selection bias are usually unknown under covariate-adaptive randomized clinical trials. In the literature, most studies are based on simulations. In this article, we provide theoretical foundation of the power of the hypothesis testing and the selection bias under covariate-adaptive randomization based on linear models. We study the asymptotic relative loss of power of hypothesis testing to compare the treatment effects and the asymptotic selection bias. Under the covariate-adaptive randomization, (i) the hypothesis testing usually losses power, the more covariates in testing model are not incorporated in the randomization procedure, the more the power is lost; (ii) the hypothesis testing is usually more powerful than the one under complete randomization; and (iii) comparing to complete randomization, most of the popular covariate-adaptive randomization procedures in the literature, for example, Pocock and Simon's marginal procedure, stratified permuted block design, etc, produce nontrivial selection bias. A new family of covariate-adaptive randomization procedures are proposed for considering the power and selection bias simultaneously, under which, the covariate imbalances are small enough so that the power of testing the treatment effects would be asymptotically the largest and at the same time, the selection bias is asymptotically the optimal. The theocratical properties give a full picture how the power of the hypothesis testing, the selection bias of the randomization procedure, and the randomization method affect each other.|http://arxiv.org/abs/1912.03636v2|Li-Xin Zhang
1088|Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks|This research on data extraction methods applies recent advances in natural language processing to evidence synthesis based on medical texts. Texts of interest include abstracts of clinical trials in English and in multilingual contexts. The main focus is on information characterized via the Population, Intervention, Comparator, and Outcome (PICO) framework, but data extraction is not limited to these fields. Recent neural network architectures based on transformers show capacities for transfer learning and increased performance on downstream natural language processing tasks such as universal reading comprehension, brought forward by this architecture's use of contextualized word embeddings and self-attention mechanisms. This paper contributes to solving problems related to ambiguity in PICO sentence prediction tasks, as well as highlighting how annotations for training named entity recognition systems are used to train a high-performing, but nevertheless flexible architecture for question answering in systematic review automation. Additionally, it demonstrates how the problem of insufficient amounts of training annotations for PICO entity extraction is tackled by augmentation. All models in this paper were created with the aim to support systematic review (semi)automation. They achieve high F1 scores, and demonstrate the feasibility of applying transformer-based classification methods to support data mining in the biomedical literature.|http://arxiv.org/abs/2001.11268v1|Lena Schmidt,Julie Weeds,Julian P. T. Higgins
1089|Assessing causal effects in the presence of treatment switching through principal stratification|Clinical trials often allow patients in the control arm to switch to the treatment arm if their physical conditions are worse than certain tolerance levels. For instance, treatment switching arises in the Concorde clinical trial, which aims to assess causal effects on the time-to-disease progression or death of immediate versus deferred treatment with zidovudine among patients with asymptomatic HIV infection. The Intention-To-Treat analysis does not measure the effect of the actual receipt of the treatment and ignores the information on treatment switching. Other existing methods reconstruct the outcome a patient would have had if they had not switched under strong assumptions. Departing from the literature, we re-define the problem of treatment switching using principal stratification and focus on causal effects for patients belonging to subpopulations defined by the switching behavior under control. We use a Bayesian approach to inference, taking into account that (i) switching happens in continuous time; (ii) switching time is not defined for patients who never switch in a particular experiment; and (iii) survival time and switching time are subject to censoring. We apply this framework to analyze synthetic data based on the Concorde study. Our data analysis reveals that immediate treatment with zidovudine increases survival time for never switcher and that treatment effects are highly heterogeneous across different types of patients defined by the switching behavior.|http://arxiv.org/abs/2002.11989v2|Alessandra Mattei,Peng Ding,Veronica Ballerini,Fabrizia Mealli
1090|Testing for Treatment Effect in Covariate-Adaptive Randomized Clinical Trials with Generalized Linear Models and Omitted Covariates|Concerns have been expressed over the validity of statistical inference under covariate-adaptive randomization despite the extensive use in clinical trials. In the literature, the inferential properties under covariate-adaptive randomization have been mainly studied for continuous responses; in particular, it is well known that the usual two sample t-test for treatment effect is typically conservative, in the sense that the actual test size is smaller than the nominal level. This phenomenon of invalid tests has also been found for generalized linear models without adjusting for the covariates and are sometimes more worrisome due to inflated Type I error. The purpose of this study is to examine the unadjusted test for treatment effect under generalized linear models and covariate-adaptive randomization. For a large class of covariate-adaptive randomization methods, we obtain the asymptotic distribution of the test statistic under the null hypothesis and derive the conditions under which the test is conservative, valid, or anti-conservative. Several commonly used generalized linear models, such as logistic regression and Poisson regression, are discussed in detail. An adjustment method is also proposed to achieve a valid size based on the asymptotic results. Numerical studies confirm the theoretical findings and demonstrate the effectiveness of the proposed adjustment method.|http://arxiv.org/abs/2009.04136v2|Li Yang,Wei Ma,Yichen Qin,Feifang Hu
1091|Automated data extraction of bar chart raster images|Objective: To develop software utilizing optical character recognition toward the automatic extraction of data from bar charts for meta-analysis. Methods: We utilized a multistep data extraction approach that included figure extraction, text detection, and image disassembly. PubMed Central papers that were processed in this manner included clinical trials regarding macular degeneration, a disease causing blindness with a heavy disease burden and many clinical trials. Bar chart characteristics were extracted in both an automated and manual fashion. These two approaches were then compared for accuracy. These characteristics were then compared using a Bland-Altman analysis. Results: Based on Bland-Altman analysis, 91.8% of data points were within the limits of agreement. By comparing our automated data extraction with manual data extraction, automated data extraction yielded the following accuracies: X-axis labels 79.5%, Y-tick values 88.6%, Y-axis label 88.6%, Bar value <5% error 88.0%. Discussion: Based on our analysis, we achieved an agreement between automated data extraction and manual data extraction. A major source of error was the incorrect delineation of 7s as 2s by optical character recognition library. We also would benefit from adding redundancy checks in the form of a deep neural network to boost our bar detection accuracy. Further refinements to this method are justified to extract tabulated and line graph data to facilitate automated data gathering for meta-analysis.|http://arxiv.org/abs/2011.04137v1|Alex Carderas,Ye Yuan,Itamar Livnat,Ryan Yanagihara,Rosita Saul,Gabrielle Montes De Oca,Kai Zheng,Andrew W. Browne
1092|Mathematical artificial intelligence design of mutation-proof COVID-19 monoclonal antibodies|Emerging severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) variants have compromised existing vaccines and posed a grand challenge to coronavirus disease 2019 (COVID-19) prevention, control, and global economic recovery. For COVID-19 patients, one of the most effective COVID-19 medications is monoclonal antibody (mAb) therapies. The United States Food and Drug Administration (U.S. FDA) has given the emergency use authorization (EUA) to a few mAbs, including those from Regeneron, Eli Elly, etc. However, they are also undermined by SARS-CoV-2 mutations. It is imperative to develop effective mutation-proof mAbs for treating COVID-19 patients infected by all emerging variants and/or the original SARS-CoV-2. We carry out a deep mutational scanning to present the blueprint of such mAbs using algebraic topology and artificial intelligence (AI). To reduce the risk of clinical trial-related failure, we select five mAbs either with FDA EUA or in clinical trials as our starting point. We demonstrate that topological AI-designed mAbs are effective to variants of concerns and variants of interest designated by the World Health Organization (WHO), as well as the original SARS-CoV-2. Our topological AI methodologies have been validated by tens of thousands of deep mutational data and their predictions have been confirmed by results from tens of experimental laboratories and population-level statistics of genome isolates from hundreds of thousands of patients.|http://arxiv.org/abs/2204.09471v1|Jiahui Chen,Guo-Wei Wei
1093|Improving Safety of the Continual Reassessment Method via a Modified Allocation Rule|This paper proposes a novel criterion for the allocation of patients in Phase~I dose-escalation clinical trials aiming to find the maximum tolerated dose (MTD). Conventionally, using a model-based approach the next patient is allocated to the dose with the toxicity estimate closest (in terms of the absolute or squared distance) to the maximum acceptable toxicity. This approach, however, ignores the uncertainty in point estimates and ethical concerns of assigning a lot of patients to overly toxic doses. Motivated by recent discussions in the theory of estimation in restricted parameter spaces, we propose a criterion which accounts for both of these issues. The criterion requires a specification of one additional parameter only which has a simple and intuitive interpretation. We incorporate the proposed criterion into the one-parameter Bayesian continual reassessment method (CRM) and show, using simulations, that it results in the same proportion of correct selections on average as the original design, but in fewer mean number of toxic responses. A comparison to other model-based dose-escalation designs demonstrates that the proposed design can result in either the same mean accuracy as alternatives but fewer number of toxic responses, or in a higher mean accuracy but the same number of toxic responses. We conclude that the new criterion makes the existing model-based designs more ethical without losing efficiency in the context of Phase I clinical trials.|http://arxiv.org/abs/1807.05781v1|Pavel Mozgunov,Thomas Jaki
1094|Bivariate network meta-analysis for surrogate endpoint evaluation|Surrogate endpoints are very important in regulatory decision-making in healthcare, in particular if they can be measured early compared to the long-term final clinical outcome and act as good predictors of clinical benefit. Bivariate meta-analysis methods can be used to evaluate surrogate endpoints and to predict the treatment effect on the final outcome from the treatment effect measured on a surrogate endpoint. However, candidate surrogate endpoints are often imperfect, and the level of association between the treatment effects on the surrogate and final outcomes may vary between treatments. This imposes a limitation on the pairwise methods which do not differentiate between the treatments. We develop bivariate network meta-analysis (bvNMA) methods which combine data on treatment effects on the surrogate and final outcomes, from trials investigating heterogeneous treatment contrasts. The bvNMA methods estimate the effects on both outcomes for all treatment contrasts individually in a single analysis. At the same time, they allow us to model the surrogacy patterns across multiple trials (different populations) within a treatment contrast and across treatment contrasts, thus enabling predictions of the treatment effect on the final outcome for a new study in a new population or investigating a new treatment. Modelling assumptions about the between-studies heterogeneity and the network consistency, and their impact on predictions, are investigated using simulated data and an illustrative example in advanced colorectal cancer. When the strength of the surrogate relationships varies across treatment contrasts, bvNMA has the advantage of identifying treatments for which surrogacy holds, thus leading to better predictions.|http://arxiv.org/abs/1807.08928v1|Sylwia Bujkiewicz,Dan Jackson,John R Thompson,Rebecca Turner,Keith R Abrams,Ian R White
1095|Empirical Likelihood Weighted Estimation of Average Treatment Effects|There has been growing attention on how to effectively and objectively use covariate information when the primary goal is to estimate the average treatment effect (ATE) in randomized clinical trials (RCTs). In this paper, we propose an effective weighting approach to extract covariate information based on the empirical likelihood (EL) method. The resulting two-sample empirical likelihood weighted (ELW) estimator includes two classes of weights, which are obtained from a constrained empirical likelihood estimation procedure, where the covariate information is effectively incorporated into the form of general estimating equations. Furthermore, this ELW approach separates the estimation of ATE from the analysis of the covariate-outcome relationship, which implies that our approach maintains objectivity. In theory, we show that the proposed ELW estimator is semiparametric efficient. We extend our estimator to tackle the scenarios where the outcomes are missing at random (MAR), and prove the double robustness and multiple robustness properties of our estimator. Furthermore, we derive the semiparametric efficiency bound of all regular and asymptotically linear semiparametric ATE estimators under MAR mechanism and prove that our proposed estimator attains this bound. We conduct simulations to make comparisons with other existing estimators, which confirm the efficiency and multiple robustness property of our proposed ELW estimator. An application to the AIDS Clinical Trials Group Protocol 175 (ACTG 175) data is conducted.|http://arxiv.org/abs/2008.12989v1|Yuanyao Tan,Xialing Wen,Wei Liang,Ying Yan
1096|The scale transformed power prior for use with historical data from a different outcome model|We develop the scale transformed power prior for settings where historical and current data involve different data types, such as binary and continuous data, respectively. This situation arises often in clinical trials, for example, when historical data involve binary responses and the current data involve time-to-event or some other type of continuous or discrete outcome. The power prior proposed by Ibrahim and Chen (2000) does not address the issue of different data types. Herein, we develop a new type of power prior, which we call the scale transformed power prior (straPP). The straPP is constructed by transforming the power prior for the historical data by rescaling the parameter using a function of the Fisher information matrices for the historical and current data models, thereby shifting the scale of the parameter vector from that of the historical to that of the current data. Examples are presented to motivate the need for a scale transformation and simulation studies are presented to illustrate the performance advantages of the straPP over the power prior and other informative and non-informative priors. A real dataset from a clinical trial undertaken to study a novel transitional care model for stroke survivors is used to illustrate the methodology.|http://arxiv.org/abs/2105.05157v1|Brady Nifong,Matthew A. Psioda,Joseph G. Ibrahim
1097|Estimation of the odds ratio in a proportional odds model with censored time-lagged outcome in a randomized clinical trial|In many randomized clinical trials of therapeutics for COVID-19, the primary outcome is an ordinal categorical variable, and interest focuses on the odds ratio (active agent vs. control) under the assumption of a proportional odds model. Although at the final analysis the outcome will be determined for all subjects, at an interim analysis, the status of some participants may not yet be determined, e.g., because ascertainment of the outcome may not be possible until some pre-specified follow-up time. Accordingly, the outcome from these subjects can be viewed as censored. A valid interim analysis can be based on data only from those subjects with full follow up; however, this approach is inefficient, as it does not exploit additional information that may be available on those for whom the outcome is not yet available at the time of the interim analysis. Appealing to the theory of semiparametrics, we propose an estimator for the odds ratio in a proportional odds model with censored, time-lagged categorical outcome that incorporates additional baseline and time-dependent covariate information and demonstrate that it can result in considerable gains in efficiency relative to simpler approaches. A byproduct of the approach is a covariate-adjusted estimator for the odds ratio based on the full data that would be available at a final analysis.|http://arxiv.org/abs/2106.15559v3|Anastasios A. Tsiatis,Marie Davidian,Shannon T. Holloway
1098|A Comparison of Various Aggregation Functions in Multi-Criteria Decision Analysis for Drug Benefit-Risk Assessment|Multi-criteria decision analysis (MCDA) is a quantitative approach to the drug benefit-risk assessment (BRA) which allows for consistent comparisons by summarising all benefits and risks in a single score. The MCDA consists of several components, one of which is the utility (or loss) score function that defines how benefits and risks are aggregated into a single quantity. While a linear utility score is one of the most widely used approach in BRA, it is recognised that it can result in counter-intuitive decisions, for example, recommending a treatment with extremely low benefits or high risks. To overcome this problem, alternative approaches to the scores construction, namely, product, multi-linear and Scale Loss Score models, were suggested. However, to date, the majority of arguments concerning the differences implied by these models are heuristic. In this work, we consider four models to calculate the aggregated utility/loss scores and compared their performance in an extensive simulation study over many different scenarios, and in a case study. It is found that the product and Scale Loss Score models provide more intuitive treatment recommendation decisions in the majority of scenarios compared to the linear and multi-linear models, and are more robust to the correlation in the criteria.|http://arxiv.org/abs/2107.12298v1|Tom Menzies,Gaelle Saint-Hilary,Pavel Mozgunov
1099|Combining cytotoxic agents with continuous dose levels in seamless phase I-II clinical trials|Phase I-II cancer clinical trial designs are intended to accelerate drug development. In cases where efficacy cannot be ascertained in a short period of time, it is common to divide the study in two stages: i) a first stage in which dose is escalated based only on toxicity data and we look for the maximum tolerated dose (MTD) set and ii) a second stage in which we search for the most efficacious dose within the MTD set. Current available approaches in the area of continuous dose levels involve fixing the MTD after stage I and discarding all collected stage I efficacy data. However, this methodology is clearly inefficient when there is a unique patient population present across stages. In this article, we propose a two-stage design for the combination of two cytotoxic agents assuming a single patient population across the entire study. In stage I, conditional escalation with overdose control (EWOC) is used to allocate successive cohorts of patients. In stage II, we employ an adaptive randomization approach to allocate patients to drug combinations along the estimated MTD curve, which is constantly updated. The proposed methodology is assessed with extensive simulations in the context of a real case study.|http://arxiv.org/abs/2109.14231v4|Jos L. Jimnez,Mourad Tighiouart
1100|Combination Chemotherapy Optimization with Discrete Dosing|Chemotherapy is one of the primary modalities of cancer treatment. Chemotherapy drug administration is a complex problem that often requires expensive clinical trials to evaluate potential regimens. One way to alleviate this burden and better inform future trials is to build reliable models for drug administration. Previous chemotherapy optimization models have mainly relied on optimal control, which does not lend itself to capturing complex and vital operational constraints in chemotherapy planning involving discrete decisions, such as doses via pills and rest periods. In addition, most of the existing models for chemotherapy optimization lack an explicit toxicity measure and impose toxicity constraints primarily through (fixed) limits on drug concentration. The existing stochastic optimization models also focus on maximizing the probability of cure when tumor heterogeneity is uncertain. In this paper, we develop a mixed-integer program for combination chemotherapy (utilization of multiple drugs) optimization that incorporates various important operational constraints and, besides dose and concentration limits, controls treatment toxicity based on its effect on the count of white blood cells. To address the uncertainty of tumor heterogeneity, we propose chance constraints that guarantee reaching an operable tumor size with a high probability in a neoadjuvant setting. We present analytical results pertinent to the accuracy of the model in representing biological processes of chemotherapy and establish its merit for clinical applications through a numerical study of breast cancer.|http://arxiv.org/abs/2111.02000v1|Temitayo Ajayi,Seyedmohammadhossein Hosseinian,Andrew J. Schaefer,Clifton D. Fuller
1101|Evaluation of Interpretability for Deep Learning algorithms in EEG Emotion Recognition: A case study in Autism|Current models on Explainable Artificial Intelligence (XAI) have shown an evident and quantified lack of reliability for measuring feature-relevance when statistically entangled features are proposed for training deep classifiers. There has been an increase in the application of Deep Learning in clinical trials to predict early diagnosis of neuro-developmental disorders, such as Autism Spectrum Disorder (ASD). However, the inclusion of more reliable saliency-maps to obtain more trustworthy and interpretable metrics using neural activity features is still insufficiently mature for practical applications in diagnostics or clinical trials. Moreover, in ASD research the inclusion of deep classifiers that use neural measures to predict viewed facial emotions is relatively unexplored. Therefore, in this study we propose the evaluation of a Convolutional Neural Network (CNN) for electroencephalography (EEG)-based facial emotion recognition decoding complemented with a novel RemOve-And-Retrain (ROAR) methodology to recover highly relevant features used in the classifier. Specifically, we compare well-known relevance maps such as Layer-Wise Relevance Propagation (LRP), PatternNet, Pattern-Attribution, and Smooth-Grad Squared. This study is the first to consolidate a more transparent feature-relevance calculation for a successful EEG-based facial emotion recognition using a within-subject-trained CNN in typically-developed and ASD individuals.|http://arxiv.org/abs/2111.13208v6|Juan Manuel Mayor-Torres,Sara Medina-DeVilliers,Tessa Clarkson,Matthew D. Lerner,Giuseppe Riccardi
1102|Bayesian semi-parametric inference for clustered recurrent events with zero-inflation and a terminal event/4163305|Recurrent event data are common in clinical studies when participants are followed longitudinally, and are often subject to a terminal event. With the increasing popularity of large pragmatic trials with a heterogeneous source population, participants are often nested in clinics and can be either susceptible or structurally unsusceptible to the recurrent process. These complications require new modeling strategies to accommodate potential zero-event inflation as well as hierarchical data structures in both the terminal and non-terminal event processes. In this paper, we develop a Bayesian semi-parametric model to jointly characterize the zero-inflated recurrent event process and the terminal event process. We use a point mass mixture of non-homogeneous Poisson processes to describe the recurrent intensity and introduce shared random effects from different sources to bridge the non-terminal and terminal event processes. To achieve robustness, we consider nonparametric Dirichlet processes to model the residual of the accelerated failure time model for the survival process as well as the cluster-specific frailty distribution, and develop a Markov Chain Monte Carlo algorithm for posterior inference. We demonstrate the superiority of our proposed model compared with competing models via simulations and apply our method to a pragmatic cluster randomized trial for fall injury prevention among the elderly.|http://arxiv.org/abs/2202.06636v2|Xinyuan Tian,Maria Ciarleglio,Jiachen Cai,Erich Greene,Denise Esserman,Fan Li,Yize Zhao
1103|Information criteria for detecting change-points in the Cox proportional hazards model|The Cox proportional hazards model, commonly used in clinical trials, assumes proportional hazards. However, it does not hold when, for example, there is a delayed onset of the treatment effect. In such a situation, an acute change in the hazard ratio function is expected to exist. This paper considers the Cox model with change-points and derives AIC-type information criteria for detecting those change-points. The change-point model does not allow for conventional statistical asymptotics due to its irregularity, thus a formal AIC that penalizes twice the number of parameters would not be analytically derived, and using it would clearly give overfitting analysis results. Therefore, we will construct specific asymptotics using the partial likelihood estimation method in the Cox model with change-points. Based on the original derivation method for AIC, we propose information criteria that are mathematically guaranteed. If the partial likelihood is used in the estimation, information criteria with penalties much larger than twice the number of parameters could be obtained in an explicit form. Numerical experiments confirm that the proposed criterion is clearly superior in terms of the original purpose of AIC, which is to provide an estimate that is close to the true structure. We also apply the proposed criterion to actual clinical trial data to indicate that it will easily lead to different results from the formal AIC.|http://arxiv.org/abs/2203.15973v2|Ryoto Ozaki,Yoshiyuki Ninomiya
1104|Hypothesis tests for multiple responses regression: effect of probiotics on addiction and binge eating disorder|Clinical trials are common in medical research where multiple non-Gaussian responses and time-dependent observations are frequent. The analysis of data from these studies requires statistical modeling techniques that take these characteristics into account. We propose a general strategy based on the Wald statistics to perform hypothesis tests like ANOVAs, MANOVAs and multiple comparison tests on regression and dispersion parameters of multivariate covariance generalized linear models (McGLMs). McGLMs provide a general statistical modeling framework for normal and non-normal multivariate data analysis along with a wide range of correlation structures. We design different simulation scenarios to verify the properties of the proposed tests. The results are promising showing that the proposed tests present the levels of confidence close to the specified one for all simulation study scenarios. Complementary to the proposal, we developed implementations in the R language to carry out the tests presented, the codes are available in the supplementary material. The proposal is motivated by the analysis of a clinical trial that aims to evaluate the effect of the use of probiotics in the control of addiction and binge eating disorder in patients undergoing bariatric surgery. The subjects were separated into two groups (placebo and treatment) and evaluated at three different times. The results indicate that addiction and binge eating disorder reduce over time, but there is no difference between groups at each time point.|http://arxiv.org/abs/2208.00027v1|Lineu Alberto Cavazani de Freitas,Ligia de Oliveira Carlos,Antnio Carlos Ligocki Campos,Wagner Hugo Bonat
1105|Identification and estimation of treatment effects on long-term outcomes in clinical trials with external observational data|In biomedical studies, estimating drug effects on chronic diseases requires a long follow-up period, which is difficult to meet in randomized clinical trials (RCTs). The use of a short-term surrogate to replace the long-term outcome for assessing the drug effect relies on stringent assumptions that empirical studies often fail to satisfy. Motivated by a kidney disease study, we investigate the drug effects on long-term outcomes by combining an RCT without observation of long-term outcome and an observational study in which the long-term outcome is observed but unmeasured confounding may exist. Under a mean exchangeability assumption weaker than the previous literature, we identify the average treatment effects in the RCT and derive the associated efficient influence function and semiparametric efficiency bound. Furthermore, we propose a locally efficient doubly robust estimator and an inverse probability weighted (IPW) estimator. The former attains the semiparametric efficiency bound if all the working models are correctly specified. The latter has a simpler form and requires much fewer model specifications. The IPW estimator using estimated propensity scores is more efficient than that using true propensity scores and achieves the semiparametric efficient bound in the case of discrete covariates and surrogates with finite support. Both estimators are shown to be consistent and asymptotically normally distributed. Extensive simulations are conducted to evaluate the finite-sample performance of the proposed estimators. We apply the proposed methods to estimate the efficacy of oral hydroxychloroquine on renal failure in a real-world data analysis.|http://arxiv.org/abs/2208.10163v2|Wenjie Hu,Xiaohua Zhou,Peng Wu
1106|Extrapolation before imputation reduces bias when imputing censored covariates|Modeling symptom progression to identify informative subjects for a new Huntington's disease clinical trial is problematic since time to diagnosis, a key covariate, can be heavily censored. Imputation is an appealing strategy where censored covariates are replaced with their conditional means, but existing methods saw over 200% bias under heavy censoring. Calculating these conditional means well requires estimating and then integrating over the survival function of the censored covariate from the censored value to infinity. To estimate the survival function flexibly, existing methods use the semiparametric Cox model with Breslow's estimator, leaving the integrand for the conditional means (the estimated survival function) undefined beyond the observed data. The integral is then estimated up to the largest observed covariate value, and this approximation can cut off the tail of the survival function and lead to severe bias, particularly under heavy censoring. We propose a hybrid approach that splices together the semiparametric survival estimator with a parametric extension, making it possible to approximate the integral up to infinity. In simulation studies, our proposed approach of extrapolation then imputation substantially reduces the bias seen with existing imputation methods, even when the parametric extension was misspecified. We further demonstrate how imputing with corrected conditional means helps to prioritize patients for future clinical trials.|http://arxiv.org/abs/2209.04716v5|Sarah C. Lotspeich,Tanya P. Garcia
1107|On sample size determination for restricted mean survival time-based tests in randomized clinical trials|Restricted mean survival time (RMST) is gaining attention as a measure to quantify the treatment effect on survival outcomes in randomized clinical trials. Several methods to determine sample size based on the RMST-based tests have been proposed. However, to the best of our knowledge, there is no discussion about the power and sample size regarding the augmented version of RMST-based tests, which utilize baseline covariates for a gain in estimation efficiency and in power for testing the no treatment effect. The conventional event-driven study design based on the log-rank test allows us to calculate the power for a given hazard ratio without specifying the survival functions. In contrast, the existing sample size determination methods for the RMST-based tests relies on the adequacy of the assumptions of the entire survival curves of two groups. Furthermore, to handle the augmented test, the correlation between the baseline covariates and the martingale residuals must be handled. To address these issues, we propose an approximated sample size formula for the augmented version of the RMST-based test, which does not require specifying the entire survival curve in the treatment group, and also a sample size recalculation approach to update the correlations between the baseline covariates and the martingale residuals with the blinded data. The proposed procedure will enable the studies to have the target power for a given RMST difference even when correct survival functions cannot be specified at the design stage.|http://arxiv.org/abs/2212.08259v1|Satoshi Hattori,Hajime Uno
1108|Causal Inference under Data Restrictions|This dissertation focuses on modern causal inference under uncertainty and data restrictions, with applications to neoadjuvant clinical trials, distributed data networks, and robust individualized decision making.   In the first project, we propose a method under the principal stratification framework to identify and estimate the average treatment effects on a binary outcome, conditional on the counterfactual status of a post-treatment intermediate response. Under mild assumptions, the treatment effect of interest can be identified. We extend the approach to address censored outcome data. The proposed method is applied to a neoadjuvant clinical trial and its performance is evaluated via simulation studies.   In the second project, we propose a tree-based model averaging approach to improve the estimation accuracy of conditional average treatment effects at a target site by leveraging models derived from other potentially heterogeneous sites, without them sharing subject-level data. The performance of this approach is demonstrated by a study of the causal effects of oxygen therapy on hospital survival rates and backed up by comprehensive simulations.   In the third project, we propose a robust individualized decision learning framework with sensitive variables to improve the worst-case outcomes of individuals caused by sensitive variables that are unavailable at the time of decision. Unlike most existing work that uses mean-optimal objectives, we propose a robust learning framework by finding a newly defined quantile- or infimum-optimal decision rule. From a causal perspective, we also generalize the classic notion of (average) fairness to conditional fairness for individual subjects. The reliable performance of the proposed method is demonstrated through synthetic experiments and three real-data applications.|http://arxiv.org/abs/2301.08788v1|Xiaoqing Tan
1109|A maximum penalised likelihood approach for semiparametric accelerated failure time models with time-varying covariates and partly interval censoring|Accelerated failure time (AFT) models are frequently used to model survival data, providing a direct quantification of the relationship between event times and covariates. These models allow for the acceleration or deceleration of failure times through a multiplicative factor that accounts for the effect of covariates. While existing literature provides numerous methods for fitting AFT models with time-fixed covariates, adapting these approaches to scenarios involving both time-varying covariates and partly interval-censored data remains challenging. Motivated by a randomised clinical trial dataset on advanced melanoma patients, we propose a maximum penalised likelihood approach for fitting a semiparametric AFT model to survival data with partly interval-censored failure times. This method also accommodates both time-fixed and time-varying covariates. We utilise Gaussian basis functions to construct a smooth approximation of the non-parametric baseline hazard and fit the model using a constrained optimisation approach. The effectiveness of our method is demonstrated through extensive simulations. Finally, we illustrate the relevance of our approach by applying it to a dataset from a randomised clinical trial involving patients with advanced melanoma.|http://arxiv.org/abs/2403.12332v2|Aishwarya Bhaskaran,Ding Ma,Benoit Liquet,Angela Hong,Stephane Heritier,Serigne N Lo,Jun Ma
1110|Deciphering autism heterogeneity: a molecular stratification approach in four mouse models|Autism spectrum disorder (ASD) is a complex neurodevelopmental condition characterized by impairments in social interaction, communication, as well as restrained or stereotyped behaviors. The inherent heterogeneity within the autism spectrum poses challenges for developing effective pharmacological treatments targeting core features. Successful clinical trials require the identification of robust markers to enable patient stratification. In this study, we explored molecular markers within the oxytocin and immediate early gene families across five interconnected brain structures of the social circuit in four distinct ASD mouse models, each exhibiting unique behavioral features along the autism spectrum. While dysregulations in the oxytocin family were model-specific, immediate early genes displayed widespread alterations, reflecting global changes in social plasticity. Through integrative analysis, we identified Egr1, Foxp1, Homer1a, Oxt and Oxtr as five robust and discriminant molecular markers facilitating successful stratification of the four models. Importantly, our stratification demonstrated predictive values when challenged with a fifth mouse model or identifying subgroups of mice potentially responsive to oxytocin treatment. Beyond providing insights into oxytocin and immediate early gene mRNA dynamics, this proof-of-concept study represents a significant step toward potential stratification of individuals with ASD. The implications extend to enhancing the success of clinical trials and guiding personalized medicine for distinct subgroups of individuals with autism.|http://arxiv.org/abs/2403.18352v1|Caroline Gora,Ana Dudas,Ocane Vaugrente,Lucile Drobecq,Emmanuel Pecnard,Galle Lefort,Lucie P. Pellissier
1111|Correcting for confounding in longitudinal experiments: positioning non-linear mixed effects modeling as implementation of standardization using latent conditional exchangeability|Non-linear mixed effects modeling and simulation (NLME M&S) is evaluated to be used for standardization with longitudinal data in presence of confounders. Standardization is a well-known method in causal inference to correct for confounding by analyzing and combining results from subgroups of patients. We show that non-linear mixed effects modeling is a particular implementation of standardization that conditions on individual parameters described by the random effects of the mixed effects model. Our motivation is that in pharmacometrics NLME M&S is routinely used to analyze clinical trials and to predict and compare potential outcomes of the same patient population under different treatment regimens. Such a comparison is a causal question sometimes referred to as causal prediction. Nonetheless, NLME M&S is rarely positioned as a method for causal prediction.   As an example, a simulated clinical trial is used that assumes treatment confounder feedback in which early outcomes can cause deviations from the planned treatment schedule. Being interested in the outcome for the hypothetical situation that patients adhere to the planned treatment schedule, we put assumptions in a causal diagram. From the causal diagram, conditional independence assumptions are derived either using latent conditional exchangeability, conditioning on the individual parameters, or using sequential conditional exchangeability, conditioning on earlier outcomes. Both conditional independencies can be used to estimate the estimand of interest, e.g., with standardization, and they give unbiased estimates.|http://arxiv.org/abs/2404.19325v1|Christian Bartels,Martina Scauda,Neva Coello,Thomas Dumortier,Bjoern Bornkamp,Giusi Moffa
1112|A stochastic algorithm approach for the elephant random walk with applications|The randomized play-the-winner rule (RPW) is a response-adaptive design proposed by Wei and Durham (1978) for sequentially randomizing patients to treatments in a two-treatment clinical trial so that more patients are assigned to the better treatment as the clinical trial goes on. The elephant random walk (ERW) proposed by Schutz and Trimper (2004) is a non-Markovian discrete-time random walk on $\mathbb Z$ which has a link to a famous saying that elephants can always remember where they have been. The asymptotic behaviors of RPW rule and ERW have been studied in litterateurs independently, and their asymptotic behaviors are very similar. In this paper, we link RPW rule and ERW with the recursive stochastic algorithm. With the help of a recursive stochastic algorithm, we obtain the Gaussian approximation of the ERW and multi-dimensional varying-memory ERW with random step sizes. By the Gaussian approximation, the central limit theorem, precise law of the iterated logarithm, and almost sure central limit theorem are obtained for the multi-dimensional ERW, the multi-dimensional ERW with random step sizes, and their centers of mass for all the diffusive, critical, and superdiffusive regimes. Based on the Gaussian approximation and the small ball probabilities for a new kind of Gaussian process, the precise Chung type laws of the iterated logarithm of the multi-dimensional ERW with random step sizes and its mass of center are also obtained for both the diffusive regime and superdiffusive regime.|http://arxiv.org/abs/2405.12495v4|Li-Xin Zhang
1113|Bayesian generalized method of moments applied to pseudo-observations in survival analysis|Bayesian inference for survival regression modeling offers numerous advantages, especially for decision-making and external data borrowing, but demands the specification of the baseline hazard function, which may be a challenging task. We propose an alternative approach that does not need the specification of this function. Our approach combines pseudo-observations to convert censored data into longitudinal data with the Generalized Methods of Moments (GMM) to estimate the parameters of interest from the survival function directly. GMM may be viewed as an extension of the Generalized Estimating Equation (GEE) currently used for frequentist pseudo-observations analysis and can be extended to the Bayesian framework using a pseudo-likelihood function. We assessed the behavior of the frequentist and Bayesian GMM in the new context of analyzing pseudo-observations. We compared their performances to the Cox, GEE, and Bayesian piecewise exponential models through a simulation study of two-arm randomized clinical trials. Frequentist and Bayesian GMM gave valid inferences with similar performances compared to the three benchmark methods, except for small sample sizes and high censoring rates. For illustration, three post-hoc efficacy analyses were performed on randomized clinical trials involving patients with Ewing Sarcoma, producing results similar to those of the benchmark methods. Through a simple application of estimating hazard ratios, these findings confirm the effectiveness of this new Bayesian approach based on pseudo-observations and the generalized method of moments. This offers new insights on using pseudo-observations for Bayesian survival analysis.|http://arxiv.org/abs/2406.03821v1|La Orsini,Caroline Brard,Emmanuel Lesaffre,Guosheng Yin,David Dejardin,Gwnal Le Teuff
1114|Semiparametric Piecewise Accelerated Failure Time Model for the Analysis of Immune-Oncology Clinical Trials|Effectiveness of immune-oncology chemotherapies has been presented in recent clinical trials. The Kaplan-Meier estimates of the survival functions of the immune therapy and the control often suggested the presence of the lag-time until the immune therapy began to act. It implies the use of hazard ratio under the proportional hazards assumption would not be appealing, and many alternatives have been investigated such as the restricted mean survival time. In addition to such overall summary of the treatment contrast, the lag-time is also an important feature of the treatment effect. Identical survival functions up to the lag-time implies patients who are likely to die before the lag-time would not benefit the treatment and identifying such patients would be very important. We propose the semiparametric piecewise accelerated failure time model and its inference procedure based on the semiparametric maximum likelihood method. It provides not only an overall treatment summary, but also a framework to identify patients who have less benefit from the immune-therapy in a unified way. Numerical experiments confirm that each parameter can be estimated with minimal bias. Through a real data analysis, we illustrate the evaluation of the effect of immune-oncology therapy and the characterization of covariates in which patients are unlikely to receive the benefit of treatment.|http://arxiv.org/abs/2407.17658v1|Hisato Sunami,Satoshi Hattori
1115|Identifying treatment response subgroups in observational time-to-event data|Identifying patient subgroups with different treatment responses is an important task to inform medical recommendations, guidelines, and the design of future clinical trials. Existing approaches for subgroup analysis primarily rely on Randomised Controlled Trials (RCTs), in which treatment assignment is randomised. RCTs' patient cohorts are often constrained by cost, rendering them not representative of the heterogeneity of patients likely to receive treatment in real-world clinical practice. When applied to observational studies, subgroup analysis approaches suffer from significant statistical biases particularly because of the non-randomisation of treatment. Our work introduces a novel, outcome-guided method for identifying treatment response subgroups in observational studies. Our approach assigns each patient to a subgroup associated with two time-to-event distributions: one under treatment and one under control regime. It hence positions itself in between individualised and average treatment effect estimation. The assumptions of our model result in a simple correction of the statistical bias from treatment non-randomisation through inverse propensity weighting. In experiments, our approach significantly outperforms the current state-of-the-art method for outcome-guided subgroup analysis in both randomised and observational treatment regimes.|http://arxiv.org/abs/2408.03463v3|Vincent Jeanselme,Chang Ho Yoon,Fabian Falck,Brian Tom,Jessica Barrett
1116|Stratification in Randomised Clinical Trials and Analysis of Covariance: Some Simple Theory and Recommendations|A simple device for balancing for a continuous covariate in clinical trials is to stratify by whether the covariate is above or below some target value, typically the predicted median. This raises an issue as to which model should be used for modelling the effect of treatment on the outcome variable, $Y$. Should one fit, the stratum indicator, $S$, the continuous covariate, $X$, both or neither?   This question has been investigated in the literature using simulations targetting the overall effect on inferences about treatment . However, when a covariate is added to a model there are three consequences for inference: 1) The mean square error effect, 2) The variance inflation factor and 3) second order precision. We consider that it is valuable to consider these three factors separately even if, ultimately, it is their joint effect that matters.   We present some simple theory, concentrating in particular on the variance inflation factor, that may be used to guide trialists in their choice of model. We also consider the case where the precise form of the relationship between the outcome and the covariate is not known. We conclude by recommending that the continuous coovariate should always be in the model but that, depending on circumstances, there may be some justification in fitting the stratum indicator also.|http://arxiv.org/abs/2408.06760v1|Stephen Senn,Franz Knig,Martin Posch
1117|ARANet: Attention-based Residual Adversarial Network with Deep Supervision for Radiotherapy Dose Prediction of Cervical Cancer|Radiation therapy is the mainstay treatment for cervical cancer, and its ultimate goal is to ensure the planning target volume (PTV) reaches the prescribed dose while reducing dose deposition of organs-at-risk (OARs) as much as possible. To achieve these clinical requirements, the medical physicist needs to manually tweak the radiotherapy plan repeatedly in a trial-anderror manner until finding the optimal one in the clinic. However, such trial-and-error processes are quite time-consuming, and the quality of plans highly depends on the experience of the medical physicist. In this paper, we propose an end-to-end Attentionbased Residual Adversarial Network with deep supervision, namely ARANet, to automatically predict the 3D dose distribution of cervical cancer. Specifically, given the computer tomography (CT) images and their corresponding segmentation masks of PTV and OARs, ARANet employs a prediction network to generate the dose maps. We also utilize a multi-scale residual attention module and deep supervision mechanism to enforce the prediction network to extract more valuable dose features while suppressing irrelevant information. Our proposed method is validated on an in-house dataset including 54 cervical cancer patients, and experimental results have demonstrated its obvious superiority compared to other state-of-the-art methods.|http://arxiv.org/abs/2408.13981v1|Lu Wen,Wenxia Yin,Zhenghao Feng,Xi Wu,Deng Xiong,Yan Wang
1118|Estimand-based Inference in Presence of Long-Term Survivors|In this article, we develop nonparametric inference methods for comparing survival data across two samples, which are beneficial for clinical trials of novel cancer therapies where long-term survival is a critical outcome. These therapies, including immunotherapies or other advanced treatments, aim to establish durable effects. They often exhibit distinct survival patterns such as crossing or delayed separation and potentially leveling-off at the tails of survival curves, clearly violating the proportional hazards assumption and rendering the hazard ratio inappropriate for measuring treatment effects. The proposed methodology utilizes the mixture cure framework to separately analyze the cure rates of long-term survivors and the survival functions of susceptible individuals. We evaluate a nonparametric estimator for the susceptible survival function in the one-sample setting. Under sufficient follow-up, it is expressed as a location-scale-shift variant of the Kaplan-Meier (KM) estimator. It retains several desirable features of the KM estimator, including inverse-probability-censoring weighting, product-limit estimation, self-consistency, and nonparametric efficiency. In scenarios of insufficient follow-up, it can easily be adapted by incorporating a suitable cure rate estimator. In the two-sample setting, besides using the difference in cure rates to measure the long-term effect, we propose a graphical estimand to compare the relative treatment effects on susceptible subgroups. This process, inspired by Kendall's tau, compares the order of survival times among susceptible individuals. The proposed methods' large-sample properties are derived for further inference, and the finite-sample properties are examined through extensive simulation studies. The proposed methodology is applied to analyze the digitized data from the CheckMate 067 immunotherapy clinical trial.|http://arxiv.org/abs/2409.02209v1|Yi-Cheng Tai,Weijing Wang,Martin T. Wells
1119|Differentiable Biomechanics for Markerless Motion Capture in Upper Limb Stroke Rehabilitation: A Comparison with Optical Motion Capture|Marker-based Optical Motion Capture (OMC) paired with biomechanical modeling is currently considered the most precise and accurate method for measuring human movement kinematics. However, combining differentiable biomechanical modeling with Markerless Motion Capture (MMC) offers a promising approach to motion capture in clinical settings, requiring only minimal equipment, such as synchronized webcams, and minimal effort for data collection. This study compares key kinematic outcomes from biomechanically modeled MMC and OMC data in 15 stroke patients performing the drinking task, a functional task recommended for assessing upper limb movement quality. We observed a high level of agreement in kinematic trajectories between MMC and OMC, as indicated by high correlations (median r above 0.95 for the majority of kinematic trajectories) and median RMSE values ranging from 2-5 degrees for joint angles, 0.04 m/s for end-effector velocity, and 6 mm for trunk displacement. Trial-to-trial biases between OMC and MMC were consistent within participant sessions, with interquartile ranges of bias around 1-3 degrees for joint angles, 0.01 m/s in end-effector velocity, and approximately 3mm for trunk displacement. Our findings indicate that our MMC for arm tracking is approaching the accuracy of marker-based methods, supporting its potential for use in clinical settings. MMC could provide valuable insights into movement rehabilitation in stroke patients, potentially enhancing the effectiveness of rehabilitation strategies.|http://arxiv.org/abs/2411.14992v1|Tim Unger,Arash Sal Moslehian,J. D. Peiffer,Johann Ullrich,Roger Gassert,Olivier Lambercy,R. James Cotton,Chris Awai Easthope
1120|Clinical evaluation of semi-automatic opensource algorithmic software segmentation of the mandibular bone: Practical feasibility and assessment of a new course of action|Computer assisted technologies based on algorithmic software segmentation are an increasing topic of interest in complex surgical cases. However - due to functional instability, time consuming software processes, personnel resources or licensed-based financial costs many segmentation processes are often outsourced from clinical centers to third parties and the industry. Therefore, the aim of this trial was to assess the practical feasibility of an easy available, functional stable and licensed-free segmentation approach to be used in the clinical practice. In this retrospective, randomized, controlled trail the accuracy and accordance of the open-source based segmentation algorithm GrowCut (GC) was assessed through the comparison to the manually generated ground truth of the same anatomy using 10 CT lower jaw data-sets from the clinical routine. Assessment parameters were the segmentation time, the volume, the voxel number, the Dice Score (DSC) and the Hausdorff distance (HD). Overall segmentation times were about one minute. Mean DSC values of over 85% and HD below 33.5 voxel could be achieved. Statistical differences between the assessment parameters were not significant (p<0.05) and correlation coefficients were close to the value one (r > 0.94). Complete functional stable and time saving segmentations with high accuracy and high positive correlation could be performed by the presented interactive open-source based approach. In the cranio-maxillofacial complex the used method could represent an algorithmic alternative for image-based segmentation in the clinical practice for e.g. surgical treatment planning or visualization of postoperative results and offers several advantages. Systematic comparisons to other segmentation approaches or with a greater data amount are areas of future works.|http://arxiv.org/abs/1805.08604v1|Jrgen Wallner,Kerstin Hochegger,Xiaojun Chen,Irene Mischak,Knut Reinbacher,Mauro Pau,Tomislav Zrnc,Katja Schwenzer-Zimmerer,Wolfgang Zemann,Dieter Schmalstieg,Jan Egger
1121|Modelling the Lymphatic Metastatic Progression Pathways of OPSCC from Multi-Institutional Datasets|The elective clinical target volume (CTV-N) in oropharyngeal squamous cell carcinoma (OPSCC) is currently based mostly on the prevalence of lymph node metastases in different lymph node levels (LNLs) for a given primary tumor location. We present a probabilistic model for ipsilateral lymphatic spread that can quantify the microscopic nodal involvement risk based on an individual patient's T-category and clinical involvement of LNLs at diagnosis. We extend a previously published hidden Markov model (HMM), which models the LNLs (I, II, III, IV, V, and VII) as hidden binary random variables (RVs). Each represents a patient's true state of lymphatic involvement. Clinical involvement at diagnosis represents the observed binary RVs linked to the true state via sensitivity and specificity. The primary tumor and the hidden RVs are connected in a graph. Each edge represents the conditional probability of metastatic spread per abstract time-step, given disease at the edge's starting node. To learn these probabilities, we draw Markov chain Monte Carlo samples from the likelihood of a dataset (686 OPSCC patients) from three institutions. We compute the model evidence using thermodynamic integration for different graphs to determine which describes the data best. The graph maximizing the model evidence connects the tumor to each LNL and the LNLs I through V in order. It predicts the risk of occult disease in level IV is below 5% if level III is clinically negative, and that the risk of occult disease in level V is below 5% except for advanced T-category (T3 and T4) patients with clinical involvement of levels II, III, and IV. The provided statistical model of nodal involvement in OPSCC patients trained on multi-institutional data may guide the design of clinical trials on volume-deescalated treatment of OPSCC and contribute to more personal guidelines on elective nodal treatment.|http://arxiv.org/abs/2312.11270v3|Roman Ludwig,Adrian Schubert,Dorothea Barbatei,Lauence Bauwens,Jean-Marc Hoffmann,Sandrine Werlen,Olgun Elicin,Matthias Dettmer,Philippe Zrounba,Bertrand Pouymayou,Panagiotis Balermpas,Vincent Grgoire,Roland Giger,Jan Unkelbach
1122|Training and Validating a Treatment Recommender with Partial Verification Evidence|Current clinical decision support systems (DSS) are trained and validated on observational data from the target clinic. This is problematic for treatments validated in a randomized clinical trial (RCT), but not yet introduced in any clinic. In this work, we report on a method for training and validating the DSS using the RCT data. The key challenges we address are of missingness -- missing rationale for treatment assignment (the assignment is at random), and missing verification evidence, since the effectiveness of a treatment for a patient can only be verified (ground truth) for treatments what were actually assigned to a patient. We use data from a multi-armed RCT that investigated the effectiveness of single- and combination- treatments for 240+ tinnitus patients recruited and treated in 5 clinical centers.   To deal with the 'missing rationale' challenge, we re-model the target variable (outcome) in order to suppress the effect of the randomly-assigned treatment, and control on the effect of treatment in general. Our methods are also robust to missing values in features and with a small number of patients per RCT arm. We deal with 'missing verification evidence' by using counterfactual treatment verification, which compares the effectiveness of the DSS recommendations to the effectiveness of the RCT assignments when they are aligned v/s not aligned.   We demonstrate that our approach leverages the RCT data for learning and verification, by showing that the DSS suggests treatments that improve the outcome. The results are limited through the small number of patients per treatment; while our ensemble is designed to mitigate this effect, the predictive performance of the methods is affected by the smallness of the data. We provide a basis for the establishment of decision supporting routines on treatments that have been tested in RCTs but have not yet been deployed clinically.|http://arxiv.org/abs/2406.06654v1|Vishnu Unnikrishnan,Clara Puga,Miro Schleicher,Uli Niemann,Berthod Langguth,Stefan Schoisswohl,Birgit Mazurek,Rilana Cima,Jose Antonio Lopez-Escamez,Dimitris Kikidis,Eleftheria Vellidou,Ruediger Pryss,Winfried Schlee,Myra Spiliopoulou
1123|Evaluating the Effectiveness of Personalized Medicine with Software|We present methodological advances in understanding the effectiveness of personalized medicine models and supply easy-to-use open-source software. Personalized medicine involves the systematic use of individual patient characteristics to determine which treatment option is most likely to result in a better outcome for the patient on average. Why is personalized medicine not done more in practice? One of many reasons is because practitioners do not have any easy way to holistically evaluate whether their personalization procedure does better than the standard of care. Our software, "Personalized Treatment Evaluator" (the R package PTE), provides inference for improvement out-of-sample in many clinical scenarios. We also extend current methodology by allowing evaluation of improvement in the case where the endpoint is binary or survival. In the software, the practitioner inputs (1) data from a single-stage randomized trial with one continuous, incidence or survival endpoint and (2) a functional form of a model for the endpoint constructed from domain knowledge. The bootstrap is then employed on data unseen during model fitting to provide confidence intervals for the improvement for the average future patient (assuming future patients are similar to the patients in the trial). One may also test against a null scenario where the hypothesized personalization are not more useful than a standard of care. We demonstrate our method's promise on simulated data as well as on data from a randomized comparative trial investigating two treatments for depression.|http://arxiv.org/abs/1404.7844v3|Adam Kapelner,Justin Bleich,Alina Levine,Zachary D. Cohen,Robert J. DeRubeis,Richard Berk
1124|Stein-like Estimators for Causal Mediation Analysis in Randomized Trials|Causal mediation analysis aims to estimate the natural direct and indirect effects under clearly specified assumptions. Traditional mediation analysis based on Ordinary Least Squares (OLS) relies on the absence of unmeasured causes of the putative mediator and outcome. When this assumption cannot be justified, Instrumental Variables (IV) estimators can be used in order to produce an asymptotically unbiased estimator of the mediator-outcome link. However, provided that valid instruments exist, bias removal comes at the cost of variance inflation for standard IV procedures such as Two-Stage Least Squares (TSLS). A Semi-Parametric Stein-Like (SPSL) estimator has been proposed in the literature that strikes a natural trade-off between the unbiasedness of the TSLS procedure and the relatively small variance of the OLS estimator. Moreover, the SPSL has the advantage that its shrinkage parameter can be directly estimated from the data. In this paper, we demonstrate how this Stein-like estimator can be implemented in the context of the estimation of natural direct and natural indirect effects of treatments in randomized controlled trials. The performance of the competing methods is studied in a simulation study, in which both the strength of hidden confounding and the strength of the instruments are independently varied. These considerations are motivated by a trial in mental health evaluating the impact of a primary care-based intervention to reduce depression in the elderly.|http://arxiv.org/abs/1707.01723v1|Cedric E. Ginestet,Richard Emsley,Sabine Landau
1125|Joint longitudinal models for dealing with missing at random data in trial-based economic evaluations|Health economic evaluations based on patient-level data collected alongside clinical trials~(e.g. health related quality of life and resource use measures) are an important component of the process which informs resource allocation decisions. Almost inevitably, the analysis is complicated by the fact that some individuals drop out from the study, which causes their data to be unobserved at some time point. Current practice performs the evaluation by handling the missing data at the level of aggregated variables (e.g. QALYs), which are obtained by combining the economic data over the duration of the study, and are often conducted under a missing at random (MAR) assumption. However, this approach may lead to incorrect inferences since it ignores the longitudinal nature of the data and may end up discarding a considerable amount of observations from the analysis. We propose the use of joint longitudinal models to extend standard cost-effectiveness analysis methods by taking into account the longitudinal structure and incorporate all available data to improve the estimation of the targeted quantities under MAR. Our approach is compared to popular missingness approaches in trial-based analyses, motivated by an exploratory simulation study, and applied to data from two real case studies.|http://arxiv.org/abs/1805.07149v2|Andrea Gabrio,Rachael Hunter,Alexina J. Mason,Gianluca Baio
1126|Properties of the weighted log-rank test in the design of confirmatory studies with delayed effects|Proportional hazards are a common assumption when designing confirmatory clinical trials in oncology. This assumption not only affects the analysis part but also the sample size calculation. The presence of delayed effects causes a change in the hazard ratio while the trial is ongoing since at the beginning we do not observe any difference between treatment arms and after some unknown time point, the differences between treatment arms will start to appear. Hence, the proportional hazards assumption no longer holds and both sample size calculation and analysis methods to be used should be reconsidered. The weighted log-rank test allows a weighting for early, middle and late differences through the Fleming and Harrington class of weights, and is proven to be more efficient when the proportional hazards assumption does not hold. The Fleming and Harrington class of weights, along with the estimated delay, can be incorporated into the sample size calculation in order to maintain the desired power once the treatment arm differences start to appear. In this article, we explore the impact of delayed effects in group sequential and adaptive group sequential designs, and make an empirical evaluation in terms of power and type-I error rate of the of the weighted log-rank test in a simulated scenario with fixed values of the Fleming and Harrington class of weights. We also give some practical recommendations regarding which methodology should be used in the presence of delayed effects depending on certain characteristics of the trial.|http://arxiv.org/abs/1806.11294v2|Jose L Jimenez,Viktoriya Stalbovskaya,Byron Jones
1127|Adaptive Clinical Trials: Exploiting Sequential Patient Recruitment and Allocation|Randomized Controlled Trials (RCTs) are the gold standard for comparing the effectiveness of a new treatment to the current one (the control). Most RCTs allocate the patients to the treatment group and the control group by uniform randomization. We show that this procedure can be highly sub-optimal (in terms of learning) if -- as is often the case -- patients can be recruited in cohorts (rather than all at once), the effects on each cohort can be observed before recruiting the next cohort, and the effects are heterogeneous across identifiable subgroups of patients. We formulate the patient allocation problem as a finite stage Markov Decision Process in which the objective is to minimize a given weighted combination of type-I and type-II errors. Because finding the exact solution to this Markov Decision Process is computationally intractable, we propose an algorithm -- \textit{Knowledge Gradient for Randomized Controlled Trials} (RCT-KG) -- that yields an approximate solution. We illustrate our algorithm on a synthetic dataset with Bernoulli outcomes and compare it with uniform randomization. For a given size of trial our method achieves significant reduction in error, and to achieve a prescribed level of confidence (in identifying whether the treatment is superior to the control), our method requires many fewer patients. Our approach uses what has been learned from the effects on previous cohorts to recruit patients to subgroups and allocate patients (to treatment/control) within subgroups in a way that promotes more efficient learning.|http://arxiv.org/abs/1810.02876v2|Onur Atan,William R. Zame,Mihaela van der Schaar
1128|Improving interim decisions in randomized trials by exploiting information on short-term outcomes and prognostic baseline covariates|Conditional power calculations are frequently used to guide the decision whether or not to stop a trial for futility or to modify planned sample size. These ignore the information in short-term endpoints and baseline covariates, and thereby do not make fully efficient use of the information in the data. We therefore propose an interim decision procedure based on the conditional power approach which exploits the information contained in baseline covariates and short-term outcomes. We will realise this by considering the estimation of the treatment effect at the interim analysis as a missing data problem. This problem is addressed by employing specific prediction models for the long-term endpoint which enable the incorporation of baseline covariates and multiple short-term endpoints. We show that the proposed procedure leads to an efficiency gain and a reduced sample size, without compromising the Type I error rate of the procedure, even when the adopted prediction models are misspecified. In particular, implementing our proposal in the conditional power approach allows earlier decisions relative to standard approaches, whilst controlling the probability of an incorrect decision. This time gain results in a lower expected number of recruited patients in case of stopping for futility, such that fewer patients receive the futile regimen. We explain how these methods can be used in adaptive designs with unblinded sample size reassessment based on the inverse normal $p$-value combination method to control type I error. We support the proposal by Monte Carlo simulations based on data from a real clinical trial.|http://arxiv.org/abs/1904.04876v1|Kelly Van Lancker,An Vandebosch,Stijn Vansteelandt
1129|Sample Size Estimation using a Latent Variable Model for Mixed Outcome Co-Primary, Multiple Primary and Composite Endpoints|Mixed outcome endpoints that combine multiple continuous and discrete components to form co-primary, multiple primary or composite endpoints are often employed as primary outcome measures in clinical trials. There are many advantages to joint modelling the individual outcomes using a latent variable framework, however in order to make use of the model in practice we require techniques for sample size estimation. In this paper we show how the latent variable model can be applied to the three types of joint endpoints and propose appropriate hypotheses, power and sample size estimation methods for each. We illustrate the techniques using a numerical example based on the four dimensional endpoint in the MUSE trial and find that the sample size required for the co-primary endpoint is larger than that required for the individual endpoint with the smallest effect size. Conversely, the sample size required for the multiple primary endpoint is reduced from that required for the individual outcome with the largest effect size. We show that the analytical technique agrees with the empirical power from simulation studies. We further illustrate the reduction in required sample size that may be achieved in trials of mixed outcome composite endpoints through a simulation study and find that the sample size primarily depends on the components driving response and the correlation structure and much less so on the treatment effect structure in the individual endpoints.|http://arxiv.org/abs/1912.05258v1|Martina McMenamin,Jessica K. Barrett,Anna Berglind,James M. S. Wason
1130|Developing a predictive signature for two trial endpoints using the cross-validated risk scores method|The existing cross-validated risk scores (CVRS) design has been proposed for developing and testing the efficacy of a treatment in a high-efficacy patient group (the sensitive group) using high-dimensional data (such as genetic data). The design is based on computing a risk score for each patient and dividing them into clusters using a non-parametric clustering procedure. In some settings it is desirable to consider the trade-off between two outcomes, such as efficacy and toxicity, or cost and effectiveness. With this motivation, we extend the CVRS design (CVRS2) to consider two outcomes. The design employs bivariate risk scores that are divided into clusters. We assess the properties of the CVRS2 using simulated data and illustrate its application on a randomised psychiatry trial. We show that CVRS2 is able to reliably identify the sensitive group (the group for which the new treatment provides benefit on both outcomes) in the simulated data. We apply the CVRS2 design to a psychology clinical trial that had offender status and substance use status as two outcomes and collected a large number of baseline covariates. The CVRS2 design yields a significant treatment effect for both outcomes, while the CVRS approach identified a significant effect for the offender status only after pre-filtering the covariates.|http://arxiv.org/abs/2007.01680v2|Svetlana Cherlin,James M. S. Wason
1131|Bayesian Sample Size Calculations for SMART Studies|In the management of most chronic conditions characterized by the lack of universally effective treatments, adaptive treatment strategies (ATSs) have been growing in popularity as they offer a more individualized approach, and sequential multiple assignment randomized trials (SMARTs) have gained attention as the most suitable clinical trial design to formalize the study of these strategies. While the number of SMARTs has increased in recent years, their design has remained limited to the frequentist setting, which may not fully or appropriately account for uncertainty in design parameters and hence not yield appropriate sample size recommendations. Specifically, standard frequentist formulae rely on several assumptions that can be easily misspecified. The Bayesian framework offers a straightforward path to alleviate some of these concerns. In this paper, we provide calculations in a Bayesian setting to allow more realistic and robust estimates that account for uncertainty in inputs through the `two priors' approach. Additionally, compared to the standard formulae, this methodology allows us to rely on fewer assumptions, integrate pre-trial knowledge, and switch the focus from the standardized effect size to the minimal detectable difference. The proposed methodology is evaluated in a thorough simulation study and is implemented to estimate the sample size for a full-scale SMART of an Internet-Based Adaptive Stress Management intervention based on a pilot SMART conducted on cardiovascular disease patients from two Canadian provinces.|http://arxiv.org/abs/2108.01041v1|Armando Turchetta,Erica E. M. Moodie,David A. Stephens,Sylvie D. Lambert
1132|A review of available software for adaptive clinical trial design|Background/Aims: The increasing expense of the drug development process has seen interest in the use of adaptive designs (ADs) grow substantially in recent years. Accordingly, much research has been conducted to identify potential barriers to increasing the use of ADs in practice, and several articles have argued that the availability of user-friendly software will be an important step in making ADs easier to implement. Therefore, in this paper we present a review of the current state of software availability for AD. Methods: We first review articles from 31 journals published in 2013-17 that relate to methodology for adaptive trials, in order to assess how often code and software for implementing novel ADs is made available at the time of publication. We contrast our findings against these journals' current policies on code distribution. Secondly, we conduct additional searches of popular code repositories, such as CRAN and GitHub, to identify further existing user-contributed software for ADs. From this, we are able to direct interested parties towards solutions for their problem of interest by classifying available code by type of adaptation. Results: Only 29% of included articles made their code available in some form. In many instances, articles published in journals that had mandatory requirements on code provision still did not make code available. There are several areas in which available software is currently limited or saturated. In particular, many packages are available to address group sequential design, but comparatively little code is present in the public domain to determine biomarker-guided ADs. Conclusions: There is much room for improvement in the provision of software alongside AD publications. Additionally, whilst progress has been made, well-established software for various types of trial adaptation remains sparsely available.|http://arxiv.org/abs/1906.05603v1|Michael J Grayling,Graham M Wheeler
1133|Applying Meta-Analytic-Predictive Priors with the R Bayesian evidence synthesis tools|Use of historical data in clinical trial design and analysis has shown various advantages such as reduction of within-study placebo-treated number of subjects and increase of study power. The meta-analytic-predictive (MAP) approach accounts with a hierarchical model for between-trial heterogeneity in order to derive an informative prior from historical (often control) data. In this paper, we introduce the package RBesT (R Bayesian Evidence Synthesis Tools) which implements the MAP approach with normal (known sampling standard deviation), binomial and Poisson endpoints. The hierarchical MAP model is evaluated by MCMC. The numerical MCMC samples representing the MAP prior are approximated with parametric mixture densities which are obtained with the expectation maximization algorithm. The parametric mixture density representation facilitates easy communication of the MAP prior and enables via fast and accurate analytical procedures to evaluate properties of trial designs with informative MAP priors. The paper first introduces the framework of robust Bayesian evidence synthesis in this setting and then explains how RBesT facilitates the derivation and evaluation of an informative MAP prior from historical control data. In addition we describe how the meta-analytic framework relates to further applications including probability of success calculations.|http://arxiv.org/abs/1907.00603v2|Sebastian Weber,Yue Li,John Seaman,Tomoyuki Kakizume,Heinz Schmidli
1134|Evaluation of the efficacy of RUTI and ID93/GLA-SE vaccines in tuberculosis treatment: in silico trial through UISS-TB simulator|Tuberculosis (TB) is one of the deadliest diseases worldwide, with 1,5 million fatalities every year along with potential devastating effects on society, families and individuals. To address this alarming burden, vaccines can play a fundamental role, even though to date no fully effective TB vaccine really exists. Current treatments involve several combinations of antibiotics administered to TB patients for up to two years, leading often to financial issues and reduced therapy adherence. Along with this, the development and spread of drug-resistant TB strains is another big complicating matter. Faced with these challenges, there is an urgent need to explore new vaccination strategies in order to boost immunity against tuberculosis and shorten the duration of treatment. Computational modeling represents an extraordinary way to simulate and predict the outcome of vaccination strategies, speeding up the arduous process of vaccine pipeline development and relative time to market. Here, we present EU - funded STriTuVaD project computational platform able to predict the artificial immunity induced by RUTI and ID93/GLA-SE, two specific tuberculosis vaccines. Such an in silico trial will be validated through a phase 2b clinical trial. Moreover, STriTuVaD computational framework is able to inform of the reasons for failure should the vaccinations strategies against M. tuberculosis under testing found not efficient, which will suggest possible improvements.|http://arxiv.org/abs/1911.00325v1|Giulia Russo,Francesco Pappalardo,Miguel A. Juarez,Marzio Pennisi,Pere Joan Cardona,Rhea Coler,Epifanio Fichera,Marco Viceconti
1135|A Simulation-free Group Sequential Design with Max-combo Tests in the Presence of Non-proportional Hazards|Non-proportional hazards (NPH) have been observed recently in many immuno-oncology clinical trials. Weighted log-rank tests (WLRT) with suitably chosen weights can be used to improve the power of detecting the difference of the two survival curves in the presence of NPH. However, it is not easy to choose a proper WLRT in practice when both robustness and efficiency are considered. A versatile maxcombo test was proposed to achieve the balance of robustness and efficiency and has received increasing attentions in both methodology development and application. However, survival trials often warrant interim analyses due to its high cost and long duration. The integration and application of maxcombo tests in interim analyses often require extensive simulation studies. In this paper, we propose a simulation-free approach for group sequential design with maxcombo test in survival trials. The simulation results support that the proposed approaches successfully control both the type I error rate and offer great accuracy and flexibility in estimating sample sizes, at the expense of light computation burden. Notably, our methods display a strong robustness towards various model misspecifications, and have been implemented in an R package for free access online.|http://arxiv.org/abs/1911.05684v7|Lili Wang,Xiaodong Luo,Cheng Zheng
|Leonhard Held
1137|Penalized Poisson model for network meta-analysis of individual patient time-to-event data|Network meta-analysis (NMA) allows the combination of direct and indirect evidence from a set of randomized clinical trials. Performing NMA using individual patient data (IPD) is considered as a "gold standard" approach as it provides several advantages over NMA based on aggregate data. For example, it allows to perform advanced modelling of covariates or covariate-treatment interactions. An important issue in IPD NMA is the selection of influential parameters among terms that account for inconsistency, covariates, covariate-by-treatment interactions or non-proportionality of treatments effect for time to event data. This issue has not been deeply studied in the literature yet and in particular not for time-to-event data. A major difficulty is to jointly account for between-trial heterogeneity which could have a major influence on the selection process. The use of penalized generalized mixed effect model is a solution, but existing implementations have several shortcomings and an important computational cost that precludes their use for complex IPD NMA. In this article, we propose a penalized Poisson regression model to perform IPD NMA of time-to-event data. It is based only on fixed effect parameters which improve its computational cost over the use of random effects. It could be easily implemented using existing penalized regression package. Computer code is shared for implementation. The methods were applied on simulated data to illustrate the importance to take into account between trial heterogeneity during the selection procedure. Finally, it was applied to an IPD NMA of overall survival of chemotherapy and radiotherapy in nasopharyngeal carcinoma.|http://arxiv.org/abs/2103.00069v2|Edouard Ollier,Pierre Blanchard,Gwnal Le Teuff,Stefan Michiels
1138|Two-Stage TMLE to Reduce Bias and Improve Efficiency in Cluster Randomized Trials|Cluster randomized trials (CRTs) randomly assign an intervention to groups of individuals (e.g., clinics or communities) and measure outcomes on individuals in those groups. While offering many advantages, this experimental design introduces challenges that are only partially addressed by existing analytic approaches. First, outcomes are often missing for some individuals within clusters. Failing to appropriately adjust for differential outcome measurement can result in biased estimates and inference. Second, CRTs often randomize limited numbers of clusters, resulting in chance imbalances on baseline outcome predictors between arms. Failing to adaptively adjust for these imbalances and other predictive covariates can result in efficiency losses. To address these methodological gaps, we propose and evaluate a novel two-stage targeted minimum loss-based estimator (TMLE) to adjust for baseline covariates in a manner that optimizes precision, after controlling for baseline and post-baseline causes of missing outcomes. Finite sample simulations illustrate that our approach can nearly eliminate bias due to differential outcome measurement, while existing CRT estimators yield misleading results and inferences. Application to real data from the SEARCH community randomized trial demonstrates the gains in efficiency afforded through adaptive adjustment for baseline covariates, after controlling for missingness on individual-level outcomes.|http://arxiv.org/abs/2106.15737v2|Laura B. Balzer,Mark van der Laan,James Ayieko,Moses Kamya,Gabriel Chamie,Joshua Schwab,Diane V. Havlir,Maya L. Petersen
1139|Optimizing Precision and Power by Machine Learning in Randomized Trials, with an Application to COVID-19|The rapid finding of effective therapeutics requires the efficient use of available resources in clinical trials. The use of covariate adjustment can yield statistical estimates with improved precision, resulting in a reduction in the number of participants required to draw futility or efficacy conclusions. We focus on time-to-event and ordinal outcomes. A key question for covariate adjustment in randomized studies is how to fit a model relating the outcome and the baseline covariates to maximize precision. We present a novel theoretical result establishing conditions for asymptotic normality of a variety of covariate-adjusted estimators that rely on machine learning (e.g., l1-regularization, Random Forests, XGBoost, and Multivariate Adaptive Regression Splines), under the assumption that outcome data is missing completely at random. We further present a consistent estimator of the asymptotic variance. Importantly, the conditions do not require the machine learning methods to converge to the true outcome distribution conditional on baseline variables, as long as they converge to some (possibly incorrect) limit. We conducted a simulation study to evaluate the performance of the aforementioned prediction methods in COVID-19 trials using longitudinal data from over 1,500 patients hospitalized with COVID-19 at Weill Cornell Medicine New York Presbyterian Hospital. We found that using l1-regularization led to estimators and corresponding hypothesis tests that control type 1 error and are more precise than an unadjusted estimator across all sample sizes tested. We also show that when covariates are not prognostic of the outcome, l1-regularization remains as precise as the unadjusted estimator, even at small sample sizes (n = 100). We give an R package adjrct that performs model-robust covariate adjustment for ordinal and time-to-event outcomes.|http://arxiv.org/abs/2109.04294v1|Nicholas Williams,Michael Rosenblum,Ivn Daz
1140|A Comparison of Estimand and Estimation Strategies for Clinical Trials in Early Parkinson's Disease|Parkinson's disease (PD) is a chronic, degenerative neurological disorder. PD cannot be prevented, slowed or cured as of today but highly effective symptomatic treatments are available. We consider relevant estimands and treatment effect estimators for randomized trials of a novel treatment which aims to slow down disease progression versus placebo in early, untreated PD. A commonly used endpoint in PD trials is the MDS-Unified Parkinson's Disease Rating Scale (MDS-UPDRS), which is longitudinally assessed at scheduled visits. The most important intercurrent events (ICEs) which affect the interpretation of the MDS-UPDRS are study treatment discontinuations and initiations of symptomatic treatment. Different estimand strategies are discussed and hypothetical or treatment policy strategies, respectively, for different types of ICEs seem most appropriate in this context. Several estimators based on multiple imputation which target these estimands are proposed and compared in terms of bias, mean-squared error, and power in a simulation study. The investigated estimators include methods based on a missing-at-random (MAR) assumption, with and without the inclusion of time-varying ICE-indicators, as well as reference-based imputation methods. Simulation parameters are motivated by data analyses of a cohort study from the Parkinson's Progression Markers Initiative (PPMI).|http://arxiv.org/abs/2112.03700v1|Alessandro Noci,Marcel Wolbers,Markus Abt,Corine Baayen,Hans Ulrich Burger,Man Jin,Weining Zhao Robieson
1141|Making SMART decisions in prophylaxis and treatment studies|The optimal prophylaxis, and treatment if the prophylaxis fails, for a disease may be best evaluated using a sequential multiple assignment randomised trial (SMART). A SMART is a multi-stage study that randomises a participant to an initial treatment, observes some response to that treatment and then, depending on their observed response, randomises the same participant to an alternative treatment. Response adaptive randomisation may, in some settings, improve the trial participants' outcomes and expedite trial conclusions, compared to fixed randomisation. But 'myopic' response adaptive randomisation strategies, blind to multistage dynamics, may also result in suboptimal treatment assignments. We propose a 'dynamic' response adaptive randomisation strategy based on Q-learning, an approximate dynamic programming algorithm. Q-learning uses stage-wise statistical models and backward induction to incorporate late-stage 'payoffs' (i.e. clinical outcomes) into early-stage 'actions' (i.e. treatments). Our real-world example consists of a COVID-19 prophylaxis and treatment SMART with qualitatively different binary endpoints at each stage. Standard Q-learning does not work with such data because it cannot be used for sequences of binary endpoints. Sequences of qualitatively distinct endpoints may also require different weightings to ensure that the design guides participants to regimens with the highest utility. We describe how a simple decision-theoretic extension to Q-learning can be used to handle sequential binary endpoints with distinct utilities. Using simulation we show that, under a set of binary utilities, the 'dynamic' approach increases expected participant utility compared to the fixed approach, sometimes markedly, for all model parameters, whereas the 'myopic' approach can actually decrease utility.|http://arxiv.org/abs/2203.12859v1|Robert K. Mahar,Katherine J. Lee,Bibhas Chakraborty,Agus Salim,Julie A. Simpson
1142|Bridged treatment comparisons: an illustrative application in HIV treatment|Comparisons of treatments, interventions, or exposures are of central interest in epidemiology, but direct comparisons are not always possible due to practical or ethical reasons. Here, we detail a fusion approach to compare treatments across studies. The motivating example entails comparing the risk of the composite outcome of death, AIDS, or greater than a 50% CD4 cell count decline in people with HIV when assigned triple versus mono antiretroviral therapy, using data from the AIDS Clinical Trial Group (ACTG) 175 (mono versus dual therapy) and ACTG 320 (dual versus triple therapy). We review a set of identification assumptions and estimate the risk difference using an inverse probability weighting estimator that leverages the shared trial arms (dual therapy). A fusion diagnostic based on comparing the shared arms is proposed that may indicate violation of the identification assumptions. Application of the data fusion estimator and diagnostic to the ACTG trials indicates triple therapy results in a reduction in risk compared to monotherapy in individuals with baseline CD4 counts between 50 and 300 cells/mm$^3$. Bridged treatment comparisons address questions that none of the constituent data sources could address alone, but valid fusion-based inference requires careful consideration of the underlying assumptions.|http://arxiv.org/abs/2206.04445v4|Paul N Zivich,Stephen R Cole,Jessie K Edwards,Bonnie E Shook-Sa,Alexander Breskin,Michael G Hudgens
1143|Analysis of two Binomial Proportions in Non-inferiority Confirmatory Trials|In this paper, we propose considering an exact likelihood score (ELS) test for non-inferiority comparison and we derive its test-based confidence interval for the difference between two independent binomial proportions. The p-value for this test is obtained by using exact binomial probabilities with the nuisance parameter being replaced by its restricted maximum likelihood estimate. Calculated type I errors revealed that the proposed ELS method has important advantages for non-inferiority comparisons over popular asymptotic methods for adequately powered confirmatory clinical trials, at 80% or 90% statistical power. For unbalanced sample sizes of the compared treatment groups, the type I errors for the asymptotic score method were shown to be higher than the nominal level in a systematic pattern over a range of the true proportions, but the ELS method did not suffer from such a problem. On average, the true type I error of the ELS method was closer to the nominal level than all considered methods in the empirical comparisons. Also, in rare cases, the type I errors of the ELS test exceeded the nominal level, but only by a small amount. In addition, the p-value and confidence interval using the ELS method can be obtained in less than 30 seconds of computer time for most confirmatory trials. The theoretical arguments and the attractive empirical evidence, along with fast computation time, should make the ELS method very attractive for consideration in statistical practice.|http://arxiv.org/abs/2207.04372v1|Hassan Lakkis,Andrew Lakkis
1144|Bayesian design and analysis of two-arm cluster randomised trials using assurance|We consider the design of a two-arm superiority cluster randomised controlled trial (RCT) with a continuous outcome. We detail Bayesian inference for the analysis of the trial using a linear mixed-effects model. The treatment is compared to control using the posterior distribution for the treatment effect. We develop the form of the assurance to choose the sample size based on this analysis, and its evaluation using a two loop Monte Carlo sampling scheme. We assess the proposed approach, considering the effect of different forms of prior distribution, and the number of Monte Carlo samples needed in both loops for accurate determination of the assurance and sample size. Based on this assessment, we provide general advice on each of these choices. We apply the approach to the choice of sample size for a cluster RCT into post-stroke incontinence, and compare the resulting sample size to those from a power calculation and assurance based on a Wald test for the treatment effect. The Bayesian approach to design and analysis developed in this paper can offer advantages in terms of an increase in the robustness of the chosen sample size to parameter mis-specification and reduced sample sizes if prior information indicates the treatment effect is likely to be larger than the minimal clinically important difference.|http://arxiv.org/abs/2208.12509v1|Kevin J Wilson
1145|A Flexible Multi-Metric Bayesian Framework for Decision-Making in Phase II Multi-Arm Multi-Stage Studies|We propose a multi-metric flexible Bayesian framework to support efficient interim decision-making in multi-arm multi-stage phase II clinical trials. Multi-arm multi-stage phase II studies increase the efficiency of drug development, but early decisions regarding the futility or desirability of a given arm carry considerable risk since sample sizes are often low and follow-up periods may be short. Further, since intermediate outcomes based on biomarkers of treatment response are rarely perfect surrogates for the primary outcome and different trial stakeholders may have different levels of risk tolerance, a single hypothesis test is insufficient for comprehensively summarizing the state of the collected evidence. We present a Bayesian framework comprised of multiple metrics based on point estimates, uncertainty, and evidence towards desired thresholds (a Target Product Profile) for 1) ranking of arms and 2) comparison of each arm against an internal control. Using a large public-private partnership targeting novel TB arms as a motivating example, we find via simulation study that our multi-metric framework provides sufficient confidence for decision-making with sample sizes as low as 30 patients per arm, even when intermediate outcomes have only moderate correlation with the primary outcome. Our reframing of trial design and the decision-making procedure has been well-received by research partners and is a practical approach to more efficient assessment of novel therapeutics.|http://arxiv.org/abs/2302.07290v2|Suzanne M. Dufault,Angela M. Crook,Katie Rolfe,Patrick P. J. Phillips
1146|Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs|Results from Randomized Controlled Trials (RCTs) establish the comparative effectiveness of interventions, and are in turn critical inputs for evidence-based care. However, results from RCTs are presented in (often unstructured) natural language articles describing the design, execution, and outcomes of trials; clinicians must manually extract findings pertaining to interventions and outcomes of interest from such articles. This onerous manual process has motivated work on (semi-)automating extraction of structured evidence from trial reports. In this work we propose and evaluate a text-to-text model built on instruction-tuned Large Language Models (LLMs) to jointly extract Interventions, Outcomes, and Comparators (ICO elements) from clinical abstracts, and infer the associated results reported. Manual (expert) and automated evaluations indicate that framing evidence extraction as a conditional generation task and fine-tuning LLMs for this purpose realizes considerable ($\sim$20 point absolute F1 score) gains over the previous SOTA. We perform ablations and error analyses to assess aspects that contribute to model performance, and to highlight potential directions for further improvements. We apply our model to a collection of published RCTs through mid-2022, and release a searchable database of structured findings: http://ico-relations.ebm-nlp.com|http://arxiv.org/abs/2305.03642v3|Somin Wadhwa,Jay DeYoung,Benjamin Nye,Silvio Amir,Byron C. Wallace
1147|Causal effect of chemotherapy received dose intensity on survival outcome: a retrospective study in osteosarcoma|This study aims to analyse the effects of reducing Received Dose Intensity (RDI) in chemotherapy treatment for osteosarcoma patients on their survival by using a novel approach. In this scenario, toxic side effects are risk factors for mortality and predictors of future exposure levels, introducing post-assignment confounding.   Chemotherapy administration data from BO03 and BO06 Randomized Clinical Trials (RCTs) in ostosarcoma are employed to emulate a target trial with three RDI-based exposure strategies: 1) standard, 2) reduced, and 3) highly-reduced RDI. Investigations are conducted between subgroups of patients characterised by poor or good Histological Responses (HRe). Inverse Probability of Treatment Weighting (IPTW) is first used to transform the original population into a pseudo-population which mimics the target randomized cohort. Then, a Marginal Structural Cox Model with effect modification is employed. Conditional Average Treatment Effects (CATEs) are ultimately measured as the difference between the Restricted Mean Survival Time of reduced/highly-reduced RDI strategy and the standard one. Confidence Intervals for CATEs are obtained using a novel IPTW-based bootstrap procedure.   Significant effect modifications based on HRe were found. Increasing RDI-reductions led to contrasting trends for poor and good responders: the higher the reduction, the better (worsen) was the survival in poor (good) reponders. This study introduces a novel approach to (i) comprehensively address the challenges related to the analysis of chemotherapy data, (ii) mitigate the toxicity-treatment-adjustment bias, and (iii) repurpose existing RCT data for retrospective analyses extending beyond the original trials' intended scopes.|http://arxiv.org/abs/2307.09405v2|Marta Spreafico,Francesca Ieva,Marta Fiocco
1148|Causal inference under transportability assumptions for conditional relative effect measures|When extending inferences from a randomized trial to a new target population, the transportability condition for conditional difference effect measures is invoked to identify the marginal causal mean difference in the target population. However, many clinical investigators believe that conditional relative effect measures are more likely to be "transportable" between populations. Here, we examine the identification and estimation of the marginal counterfactual mean difference and ratio under the transportability condition for conditional relative effect measures. We obtain identification results for two scenarios that often arise in practice when individuals in the target population (1) only have access to the control treatment, and (2) have access to the control and other treatments but not necessarily the experimental treatment evaluated in the trial. We then propose model and rate multiply robust and nonparametric efficient estimators that allow for the use of data-adaptive methods to model the nuisance functions. We examine the performance of the methods in simulation studies and illustrate their use with data from two trials of paliperidone for patients with schizophrenia. We conclude that the proposed methods are attractive when background knowledge suggests that the transportability condition for conditional relative effect measures is more plausible than alternative conditions.|http://arxiv.org/abs/2402.02702v2|Guanbo Wang,Alexander Levis,Jon Steingrimsson,Issa Dahabreh
1149|Nested Instrumental Variables Design: Switcher Average Treatment Effect, Identification, Efficient Estimation and Generalizability|Instrumental variables (IV) are a commonly used tool to estimate causal effects from non-randomized data. An archetype of an IV is a randomized trial with non-compliance where the randomized treatment assignment serves as an IV for the non-ignorable treatment received. Under a monotonicity assumption, a valid IV non-parametrically identifies the average treatment effect among a non-identified, latent complier subgroup, whose generalizability is often under debate. In many studies, there could exist multiple versions of an IV, for instance, different nudges to take the same treatment in different study sites in a multicentre clinical trial. These different versions of an IV may result in different compliance rates and offer a unique opportunity to study IV estimates' generalizability. In this article, we introduce a novel nested IV assumption and study identification of the average treatment effect among two latent subgroups: always-compliers and switchers, who are defined based on the joint potential treatment received under two versions of a binary IV. We derive the efficient influence function for the SWitcher Average Treatment Effect (SWATE) and propose efficient estimators. We then propose formal statistical tests of the generalizability of IV estimates under the nested IV framework. We apply the proposed method to the Prostate, Lung, Colorectal and Ovarian (PLCO) Cancer Screening Trial and study the causal effect of colorectal cancer screening and its generalizability.|http://arxiv.org/abs/2405.07102v2|Rui Wang,Ying-Qi Zhao,Oliver Dukes,Bo Zhang
1150|An adaptive enrichment design using Bayesian model averaging for selection and threshold-identification of tailoring variables|Precision medicine stands as a transformative approach in healthcare, offering tailored treatments that can enhance patient outcomes and reduce healthcare costs. As understanding of complex disease improves, clinical trials are being designed to detect subgroups of patients with enhanced treatment effects. Biomarker-driven adaptive enrichment designs, which enroll a general population initially and later restrict accrual to treatment-sensitive patients, are gaining popularity. Current practice often assumes either pre-trial knowledge of biomarkers defining treatment-sensitive subpopulations or a simple, linear relationship between continuous markers and treatment effectiveness. Motivated by a trial studying rheumatoid arthritis treatment, we propose a Bayesian adaptive enrichment design which identifies important tailoring variables out of a larger set of candidate biomarkers. Our proposed design is equipped with a flexible modelling framework where the effects of continuous biomarkers are introduced using free knot B-splines. The parameters of interest are then estimated by marginalizing over the space of all possible variable combinations using Bayesian model averaging. At interim analyses, we assess whether a biomarker-defined subgroup has enhanced or reduced treatment effects, allowing for early termination due to efficacy or futility and restricting future enrollment to treatment-sensitive patients. We consider pre-categorized and continuous biomarkers, the latter of which may have complex, nonlinear relationships to the outcome and treatment effect. Using simulations, we derive the operating characteristics of our design and compare its performance to two existing approaches.|http://arxiv.org/abs/2405.08180v1|Lara Maleyeff,Shirin Golchi,Erica E. M. Moodie,Marie Hudson
1151|A comparison of methods for estimating the average treatment effect on the treated for externally controlled trials|While randomized trials may be the gold standard for evaluating the effectiveness of the treatment intervention, in some special circumstances, single-arm clinical trials utilizing external control may be considered. The causal treatment effect of interest for single-arm studies is usually the average treatment effect on the treated (ATT) rather than the average treatment effect (ATE). Although methods have been developed to estimate the ATT, the selection and use of these methods require a thorough comparison and in-depth understanding of the advantages and disadvantages of these methods. In this study, we conducted simulations under different identifiability assumptions to compare the performance metrics (e.g., bias, standard deviation (SD), mean squared error (MSE), type I error rate) for a variety of methods, including the regression model, propensity score matching, Mahalanobis distance matching, coarsened exact matching, inverse probability weighting, augmented inverse probability weighting (AIPW), AIPW with SuperLearner, and targeted maximum likelihood estimator (TMLE) with SuperLearner.   Our simulation results demonstrate that the doubly robust methods in general have smaller biases than other methods. In terms of SD, nonmatching methods in general have smaller SDs than matching-based methods. The performance of MSE is a trade-off between the bias and SD, and no method consistently performs better in term of MSE. The identifiability assumptions are critical to the models' performance: violation of the positivity assumption can lead to a significant inflation of type I errors in some methods; violation of the unconfoundedness assumption can lead to a large bias for all methods... (Further details are available in the main body of the paper).|http://arxiv.org/abs/2408.07193v1|Huan Wang,Fei Wu,Yeh-Fong Chen
1152|Randomization-based Inference for MCP-Mod|Dose selection is critical in pharmaceutical drug development, as it directly impacts therapeutic efficacy and patient safety of a drug. The Generalized Multiple Comparison Procedures and Modeling (MCP-Mod) approach is commonly used in Phase II trials for testing and estimation of dose-response relationships. However, its effectiveness in small sample sizes, particularly with binary endpoints, is hindered by issues like complete separation in logistic regression, leading to non-existence of estimates. Motivated by an actual clinical trial using the MCP-Mod approach, this paper introduces penalized maximum likelihood estimation (MLE) and randomization-based inference techniques to address these challenges. Randomization-based inference allows for exact finite sample inference, while population-based inference for MCP-Mod typically relies on asymptotic approximations. Simulation studies demonstrate that randomization-based tests can enhance statistical power in small to medium-sized samples while maintaining control over type-I error rates, even in the presence of time trends. Our results show that residual-based randomization tests using penalized MLEs not only improve computational efficiency but also outperform standard randomization-based methods, making them an adequate choice for dose-finding analyses within the MCP-Mod framework. Additionally, we apply these methods to pharmacometric settings, demonstrating their effectiveness in such scenarios. The results in this paper underscore the potential of randomization-based inference for the analysis of dose-finding trials, particularly in small sample contexts.|http://arxiv.org/abs/2410.11716v1|Lukas Pin,Oleksandr Sverdlov,Frank Bretz,Bjrn Bornkamp
1153|Subgroup analysis in multi level hierarchical cluster randomized trials|Cluster or group randomized trials (CRTs) are increasingly used for both behavioral and system-level interventions, where entire clusters are randomly assigned to a study condition or intervention. Apart from the assigned cluster-level analysis, investigating whether an intervention has a differential effect for specific subgroups remains an important issue, though it is often considered an afterthought in pivotal clinical trials. Determining such subgroup effects in a CRT is a challenging task due to its inherent nested cluster structure. Motivated by a real-life HIV prevention CRT, we consider a three-level cross-sectional CRT, where randomization is carried out at the highest level and subgroups may exist at different levels of the hierarchy. We employ a linear mixed-effects model to estimate the subgroup-specific effects through their maximum likelihood estimators (MLEs). Consequently, we develop a consistent test for the significance of the differential intervention effect between two subgroups at different levels of the hierarchy, which is the key methodological contribution of this work. We also derive explicit formulae for sample size determination to detect a differential intervention effect between two subgroups, aiming to achieve a given statistical power in the case of a planned confirmatory subgroup analysis. The application of our methodology is illustrated through extensive simulation studies using synthetic data, as well as with real-world data from an HIV prevention CRT in The Bahamas.|http://arxiv.org/abs/2411.11301v1|Shubhadeep Chakraborty,Bo Wang,Ram Tiwari,Samiran Ghosh
1154|Stabilizing Machine Learning for Reproducible and Explainable Results: A Novel Validation Approach to Subject-Specific Insights|Machine Learning is transforming medical research by improving diagnostic accuracy and personalizing treatments. General ML models trained on large datasets identify broad patterns across populations, but their effectiveness is often limited by the diversity of human biology. This has led to interest in subject-specific models that use individual data for more precise predictions. However, these models are costly and challenging to develop. To address this, we propose a novel validation approach that uses a general ML model to ensure reproducible performance and robust feature importance analysis at both group and subject-specific levels. We tested a single Random Forest (RF) model on nine datasets varying in domain, sample size, and demographics. Different validation techniques were applied to evaluate accuracy and feature importance consistency. To introduce variability, we performed up to 400 trials per subject, randomly seeding the ML algorithm for each trial. This generated 400 feature sets per subject, from which we identified top subject-specific features. A group-specific feature importance set was then derived from all subject-specific results. We compared our approach to conventional validation methods in terms of performance and feature importance consistency. Our repeated trials approach, with random seed variation, consistently identified key features at the subject level and improved group-level feature importance analysis using a single general model. Subject-specific models address biological variability but are resource-intensive. Our novel validation technique provides consistent feature importance and improved accuracy within a general ML model, offering a practical and explainable alternative for clinical research.|http://arxiv.org/abs/2412.16199v1|Gideon Vos,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi
1155|Sequential tests and estimates after overrunning based on $p$-value combination|Often in sequential trials additional data become available after a stopping boundary has been reached. A method of incorporating such information from overrunning is developed, based on the ``adding weighted Zs'' method of combining $p$-values. This yields a combined $p$-value for the primary test and a median-unbiased estimate and confidence bounds for the parameter under test. When the amount of overrunning information is proportional to the amount available upon terminating the sequential test, exact inference methods are provided; otherwise, approximate methods are given and evaluated. The context is that of observing a Brownian motion with drift, with either linear stopping boundaries in continuous time or discrete-time group-sequential boundaries. The method is compared with other available methods and is exemplified with data from two sequential clinical trials.|http://arxiv.org/abs/0805.3070v1|W. J. Hall,Keyue Ding
1156|The Future of Indirect Evidence|Familiar statistical tests and estimates are obtained by the direct observation of cases of interest: a clinical trial of a new drug, for instance, will compare the drug's effects on a relevant set of patients and controls. Sometimes, though, indirect evidence may be temptingly available, perhaps the results of previous trials on closely related drugs. Very roughly speaking, the difference between direct and indirect statistical evidence marks the boundary between frequentist and Bayesian thinking. Twentieth-century statistical practice focused heavily on direct evidence, on the grounds of superior objectivity. Now, however, new scientific devices such as microarrays routinely produce enormous data sets involving thousands of related situations, where indirect evidence seems too important to ignore. Empirical Bayes methodology offers an attractive direct/indirect compromise. There is already some evidence of a shift toward a less rigid standard of statistical objectivity that allows better use of indirect evidence. This article is basically the text of a recent talk featuring some examples from current practice, with a little bit of futuristic speculation.|http://arxiv.org/abs/1012.1161v2|Bradley Efron
1157|Sequential estimation for covariate-adjusted response-adaptive designs|In clinical trials, a covariate-adjusted response-adaptive (CARA) design allows a subject newly entering a trial a better chance of being allocated to a superior treatment regimen based on cumulative information from previous subjects, and adjusts the allocation according to individual covariate information.   Since this design allocates subjects sequentially, it is natural to apply a sequential method for estimating the treatment effect in order to make the data analysis more efficient.   In this paper, we study the sequential estimation of treatment effect for a general CARA design. A stopping criterion is proposed such that the estimates satisfy a prescribed precision when the sampling is stopped. The properties of estimates and stopping time} are obtained under the proposed stopping rule. In addition, we show that the asymptotic properties of the allocation function, under the proposed stopping rule, are the same as those obtained in the non-sequential/fixed sample size counterpart.   We then illustrate the performance of the proposed procedure with some simulation results using logistic models. The properties, such as the coverage probability of treatment effect, correct allocation proportion and average sample size, for diverse combinations of initial sample sizes and tuning parameters in the utility function are discussed.|http://arxiv.org/abs/1106.3814v1|Yuan-chin Ivan Chang,Eunsik Park
1158|Q-learning with censored data|We develop methodology for a multistage decision problem with flexible number of stages in which the rewards are survival times that are subject to censoring. We present a novel Q-learning algorithm that is adjusted for censored data and allows a flexible number of stages. We provide finite sample bounds on the generalization error of the policy learned by the algorithm, and show that when the optimal Q-function belongs to the approximation space, the expected survival time for policies obtained by the algorithm converges to that of the optimal policy. We simulate a multistage clinical trial with flexible number of stages and apply the proposed censored-Q-learning algorithm to find individualized treatment regimens. The methodology presented in this paper has implications in the design of personalized medicine trials in cancer and in other life-threatening diseases.|http://arxiv.org/abs/1205.6659v1|Yair Goldberg,Michael R. Kosorok
1159|Testing for Efficacy in Single-Subject Trials with Intervention Analysis|Single subject or n-of-1 research designs have been widely used to evaluate treatment interventions. Many statistical procedures such as split-middle trend lines, regression trend line, Shewart-chart trend line, binomial tests, randomization tests and Tryon C-statistics have been used to analyze single-subject data, but they fail to control Type I error due to serially-dependent time-series observations. The interrupted time series analysis maintains Type I error but assumes that the intervention effect to be a linear trend change from baseline. In this paper, we consider an improved intervention analysis model (Box and Tiao, 1975) for dynamic characteristics of an intervention effect in a short series of single-subject data. The maximum likelihood estimates are derived and a hypothesis testing procedure is proposed. The method is illustrated with a real clinical trial on constraint induced language therapy for aphasia patients.|http://arxiv.org/abs/1403.4309v2|A. Savenkov,S. Wu,D. Neal
1160|Combining nonexchangeable functional or survival data sources in oncology using generalized mixture commensurate priors|Conventional approaches to statistical inference preclude structures that facilitate incorporation of supplemental information acquired from similar circumstances. For example, the analysis of data obtained using perfusion computed tomography to characterize functional imaging biomarkers in cancerous regions of the liver can benefit from partially informative data collected concurrently in noncancerous regions. This paper presents a hierarchical model structure that leverages all available information about a curve, using penalized splines, while accommodating important between-source features. Our proposed methods flexibly borrow strength from the supplemental data to a degree that reflects the commensurability of the supplemental curve with the primary curve. We investigate our method's properties for nonparametric regression via simulation, and apply it to a set of liver cancer data. We also apply our method for a semiparametric hazard model to data from a clinical trial that compares time to disease progression for three colorectal cancer treatments, while supplementing inference with information from a previous trial that tested the current standard of care.|http://arxiv.org/abs/1511.05367v1|Thomas A. Murray,Brian P. Hobbs,Bradley P. Carlin
1161|An efficient multiple imputation algorithm for control-based and delta-adjusted pattern mixture models using SAS|In clinical trials, mixed effects models for repeated measures (MMRM) and pattern mixture models (PMM) are often used to analyze longitudinal continuous outcomes. We describe a simple missing data imputation algorithm for the MMRM that can be easily implemented in standard statistical software packages such as SAS PROC MI. We explore the relationship of the missing data distribution in the control-based and delta-adjusted PMMs with that in the MMRM, and suggest an efficient imputation algorithm for these PMMs. The unobserved values in PMMs can be imputed by subtracting the mean difference in the posterior predictive distributions of missing data from the imputed values in MMRM. We also suggest a modification of the copy reference imputation procedure to avoid the possibility that after dropout, subjects from the active treatment arm will have better mean response trajectory than subjects who stay on the active treatment. The proposed methods are illustrated by the analysis of an antidepressant trial.|http://arxiv.org/abs/1610.03580v1|Yongqiang Tang
1162|Function Driven Diffusion for Personalized Counterfactual Inference|We consider the problem of constructing diffusion operators high dimensional data $X$ to address counterfactual functions $F$, such as individualized treatment effectiveness. We propose and construct a new diffusion metric $K_F$ that captures both the local geometry of $X$ and the directions of variance of $F$. The resulting diffusion metric is then used to define a localized filtration of $F$ and answer counterfactual questions pointwise, particularly in situations such as drug trials where an individual patient's outcomes cannot be studied long term both taking and not taking a medication. We validate the model on synthetic and real world clinical trials, and create individualized notions of benefit from treatment.|http://arxiv.org/abs/1610.10025v5|Alexander Cloninger
1163|A causal modelling framework for reference-based imputation and tipping point analysis|We consider estimating the "de facto" or effectiveness estimand in a randomised placebo-controlled or standard-of-care-controlled drug trial with quantitative outcome, where participants who discontinue an investigational treatment are not followed up thereafter. Carpenter et al (2013) proposed reference-based imputation methods which use a reference arm to inform the distribution of post-discontinuation outcomes and hence to inform an imputation model. However, the reference-based imputation methods were not formally justified. We present a causal model which makes an explicit assumption in a potential outcomes framework about the maintained causal effect of treatment after discontinuation. We show that the "jump to reference", "copy reference" and "copy increments in reference" reference-based imputation methods, with the control arm as the reference arm, are special cases of the causal model with specific assumptions about the causal treatment effect. Results from simulation studies are presented. We also show that the causal model provides a flexible and transparent framework for a tipping point sensitivity analysis in which we vary the assumptions made about the causal effect of discontinued treatment. We illustrate the approach with data from two longitudinal clinical trials.|http://arxiv.org/abs/1705.04506v1|Ian R. White,Royes Joseph,Nicky Best
1164|Instrument-Armed Bandits|We extend the classic multi-armed bandit (MAB) model to the setting of noncompliance, where the arm pull is a mere instrument and the treatment applied may differ from it, which gives rise to the instrument-armed bandit (IAB) problem. The IAB setting is relevant whenever the experimental units are human since free will, ethics, and the law may prohibit unrestricted or forced application of treatment. In particular, the setting is relevant in bandit models of dynamic clinical trials and other controlled trials on human interventions. Nonetheless, the setting has not been fully investigate in the bandit literature. We show that there are various and divergent notions of regret in this setting, all of which coincide only in the classic MAB setting. We characterize the behavior of these regrets and analyze standard MAB algorithms. We argue for a particular kind of regret that captures the causal effect of treatments but show that standard MAB algorithms cannot achieve sublinear control on this regret. Instead, we develop new algorithms for the IAB problem, prove new regret bounds for them, and compare them to standard MAB algorithms in numerical examples.|http://arxiv.org/abs/1705.07377v1|Nathan Kallus
1165|Some methods for heterogeneous treatment effect estimation in high-dimensions|When devising a course of treatment for a patient, doctors often have little quantitative evidence on which to base their decisions, beyond their medical education and published clinical trials. Stanford Health Care alone has millions of electronic medical records (EMRs) that are only just recently being leveraged to inform better treatment recommendations. These data present a unique challenge because they are high-dimensional and observational. Our goal is to make personalized treatment recommendations based on the outcomes for past patients similar to a new patient. We propose and analyze three methods for estimating heterogeneous treatment effects using observational data. Our methods perform well in simulations using a wide variety of treatment effect functions, and we present results of applying the two most promising methods to data from The SPRINT Data Analysis Challenge, from a large randomized trial of a treatment for high blood pressure.|http://arxiv.org/abs/1707.00102v1|Scott Powers,Junyang Qian,Kenneth Jung,Alejandro Schuler,Nigam H. Shah,Trevor Hastie,Robert Tibshirani
1166|Diagnosing Glaucoma Progression with Visual Field Data Using a Spatiotemporal Boundary Detection Method|Diagnosing glaucoma progression is critical for limiting irreversible vision loss. A common method for assessing glaucoma progression uses a longitudinal series of visual fields (VF) acquired at regular intervals. VF data are characterized by a complex spatiotemporal structure due to the data generating process and ocular anatomy. Thus, advanced statistical methods are needed to make clinical determinations regarding progression status. We introduce a spatiotemporal boundary detection model that allows the underlying anatomy of the optic disc to dictate the spatial structure of the VF data across time. We show that our new method provides novel insight into vision loss that improves diagnosis of glaucoma progression using data from the Vein Pulsation Study Trial in Glaucoma and the Lions Eye Institute trial registry. Simulations are presented, showing the proposed methodology is preferred over existing spatial methods for VF data. Supplementary materials for this article are available online and the method is implemented in the R package womblR.|http://arxiv.org/abs/1805.11636v1|Samuel I. Berchuck,Jean-Claude Mwanza,Joshua L. Warren
1167|Joint modelling of progression-free and overall survival and computation of correlation measures|In this paper, we derive the joint distribution of progression-free and overall survival as a function of transition probabilities in a multistate model. No assumptions on copulae or latent event times are needed and the model is allowed to be non-Markov. From the joint distribution, statistics of interest can then be computed. As an example, we provide closed formulas and statistical inference for Pearson's correlation coefficient between progression-free and overall survival in a parametric framework. The example is inspired by recent approaches to quantify the dependence between progression-free survival, a common primary outcome in phase III trials in oncology, and overall survival. We complement these approaches by providing methods of statistical inference while at the same time working within a much more parsimonious modelling framework. Our approach is completely general and can be applied to other measures of dependence. We also discuss extensions to nonparametric inference. Our analytical results are illustrated using a large randomized clinical trial in breast cancer.|http://arxiv.org/abs/1810.10722v1|Matthias Meller,Jan Beyersmann,Kaspar Rufibach
1168|Adaptive multicenter designs for continuous response clinical trials in the presence of an unknown sensitive subgroup|The partial effectiveness of drugs is of importance to the pharmaceutical industry. Randomized controlled trials (RCTs) assuming the existence of a subgroup sensitive to the treatment are already used. These designs, however, are available only if there is a known marker for identifying subjects in the subgroup. In this paper we investigate a model in which the response in the treatment group $Z^T$ has a two-component mixture density $(1-p)\mathcal N(\mu^C, \sigma^2)+p\mathcal N(\mu^T, \sigma^2)$ representing the treatment responses of \emph{placebo responders} and \emph{drug responders}. The treatment-specific effect is $\mu = \frac{\mu^T-\mu^C}{\sigma}$ and $p$ is the prevalence of the drug responders in the population. Other patients in the treatment group react as if they had received a placebo.   We develop one- and two-stage RCT designs that are able to detect a sensitive subgroup based solely on the responses. We also extend them to a multicenter RCTs using Hochberg's step-up procedure. We avoid extensive simulations and use simple and quick numerical optimization methods.|http://arxiv.org/abs/1812.02687v1|Daria Rukina
1169|Estimating Malaria Vaccine Efficacy in the Absence of a Gold Standard Case Definition: Mendelian Factorial Design|Accurate estimates of malaria vaccine efficacy require a reliable definition of a malaria case. However, the symptoms of clinical malaria are unspecific, overlapping with other childhood illnesses. Additionally, children in endemic areas tolerate varying levels of parasitemia without symptoms. Together, this makes finding a gold-standard case definition challenging. We present a method to identify and estimate malaria vaccine efficacy that does not require an observable gold-standard case definition. Instead, we leverage genetic traits that are protective against malaria but not against other illnesses, e.g., the sickle cell trait, to identify vaccine efficacy in a randomized trial. Inspired by Mendelian randomization, we introduce Mendelian factorial design, a method that augments a randomized trial with genetic variation to produce a natural factorial experiment, which identifies vaccine efficacy under realistic assumptions. A robust, covariance adjusted estimation procedure is developed for estimating vaccine efficacy on the risk ratio and incidence ratio scales. Simulations suggest that our estimator has good performance whereas standard methods are systematically biased. We demonstrate that a combined estimator using both our proposed estimator and the standard approach yields significant improvements when the Mendelian factor is only weakly protective.|http://arxiv.org/abs/1908.09425v1|Raiden B. Hasegawa,Dylan S. Small
1170|Assessing effect heterogeneity of a randomized treatment using conditional inference trees|Treatment effect heterogeneity occurs when individual characteristics influence the effect of a treatment. We propose a novel approach that combines prognostic score matching and conditional inference trees to characterize effect heterogeneity of a randomized binary treatment. One key feature that distinguishes our method from alternative approaches is that it controls the Type I error rate, i.e., the probability of identifying effect heterogeneity if none exists and retains the underlying subgroups. This feature makes our technique particularly appealing in the context of clinical trials, where there may be significant costs associated with erroneously declaring that effects differ across population subgroups. TEHTrees are able to identify heterogeneous subgroups, characterize the relevant subgroups and estimate the associated treatment effects. We demonstrate the efficacy of the proposed method using a comprehensive simulation study and illustrate our method using a nutrition trial dataset to evaluate effect heterogeneity within a patient population.|http://arxiv.org/abs/1912.06313v2|Ashwini Venkatasubramaniam,Brandon Koch,Lauren Erickson,Simone French,David Vock,Julian Wolfson
1171|Finite-Sample Two-Group Composite Hypothesis Testing via Machine Learning|In the problem of composite hypothesis testing, identifying the potential uniformly most powerful (UMP) unbiased test is of great interest. Beyond typical hypothesis settings with exponential family, it is usually challenging to prove the existence and further construct such UMP unbiased tests with finite sample size. For example in the COVID-19 pandemic with limited previous assumptions on the treatment for investigation and the standard of care, adaptive clinical trials are appealing due to ethical considerations, and the ability to accommodate uncertainty while conducting the trial. Although several methods have been proposed to control type I error rates, how to find a more powerful hypothesis testing strategy is still an open question. Motivated by this problem, we propose an automatic framework of constructing test statistics and corresponding critical values via machine learning methods to enhance power in a finite sample. In this article, we particularly illustrate the performance using Deep Neural Networks (DNN) and discuss its advantages. Simulations and two case studies of adaptive designs demonstrate that our method is automatic, general and pre-specified to construct statistics with satisfactory power in finite-sample. Supplemental materials are available online including R code and an R shiny app.|http://arxiv.org/abs/1912.07433v2|Tianyu Zhan,Jian Kang
1172|Approximate Bayesian Bootstrap Procedures to Estimate Multilevel Treatment Effects in Observational Studies with Application to Type 2 Diabetes Treatment Regimens|Randomized clinical trials are considered the gold standard for estimating causal effects. Nevertheless, in studies that are aimed at examining adverse effects of interventions, such trials are often impractical because of ethical and financial considerations. In observational studies, matching on the generalized propensity scores was proposed as a possible solution to estimate the treatment effects of multiple interventions. However, the derivation of point and interval estimates for these matching procedures can become complex with non-continuous or censored outcomes. We propose a novel Approximate Bayesian Bootstrap algorithm that result in statistically valid point and interval estimates of the treatment effects with categorical outcomes. The procedure relies on the estimated generalized propensity scores and multiply imputes the unobserved potential outcomes for each unit. In addition, we describe a corresponding interpretable sensitivity analysis to examine the unconfoundedness assumption. We apply this approach to examines the cardiovascular safety of common, real-world anti-diabetic treatment regimens for Type 2 diabetes mellitus in a large observational database.|http://arxiv.org/abs/2001.06125v1|Anthony D. Scotina,Andrew R. Zullo,Robert J. Smith,Roee Gutman
1173|Borrowing from Supplemental Sources to Estimate Causal Effects from a Primary Data Source|The increasing multiplicity of data sources offers exciting possibilities in estimating the effects of a treatment, intervention, or exposure, particularly if observational and experimental sources could be used simultaneously. Borrowing between sources can potentially result in more efficient estimators, but it must be done in a principled manner to mitigate increased bias and Type I error. Furthermore, when the effect of treatment is confounded, as in observational sources or in clinical trials with noncompliance, causal effect estimators are needed to simultaneously adjust for confounding and to estimate effects across data sources. We consider the problem of estimating causal effects from a primary source and borrowing from any number of supplemental sources. We propose using regression-based estimators that borrow based on assuming exchangeability of the regression coefficients and parameters between data sources. Borrowing is accomplished with multisource exchangeability models and Bayesian model averaging. We show via simulation that a Bayesian linear model and Bayesian additive regression trees both have desirable properties and borrow under appropriate circumstances. We apply the estimators to recently completed trials of very low nicotine content cigarettes investigating their impact on smoking behavior.|http://arxiv.org/abs/2003.09680v1|Jeffrey A. Boatman,David M. Vock,Joseph S. Koopmeiners
1174|Unstructured Primary Outcome in Randomized Controlled Trials|The primary outcome of Randomized clinical Trials (RCTs) are typically dichotomous, continuous, multivariate continuous, or time-to-event. However, what if this outcome is unstructured, e.g., a list of variables of mixed types, longitudinal sequences, images, audio recordings, etc. When the outcome is unstructured it is unclear how to assess RCT success and how to compute sample size. We show that kernel methods offer natural extensions to traditional biostatistics methods. We demonstrate our approach with the measurements of computer usage in a cohort of aging participants, some of which will become cognitively impaired. Simulations as well as a real data experiment show the superiority of the proposed approach compared to the standard in this situation: generalized mixed effect models.|http://arxiv.org/abs/2011.12901v1|Daniel Taylor-Rodriguez,David Lovitz,Nora Mattek,Chao-Yi Wu,Hiroko Dodge,Jeffrey Kaye,Bruno M. Jedynak
1175|Nonparametric Analysis of Delayed Treatment Effects using Single-Crossing Constraints|Clinical trials involving novel immuno-oncology (IO) therapies frequently exhibit survival profiles which violate the proportional hazards assumption due to a delay in treatment effect, and in such settings, the survival curves in the two treatment arms may have a crossing before the two curves eventually separate. To flexibly model such scenarios, we describe a nonparametric approach for estimating the treatment arm-specific survival functions which constrains these two survival functions to cross at most once without making any additional assumptions about how the survival curves are related. A main advantage of our approach is that it provides an estimate of a crossing time if such a crossing exists, and moreover, our method generates interpretable measures of treatment benefit including crossing-conditional survival probabilities and crossing-conditional estimates of restricted residual mean life. We demonstrate the use and effectiveness of our approach with a large simulation study and an analysis of reconstructed outcomes from a recent combination-therapy trial.|http://arxiv.org/abs/2102.00409v1|Nicholas C. Henderson,Kijoeng Nam,Dai Feng
1176|Estimating the treatment effect for adherers using multiple imputation|Randomized controlled trials are considered the gold standard to evaluate the treatment effect (estimand) for efficacy and safety. According to the recent International Council on Harmonisation (ICH)-E9 addendum (R1), intercurrent events (ICEs) need to be considered when defining an estimand, and principal stratum is one of the five strategies to handle ICEs. Qu et al. (2020, Statistics in Biopharmaceutical Research 12:1-18) proposed estimators for the adherer average causal effect (AdACE) for estimating the treatment difference for those who adhere to one or both treatments based on the causal-inference framework, and demonstrated the consistency of those estimators; however, this method requires complex custom programming related to high-dimensional numeric integrations. In this article, we implemented the AdACE estimators using multiple imputation (MI) and constructs CI through bootstrapping. A simulation study showed that the MI-based estimators provided consistent estimators with the nominal coverage probabilities of CIs for the treatment difference for the adherent populations of interest. As an illustrative example, the new method was applied to data from a real clinical trial comparing 2 types of basal insulin for patients with type 1 diabetes.|http://arxiv.org/abs/2102.03499v2|Junxiang Luo,Stephen J. Ruberg,Yongming Qu
1177|On the Evaluation of Surrogate Markers in Real World Data Settings|Shortcomings of randomized clinical trials are pronounced in urgent health crises, when rapid identification of effective treatments is critical. Leveraging short-term surrogates in real-world data (RWD) can guide policymakers evaluating new treatments. In this paper, we develop novel estimators for the proportion of treatment effect (PTE) on the true outcome explained by a surrogate in RWD settings. We propose inverse probability weighted and doubly robust (DR) estimators of an optimal transformation of the surrogate and PTE by semi-nonparametrically modeling the relationship between the true outcome and surrogate given baseline covariates. We show that our estimators are consistent and asymptotically normal, and the DR estimator is consistent when either the propensity score model or outcome regression model is correctly specified. We compare our proposed estimators to existing estimators and show a reduction in bias and gains in efficiency through simulations. We illustrate the utility of our method in obtaining an interpretable PTE by conducting a cross-trial comparison of two biologic therapies for ulcerative colitis.|http://arxiv.org/abs/2104.05513v1|Larry Han,Xuan Wang,Tianxi Cai
1178|Evidence Aggregation for Treatment Choice|Consider a planner who has limited knowledge of the policy's causal impact on a certain local population of interest due to a lack of data, but does have access to the publicized intervention studies performed for similar policies on different populations. How should the planner make use of and aggregate this existing evidence to make her policy decision? Following Manski (2020; Towards Credible Patient-Centered Meta-Analysis, \textit{Epidemiology}), we formulate the planner's problem as a statistical decision problem with a social welfare objective, and solve for an optimal aggregation rule under the minimax-regret criterion. We investigate the analytical properties, computational feasibility, and welfare regret performance of this rule. We apply the minimax regret decision rule to two settings: whether to enact an active labor market policy based on 14 randomized control trial studies; and whether to approve a drug (Remdesivir) for COVID-19 treatment using a meta-database of clinical trials.|http://arxiv.org/abs/2108.06473v2|Takuya Ishihara,Toru Kitagawa
1179|Familywise error rate control for block response-adaptive randomization|Response-adaptive randomization allows the probabilities of allocating patients to treatments in a clinical trial to change based on the previously observed response data, in order to achieve different experimental goals. One concern over the use of such designs in practice, particularly from a regulatory viewpoint, is controlling the type I error rate. To address this, Robertson and Wason (Biometrics, 2019) proposed methodology that guarantees familywise error rate control for a large class of response-adaptive designs. In this paper, we propose an improvement of their proposal that is conceptually simpler, in the specific context of block-randomised trials with a fixed allocation to the control arm. We show the modified method guarantees that there will never be negative weights for blocks of data, and can also provide a substantial power advantage in practice.|http://arxiv.org/abs/2204.05734v1|Ekkehard Glimm,David Robertson
1180|Combining experimental and observational data through a power likelihood|Randomized controlled trials are the gold standard for causal inference and play a pivotal role in modern evidence-based medicine. However, the sample sizes they use are often too limited to draw significant causal conclusions for subgroups that are less prevalent in the population. In contrast, observational data are becoming increasingly accessible in large volumes but can be subject to bias as a result of hidden confounding. Given these complementary features, we propose a power likelihood approach to augmenting RCTs with observational data to improve the efficiency of treatment effect estimation. We provide a data-adaptive procedure for maximizing the expected log predictive density (ELPD) to select the learning rate that best regulates the information from the observational data. We validate our method through a simulation study that shows increased power while maintaining an approximate nominal coverage rate. Finally, we apply our method in a real-world data fusion study augmenting the PIONEER 6 clinical trial with a US health claims dataset, demonstrating the effectiveness of our method and providing detailed guidance on how to address practical considerations in its application.|http://arxiv.org/abs/2304.02339v2|Xi Lin,Jens Magelund Tarp,Robin J. Evans
1181|Bayesian Sequentially Monitored Multi-arm Experiments with Multiple Comparison Adjustments|Randomized experiments play a major role in data-driven decision making across many different fields and disciplines. In medicine, for example, randomized controlled trials (RCTs) are the backbone of clinical trial methodology for testing the efficacy of new drugs and therapies versus existing treatments or placebo. In business and marketing, randomized experiments are typically referred to as A/B tests when there are only two arms, or variants, in the experiment, and as multivariate A/B tests when there are more than two arms. Typical applications of A/B tests include comparing the effectiveness of different ad campaigns, evaluating how people respond to different website layouts, or comparing different customer subpopulations to each other.   This paper focuses on multivariate A/B testing from a digital marketing perspective, and presents a method for the sequential monitoring of such experiments while accounting for the issue of multiple comparisons. In adapting and combining the methods of two previous works, the method presented herein is straightforward to implement using standard statistical software and performs quite well in various simulation studies, exhibiting better power and smaller average sample sizes than comparable methods.|http://arxiv.org/abs/1608.08076v1|Andrew W. Correia
1182|A Matching Procedure for Sequential Experiments that Iteratively Learns which Covariates Improve Power|We propose a dynamic allocation procedure that increases power and efficiency when measuring an average treatment effect in sequential randomized trials exploiting some subjects' previous assessed responses. Subjects arrive sequentially and are either randomized or paired to a previously randomized subject and administered the alternate treatment. The pairing is made via a dynamic matching criterion that iteratively learns which specific covariates are important to the response. We develop estimators for the average treatment effect as well as an exact test. We illustrate our method's increase in efficiency and power over other allocation procedures in both simulated scenarios and a clinical trial dataset. An R package "SeqExpMatch" for use by practitioners is available.|http://arxiv.org/abs/2010.05980v2|Adam Kapelner,Abba Krieger
1183|Prospect for application of mathematical models in combination cancer treatments|The long-term efficacy of targeted therapeutics for cancer treatment can be significantly limited by the type of therapy and development of drug resistance, inter alia. Experimental studies indicate that the factors enhancing acquisition of drug resistance in cancer cells include cell heterogeneity, drug target alteration, drug inactivation, DNA damage repair, drug efflux, cell death inhibition, as well as microenvironmental adaptations to targeted therapy, among others. Combination cancer therapies (CCTs) are employed to overcome these molecular and pathophysiological bottlenecks and improve the overall survival of cancer patients. CCTs often utilize multiple combinatorial modes of action and thus potentially constitute a promising approach to overcome drug resistance. Considering the colossal cost, human effort, time and ethical issues involved in clinical drug trials and basic medical research, mathematical modeling and analysis can potentially contribute immensely to the discovery of better cancer treatment regimens. In this article, we review mathematical models on CCTs developed thus far for cancer management. Open questions are highlighted and plausible combinations are discussed based on the level of toxicity, drug resistance, survival benefits, preclinical trials and other side effects.|http://arxiv.org/abs/2012.00683v2|Joseph Malinzi,Kevin Bosire Basita,Sara Padidar,Henry A. Adeola
1184|Sample size re-estimation in Phase 2 Dose-Finding: Conditional power vs. Bayesian predictive power|Unblinded sample size re-estimation (SSR) is often planned in a clinical trial when there is large uncertainty about the true treatment effect. For Proof-of Concept (PoC) in a Phase II dose finding study, contrast test can be adopted to leverage information from all treatment groups. In this article, we propose two-stage SSR designs using frequentist conditional power and Bayesian posterior predictive power for both single and multiple contrast tests. The Bayesian SSR can be implemented under a wide range of prior settings to incorporate different prior knowledge. Taking the adaptivity into account, all type I errors of final analysis in this paper are rigorously protected. Simulation studies are carried out to demonstrate the advantages of unblinded SSR in multi-arm trials.|http://arxiv.org/abs/2012.14589v3|Qingyang Liu,Guanyu Hu,Binqi Ye,Susan Wang,Yaoshi Wu
1185|Improved inference for vaccine-induced immune responses via shape-constrained methods|We study the performance of shape-constrained methods for evaluating immune response profiles from early-phase vaccine trials. The motivating problem for this work involves quantifying and comparing the IgG binding immune responses to the first and second variable loops (V1V2 region) arising in HVTN 097 and HVTN 100 HIV vaccine trials. We consider unimodal and log-concave shape-constrained methods to compare the immune profiles of the two vaccines, which is reasonable because the data support that the underlying densities of the immune responses could have these shapes. To this end, we develop novel shape-constrained tests of stochastic dominance and shape-constrained plug-in estimators of the Hellinger distance between two densities. Our techniques are either tuning parameter free, or rely on only one tuning parameter, but their performance is either better (the tests of stochastic dominance) or comparable with the nonparametric methods (the estimators of Hellinger distance). The minimal dependence on tuning parameters is especially desirable in clinical contexts where analyses must be prespecified and reproducible. Our methods are supported by theoretical results and simulation studies.|http://arxiv.org/abs/2107.11546v3|Nilanjana Laha,Zoe Moodie,Ying Huang,Alex Luedtke
1186|Standard and reference-based conditional mean imputation|Clinical trials with longitudinal outcomes typically include missing data due to missed assessments or structural missingness of outcomes after intercurrent events handled with a hypothetical strategy. Approaches based on Bayesian random multiple imputation and Rubin's rule for pooling results across multiple imputed datasets are increasingly used in order to align the analysis of these trials with the targeted estimand. We propose and justify deterministic conditional mean imputation combined with the jackknife for inference as an alternative approach. The method is applicable to imputations under a missing-at-random assumption as well as for reference-based imputation approaches. In an application and a simulation study, we demonstrate that it provides consistent treatment effect estimates with the Bayesian approach and reliable frequentist inference with accurate standard error estimation and type I error control. A further advantage of the method is that it does not rely on random sampling and is therefore replicable and unaffected by Monte Carlo error.|http://arxiv.org/abs/2109.11162v3|Marcel Wolbers,Alessandro Noci,Paul Delmar,Craig Gower-Page,Sean Yiu,Jonathan W. Bartlett
1187|Product recalls, market size and innovation in the pharmaceutical industry|The idea that research investments respond to market rewards is well established in the literature on markets for innovation (Schmookler, 1966; Acemoglu and Linn, 2004; Bryan and Williams, 2021). Empirical evidence tells us that a change in market size, such as the one measured by demographical shifts, is associated with an increase in the number of new drugs available (Acemoglu and Linn, 2004; Dubois et al., 2015). However, the debate about potential reverse causality is still open (Cerda et al., 2007). In this paper we analyze market size's effect on innovation as measured by active clinical trials. The idea is to exploit product recalls an innovative instrument tested to be sharp, strong, and unexpected. The work analyses the relationship between US market size and innovation at ATC-3 level through an original dataset and the two-step IV methodology proposed by Wooldridge et al. (2019). The results reveal a robust and significantly positive response of number of active trials to market size.|http://arxiv.org/abs/2111.15389v1|Federico Nutarelli,Massimo Riccaboni,Andrea Morescalchi
1188|Statistical Methods for Accommodating Immortal Time: A Selective Review and Comparison|Epidemiologic studies and clinical trials with a survival outcome are often challenged by immortal time (IMT), a period of follow-up during which the survival outcome cannot occur because of the observed later treatment initiation. It has been well recognized that failing to properly accommodate IMT leads to biased estimation and misleading inference. Accordingly, a series of statistical methods have been developed, from the simplest by including or excluding IMT to various weightings and the more recent sequential methods. Our literature review suggests that the existing developments are often "scattered", and there is a lack of comprehensive review and direct comparison. To fill this knowledge gap and better introduce this important topic especially to biomedical researchers, we provide this review to comprehensively describe the available methods, discuss their advantages and disadvantages, and equally important, directly compare their performance via simulation and the analysis of the Stanford heart transplant data. The key observation is that the time-varying treatment modeling and sequential trial methods tend to provide unbiased estimation, while the other methods may result in substantial bias. We also provide an in-depth discussion on the interconnections with causal inference.|http://arxiv.org/abs/2202.02369v1|Jiping Wang,Peter Peduzzi,Michael Wininger,Shuangge Ma
1189|A comparison of priors for variance parameters in Bayesian basket trials|Phase II basket trials are popular tools to evaluate efficacy of a new treatment targeting genetic alteration common to a set of different cancer histologies. Efficient designs are obtained by pooling data from the different arms (e.g., cancer histologies) via Bayesian hierarchical modelling, with a variance parameter controlling the strength of shrinkage of each arm treatment effect to the overall treatment effect. One critical aspect of this approach is that prior choice on the variance plays a major role in determining the strength of shrinkage and impacts the operating characteristics of the design. We review the priors most commonly adopted in previous works and compare them with the recently introduced penalized complexity (PC) priors. Our simulation study shows comparable behaviour for the PC prior and the gold standard choice half-t prior, with the former performing better in the homogeneous scenario where all histologies respond similarly to the treatment. We argue that PC priors offer advantages over other priors because they allow the user to handle the degree of shrinkage by means of only one parameter and can be elicited based on clinical opinion when available.|http://arxiv.org/abs/2210.16685v1|Massimo Ventrucci,Alessandro Vagheggini
1190|Modeling and Predictive Control for the Treatment of Hyperthyroidism|In this work, we propose an approach to determine the dosages of antithyroid agents to treat hyperthyroid patients. Instead of relying on a trial-and-error approach as it is commonly done in clinical practice, we suggest to determine the dosages by means of a model predictive control (MPC) scheme. To this end, we first extend a mathematical model of the pituitary-thyroid feedback loop such that the intake of methimazole, a common antithyroid agent, can be considered. Second, based on the extended model, we develop an MPC scheme to determine suitable dosages. In numerical simulations, we consider scenarios in which (i) patients are affected by Graves' disease and take the medication orally and (ii) patients suffering from a life-threatening thyrotoxicosis, in which the medication is usually given intravenously. Our conceptual study suggests that determining the medication dosages by means of an MPC scheme could be a promising alternative to the currently applied trial-and-error approach.|http://arxiv.org/abs/2212.10096v3|Tobias M. Wolff,Maylin Menzel,Johannes W. Dietrich,Matthias A. Mller
1191|Principal Stratification with Time-to-Event Outcomes|Post-randomization events, also known as intercurrent events, such as treatment noncompliance and censoring due to a terminal event, are common in clinical trials. Principal stratification is a framework for causal inference in the presence of intercurrent events. Despite the extensive existing literature, there lacks generally applicable and accessible methods for principal stratification analysis with time-to-event outcomes. In this paper, we specify two causal estimands for time-to-event outcomes in principal stratification. For estimation, we adopt the general strategy of latent mixture modeling and derive the corresponding likelihood function. For computational convenience, we illustrate the general strategy with a mixture of Bayesian parametric Weibull-Cox proportional model for the outcome. We utilize the Stan programming language to obtain automatic posterior sampling of the model parameters via the Hamiltonian Monte Carlo. We provide the analytical forms of the causal estimands as functions of the model parameters and an alternative numerical method when analytical forms are not available. We apply the proposed method to the ADAPTABLE trial to evaluate the causal effect of taking 81 mg versus 325 mg aspirin on the risk of major adverse cardiovascular events.|http://arxiv.org/abs/2301.07672v1|Bo Liu,Lisa Wruck,Fan Li
1192|Chromium Supplementation And The Essentiality Of Chromium To Human Nutrition: A Narrative Review|This narrative review evaluates the effect of chromium supplementation on glycemia and serum lipids, with an emphasis on patients with type 2 diabetes mellitus (T2DM). Additionally, this narrative review evaluates the essentiality of the trace mineral chromium to human nutrition. Meta-analyses and reviews were included, while certain clinical trials were specifically included to discuss flaws or impact. Overall, this narrative review concludes that chromium supplementation likely has no beneficial effect on glycemia or serum lipids (in subjects with or without T2DM). This narrative review also concludes the essentiality of chromium to human nutrition has become increasingly challenged over time, with some investigators postulating that chromium is pharmacologically active rather than an essential trace mineral. However, further randomized controlled trials (RCTs) are necessary to come to solid conclusions about the effect of chromium supplementation and the essentiality of chromium to human nutrition. This investigation is necessary as manufacturers continue to market chromium supplements as helpful aids to T2DM patients based on flawed studies.|http://arxiv.org/abs/2309.10820v1|Matthew Tirona
1193|Power and sample size calculation of two-sample projection-based testing for sparsely observed functional data|Projection-based testing for mean trajectory differences in two groups of irregularly and sparsely observed functional data has garnered significant attention in the literature because it accommodates a wide spectrum of group differences and (non-stationary) covariance structures. This article presents the derivation of the theoretical power function and the introduction of a comprehensive power and sample size (PASS) calculation toolkit tailored to the projection-based testing method developed by Wang (2021). Our approach accommodates a wide spectrum of group difference scenarios and a broad class of covariance structures governing the underlying processes. Through extensive numerical simulation, we demonstrate the robustness of this testing method by showcasing that its statistical power remains nearly unaffected even when a certain percentage of observations are missing, rendering it 'missing-immune'. Furthermore, we illustrate the practical utility of this test through analysis of two randomized controlled trials of Parkinson's disease. To facilitate implementation, we provide a user-friendly R package fPASS, complete with a detailed vignette to guide users through its practical application. We anticipate that this article will significantly enhance the usability of this potent statistical tool across a range of biostatistical applications, with a particular focus on its relevance in the design of clinical trials.|http://arxiv.org/abs/2310.06252v1|Salil Koner,Sheng Luo
1194|Hidden yet quantifiable: A lower bound for confounding strength using randomized trials|In the era of fast-paced precision medicine, observational studies play a major role in properly evaluating new treatments in clinical practice. Yet, unobserved confounding can significantly compromise causal conclusions drawn from non-randomized data. We propose a novel strategy that leverages randomized trials to quantify unobserved confounding. First, we design a statistical test to detect unobserved confounding with strength above a given threshold. Then, we use the test to estimate an asymptotically valid lower bound on the unobserved confounding strength. We evaluate the power and validity of our statistical test on several synthetic and semi-synthetic datasets. Further, we show how our lower bound can correctly identify the absence and presence of unobserved confounding in a real-world setting.|http://arxiv.org/abs/2312.03871v3|Piersilvio De Bartolomeis,Javier Abad,Konstantin Donhauser,Fanny Yang
1195|Optimal Adaptive SMART Designs with Binary Outcomes|In a sequential multiple-assignment randomized trial (SMART), a sequence of treatments is given to a patient over multiple stages. In each stage, randomization may be done to allocate patients to different treatment groups. Even though SMART designs are getting popular among clinical researchers, the methodologies for adaptive randomization at different stages of a SMART are few and not sophisticated enough to handle the complexity of optimal allocation of treatments at every stage of a trial. Lack of optimal allocation methodologies can raise serious concerns about SMART designs from an ethical point of view. In this work, we develop an optimal adaptive allocation procedure to minimize the expected number of treatment failures for a SMART with a binary primary outcome. Issues related to optimal adaptive allocations are explored theoretically with supporting simulations. The applicability of the proposed methodology is demonstrated using a recently conducted SMART study named M-Bridge for developing universal and resource-efficient dynamic treatment regimes (DTRs) for incoming first-year college students as a bridge to desirable treatments to address alcohol-related risks.|http://arxiv.org/abs/2401.03326v1|Rik Ghosh,Bibhas Chakraborty,Inbal Nahum-Shani,Megan E. Patrick,Palash Ghosh
1196|A Unified Three-State Model Framework for Analysis of Treatment Crossover in Survival Trials|We present a unified three-state model (TSM) framework for evaluating treatment effects in clinical trials in the presence of treatment crossover. Researchers have proposed diverse methodologies to estimate the treatment effect that would have hypothetically been observed if treatment crossover had not occurred. However, there is little work on understanding the connections between these different approaches from a statistical point of view. The proposed TSM framework unifies existing methods, effectively identifying potential biases, model assumptions, and inherent limitations for each method. This can guide researchers in understanding when these methods are appropriate and choosing a suitable approach for their data. The TSM framework also facilitates the creation of new methods to adjust for confounding effects from treatment crossover. To illustrate this capability, we introduce a new imputation method that falls under its scope. Through simulation experiments, we demonstrate the performance of different approaches for estimating the treatment effects. Codes for implementing the methods within the TSM framework are available at https://github.com/JasonZhao111/TSM.|http://arxiv.org/abs/2401.17008v2|Zile Zhao,Ye Li,Xiaodong Luo,Ray Bai
1197|On two-sample testing for data with arbitrarily missing values|We develop a new rank-based approach for univariate two-sample testing in the presence of missing data which makes no assumptions about the missingness mechanism. This approach is a theoretical extension of the Wilcoxon-Mann-Whitney test that controls the Type I error by providing exact bounds for the test statistic after accounting for the number of missing values. Greater statistical power is shown when the method is extended to account for a bounded domain. Furthermore, exact bounds are provided on the proportions of data that can be missing in the two samples while yielding a significant result. Simulations demonstrate that our method has good power, typically for cases of $10\%$ to $20\%$ missing data, while standard imputation approaches fail to control the Type I error. We illustrate our method on complex clinical trial data in which patients' withdrawal from the trial lead to missing values.|http://arxiv.org/abs/2403.15327v1|Yijin Zeng,Niall M. Adams,Dean A. Bodenham
1198|A Bayesian Estimator of Sample Size|We consider a Bayesian estimator of sample size (BESS) and an application to oncology dose optimization clinical trials. BESS is built upon three pillars, Sample size, Evidence from observed data, and Confidence in posterior inference. It uses a simple logic of "given the evidence from data, a specific sample size can achieve a degree of confidence in the posterior inference." The key distinction between BESS and standard sample size estimation (SSE) is that SSE, typically based on Frequentist inference, specifies the true parameters values in its calculation while BESS assumes possible outcome from the observed data. As a result, the calibration of the sample size is not based on type I or type II error rates, but on posterior probabilities. We demonstrate that BESS leads to a more interpretable statement for investigators, and can easily accommodates prior information as well as sample size re-estimation. We explore its performance in comparison to the standard SSE and demonstrate its usage through a case study of oncology optimization trial. BESS can be applied to general hypothesis tests. An R tool is available at https://ccte.uchicago.edu/BESS.|http://arxiv.org/abs/2404.07923v2|Dehua Bi,Yuan Ji
1199|Pharmacokinetic Measurements in Dose Finding Model Guided by Escalation with Overdose Control|Oncology drug development starts with a dose escalation phase to find the maximal tolerable dose (MTD). Dose limiting toxicity (DLT) is the primary endpoint for dose escalation phase. Traditionally, model-based dose escalation trial designs recommend a dose for escalation based on an assumed dose-DLT relationship. Pharmacokinetic (PK) data are often available but are currently only used by clinical teams in a subjective manner to aid decision making. Formal incorporation of PK data in dose-escalation models can make the decision process more efficient and lead to an increase in precision. In this talk we present a Bayesian joint modeling framework for incorporating PK data in Oncology dose escalation trials. This framework explores the dose-PK and PK-DLT relationships jointly for better model informed dose escalation decisions. Utility of the proposed model is demonstrated through a real-life case study along with simulation.|http://arxiv.org/abs/2404.11406v1|Arnab Kumar Maity,Satrajit Roy Chowdhury,Ray Li,Lada Markovtsova,Roberto Bugarini
1200|Detecting critical treatment effect bias in small subgroups|Randomized trials are considered the gold standard for making informed decisions in medicine, yet they often lack generalizability to the patient populations in clinical practice. Observational studies, on the other hand, cover a broader patient population but are prone to various biases. Thus, before using an observational study for decision-making, it is crucial to benchmark its treatment effect estimates against those derived from a randomized trial. We propose a novel strategy to benchmark observational studies beyond the average treatment effect. First, we design a statistical test for the null hypothesis that the treatment effects estimated from the two studies, conditioned on a set of relevant features, differ up to some tolerance. We then estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup in the observational study. Finally, we validate our benchmarking strategy in a real-world setting and show that it leads to conclusions that align with established medical knowledge.|http://arxiv.org/abs/2404.18905v2|Piersilvio De Bartolomeis,Javier Abad,Konstantin Donhauser,Fanny Yang
1201|Transportability of Principal Causal Effects|Recent research in causal inference has made important progress in addressing challenges to the external validity of trial findings. Such methods weight trial participant data to more closely resemble the distribution of effect-modifying covariates in a well-defined target population. In the presence of participant non-adherence to study medication, these methods effectively transport an intention-to-treat effect that averages over heterogeneous compliance behaviors. In this paper, we develop a principal stratification framework to identify causal effects conditioning on both compliance behavior and membership in the target population. We also develop non-parametric efficiency theory for and construct efficient estimators of such "transported" principal causal effects and characterize their finite-sample performance in simulation experiments. While this work focuses on treatment non-adherence, the framework is applicable to a broad class of estimands that target effects in clinically-relevant, possibly latent subsets of a target population.|http://arxiv.org/abs/2405.04419v2|Justin M. Clark,Kollin W. Rott,James S. Hodges,Jared D. Huling
1202|Starting Small: Prioritizing Safety over Efficacy in Randomized Experiments Using the Exact Finite Sample Likelihood|We use the exact finite sample likelihood and statistical decision theory to answer questions of ``why?'' and ``what should you have done?'' using data from randomized experiments and a utility function that prioritizes safety over efficacy. We propose a finite sample Bayesian decision rule and a finite sample maximum likelihood decision rule. We show that in finite samples from 2 to 50, it is possible for these rules to achieve better performance according to established maximin and maximum regret criteria than a rule based on the Boole-Frechet-Hoeffding bounds. We also propose a finite sample maximum likelihood criterion. We apply our rules and criterion to an actual clinical trial that yielded a promising estimate of efficacy, and our results point to safety as a reason for why results were mixed in subsequent trials.|http://arxiv.org/abs/2407.18206v1|Neil Christy,A. E. Kowalski
1203|Bootstrap Matching: a robust and efficient correction for non-random A/B test, and its applications|A/B testing, a widely used form of Randomized Controlled Trial (RCT), is a fundamental tool in business data analysis and experimental design. However, despite its intent to maintain randomness, A/B testing often faces challenges that compromise this randomness, leading to significant limitations in practice. In this study, we introduce Bootstrap Matching, an innovative approach that integrates Bootstrap resampling, Matching techniques, and high-dimensional hypothesis testing to address the shortcomings of A/B tests when true randomization is not achieved. Unlike traditional methods such as Difference-in-Differences (DID) and Propensity Score Matching (PSM), Bootstrap Matching is tailored for large-scale datasets, offering enhanced robustness and computational efficiency. We illustrate the effectiveness of this methodology through a real-world application in online advertising and further discuss its potential applications in digital marketing, empirical economics, clinical trials, and high-dimensional bioinformatics.|http://arxiv.org/abs/2408.05297v1|Zihao Zheng,Carol Liu
1204|Development of COVID-19 Booster Vaccine Policy by Microsimulation and Q-learning|The COVID-19 pandemic highlighted the urgent need for effective vaccine policies, but traditional clinical trials often lack sufficient data to capture the diverse population characteristics necessary for comprehensive public health strategies. Ethical concerns around randomized trials during a pandemic further complicate policy development for public health. Reinforcement Learning (RL) offers a promising alternative for vaccine policy development. However, direct online RL exploration in real-world scenarios can result in suboptimal and potentially harmful decisions. This study proposes a novel framework combining tabular Q-learning with microsimulation (i.e., a Recurrent Neural Network (RNN) environment simulator) to address these challenges in public health vaccine policymaking, which enables effective vaccine policy learning without real-world interaction, addressing both ethical and exploration challenges. The RNN environment simulator captures temporal associations between infection and patient characteristics, generating realistic simulation data. Our tabular Q-learning model produces an interpretable policy table that balances the risks of severe infection against vaccination side effects. Applied to COVID-19 booster policies, the learned Q-learning-based policy outperforms current practices, offering a path toward more effective vaccination strategies.|http://arxiv.org/abs/2410.12936v2|Guoxuan Ma,Lili Zhao,Jian Kang
1205|Semiparametric principal stratification analysis beyond monotonicity|Intercurrent events, common in clinical trials and observational studies, affect the existence or interpretation of final outcomes. Principal stratification addresses these challenges by defining local average treatment effects within latent subpopulations, but often relies on restrictive assumptions such as monotonicity and counterfactual intermediate independence. To address these limitations, we propose a unified semiparametric framework for principal stratification analysis leveraging a margin-free, conditional odds ratio sensitivity parameter. Under principal ignorability, we derive nonparametric identification formulas and develop efficient estimation methods, including a conditionally doubly robust parametric estimator and a de-biased machine learning estimator with data-adaptive nuisance estimators. Simulations show that incorrectly assuming monotonicity can often lead to suboptimal inference, while specifying non-trivial odds ratio sensitivity parameter can enable approximately valid inference under monotonicity. We apply our methods to a critical care trial and further suggest a semiparametric sensitivity analysis approach under violation of principal ignorability.|http://arxiv.org/abs/2501.17514v1|Jiaqi Tong,Brennan Kahan,Michael O. Harhay,Fan Li
1206|Adapting Machine Learning Diagnostic Models to New Populations Using a Small Amount of Data: Results from Clinical Neuroscience|Machine learning (ML) has shown great promise for revolutionizing a number of areas, including healthcare. However, it is also facing a reproducibility crisis, especially in medicine. ML models that are carefully constructed from and evaluated on a training set might not generalize well on data from different patient populations or acquisition instrument settings and protocols. We tackle this problem in the context of neuroimaging of Alzheimer's disease (AD), schizophrenia (SZ) and brain aging. We develop a weighted empirical risk minimization approach that optimally combines data from a source group, e.g., subjects are stratified by attributes such as sex, age group, race and clinical cohort to make predictions on a target group, e.g., other sex, age group, etc. using a small fraction (10%) of data from the target group. We apply this method to multi-source data of 15,363 individuals from 20 neuroimaging studies to build ML models for diagnosis of AD and SZ, and estimation of brain age. We found that this approach achieves substantially better accuracy than existing domain adaptation techniques: it obtains area under curve greater than 0.95 for AD classification, area under curve greater than 0.7 for SZ classification and mean absolute error less than 5 years for brain age prediction on all target groups, achieving robustness to variations of scanners, protocols, and demographic or clinical characteristics. In some cases, it is even better than training on all data from the target group, because it leverages the diversity and size of a larger training set. We also demonstrate the utility of our models for prognostic tasks such as predicting disease progression in individuals with mild cognitive impairment. Critically, our brain age prediction models lead to new clinical insights regarding correlations with neurophysiological tests.|http://arxiv.org/abs/2308.03175v2|Rongguang Wang,Guray Erus,Pratik Chaudhari,Christos Davatzikos
1207|A threshold-free summary index for quantifying the capacity of covariates to yield efficient treatment rules|The focus of this paper is on quantifying the capacity of covariates in devising efficient treatment rules when data from a randomized trial are available. Conventional one-variable-at-a-time subgroup analysis based on statistical hypothesis testing of covariate-by-treatment interaction is ill-suited for this purpose. The application of decision theory results in treatment rules that compare the expected benefit of treatment given the patient's covariates against a treatment threshold. However, determining treatment threshold is often context-specific, and any given threshold might seem arbitrary at the reporting stages of a clinical trial. We propose a threshold-free metric that quantifies the capacity of a set of covariates towards finding individuals who will benefit the most from treatment. The construct of the proposed metric is comparing the expected outcomes with and without knowledge of covariates when one of a two randomly selected patients are to be treated. We show that the resulting index can also be expressed in terms of integrated treatment benefit as a function of covariates over the entire range of treatment thresholds. We also propose a semi-parametric estimation method suitable for out-of-sample validation and adjustment for optimism. We use data from a clinical trial of preventive antibiotic therapy for reducing exacerbation rate in Chronic Obstructive Pulmonary Disease to demonstrate the calculations in a step-by-step fashion. The proposed index has intuitive and theoretically sound interpretation and can be estimated with relative ease for a wide class of regression models. Beyond the conceptual developments presented in this work, various aspects of estimation and inference for such metrics need to be pursued in future research.|http://arxiv.org/abs/1901.05124v3|Mohsen Sadatsafavi,Mohammad Mansournia,Paul Gustafson
1208|Covariate adjustment in subgroup analyses of randomized clinical trials: A propensity score approach|Background: Subgroup analyses are frequently conducted in randomized clinical trials to assess evidence of heterogeneous treatment effect across patient subpopulations. Although randomization balances covariates within subgroups in expectation, chance imbalance may be amplified in small subgroups and harm the precision. Covariate adjustment in overall analysis of RCT is often conducted, via either ANCOVA or propensity score weighting, but for subgroup analysis has been rarely discussed. In this article, we develop propensity score weighting methodology for covariate adjustment to improve the precision and power of subgroup analyses in RCTs. Methods: We extend the propensity score weighting methodology to subgroup analyses by fitting a logistic propensity model with pre-specified covariate-subgroup interactions. We show that, by construction, overlap weighting exactly balances the covariates with interaction terms in each subgroup. Extensive simulations were performed to compare the operating characteristics of unadjusted, different propensity score weighting and the ANCOVA estimator. We apply these methods to the HF-ACTION trial to evaluate the effect of exercise training on 6-minute walk test in pre-specified subgroups. Results: Standard errors of the adjusted estimators are smaller than those of the unadjusted estimator. The propensity score weighting estimator is as efficient as ANCOVA, and is often more efficient when subgroup sample size is small (e.g.<125), and/or when outcome model is misspecified. The weighting estimators with full-interaction propensity model consistently outperform the standard main-effect propensity model. Conclusion: Propensity score weighting is a transparent and objective method to adjust chance imbalance of important covariates in subgroup analyses of RCTs. It is crucial to include the full covariate-subgroup interactions in the propensity score model.|http://arxiv.org/abs/2102.02285v4|Siyun Yang,Fan Li,Laine E. Thomas,Fan Li
1209|Survival analysis for AdVerse events with VarYing follow-up times (SAVVY) -- comparison of adverse event risks in randomized controlled trials|Analyses of adverse events (AEs) are an important aspect of the evaluation of experimental therapies. The SAVVY (Survival analysis for AdVerse events with Varying follow-up times) project aims to improve the analyses of AE data in clinical trials through the use of survival techniques appropriately dealing with varying follow-up times, censoring, and competing events (CE). In an empirical study including seventeen randomized clinical trials the effect of varying follow-up times, censoring, and competing events on comparisons of two treatment arms with respect to AE risks is investigated. The comparisons of relative risks (RR) of standard probability-based estimators to the gold-standard Aalen-Johansen estimator or hazard-based estimators to an estimated hazard ratio (HR) from Cox regression are done descriptively, with graphical displays, and using a random effects meta-analysis on AE level. The influence of different factors on the size of the bias is investigated in a meta-regression. We find that for both, avoiding bias and categorization of evidence with respect to treatment effect on AE risk into categories, the choice of the estimator is key and more important than features of the underlying data such as percentage of censoring, CEs, amount of follow-up, or value of the gold-standard RR. There is an urgent need to improve the guidelines of reporting AEs so that incidence proportions are finally replaced by the Aalen-Johansen estimator - rather than by Kaplan-Meier - with appropriate definition of CEs. For RRs based on hazards, the HR based on Cox regression has better properties than the ratio of incidence densities.|http://arxiv.org/abs/2008.07881v2|Kaspar Rufibach,Regina Stegherr,Claudia Schmoor,Valentine Jehl,Arthur Allignol,Annette Boeckenhoff,Cornelia Dunger-Baldauf,Lewin Eisele,Thomas Knzel,Katrin Kupas,Friedhelm Leverkus,Matthias Trampisch,Yumin Zhao,Tim Friede,Jan Beyersmann
1210|Statistical analyses of ordinal outcomes in randomised controlled trials: protocol for a scoping review|Randomised controlled trials aim to assess the impact of one (or more) health interventions relative to other standard interventions. RCTs sometimes use an ordinal outcome, which is an endpoint that comprises of multiple, monotonically ordered categories that are not necessarily separated by a quantifiable distance. Ordinal outcomes are appealing in clinical settings as disease states can represent meaningful categories that may be of clinical importance. They can also retain information and increase statistical power compared to dichotomised outcomes. Target parameters for ordinal outcomes in RCTs may vary depending on the nature of the research question, the modelling assumptions, and the expertise of the data analyst. The aim of this scoping review is to systematically describe the use of ordinal outcomes in contemporary RCTs. Specifically, we aim to (i) identify which target parameters are of interest in trials that use an ordinal outcome; (ii) describe how ordinal outcomes are analysed in RCTs to estimate a treatment effect; and (iii) describe whether RCTs that use an ordinal outcome adequately report key methodological aspects specific to the analysis of the outcome.   Results from this review will outline the current state of practice of the use of ordinal outcomes in RCTs. Ways to improve the analysis and reporting of ordinal outcomes in RCTs will be discussed. We will review RCTs that are published in the top four medical journals (BMJ, NEJM, The Lancet and JAMA) between 1 January 2012 and 31 July 2022 that use an ordinal outcome. The review will be conducted using PubMed. Our review will adhere to guidelines for scoping reviews as described in the PRISMA-ScR checklist. The study characteristics and design, including the target parameter(s) and statistical methods will be extracted from eligible studies. The data will be summarised using descriptive statistics.|http://arxiv.org/abs/2208.06154v1|Chris J. Selman,Katherine J. Lee,Robert K. Mahar
1211|Towards Efficient Patient Recruitment for Clinical Trials: Application of a Prompt-Based Learning Model|Objective: Clinical trials are essential for advancing pharmaceutical interventions, but they face a bottleneck in selecting eligible participants. Although leveraging electronic health records (EHR) for recruitment has gained popularity, the complex nature of unstructured medical texts presents challenges in efficiently identifying participants. Natural Language Processing (NLP) techniques have emerged as a solution with a recent focus on transformer models. In this study, we aimed to evaluate the performance of a prompt-based large language model for the cohort selection task from unstructured medical notes collected in the EHR. Methods: To process the medical records, we selected the most related sentences of the records to the eligibility criteria needed for the trial. The SNOMED CT concepts related to each eligibility criterion were collected. Medical records were also annotated with MedCAT based on the SNOMED CT ontology. Annotated sentences including concepts matched with the criteria-relevant terms were extracted. A prompt-based large language model (Generative Pre-trained Transformer (GPT) in this study) was then used with the extracted sentences as the training set. To assess its effectiveness, we evaluated the model's performance using the dataset from the 2018 n2c2 challenge, which aimed to classify medical records of 311 patients based on 13 eligibility criteria through NLP techniques. Results: Our proposed model showed the overall micro and macro F measures of 0.9061 and 0.8060 which were among the highest scores achieved by the experiments performed with this dataset. Conclusion: The application of a prompt-based large language model in this study to classify patients based on eligibility criteria received promising scores. Besides, we proposed a method of extractive summarization with the aid of SNOMED CT ontology that can be also applied to other medical texts.|http://arxiv.org/abs/2404.16198v1|Mojdeh Rahmanian,Seyed Mostafa Fakhrahmad,Seyedeh Zahra Mousavi
1212|Using shrinkage methods to estimate treatment effects in overlapping subgroups in randomized clinical trials with a time-to-event endpoint|In randomized controlled trials, forest plots are frequently used to investigate the homogeneity of treatment effect estimates in subgroups. However, the interpretation of subgroup-specific treatment effect estimates requires great care due to the smaller sample size of subgroups and the large number of investigated subgroups. Bayesian shrinkage methods have been proposed to address these issues, but they often focus on disjoint subgroups while subgroups displayed in forest plots are overlapping, i.e., each subject appears in multiple subgroups. In our approach, we first build a flexible Cox model based on all available observations, including categorical covariates that identify the subgroups of interest and their interactions with the treatment group variable. We explore both penalized partial likelihood estimation with a lasso or ridge penalty for treatment-by-covariate interaction terms, and Bayesian estimation with a regularized horseshoe prior. One advantage of the Bayesian approach is the ability to derive credible intervals for shrunken subgroup-specific estimates. In a second step, the Cox model is marginalized to obtain treatment effect estimates for all subgroups. We illustrate these methods using data from a randomized clinical trial in follicular lymphoma and evaluate their properties in a simulation study. In all simulation scenarios, the overall mean-squared error is substantially smaller for penalized and shrinkage estimators compared to the standard subgroup-specific treatment effect estimator but leads to some bias for heterogeneous subgroups. We recommend that subgroup-specific estimators, which are typically displayed in forest plots, are more routinely complemented by treatment effect estimators based on shrinkage methods. The proposed methods are implemented in the R package bonsaiforest.|http://arxiv.org/abs/2407.11729v1|Marcel Wolbers,Mar Vzquez Rabual,Ke Li,Kaspar Rufibach,Daniel Sabans Bov
1213|Chauhan Weighted Trajectory Analysis of combined efficacy and safety outcomes for risk-benefit analysis|Analyzing and effectively communicating the efficacy and toxicity of treatment is the basis of risk benefit analysis (RBA). More efficient and objective tools are needed. We apply Chauhan Weighted Trajectory Analysis (CWTA) to perform RBA with superior objectivity, power, and clarity.   We used CWTA to perform 1000-fold simulations of RCTs using ordinal endpoints for both treatment efficacy and toxicity. RCTs were simulated with 1:1 allocation at defined sample sizes and hazard ratios. We studied the simplest case of 3 levels each of toxicity and efficacy and the general case of the advanced cancer trial, with efficacy graded by five RECIST 1.1 health statuses and toxicity by the six-point CTCAE scale (6 x 5 matrix). The latter model was applied to a real-world dose escalation phase I trial in advanced cancer.   Simulations in both the 3 x 3 and the 6 x 5 advanced cancer matrix confirmed that drugs with both superior efficacy and toxicity profiles synergize for greater statistical power with CWTA-RBA. The CWTA-RBA 6 x 5 matrix reduced sample size requirements over CWTA efficacy-only analysis. Application to the dose finding phase I clinical trial provided objective, statistically significant validation for the selected dose.   CWTA-RBA, by incorporating both drug efficacy and toxicity, provides a single test statistic and plot that analyzes and effectively communicates therapeutic risks and benefits. CWTA-RBA requires fewer patients than CWTA efficacy-only analysis when the experimental drug is both more effective and less toxic. CWTA-RBA facilitates the objective and efficient assessment of new therapies throughout the drug development pathway. Furthermore, several advantages over competing tests in communicating risk-benefit will assist regulatory review, clinical adoption, and understanding of therapeutic risks and benefits by clinicians and patients alike.|http://arxiv.org/abs/2409.13946v1|Utkarsh Chauhan,Daylen Mackey,John R Mackey
1214|Semiparametric Joint Modeling to Estimate the Treatment Effect on a Longitudinal Surrogate with Application to Chronic Kidney Disease Trials|In clinical trials where long follow-up is required to measure the primary outcome of interest, there is substantial interest in using an accepted surrogate outcome that can be measured earlier in time or with less cost to estimate a treatment effect. For example, in clinical trials of chronic kidney disease (CKD), the effect of a treatment is often demonstrated on a surrogate outcome, the longitudinal trajectory of glomerular filtration rate (GFR). However, estimating the effect of a treatment on the GFR trajectory is complicated by the fact that GFR measurement can be terminated by the occurrence of a terminal event, such as death or kidney failure. Thus, to estimate this effect, one must consider both the longitudinal outcome of GFR, and the terminal event process. Available estimation methods either impose restrictive parametric assumptions with corresponding maximum likelihood estimation that is computationintensive or other assumptions not appropriate for the GFR setting. In this paper, we build a semiparametric framework to jointly model the longitudinal outcome and the terminal event, where the model for the longitudinal outcome is semiparametric, and the relationship between the longitudinal outcome and the terminal event is nonparametric. The proposed semiparametric joint model is flexible and can be extended to include nonlinear trajectory of the longitudinal outcome easily. An estimating equation based method is proposed to estimate the treatment effect on the slope of the longitudinal outcome (e.g., GFR slope). Theoretical properties of the proposed estimators are derived. Finite sample performance of the proposed method is evaluated through simulation studies. We illustrate the proposed method using data from the Reduction of Endpoints in NIDDM with the Angiotensin II Antagonist Losartan (RENAAL) trail to examine the effect of Losartan on GFR slope.|http://arxiv.org/abs/2412.14124v1|Xuan Wang,Jie Zhou,Layla Parast,Tom Greene
1215|Asymptotic Properties of Covariate-Adjusted Adaptive Designs|Response-adaptive designs have been extensively studied and used in clinical trials. However, there is a lack of a comprehensive study of response-adaptive designs that include covariates, despite their importance in clinical experiments. Because the allocation scheme and the estimation of parameters are affected by both the responses and the covariates, covariate-adjusted response-adaptive (CARA) designs are very complex to formulate. In this paper, we overcome the technical hurdles and lay out a framework for general CARA designs for the allocation of subjects to $K (\geq 2)$ treatments. The asymptotic properties are studied under certain widely satisfied conditions. The proposed CARA designs can be applied to generalized linear models. Two important special cases, the linear model and the logistic regression model, are considered in detail.|http://arxiv.org/abs/math/0610518v1|Li-Xin Zhang,Feifang Hu,Siu Hung Cheung,Wai Sum Chan
1216|Estimating medical costs from a transition model|Nonparametric estimators of the mean total cost have been proposed in a variety of settings. In clinical trials it is generally impractical to follow up patients until all have responded, and therefore censoring of patient outcomes and total cost will occur in practice. We describe a general longitudinal framework in which costs emanate from two streams, during sojourn in health states and in transition from one health state to another. We consider estimation of net present value for expenditures incurred over a finite time horizon from medical cost data that might be incompletely ascertained in some patients. Because patient specific demographic and clinical characteristics would influence total cost, we use a regression model to incorporate covariates. We discuss similarities and differences between our net present value estimator and other widely used estimators of total medical costs. Our model can accommodate heteroscedasticity, skewness and censoring in cost data and provides a flexible approach to analyses of health care cost.|http://arxiv.org/abs/0805.2496v1|Joseph C. Gardiner,Lin Liu,Zhehui Luo
1217|Immigrated urn models - asymptotic properties and applications|Urn models have been widely studied and applied in both scientific and social science disciplines. In clinical studies, the adoption of urn models in treatment allocation schemes has been proved to be beneficial to both researchers, by providing more efficient clinical trials, and patients, by increasing the likelihood of receiving the better treatment. In this paper, we propose a new and general class of immigrated urn (IMU) models that incorporates the immigration mechanism into the urn process. Theoretical properties are developed and the advantages of the IMU models are discussed. In general, the IMU models have smaller variabilities than the classical urn models, yielding more powerful statistical inferences in applications.   Illustrative examples are presented to demonstrate the wide applicability of the IMU models. The proposed IMU framework, including many popular classical urn models, not only offers a unify perspective for us to comprehend the urn process, but also enables us to generate several novel urn models with desirable properties.|http://arxiv.org/abs/0812.3698v2|Li-Xin Zhang,Feifang Hu,Siu Hung Cheung,Wei Sum Chan
1218|A new latent cure rate marker model for survival data|To address an important risk classification issue that arises in clinical practice, we propose a new mixture model via latent cure rate markers for survival data with a cure fraction. In the proposed model, the latent cure rate markers are modeled via a multinomial logistic regression and patients who share the same cure rate are classified into the same risk group. Compared to available cure rate models, the proposed model fits better to data from a prostate cancer clinical trial. In addition, the proposed model can be used to determine the number of risk groups and to develop a predictive classification algorithm.|http://arxiv.org/abs/0910.1660v1|Sungduk Kim,Yingmei Xi,Ming-Hui Chen
1219|Influence of study type on Twitter activity for medical research papers|Twitter has been identified as one of the most popular and promising altmetrics data sources, as it possibly reflects a broader use of research articles by the general public. Several factors, such as document age, scientific discipline, number of authors and document type, have been shown to affect the number of tweets received by scientific documents. The particular meaning of tweets mentioning scholarly papers is, however, not entirely understood and their validity as impact indicators debatable. This study contributes to the understanding of factors influencing Twitter popularity of medical papers investigating differences between medical study types. 162,830 documents indexed in Embase to a medical study type have been analysed for the study type specific tweet frequency. Meta-analyses, systematic reviews and clinical trials were found to be tweeted substantially more frequently than other study types, while all basic research received less attention than the average. The findings correspond well with clinical evidence hierarchies. It is suggested that interest from laymen and patients may be a factor in the observed effects.|http://arxiv.org/abs/1507.00154v1|Jens Peter Andersen,Stefanie Haustein
1220|Model Selection versus Model Averaging in Dose Finding Studies|Phase II dose finding studies in clinical drug development are typically conducted to adequately characterize the dose response relationship of a new drug. An important decision is then on the choice of a suitable dose response function to support dose selection for the subsequent Phase III studies. In this paper we compare different approaches for model selection and model averaging using mathematical properties as well as simulations. Accordingly, we review and illustrate asymptotic properties of model selection criteria and investigate their behavior when changing the sample size but keeping the effect size constant. In a large scale simulation study we investigate how the various approaches perform in realistically chosen settings. Finally, the different methods are illustrated with a recently conducted Phase II dosefinding study in patients with chronic obstructive pulmonary disease.|http://arxiv.org/abs/1508.00281v1|Kirsten Schorning,Bjrn Bornkamp,Frank Bretz,Holger Dette
1221|A Metric for Evaluating and Comparing Closed-Loop Deep Brain Stimulation Algorithms|Objective: Closed-loop deep brain stimulation (DBS) may improve current clinical DBS treatment for neurological movement disorders, but control algorithms may perform differently across patients. New metrics are needed for comparing and evaluating closed-loop algorithm performance that address the specific needs of closed-loop neuromodulation controllers. Approach: A metric is proposed for system performance that includes normalized terms that can be used to compare algorithm performance for a patient. This metric was evaluated using two closed-loop control algorithms that were tested in patients with Parkinson's Disease (PD) who experience rest tremor. Main Results: The metric's resulting balance between tremor treatment and power savings varied on a per patient and algorithm basis. This was expected given how each trial resulted in a variable reduction in stimulation power at the cost of additional tremor for the patient when compared to open-loop stimulation. Significance: The proposed metric will aid in clinical evaluation of new algorithms and provide a benchmark for future system designers. This will be important given the growing potential applications of dynamically adjusted neural stimulation. ClinicalTrials.gov Identifier: NCT02384421.|http://arxiv.org/abs/1605.09312v1|Jeffrey Herron,Anca Velisar,Mahsa Malekmohammadi,Helen Bronte-Stewart,Howard Jay Chizeck
1222|Future of Flexible Robotic Endoscopy Systems|Robotics enables a variety of unconventional actuation strategies to be used for endoscopes, resulting in reduced trauma to the GI tract. For transmission of force to distally mounted endoscopic instruments, robotically actuated tendon sheath mechanisms are the current state of the art. Robotics in surgical endoscopy enables an ergonomic mapping of the surgeon movements to remotely controlled slave arms, facilitating tissue manipulation. The learning curve for difficult procedures such as endoscopic submucosal dissection and full-thickness resection can be significantly reduced. Improved surgical outcomes are also observed from clinical and pre-clinical trials. The technology behind master-slave surgical robotics will continue to mature, with the addition of position and force sensors enabling better control and tactile feedback. More robotic assisted GI luminal and NOTES surgeries are expected to be conducted in future, and gastroenterologists will have a key collaborative role to play.|http://arxiv.org/abs/1703.05569v1|Tian En Timothy Seah,Thanh Nho Do,Nobuyoshi Takeshita,Khek Yu Ho,Soo Jay Phee
1223|A Novel Method of Subgroup Identification by Combining Virtual Twins with GUIDE (VG) for Development of Precision Medicines|A lack of understanding of human biology creates a hurdle for the development of precision medicines. To overcome this hurdle we need to better understand the potential synergy between a given investigational treatment (vs. placebo or active control) and various demographic or genetic factors, disease history and severity, etc., with the goal of identifying those patients at increased risk of exhibiting clinically meaningful treatment benefit. For this reason, we propose the VG method, which combines the idea of an individual treatment effect (ITE) from Virtual Twins (Foster, et al., 2011) with the unbiased variable selection and cutoff value determination algorithm from GUIDE (Loh, et al., 2015). Simulation results show the VG method has less variable selection bias than Virtual Twins and higher statistical power than GUIDE Interaction in the presence of prognostic variables with strong treatment effects. Type I error and predictive performance of Virtual Twins, GUIDE and VG are compared through the use of simulation studies. Results obtained after retrospectively applying VG to data from a clinical trial also are discussed.|http://arxiv.org/abs/1708.04741v1|Jia Jia,Qi Tang,Wangang Xie,Richard Rode
1224|Riemannian tangent space mapping and elastic net regularization for cost-effective EEG markers of brain atrophy in Alzheimer's disease|The diagnosis of Alzheimer's disease (AD) in routine clinical practice is most commonly based on subjective clinical interpretations. Quantitative electroencephalography (QEEG) measures have been shown to reflect neurodegenerative processes in AD and might qualify as affordable and thereby widely available markers to facilitate the objectivization of AD assessment. Here, we present a novel framework combining Riemannian tangent space mapping and elastic net regression for the development of brain atrophy markers. While most AD QEEG studies are based on small sample sizes and psychological test scores as outcome measures, here we train and test our models using data of one of the largest prospective EEG AD trials ever conducted, including MRI biomarkers of brain atrophy.|http://arxiv.org/abs/1711.08359v1|Wolfgang Fruehwirt,Matthias Gerstgrasser,Pengfei Zhang,Leonard Weydemann,Markus Waser,Reinhold Schmidt,Thomas Benke,Peter Dal-Bianco,Gerhard Ransmayr,Dieter Grossegger,Heinrich Garn,Gareth W. Peters,Stephen Roberts,Georg Dorffner
1225|Estimating the treatment effect in a subgroup defined by an early post-baseline biomarker measurement in randomized clinical trials with time-to-event endpoint|Biomarker measurements can be relatively easy and quick to obtain and they are useful to investigate whether a compound works as intended on a mechanistic, pharmacological level. In some situations, it is realistic to assume that patients, whose post-baseline biomarker levels indicate that they do not sufficiently respond to the drug, are also unlikely to respond on clinically relevant long term outcomes (such as time-to-event). However the determination of the treatment effect in the subgroup of patients that sufficiently respond to the drug according to their biomarker levels is not straightforward: It is unclear which patients on placebo would have responded had they been given the treatment, so that naive comparisons between treatment and placebo will not estimate the treatment effect of interest. The purpose of this paper is to investigate assumptions necessary to obtain causal conclusions in such a setting, utilizing the formalism of causal inference. Three approaches for estimation of subgroup effects will be developed and illustrated using simulations and a case-study.|http://arxiv.org/abs/1806.08807v1|Bjrn Bornkamp,Georgina Bermann
1226|More robust estimation of sample average treatment effects using Kernel Optimal Matching in an observational study of spine surgical interventions|Inverse probability of treatment weighting (IPTW), which has been used to estimate sample average treatment effects (SATE) using observational data, tenuously relies on the positivity assumption and the correct specification of the treatment assignment model, both of which are problematic assumptions in many observational studies. Various methods have been proposed to overcome these challenges, including truncation, covariate-balancing propensity scores, and stable balancing weights. Motivated by an observational study in spine surgery, in which positivity is violated and the true treatment assignment model is unknown, we present the use of optimal balancing by Kernel Optimal Matching (KOM) to estimate SATE. By uniformly controlling the conditional mean squared error of a weighted estimator over a class of models, KOM simultaneously mitigates issues of possible misspecification of the treatment assignment model and is able to handle practical violations of the positivity assumption, as shown in our simulation study. Using data from a clinical registry, we apply KOM to compare two spine surgical interventions and demonstrate how the result matches the conclusions of clinical trials that IPTW estimates spuriously refute.|http://arxiv.org/abs/1811.04274v2|Nathan Kallus,Brenton Pennicooke,Michele Santacatterina
1227|A Family-based Graphical Approach for Testing Hierarchically Ordered Families of Hypotheses|In applications of clinical trials, tested hypotheses are often grouped as multiple hierarchically ordered families. To test such structured hypotheses, various gatekeeping strategies have been developed in the literature, such as series gatekeeping, parallel gatekeeping, tree-structured gatekeeping strategies, etc. However, these gatekeeping strategies are often either non-intuitive or less flexible when addressing increasingly complex logical relationships among families of hypotheses. In order to overcome the issue, in this paper, we develop a new family-based graphical approach, which can easily derive and visualize different gatekeeping strategies. In the proposed approach, a directed and weighted graph is used to represent the generated gatekeeping strategy where each node corresponds to a family of hypotheses and two simple updating rules are used for updating the critical value of each family and the transition coefficient between any two families. Theoretically, we show that the proposed graphical approach strongly controls the overall familywise error rate at a pre-specified level. Through some case studies and a real clinical example, we demonstrate simplicity and flexibility of the proposed approach.|http://arxiv.org/abs/1812.00250v1|Zhiying Qiu,Li Yu,Wenge Guo
1228|Multimodal Machine Learning-based Knee Osteoarthritis Progression Prediction from Plain Radiographs and Clinical Data|Knee osteoarthritis (OA) is the most common musculoskeletal disease without a cure, and current treatment options are limited to symptomatic relief. Prediction of OA progression is a very challenging and timely issue, and it could, if resolved, accelerate the disease modifying drug development and ultimately help to prevent millions of total joint replacement surgeries performed annually. Here, we present a multi-modal machine learning-based OA progression prediction model that utilizes raw radiographic data, clinical examination results and previous medical history of the patient. We validated this approach on an independent test set of 3,918 knee images from 2,129 subjects. Our method yielded area under the ROC curve (AUC) of 0.79 (0.78-0.81) and Average Precision (AP) of 0.68 (0.66-0.70). In contrast, a reference approach, based on logistic regression, yielded AUC of 0.75 (0.74-0.77) and AP of 0.62 (0.60-0.64). The proposed method could significantly improve the subject selection process for OA drug-development trials and help the development of personalized therapeutic plans.|http://arxiv.org/abs/1904.06236v2|Aleksei Tiulpin,Stefan Klein,Sita M. A. Bierma-Zeinstra,Jrme Thevenot,Esa Rahtu,Joyce van Meurs,Edwin H. G. Oei,Simo Saarakkala
1229|Patient Specific Biomechanics Are Clinically Significant In Accurate Computer Aided Surgical Image Guidance|Augmented Reality is used in Image Guided surgery (AR IG) to fuse surgical landmarks from preoperative images into a video overlay. Physical simulation is essential to maintaining accurate position of the landmarks as surgery progresses and ensuring patient safety by avoiding accidental damage to vessels etc. In liver procedures, AR IG simulation accuracy is hampered by an inability to model stiffness variations unique to the patients disease. We introduce a novel method to account for patient specific stiffness variation based on Magnetic Resonance Elastography (MRE) data. To the best of our knowledge we are the first to demonstrate the use of in-vivo biomechanical data for AR IG landmark placement. In this early work, a comparative evaluation of our MRE data driven simulation and the traditional method shows clinically significant differences in accuracy during landmark placement and motivates further animal model trials.|http://arxiv.org/abs/2001.10717v1|Michael Barrow,Alice Chao,Qizhi He,Sonia Ramamoorthy,Claude Sirlin,Ryan Kastner
1230|Prediction of Thrombectomy Functional Outcomes using Multimodal Data|Recent randomised clinical trials have shown that patients with ischaemic stroke {due to occlusion of a large intracranial blood vessel} benefit from endovascular thrombectomy. However, predicting outcome of treatment in an individual patient remains a challenge. We propose a novel deep learning approach to directly exploit multimodal data (clinical metadata information, imaging data, and imaging biomarkers extracted from images) to estimate the success of endovascular treatment. We incorporate an attention mechanism in our architecture to model global feature inter-dependencies, both channel-wise and spatially. We perform comparative experiments using unimodal and multimodal data, to predict functional outcome (modified Rankin Scale score, mRS) and achieve 0.75 AUC for dichotomised mRS scores and 0.35 classification accuracy for individual mRS scores.|http://arxiv.org/abs/2005.13061v2|Zeynel A. Samak,Philip Clatworthy,Majid Mirmehdi
1231|Causal Network Models of SARS-CoV-2 Expression and Aging to Identify Candidates for Drug Repurposing|Given the severity of the SARS-CoV-2 pandemic, a major challenge is to rapidly repurpose existing approved drugs for clinical interventions. While a number of data-driven and experimental approaches have been suggested in the context of drug repurposing, a platform that systematically integrates available transcriptomic, proteomic and structural data is missing. More importantly, given that SARS-CoV-2 pathogenicity is highly age-dependent, it is critical to integrate aging signatures into drug discovery platforms. We here take advantage of large-scale transcriptional drug screens combined with RNA-seq data of the lung epithelium with SARS-CoV-2 infection as well as the aging lung. To identify robust druggable protein targets, we propose a principled causal framework that makes use of multiple data modalities. Our analysis highlights the importance of serine/threonine and tyrosine kinases as potential targets that intersect the SARS-CoV-2 and aging pathways. By integrating transcriptomic, proteomic and structural data that is available for many diseases, our drug discovery platform is broadly applicable. Rigorous in vitro experiments as well as clinical trials are needed to validate the identified candidate drugs.|http://arxiv.org/abs/2006.03735v1|Anastasiya Belyaeva,Louis Cammarata,Adityanarayanan Radhakrishnan,Chandler Squires,Karren Dai Yang,G. V. Shivashankar,Caroline Uhler
|Blake Mason,Lalit Jain,Ardhendu Tripathy,Robert Nowak
1233|When deep learning meets causal inference: a computational framework for drug repurposing from real-world data|Drug repurposing is an effective strategy to identify new uses for existing drugs, providing the quickest possible transition from bench to bedside. Existing methods for drug repurposing that mainly focus on pre-clinical information may exist translational issues when applied to human beings. Real world data (RWD), such as electronic health records and insurance claims, provide information on large cohorts of users for many drugs. Here we present an efficient and easily-customized framework for generating and testing multiple candidates for drug repurposing using a retrospective analysis of RWDs. Building upon well-established causal inference and deep learning methods, our framework emulates randomized clinical trials for drugs present in a large-scale medical claims database. We demonstrate our framework in a case study of coronary artery disease (CAD) by evaluating the effect of 55 repurposing drug candidates on various disease outcomes. We achieve 6 drug candidates that significantly improve the CAD outcomes but not have been indicated for treating CAD, paving the way for drug repurposing.|http://arxiv.org/abs/2007.10152v1|Ruoqi Liu,Lai Wei,Ping Zhang
1234|The Impact of the COVID-19 Pandemic on Scientific Research in the Life Sciences|The COVID-19 outbreak has posed an unprecedented challenge to humanity and science. On the one side, public and private incentives have been put in place to promptly allocate resources toward research areas strictly related to the COVID-19 emergency. But on the flip side, research in many fields not directly related to the pandemic has lagged behind. In this paper, we assess the impact of COVID-19 on world scientific production in the life sciences. We investigate how the usage of medical subject headings (MeSH) has changed following the outbreak. We estimate through a difference-in-differences approach the impact of COVID-19 on scientific production through PubMed. We find that COVID-related research topics have risen to prominence, displaced clinical publications, diverted funds away from research areas not directly related to COVID-19 and that the number of publications on clinical trials in unrelated fields has contracted. Our results call for urgent targeted policy interventions to reactivate biomedical research in areas that have been neglected by the COVID-19 emergency.|http://arxiv.org/abs/2102.00497v1|Massimo Riccaboni,Luca Verginer
1235|Molecular origin of blood-based infrared fingerprints|Previous studies demonstrated that infrared absorption spectra of blood sera may help disease detection. For clinical translation of this approach, it is important to determine the molecular origin of disease-related spectral perturbations. To that end, we supplemented infrared spectroscopy with biochemical fractionation and proteomic profiling, which provide detailed information about blood serum composition. We built a model to describe serum absorption based on the concentrations of the highly-abundant proteins and applied this framework to lung cancer detection. We find that it is the levels of acute-phase proteins that change most in the presence of the disease and generate its infrared signature. These findings inform future clinical trials and establish a framework that could be applied to probing of any disease.|http://arxiv.org/abs/2102.00765v1|Liudmila Voronina,Cristina Leonardo,Johannes B. Mueller-Reif,Philipp E. Geyer,Marinus Huber,Michael Trubetskov,Kosmas V. Kepesidis,Jurgen Behr,Matthias Mann,Ferenc Krausz,Mihaela Zigman
1236|Applications of Artificial Intelligence to aid detection of dementia: a narrative review on current capabilities and future directions|With populations ageing, the number of people with dementia worldwide is expected to triple to 152 million by 2050. Seventy percent of cases are due to Alzheimer's disease (AD) pathology and there is a 10-20 year 'pre-clinical' period before significant cognitive decline occurs. We urgently need, cost effective, objective methods to detect AD, and other dementias, at an early stage. Risk factor modification could prevent 40% of cases and drug trials would have greater chances of success if participants are recruited at an earlier stage. Currently, detection of dementia is largely by pen and paper cognitive tests but these are time consuming and insensitive to pre-clinical phases. Specialist brain scans and body fluid biomarkers can detect the earliest stages of dementia but are too invasive or expensive for widespread use. With the advancement of technology, Artificial Intelligence (AI) shows promising results in assisting with detection of early-stage dementia. Existing AI-aided methods and potential future research directions are reviewed and discussed.|http://arxiv.org/abs/2104.14073v1|Renjie Li,Xinyi Wang,Katherine Lawler,Saurabh Garg,Quan Bai,Jane Alty
1237|CASOG: Conservative Actor-critic with SmOoth Gradient for Skill Learning in Robot-Assisted Intervention|Robot-assisted intervention has shown reduced radiation exposure to physicians and improved precision in clinical trials. However, existing vascular robotic systems follow master-slave control mode and entirely rely on manual commands. This paper proposes a novel offline reinforcement learning algorithm, Conservative Actor-critic with SmOoth Gradient (CASOG), to learn manipulation skills from human demonstrations on vascular robotic systems. The proposed algorithm conservatively estimates Q-function and smooths gradients of convolution layers to deal with distribution shift and overfitting issues. Furthermore, to focus on complex manipulations, transitions with larger temporal-difference error are sampled with higher probability. Comparative experiments in a pre-clinical environment demonstrate that CASOG can deliver guidewire to the target at a success rate of 94.00\% and mean backward steps of 14.07, performing closer to humans and better than prior offline reinforcement learning methods. These results indicate that the proposed algorithm is promising to improve the autonomy of vascular robotic systems.|http://arxiv.org/abs/2304.09632v1|Hao Li,Xiao-Hu Zhou,Xiao-Liang Xie,Shi-Qi Liu,Zhen-Qiu Feng,Zeng-Guang Hou
1238|A monotone data augmentation algorithm for longitudinal data analysis via multivariate skew-t, skew-normal or t distributions|The mixed effects model for repeated measures (MMRM) has been widely used for the analysis of longitudinal clinical data collected at a number of fixed time points. We propose a robust extension of the MMRM for skewed and heavy-tailed data on basis of the multivariate skew-t distribution, and it includes the multivariate normal, t, and skew-normal distributions as special cases. An efficient Markov chain Monte Carlo algorithm is developed using the monotone data augmentation and parameter expansion techniques. We employ the algorithm to perform controlled pattern imputations for sensitivity analyses of longitudinal clinical trials with nonignorable dropouts. The proposed methods are illustrated by real data analyses. Sample SAS programs for the analyses are provided in the online supplementary material.|http://arxiv.org/abs/1906.04844v2|Yongqiang Tang
1239|Online Learning to Estimate Warfarin Dose with Contextual Linear Bandits|Warfarin is one of the most commonly used oral blood anticoagulant agent in the world, the proper dose of Warfarin is difficult to establish not only because it is substantially variant among patients, but also adverse even severe consequences of taking an incorrect dose. Typical practice is to prescribe an initial dose, then doctor closely monitor patient response and adjust accordingly to the correct dosage. The three commonly used strategies for an initial dosage are the fixed-dose approach, the Warfarin Clinical algorithm, and the Pharmacogenetic algorithm developed by the IWPC (International Warfarin Pharmacogenetics Consortium). It is always best to prescribe correct initial dosage, motivated by this challenge, this work explores the performance of multi-armed bandit algorithms to best predict the correct dosage of Warfarin instead of trial-and-error procedure. Real data from the Pharmacogenetics and Pharmacogenomics Knowledge Base (PharmGKB) is used, with it a series of linear bandit algorithms and variants are developed and evaluated on Warfarin dataset. All proposed algorithms outperformed the fixed-dose baseline algorithm, and some even matched up the Warfarin Clinical Dosing Algorithm. In addition, a few promising future directions are given for further exploration and development.|http://arxiv.org/abs/1907.05496v1|Hai Xiao
1240|A novel hand-held interface supporting the self-management of Type 1 diabetes|The paper describes the interaction design of a hand-held interface supporting the self-management of Type 1 diabetes. It addresses well-established clinical and human-computer interaction requirements.   The design exploits three opportunities. One is associated with visible context, whether conspicuous or inconspicuous. A second arises from the design freedom made possible by the user's anticipated focus of attention during certain interactions. A third opportunity to provide valuable functionality arises from wearable sensors and machine learning algorithms. The resulting interface permits ``What if?'' questions: it allows a user to dynamically and manually explore predicted short-term (e.g., 2 hours) relationships between an intended meal, blood glucose level and recommended insulin dosage, and thereby readily make informed food and exercise decisions. Design activity has been informed throughout by focus groups comprising people with Type 1 diabetes in addition to experts in diabetes, interaction design and machine learning. The design is being implemented prior to a clinical trial.|http://arxiv.org/abs/2008.03550v1|Robert Spence,Chukwuma Uduku,Kezhi Li,Nick Oliver,Pantelis Georgiou
1241|The use of restricted mean time lost under competing risks data|Background: Under competing risks, the commonly used sub-distribution hazard ratio (SHR) is not easy to interpret clinically and is valid only under the proportional sub-distribution hazard (SDH) assumption. This paper introduces an alternative statistical measure: the restricted mean time lost (RMTL). Methods: First, the definition and estimation methods of the measures are introduced. Second, based on the differences in RMTLs, a basic difference test (Diff) and a supremum difference test (sDiff) are constructed. Then, the corresponding sample size estimation method is proposed. The statistical properties of the methods and the estimated sample size are evaluated using Monte Carlo simulations, and these methods are also applied to two real examples. Results: The simulation results show that sDiff performs well and has relatively high test efficiency in most situations. Regarding sample size calculation, sDiff exhibits good performance in various situations. The methods are illustrated using two examples. Conclusions: RMTL can meaningfully summarize treatment effects for clinical decision making, which can then be reported with the SDH ratio for competing risks data. The proposed sDiff test and the two calculated sample size formulas have wide applicability and can be considered in real data analysis and trial design.|http://arxiv.org/abs/2010.02071v1|Jingjing Lyu,Yawen Hou,Zheng Chen
1242|Towards Social HRI for Improving Children's Healthcare Experiences|This paper describes a new research project that aims to develop a social robot designed to help children cope with painful and distressing medical procedures in a clinical setting. While robots have previously been trialled for this task, with promising initial results, the systems have tended to be teleoperated, limiting their flexibility and robustness. This project will use epistemic planning techniques as a core component for action selection in the robot system, in order to generate plans that include physical, sensory, and social actions for interacting with humans. The robot will operate in a task environment where appropriate and safe interaction with children, parents/caregivers, and healthcare professionals is required. In addition to addressing the core technical challenge of building an autonomous social robot, the project will incorporate co-design techniques involving all participant groups, and the final robot system will be evaluated in a two-site clinical trial.|http://arxiv.org/abs/2010.04652v1|Mary Ellen Foster,Ronald P. A. Petrick
1243|Discovering Clinically Meaningful Shape Features for the Analysis of Tumor Pathology Images|With the advanced imaging technology, digital pathology imaging of tumor tissue slides is becoming a routine clinical procedure for cancer diagnosis. This process produces massive imaging data that capture histological details in high resolution. Recent developments in deep-learning methods have enabled us to automatically detect and characterize the tumor regions in pathology images at large scale. From each identified tumor region, we extracted 30 well-defined descriptors that quantify its shape, geometry, and topology. We demonstrated how those descriptor features were associated with patient survival outcome in lung adenocarcinoma patients from the National Lung Screening Trial (n=143). Besides, a descriptor-based prognostic model was developed and validated in an independent patient cohort from The Cancer Genome Atlas Program program (n=318). This study proposes new insights into the relationship between tumor shape, geometrical, and topological features and patient prognosis. We provide software in the form of R code on GitHub: https://github.com/estfernandez/Slide_Image_Segmentation_and_Extraction.|http://arxiv.org/abs/2012.04878v1|Esteban Fernndez Morales,Cong Zhang,Guanghua Xiao,Chul Moon,Qiwei Li
1244|A selective review on calibration information from similar studies based on parametric likelihood or empirical likelihood|In multi-center clinical trials, due to various reasons, the individual-level data are strictly restricted to be assessed publicly. Instead, the summarized information is widely available from published results. With the advance of computational technology, it has become very common in data analyses to run on hundreds or thousands of machines simultaneous, with the data distributed across those machines and no longer available in a single central location. How to effectively assemble the summarized clinical data information or information from each machine in parallel computation has become a challenging task for statisticians and computer scientists. In this paper, we selectively review some recently-developed statistical methods, including communication efficient distributed statistical inference, and renewal estimation and incremental inference, which can be regarded as the latest development of calibration information methods in the era of big data. Even though those methods were developed in different fields and in different statistical frameworks, in principle, they are asymptotically equivalent to those well known methods developed in meta analysis. Almost no or little information is lost compared with the case when full data are available. As a general tool to integrate information, we also review the generalized method of moments and estimating equations approach by using empirical likelihood method.|http://arxiv.org/abs/2101.00105v1|Jing Qin,Yukun Liu,Pengfei Li
1245|Meta-analysis of Censored Adverse Events|Meta-analysis is a powerful tool for assessing drug safety by combining treatment-related toxicological findings across multiple studies, as clinical trials are typically underpowered for detecting adverse drug effects. However, incomplete reporting of adverse events (AEs) in published clinical studies is a frequent issue, especially if the observed number of AEs is below a pre-specified study-dependent threshold. Ignoring the censored AE information, often found in lower frequency, can significantly bias the estimated incidence rate of AEs. Despite its importance, this common meta-analysis problem has received little statistical or analytic attention in the literature. To address this challenge, we propose a Bayesian approach to accommodating the censored and possibly rare AEs for meta-analysis of safety data. Through simulation studies, we demonstrate that the proposed method can improves accuracy in point and interval estimation of incidence probabilities, particularly in the presence of censored data. Overall, the proposed method provides a practical solution that can facilitate better-informed decisions regarding drug safety.|http://arxiv.org/abs/2101.07934v3|Xinyue Qi,Shouhao Zhou,Christine B. Peterson,Yucai Wang,Xinying Fang,Michael L. Wang,Chan Shen
1246|Synthesized Difference in Differences|We consider estimating the conditional average treatment effect for everyone by eliminating confounding and selection bias. Unfortunately, randomized clinical trials (RCTs) eliminate confounding but impose strict exclusion criteria that prevent sampling of the entire clinical population. Observational datasets are more inclusive but suffer from confounding. We therefore analyze RCT and observational data simultaneously in order to extract the strengths of each. Our solution builds upon Difference in Differences (DD), an algorithm that eliminates confounding from observational data by comparing outcomes before and after treatment administration. DD requires a parallel slopes assumption that may not apply in practice when confounding shifts across time. We instead propose Synthesized Difference in Differences (SDD) that infers the correct (possibly non-parallel) slopes by linearly adjusting a conditional version of DD using additional RCT data. The algorithm achieves state of the art performance across multiple synthetic and real datasets even when the RCT excludes the majority of patients.|http://arxiv.org/abs/2105.00455v2|Eric V. Strobl,Thomas A. Lasko
1247|Machine Learning Applications for Therapeutic Tasks with Genomics Data|Thanks to the increasing availability of genomics and other biomedical data, many machine learning approaches have been proposed for a wide range of therapeutic discovery and development tasks. In this survey, we review the literature on machine learning applications for genomics through the lens of therapeutic development. We investigate the interplay among genomics, compounds, proteins, electronic health records (EHR), cellular images, and clinical texts. We identify twenty-two machine learning in genomics applications across the entire therapeutics pipeline, from discovering novel targets, personalized medicine, developing gene-editing tools all the way to clinical trials and post-market studies. We also pinpoint seven important challenges in this field with opportunities for expansion and impact. This survey overviews recent research at the intersection of machine learning, genomics, and therapeutic development.|http://arxiv.org/abs/2105.01171v1|Kexin Huang,Cao Xiao,Lucas M. Glass,Cathy W. Critchlow,Greg Gibson,Jimeng Sun
1248|An Empirical Bayes Robust Meta-Analytical-Predictive Prior to Adaptively Leverage External Data|We propose a novel empirical Bayes robust MAP (EB-rMAP) prior to adaptively leverage external/historical data. Built on Box's prior predictive p-value, the EB-rMAP prior framework balances between model parsimony and flexibility through a tuning parameter. The proposed framework can be applied to binary, normal, and time-to-event endpoints. Computational aspects of the framework are efficient. Simulations results with different endpoints demonstrate that the EB-rMAP prior is robust in the presence of prior-data conflict while preserving statistical power. The proposed EB-rMAP prior is then applied to a clinical dataset that comprises of ten oncology clinical trials, including the perspective study.|http://arxiv.org/abs/2109.10237v2|Hongtao Zhang,Yueqi Shen,Alan Y Chiang,Judy Li
1249|Counterfactual Phenotyping with Censored Time-to-Events|Estimation of treatment efficacy of real-world clinical interventions involves working with continuous outcomes such as time-to-death, re-hospitalization, or a composite event that may be subject to censoring. Counterfactual reasoning in such scenarios requires decoupling the effects of confounding physiological characteristics that affect baseline survival rates from the effects of the interventions being assessed. In this paper, we present a latent variable approach to model heterogeneous treatment effects by proposing that an individual can belong to one of latent clusters with distinct response characteristics. We show that this latent structure can mediate the base survival rates and helps determine the effects of an intervention. We demonstrate the ability of our approach to discover actionable phenotypes of individuals based on their treatment response on multiple large randomized clinical trials originally conducted to assess appropriate treatments to reduce cardiovascular risk.|http://arxiv.org/abs/2202.11089v3|Chirag Nagpal,Mononito Goswami,Keith Dufendach,Artur Dubrawski
1250|PathologyBERT -- Pre-trained Vs. A New Transformer Language Model for Pathology Domain|Pathology text mining is a challenging task given the reporting variability and constant new findings in cancer sub-type definitions. However, successful text mining of a large pathology database can play a critical role to advance 'big data' cancer research like similarity-based treatment selection, case identification, prognostication, surveillance, clinical trial screening, risk stratification, and many others. While there is a growing interest in developing language models for more specific clinical domains, no pathology-specific language space exist to support the rapid data-mining development in pathology space. In literature, a few approaches fine-tuned general transformer models on specialized corpora while maintaining the original tokenizer, but in fields requiring specialized terminology, these models often fail to perform adequately. We propose PathologyBERT - a pre-trained masked language model which was trained on 347,173 histopathology specimen reports and publicly released in the Huggingface repository. Our comprehensive experiments demonstrate that pre-training of transformer model on pathology corpora yields performance improvements on Natural Language Understanding (NLU) and Breast Cancer Diagnose Classification when compared to nonspecific language models.|http://arxiv.org/abs/2205.06885v1|Thiago Santos,Amara Tariq,Susmita Das,Kavyasree Vayalpati,Geoffrey H. Smith,Hari Trivedi,Imon Banerjee
1251|Accurate collection of reasons for treatment discontinuation to better define estimands in clinical trials|Background: Reasons for treatment discontinuation are important not only to understand the benefit and risk profile of experimental treatments, but also to help choose appropriate strategies to handle intercurrent events in defining estimands. The current case report form (CRF) commonly in use mixes the underlying reasons for treatment discontinuation and who makes the decision for treatment discontinuation, often resulting in an inaccurate collection of reasons for treatment discontinuation. Methods and results: We systematically reviewed and analyzed treatment discontinuation data from nine phase 2 and phase 3 studies for insulin peglispro. A total of 857 participants with treatment discontinuation were included in the analysis. Our review suggested that, due to the vague multiple-choice options for treatment discontinuation present in the CRF, different reasons were sometimes recorded for the same underlying reason for treatment discontinuation. Based on our review and analysis, we suggest an intermediate solution and a more systematic way to improve the current CRF for treatment discontinuations. Conclusion: This research provides insight and directions on how to optimize the CRF for recording treatment discontinuation. Further work needs to be done to build the learning into Clinical Data Interchange Standards Consortium standards.|http://arxiv.org/abs/2206.01659v2|Yongming Qu,Robin D. White,Stephen J. Ruberg
1252|Statistical Methods for Selective Biomarker Testing|Biomarker is a critically important tool in modern clinical diagnosis, prognosis, and classification/prediction. However, there are fiscal and analytical barriers to biomarker research. Selective Genotyping is an approach to increasing study power and efficiency where individuals with the most extreme phenotype (response) are chosen for genotyping (exposure) in order to maximize the information in the sample. In this article, we describe an analogous procedure in the biomarker testing landscape where both response and biomarker (exposure) are continuous. We propose an intuitive reverse-regression least squares estimator for the parameters relating biomarker value to response. Monte Carlo simulations show that this method is unbiased and efficient relative to estimates from random sampling when the joint normal distribution assumption is met. We illustrate application of proposed methods on data from a chronic pain clinical trial.|http://arxiv.org/abs/2208.00353v1|A. Adam Ding,Natalie DelRocco,Samuel Wu
1253|Perpetual Observational Studies: New strategies to support efficient implementation of observational studies and randomized trials in the infectious diseases arena|The increasing threat of emerging infectious diseases and antimicrobial resistance requires more efficient, high-quality research. Perpetual Observational Studies (POS) nested within a clinical research network can improve planning, quality and efficiency of interventional and observational studies, although real-life benefits and challenges need to be assessed. Ecraid (European Clinical Research Alliance on Infectious Diseases) has initiated POS and will monitor the impact for five specific infectious syndromes.|http://arxiv.org/abs/2208.03992v2|N. Hassoun-Kheir,C. H. van Werkhoven,J. Dunning,T. Jaenisch,J. van Beek,J. Bielicki,C. Butler,B. Francois,S. Harbarth,A. C. Hernandez Padilla,P. Horby,M. Koopmans,J. Lee,J. Rodriguez-Bao,E. Tacconelli,Y. Themistocleous,A. W. van der Velden,M. Bonten,H. Goossens,M. E. A. de Kraker
1254|Deep Computational Model for the Inference of Ventricular Activation Properties|Patient-specific cardiac computational models are essential for the efficient realization of precision medicine and in-silico clinical trials using digital twins. Cardiac digital twins can provide non-invasive characterizations of cardiac functions for individual patients, and therefore are promising for the patient-specific diagnosis and therapy stratification. However, current workflows for both the anatomical and functional twinning phases, referring to the inference of model anatomy and parameter from clinical data, are not sufficiently efficient, robust, and accurate. In this work, we propose a deep learning based patient-specific computational model, which can fuse both anatomical and electrophysiological information for the inference of ventricular activation properties, i.e., conduction velocities and root nodes. The activation properties can provide a quantitative assessment of cardiac electrophysiological function for the guidance of interventional procedures. We employ the Eikonal model to generate simulated electrocardiogram (ECG) with ground truth properties to train the inference model, where specific patient information has also been considered. For evaluation, we test the model on the simulated data and obtain generally promising results with fast computational time.|http://arxiv.org/abs/2208.04028v1|Lei Li,Julia Camps,Abhirup Banerjee,Marcel Beetz,Blanca Rodriguez,Vicente Grau
1255|Echocardiographic Image Quality Assessment Using Deep Neural Networks|Echocardiography image quality assessment is not a trivial issue in transthoracic examination. As the in vivo examination of heart structures gained prominence in cardiac diagnosis, it has been affirmed that accurate diagnosis of the left ventricle functions is hugely dependent on the quality of echo images. Up till now, visual assessment of echo images is highly subjective and requires specific definition under clinical pathologies. While poor-quality images impair quantifications and diagnosis, the inherent variations in echocardiographic image quality standards indicates the complexity faced among different observers and provides apparent evidence for incoherent assessment under clinical trials, especially with less experienced cardiologists. In this research, our aim was to analyse and define specific quality attributes mostly discussed by experts and present a fully trained convolutional neural network model for assessing such quality features objectively.|http://arxiv.org/abs/2209.00959v1|Robert B. Labs,Massoud Zolgharni,Jonathan P. Loo
1256|Contrastive learning for unsupervised medical image clustering and reconstruction|The lack of large labeled medical imaging datasets, along with significant inter-individual variability compared to clinically established disease classes, poses significant challenges in exploiting medical imaging information in a precision medicine paradigm, where in principle dense patient-specific data can be employed to formulate individual predictions and/or stratify patients into finer-grained groups which may follow more homogeneous trajectories and therefore empower clinical trials. In order to efficiently explore the effective degrees of freedom underlying variability in medical images in an unsupervised manner, in this work we propose an unsupervised autoencoder framework which is augmented with a contrastive loss to encourage high separability in the latent space. The model is validated on (medical) benchmark datasets. As cluster labels are assigned to each example according to cluster assignments, we compare performance with a supervised transfer learning baseline. Our method achieves similar performance to the supervised architecture, indicating that separation in the latent space reproduces expert medical observer-assigned labels. The proposed method could be beneficial for patient stratification, exploring new subdivisions of larger classes or pathological continua or, due to its sampling abilities in a variation setting, data augmentation in medical image processing.|http://arxiv.org/abs/2209.12005v1|Matteo Ferrante,Tommaso Boccato,Simeon Spasov,Andrea Duggento,Nicola Toschi
1257|Towards Safe Mechanical Ventilation Treatment Using Deep Offline Reinforcement Learning|Mechanical ventilation is a key form of life support for patients with pulmonary impairment. Healthcare workers are required to continuously adjust ventilator settings for each patient, a challenging and time consuming task. Hence, it would be beneficial to develop an automated decision support tool to optimize ventilation treatment. We present DeepVent, a Conservative Q-Learning (CQL) based offline Deep Reinforcement Learning (DRL) agent that learns to predict the optimal ventilator parameters for a patient to promote 90 day survival. We design a clinically relevant intermediate reward that encourages continuous improvement of the patient vitals as well as addresses the challenge of sparse reward in RL. We find that DeepVent recommends ventilation parameters within safe ranges, as outlined in recent clinical trials. The CQL algorithm offers additional safety by mitigating the overestimation of the value estimates of out-of-distribution states/actions. We evaluate our agent using Fitted Q Evaluation (FQE) and demonstrate that it outperforms physicians from the MIMIC-III dataset.|http://arxiv.org/abs/2210.02552v1|Flemming Kondrup,Thomas Jiralerspong,Elaine Lau,Nathan de Lara,Jacob Shkrob,My Duc Tran,Doina Precup,Sumana Basu
1258|A perspective on the use of health digital twins in computational pathology|A digital health twin can be defined as a virtual model of a physical person, in this specific case, a patient. This virtual model is constituted by multidimensional data that can host from clinical, molecular and therapeutic parameters to sensor data and living conditions. Given that in computational pathology, it is very important to have the information from image donors to create computational models, the integration of digital twins in this field could be crucial. However, since these virtual entities collect sensitive data from physical people, privacy safeguards must also be considered and implemented. With these data safeguards in place, health digital twins could integrate digital clinical trials and be necessary participants in the generation of real-world evidence, which could positively change both fields.|http://arxiv.org/abs/2212.00573v1|Manuel Cossio
1259|Identifying Heterogeneous Treatment Effects in Multiple Outcomes using Joint Confidence Intervals|Heterogeneous treatment effects (HTEs) are commonly identified during randomized controlled trials (RCTs). Identifying subgroups of patients with similar treatment effects is of high interest in clinical research to advance precision medicine. Often, multiple clinical outcomes are measured during an RCT, each having a potentially heterogeneous effect. Recently there has been high interest in identifying subgroups from HTEs, however, there has been less focus on developing tools in settings where there are multiple outcomes. In this work, we propose a framework for partitioning the covariate space to identify subgroups across multiple outcomes based on the joint CIs. We test our algorithm on synthetic and semi-synthetic data where there are two outcomes, and demonstrate that our algorithm is able to capture the HTE in both outcomes simultaneously.|http://arxiv.org/abs/2212.01437v1|Peniel N. Argaw,Elizabeth Healey,Isaac S. Kohane
1260|Simulating the Evolution of Signaling Signatures during CART-Cell -- Tumor Cell Interactions|Immunotherapies have been proven to have significant therapeutic efficacy in the treatment of cancer. The last decade has seen adoptive cell therapies, such as chimeric antigen receptor T-cell (CART-cell) therapy, gain FDA approval against specific cancers. Additionally, there are numerous clinical trials ongoing investigating additional designs and targets. Nevertheless, despite the excitement and promising potential of CART-cell therapy, response rates to therapy vary greatly between studies, patients, and cancers. There remains an unmet need to develop computational frameworks that more accurately predict CART-cell function and clinical efficacy. Here we present a coarse-grained model simulated with logical rules that demonstrates the evolution of signaling signatures following the inter-action between CART-cells and tumor cells and allows for in silico based prediction of CART-cell functionality prior to experimentation.|http://arxiv.org/abs/2302.04338v1|Viren Shah,Justin Womack,Anthony E. Zamora,Scott S. Terhune,Ranjan K. Dash
1261|Permutation tests for assessing potential non-linear associations between treatment use and multivariate clinical outcomes|In many psychometric applications, the relationship between the mean of an outcome and a quantitative covariate is too complex to be described by simple parametric functions; instead, flexible nonlinear relationships can be incorporated using penalized splines. Penalized splines can be conveniently represented as a linear mixed effects model (LMM), where the coefficients of the spline basis functions are random effects. The LMM representation of penalized splines makes the extension to multivariate outcomes relatively straightforward. In the LMM, no effect of the quantitative covariate on the outcome corresponds to the null hypothesis that a fixed effect and a variance component are both zero. Under the null, the usual asymptotic chi-square distribution of the likelihood ratio test for the variance component does not hold. Therefore, we propose three permutation tests for the likelihood ratio test statistic: one based on permuting the quantitative covariate, the other two based on permuting residuals. We compare via simulation the Type I error rate and power of the three permutation tests obtained from joint models for multiple outcomes, as well as a commonly used parametric test. The tests are illustrated using data from a stimulant use disorder psychosocial clinical trial.|http://arxiv.org/abs/2302.12866v1|Boyu Ren,Stuart R. Lipsitz,Garrett M. Fitzmaurice,Roger D. Weiss
1262|Chemosensitivity testing of revived fresh-frozen biopsies using digital speckle holography|Enrolling patients in clinical trials to obtain fresh tumor biopsies to profile anticancer agents can be slow and expensive. However, if flash-frozen biopsies can be thawed to produce viable living tissue with relevant biodynamic profiles, then a large reservoir of tissue-banked samples could become available for phenotypic library building. Here, we report biodynamic profiles acquired from revived flash-frozen canine B-cell lymphoma biopsies using digital speckle holography. We compared the thawed-tissue drug-response spectrograms to spectrograms from fresh tissues in a study of canine B-cell lymphoma. By compensating for tissue trauma in the thawed sample, patient clustering of both the fresh and thawed samples were found to be in general agreement with clinical outcomes. This study indicates that properly frozen tumor specimens are a viable proxy for fresh specimens in the context of chemosensitivity testing, and that thawed samples from tissue banks contain sufficient viable cells to evaluate phenotypic drug response.|http://arxiv.org/abs/2303.12339v1|Zhen Hua,John Turek,Mike Childress,David Nolte
1263|On the Confidence Intervals in Bioequivalence Studies|A bioequivalence study is a type of clinical trial designed to compare the biological equivalence of two different formulations of a drug. Such studies are typically conducted in controlled clinical settings with human subjects, who are randomly assigned to receive two formulations. The two formulations are then compared with respect to their pharmacokinetic profiles, which encompass the absorption, distribution, metabolism, and elimination of the drug. Under the guidance from Food and Drug Administration (FDA), for a size-$\alpha$ bioequivalence test, the standard approach is to construct a $100(1-2\alpha)\%$ confidence interval and verify if the confidence interval falls with the critical region. In this work, we clarify that $100(1-2\alpha)\%$ confidence interval approach for bioequivalence testing yields a size-$\alpha$ test only when the two one-sided tests in TOST are ``equal-tailed''. Furthermore, a $100(1-\alpha)\%$ confidence interval approach is also discussed in the bioequivalence study.|http://arxiv.org/abs/2306.06698v1|Kexuan Li,Susie Sinks,Peng Sun,Lingli Yang
1264|Guidance on Individualized Treatment Rule Estimation in High Dimensions|Individualized treatment rules, cornerstones of precision medicine, inform patient treatment decisions with the goal of optimizing patient outcomes. These rules are generally unknown functions of patients' pre-treatment covariates, meaning they must be estimated from clinical or observational study data. Myriad methods have been developed to learn these rules, and these procedures are demonstrably successful in traditional asymptotic settings with moderate number of covariates. The finite-sample performance of these methods in high-dimensional covariate settings, which are increasingly the norm in modern clinical trials, has not been well characterized, however. We perform a comprehensive comparison of state-of-the-art individualized treatment rule estimators, assessing performance on the basis of the estimators' accuracy, interpretability, and computational efficacy. Sixteen data-generating processes with continuous outcomes and binary treatment assignments are considered, reflecting a diversity of randomized and observational studies. We summarize our findings and provide succinct advice to practitioners needing to estimate individualized treatment rules in high dimensions. All code is made publicly available, facilitating modifications and extensions to our simulation study. A novel pre-treatment covariate filtering procedure is also proposed and is shown to improve estimators' accuracy and interpretability.|http://arxiv.org/abs/2306.16402v2|Philippe Boileau,Ning Leng,Sandrine Dudoit
1265|Artificial Intelligence for Drug Discovery: Are We There Yet?|Drug discovery is adapting to novel technologies such as data science, informatics, and artificial intelligence (AI) to accelerate effective treatment development while reducing costs and animal experiments. AI is transforming drug discovery, as indicated by increasing interest from investors, industrial and academic scientists, and legislators. Successful drug discovery requires optimizing properties related to pharmacodynamics, pharmacokinetics, and clinical outcomes. This review discusses the use of AI in the three pillars of drug discovery: diseases, targets, and therapeutic modalities, with a focus on small molecule drugs. AI technologies, such as generative chemistry, machine learning, and multi-property optimization, have enabled several compounds to enter clinical trials. The scientific community must carefully vet known information to address the reproducibility crisis. The full potential of AI in drug discovery can only be realized with sufficient ground truth and appropriate human intervention at later pipeline stages.|http://arxiv.org/abs/2307.06521v1|Catrin Hasselgren,Tudor I. Oprea
1266|Targeting relative risk heterogeneity with causal forests|The estimation of heterogeneous treatment effects (HTE) across different subgroups in a population is of significant interest in clinical trial analysis. State-of-the-art HTE estimation methods, including causal forests (Wager and Athey, 2018), generally rely on recursive partitioning for non-parametric identification of relevant covariates and interactions. However, like many other methods in this area, causal forests partition subgroups based on differences in absolute risk. This can dilute statistical power by masking variability in the relative risk, which is often a more appropriate quantity of clinical interest. In this work, we propose and implement a methodology for modifying causal forests to target relative risk, using a novel node-splitting procedure based on exhaustive generalized linear model comparison. We present results that suggest relative risk causal forests can capture otherwise undetected sources of heterogeneity.|http://arxiv.org/abs/2309.15793v2|Vik Shirvaikar,Xi Lin,Chris Holmes
1267|Surpassing GPT-4 Medical Coding with a Two-Stage Approach|Recent advances in large language models (LLMs) show potential for clinical applications, such as clinical decision support and trial recommendations. However, the GPT-4 LLM predicts an excessive number of ICD codes for medical coding tasks, leading to high recall but low precision. To tackle this challenge, we introduce LLM-codex, a two-stage approach to predict ICD codes that first generates evidence proposals using an LLM and then employs an LSTM-based verification stage. The LSTM learns from both the LLM's high recall and human expert's high precision, using a custom loss function. Our model is the only approach that simultaneously achieves state-of-the-art results in medical coding accuracy, accuracy on rare codes, and sentence-level evidence identification to support coding decisions without training on human-annotated evidence according to experiments on the MIMIC dataset.|http://arxiv.org/abs/2311.13735v1|Zhichao Yang,Sanjit Singh Batra,Joel Stremmel,Eran Halperin
1268|Investigating the heterogeneity of "study twins"|Meta-analyses are commonly performed based on random-effects models, while in certain cases one might also argue in favour of a common-effect model. One such case may be given by the example of two "study twins" that are performed according to a common (or at least very similar) protocol. Here we investigate the particular case of meta-analysis of a pair of studies, e.g. summarizing the results of two confirmatory clinical trials in phase III of a clinical development programme. Thereby, we focus on the question of to what extent homogeneity or heterogeneity may be discernible, and include an empirical investigation of published ("twin") pairs of studies. A pair of estimates from two studies only provides very little evidence on homogeneity or heterogeneity of effects, and ad-hoc decision criteria may often be misleading.|http://arxiv.org/abs/2312.09884v2|Christian Rver,Tim Friede
1269|Privacy Preserving Adaptive Experiment Design|Adaptive experiment is widely adopted to estimate conditional average treatment effect (CATE) in clinical trials and many other scenarios. While the primary goal in experiment is to maximize estimation accuracy, due to the imperative of social welfare, it's also crucial to provide treatment with superior outcomes to patients, which is measured by regret in contextual bandit framework. These two objectives often lead to contrast optimal allocation mechanism. Furthermore, privacy concerns arise in clinical scenarios containing sensitive data like patients health records. Therefore, it's essential for the treatment allocation mechanism to incorporate robust privacy protection measures. In this paper, we investigate the tradeoff between loss of social welfare and statistical power in contextual bandit experiment. We propose a matched upper and lower bound for the multi-objective optimization problem, and then adopt the concept of Pareto optimality to mathematically characterize the optimality condition. Furthermore, we propose differentially private algorithms which still matches the lower bound, showing that privacy is "almost free". Additionally, we derive the asymptotic normality of the estimator, which is essential in statistical inference and hypothesis testing.|http://arxiv.org/abs/2401.08224v4|Jiachun Li,Kaining Shi,David Simchi-Levi
1270|Cumulative Incidence Function Estimation Based on Population-Based Biobank Data|Many countries have established population-based biobanks, which are being used increasingly in epidemiolgical and clinical research. These biobanks offer opportunities for large-scale studies addressing questions beyond the scope of traditional clinical trials or cohort studies. However, using biobank data poses new challenges. Typically, biobank data is collected from a study cohort recruited over a defined calendar period, with subjects entering the study at various ages falling between $c_L$ and $c_U$. This work focuses on biobank data with individuals reporting disease-onset age upon recruitment, termed prevalent data, along with individuals initially recruited as healthy, and their disease onset observed during the follow-up period. We propose a novel cumulative incidence function (CIF) estimator that efficiently incorporates prevalent cases, in contrast to existing methods, providing two advantages: (1) increased efficiency, and (2) CIF estimation for ages before the lower limit, $c_L$.|http://arxiv.org/abs/2403.18464v1|Malka Gorfine,David M. Zucker,Shoval Shoham
1271|Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4|The NLI4CT task assesses Natural Language Inference systems in predicting whether hypotheses entail or contradict evidence from Clinical Trial Reports. In this study, we evaluate various Large Language Models (LLMs) with multiple strategies, including Chain-of-Thought, In-Context Learning, and Parameter-Efficient Fine-Tuning (PEFT). We propose a PEFT method to improve the consistency of LLMs by merging adapters that were fine-tuned separately using triplet and language modelling objectives. We found that merging the two PEFT adapters improves the F1 score (+0.0346) and consistency (+0.152) of the LLMs. However, our novel methods did not produce more accurate results than GPT-4 in terms of faithfulness and consistency. Averaging the three metrics, GPT-4 ranks joint-first in the competition with 0.8328. Finally, our contamination analysis with GPT-4 indicates that there was no test data leakage.|http://arxiv.org/abs/2404.00484v1|Aryo Pradipta Gema,Giwon Hong,Pasquale Minervini,Luke Daines,Beatrice Alex
1272|D-NLP at SemEval-2024 Task 2: Evaluating Clinical Inference Capabilities of Large Language Models|Large language models (LLMs) have garnered significant attention and widespread usage due to their impressive performance in various tasks. However, they are not without their own set of challenges, including issues such as hallucinations, factual inconsistencies, and limitations in numerical-quantitative reasoning. Evaluating LLMs in miscellaneous reasoning tasks remains an active area of research. Prior to the breakthrough of LLMs, Transformers had already proven successful in the medical domain, effectively employed for various natural language understanding (NLU) tasks. Following this trend, LLMs have also been trained and utilized in the medical domain, raising concerns regarding factual accuracy, adherence to safety protocols, and inherent limitations. In this paper, we focus on evaluating the natural language inference capabilities of popular open-source and closed-source LLMs using clinical trial reports as the dataset. We present the performance results of each LLM and further analyze their performance on a development set, particularly focusing on challenging instances that involve medical abbreviations and require numerical-quantitative reasoning. Gemini, our leading LLM, achieved a test set F1-score of 0.748, securing the ninth position on the task scoreboard. Our work is the first of its kind, offering a thorough examination of the inference capabilities of LLMs within the medical domain.|http://arxiv.org/abs/2405.04170v1|Duygu Altinok
1273|RadEx: A Framework for Structured Information Extraction from Radiology Reports based on Large Language Models|Annually and globally, over three billion radiography examinations and computer tomography scans result in mostly unstructured radiology reports containing free text. Despite the potential benefits of structured reporting, its adoption is limited by factors such as established processes, resource constraints and potential loss of information. However, structured information would be necessary for various use cases, including automatic analysis, clinical trial matching, and prediction of health outcomes. This study introduces RadEx, an end-to-end framework comprising 15 software components and ten artifacts to develop systems that perform automated information extraction from radiology reports. It covers the complete process from annotating training data to extracting information by offering a consistent generic information model and setting boundaries for model development. Specifically, RadEx allows clinicians to define relevant information for clinical domains (e.g., mammography) and to create report templates. The framework supports both generative and encoder-only models and the decoupling of information extraction from template filling enables independent model improvements. Developing information extraction systems according to the RadEx framework facilitates implementation and maintenance as components are easily exchangeable, while standardized artifacts ensure interoperability between components.|http://arxiv.org/abs/2406.15465v1|Daniel Reichenpfader,Jonas Knupp,Andr Sander,Kerstin Denecke
1274|Optimal treatment strategies for prioritized outcomes|Dynamic treatment regimes formalize precision medicine as a sequence of decision rules, one for each stage of clinical intervention, that map current patient information to a recommended intervention. Optimal regimes are typically defined as maximizing some functional of a scalar outcome's distribution, e.g., the distribution's mean or median. However, in many clinical applications, there are multiple outcomes of interest. We consider the problem of estimating an optimal regime when there are multiple outcomes that are ordered by priority but which cannot be readily combined by domain experts into a meaningful single scalar outcome. We propose a definition of optimality in this setting and show that an optimal regime with respect to this definition leads to maximal mean utility under a large class of utility functions. Furthermore, we use inverse reinforcement learning to identify a composite outcome that most closely aligns with our definition within a pre-specified class. Simulation experiments and an application to data from a sequential multiple assignment randomized trial (SMART) on HIV/STI prevention illustrate the usefulness of the proposed approach.|http://arxiv.org/abs/2407.05537v1|Kyle Duke,Eric B. Laber,Marie Davidian,Michael Newcomb,Brian Mustanksi
1275|Addressing Outcome Reporting Bias in Meta-analysis: A Selection Model Perspective|Outcome Reporting Bias (ORB) poses significant threats to the validity of meta-analytic findings. It occurs when researchers selectively report outcomes based on the significance or direction of results, potentially leading to distorted treatment effect estimates. Despite its critical implications, ORB remains an under-recognized issue, with few comprehensive adjustment methods available. The goal of this research is to investigate ORB-adjustment techniques through a selection model lens, thereby extending some of the existing methodological approaches available in the literature. To gain a better insight into the effects of ORB in meta-analysis of clinical trials, specifically in the presence of heterogeneity, and to assess the effectiveness of ORB-adjustment techniques, we apply the methodology to real clinical data affected by ORB and conduct a simulation study focusing on treatment effect estimation with a secondary interest in heterogeneity quantification.|http://arxiv.org/abs/2408.05747v1|Alessandra Gaia Saracini,Leonhard Held
1276|Opportunities and challenges of mRNA technologies in development of Dengue Virus Vaccine|Dengue virus (DENV) is a mosquito-borne virus with a significant human health concern. With 390 million infections annually and 96 million showing clinical symptoms, severe dengue can lead to life-threatening conditions like dengue hemorrhagic fever (DHF) and dengue shock syndrome (DSS). The only FDA-approved vaccine, Dengvaxia, has limitations due to antibody-dependent enhancement (ADE), necessitating careful administration. The recent pre-approval of TAK-003 by WHO in 2024 highlights ongoing efforts to improve vaccine options. This review explores recent advancements in dengue vaccine development, emphasizing potential utility of mRNA-based vaccines. By examining current clinical trial data and innovations, we aim to identify promising strategies to address the limitations of existing vaccines and enhance global dengue prevention efforts.|http://arxiv.org/abs/2409.10805v1|Xiaoyang Liu,Daniel Salmon
1277|Comorbid anxiety predicts lower odds of depression improvement during smartphone-delivered psychotherapy|Comorbid anxiety disorders are common among patients with major depressive disorder (MDD), and numerous studies have identified an association between comorbid anxiety and resistance to pharmacological depression treatment. However, the impact of anxiety on the effectiveness of non-pharmacological interventions for MDD is not as well understood. In this study, we applied machine learning techniques to predict treatment responses in a large-scale clinical trial (n=493) of individuals with MDD, who were recruited online and randomly assigned to one of three smartphone-based interventions. Our analysis reveals that a baseline GAD-7 questionnaire score in the moderate to severe range (>10) predicts reduced probability of recovery from MDD. Our findings suggest that depressed individuals with comorbid anxiety face lower odds of substantial improvement in the context of smartphone-based therapeutic interventions for depression. Our work highlights a methodology that can identify simple, clinically useful "rules of thumb" for treatment response prediction using interpretable machine learning models.|http://arxiv.org/abs/2409.11183v2|Morgan B. Talbot,Jessica M. Lipschitz,Omar Costilla-Reyes
1278|Mitigating the Risk of Health Inequity Exacerbated by Large Language Models|Recent advancements in large language models have demonstrated their potential in numerous medical applications, particularly in automating clinical trial matching for translational research and enhancing medical question answering for clinical decision support. However, our study shows that incorporating non decisive sociodemographic factors such as race, sex, income level, LGBT+ status, homelessness, illiteracy, disability, and unemployment into the input of LLMs can lead to incorrect and harmful outputs for these populations. These discrepancies risk exacerbating existing health disparities if LLMs are widely adopted in healthcare. To address this issue, we introduce EquityGuard, a novel framework designed to detect and mitigate the risk of health inequities in LLM based medical applications. Our evaluation demonstrates its efficacy in promoting equitable outcomes across diverse populations.|http://arxiv.org/abs/2410.05180v2|Yuelyu Ji,Wenhe Ma,Sonish Sivarajkumar,Hang Zhang,Eugene Mathew Sadhu,Zhuochun Li,Xizhi Wu,Shyam Visweswaran,Yanshan Wang
1279|Improve the Precision of Area Under the Curve Estimation for Recurrent Events Through Covariate Adjustment|The area under the curve (AUC) of the mean cumulative function (MCF) has recently been introduced as a novel estimand for evaluating treatment effects in recurrent event settings, capturing a totality of evidence in relation to disease progression. While the Lin-Wei-Yang-Ying (LWYY) model is commonly used for analyzing recurrent events, it relies on the proportional rate assumption between treatment arms, which might be violated in practice. In contrast, the AUC under MCFs does not depend on such proportionality assumptions and offers a clinically interpretable measure of treatment effect. To improve the precision of the AUC estimation while preserving its unconditional interpretability, we propose a nonparametric covariate adjustment approach. This approach guarantees efficiency gain compared to unadjusted analysis, as demonstrated by theoretical asymptotic distributions, and is universally applicable to various randomization schemes, including both simple and covariate-adaptive designs. Extensive simulations across different scenarios further support its advantage in increasing statistical power. Our findings highlight the importance of covariate adjustment for the analysis of AUC in recurrent event settings, offering practical guidance for its application in randomized clinical trials.|http://arxiv.org/abs/2410.24163v2|Jiren Sun,Tuo Wang,Yanyao Yi,Ting Ye,Jun Shao,Yu Du
1280|Uncertainty quantification and multi-stage variable selection for personalized treatment regimes|A dynamic treatment regime is a sequence of medical decisions that adapts to the evolving clinical status of a patient over time. To facilitate personalized care, it is crucial to assess the probability of each available treatment option being optimal for a specific patient, while also identifying the key prognostic factors that determine the optimal sequence of treatments. This task has become increasingly challenging due to the growing number of individual prognostic factors typically available. In response to these challenges, we propose a Bayesian model for optimizing dynamic treatment regimes that addresses the uncertainty in identifying optimal decision sequences and incorporates dimensionality reduction to manage high-dimensional individual covariates. The first task is achieved through a suitable augmentation of the model to handle counterfactual variables. For the second, we introduce a novel class of spike-and-slab priors for the multi-stage selection of significant factors, to favor the sharing of information across stages. The effectiveness of the proposed approach is demonstrated through extensive simulation studies and illustrated using clinical trial data on severe acute arterial hypertension.|http://arxiv.org/abs/2411.02123v1|Jiefeng Bi,Matteo Borrotti,Bernardo Nipoti
1281|Geometric-Based Nail Segmentation for Clinical Measurements|A robust segmentation method that can be used to perform measurements on toenails is presented. The proposed method is used as the first step in a clinical trial to objectively quantify the incidence of a particular pathology. For such an assessment, it is necessary to distinguish a nail, which locally appears to be similar to the skin. Many algorithms have been used, each of which leverages different aspects of toenail appearance. We used the Hough transform to locate the tip of the toe and estimate the nail location and size. Subsequently, we classified the super-pixels of the image based on their geometric and photometric information. Thereafter, the watershed transform delineated the border of the nail. The method was validated using a 348-image medical dataset, achieving an accuracy of 0.993 and an F-measure of 0.925. The proposed method is considerably robust across samples, with respect to factors such as nail shape, skin pigmentation, illumination conditions, and appearance of large regions affected by a medical condition|http://arxiv.org/abs/2501.06027v1|Bernat Galms,Gabriel Moy-Alcover,Pedro Bibiloni,Javier Varona,Antoni Jaume-i-Cap
1282|Screening and evaluation of potential clinically significant HIV drug combinations against SARS-CoV-2 virus|In this study, we investigated the inhibition of SARS-CoV-2 spike glycoprotein with HIV drugs and their combinations. This glycoprotein is essential for the reproduction of the SARS-COV-2 virus, so its inhibition opens new avenues for the treatment of patients with COVID-19 disease. In doing so, we used the VINI in silico model of cancer, whose high accuracy in finding effective drugs and their combinations was confirmed in vitro by comparison with existing results from NCI-60 bases, and in vivo by comparison with existing clinical trial results. In the first step, the VINI model calculated the inhibition efficiency of SARS-CoV-2 spike glycoprotein with 44 FDA-approved antiviral drugs. Of these drugs, HIV drugs have been shown to be effective, while others mainly have shown weak or no efficiency. Subsequently, the VINI model calculated the inhibition efficiency of all possible double and triple HIV drug combinations, and among them identified ten with the highest inhibition efficiency. These ten combinations were analyzed by Medscape drug-drug interaction software and LexiComp Drug Interactions. All combinations except the combination of cobicistat_abacavir_rilpivirine appear to have serious interactions (risk rating category D) when dosage adjustments/reductions are required for possible toxicity. Finally, the VINI model compared the inhibition efficiency of cobicistat_abacivir_rilpivirine combination with cocktails and individual drugs already used or planned to be tested against SARS-CoV-2. Combination cobicistat_abacivir_rilpivirine demonstrated the highest inhibition of SARS-CoV-2 spike glycoprotein over others. Thus, this combination seems to be a promising candidate for the further in vitro testing and clinical trials.|http://arxiv.org/abs/2007.16177v1|Drako Tomi,Karolj Skala,Attila Marcel Szasz,Melinda Rezeli,Vesna Bai Vrca,Boris Pirki,Jozsef Petrik,Vladimir Janel,Marija Milkovi Peria,Branka Medved Rogina,Josip Mesari,Davor Davidovi,Tomislav Lipi
1283|Can Autism be Catered with Artificial Intelligence-Assisted Intervention Technology? A Literature Review|This article presents an extensive literature review of technology based intervention methodologies for individuals facing Autism Spectrum Disorder (ASD). Reviewed methodologies include: contemporary Computer Aided Systems (CAS), Computer Vision Assisted Technologies (CVAT) and Virtual Reality (VR) or Artificial Intelligence (AI)-Assisted interventions. The research over the past decade has provided enough demonstrations that individuals with ASD have a strong interest in technology based interventions, which are useful in both, clinical settings as well as at home and classrooms. Despite showing great promise, research in developing an advanced technology based intervention that is clinically quantitative for ASD is minimal. Moreover, the clinicians are generally not convinced about the potential of the technology based interventions due to non-empirical nature of published results. A major reason behind this lack of acceptability is that a vast majority of studies on distinct intervention methodologies do not follow any specific standard or research design. We conclude from our findings that there remains a gap between the research community of computer science, psychology and neuroscience to develop an AI assisted intervention technology for individuals suffering from ASD. Following the development of a standardized AI based intervention technology, a database needs to be developed, to devise effective AI algorithms.|http://arxiv.org/abs/1803.05181v5|Muhammad Shoaib Jaliawala,Rizwan Ahmed Khan
1284|Detection of REM Sleep Behaviour Disorder by Automated Polysomnography Analysis|Evidence suggests Rapid-Eye-Movement (REM) Sleep Behaviour Disorder (RBD) is an early predictor of Parkinson's disease. This study proposes a fully-automated framework for RBD detection consisting of automated sleep staging followed by RBD identification. Analysis was assessed using a limited polysomnography montage from 53 participants with RBD and 53 age-matched healthy controls. Sleep stage classification was achieved using a Random Forest (RF) classifier and 156 features extracted from electroencephalogram (EEG), electrooculogram (EOG) and electromyogram (EMG) channels. For RBD detection, a RF classifier was trained combining established techniques to quantify muscle atonia with additional features that incorporate sleep architecture and the EMG fractal exponent. Automated multi-state sleep staging achieved a 0.62 Cohen's Kappa score. RBD detection accuracy improved by 10% to 96% (compared to individual established metrics) when using manually annotated sleep staging. Accuracy remained high (92%) when using automated sleep staging. This study outperforms established metrics and demonstrates that incorporating sleep architecture and sleep stage transitions can benefit RBD detection. This study also achieved automated sleep staging with a level of accuracy comparable to manual annotation. This study validates a tractable, fully-automated, and sensitive pipeline for RBD identification that could be translated to wearable take-home technology.|http://arxiv.org/abs/1811.04662v1|Navin Cooray,Fernando Andreotti,Christine Lo,Mkael Symmonds,Michele T. M. Hu,Maarten De Vos
1285|Double diffusion encoding and applications for biomedical imaging|Diffusion Magnetic Resonance Imaging (dMRI) is one of the most important contemporary non-invasive modalities for probing tissue structure at the microscopic scale. The majority of dMRI techniques employ standard single diffusion encoding (SDE) measurements, covering different sequence parameter ranges depending on the complexity of the method. Although many signal representations and biophysical models have been proposed for SDE data, they are intrinsically limited by a lack of specificity. Advanced dMRI methods have been proposed to provide additional microstructural information beyond what can be inferred from SDE. These enhanced contrasts can play important roles in characterizing biological tissues, for instance upon diseases (e.g. neurodegenerative, cancer, stroke), aging, learning, and development.   In this review we focus on double diffusion encoding (DDE), which stands out among other advanced acquisitions for its versatility, ability to probe more specific diffusion correlations, and feasibility for preclinical and clinical applications. Various DDE methodologies have been employed to probe compartment sizes (Section 3), decouple the effects of microscopic diffusion anisotropy from orientation dispersion (Section 4), probe displacement correlations, study exchange, or suppress fast diffusing compartments (Section 6). DDE measurements can also be used to improve the robustness of biophysical models (Section 5) and study intra-cellular diffusion via magnetic resonance spectroscopy of metabolites (Section 7). This review discusses all these topics as well as important practical aspects related to the implementation and contrast in preclinical and clinical settings (Section 9) and aims to provide the readers a guide for deciding on the right DDE acquisition for their specific application.|http://arxiv.org/abs/2011.02913v1|Rafael N. Henriques,Marco Palombo,Sune N. Jespersen,Noam Shemesh,Henrik Lundell,Andrada Ianu
1286|Learning joint lesion and tissue segmentation from task-specific hetero-modal datasets|Brain tissue segmentation from multimodal MRI is a key building block of many neuroscience analysis pipelines. It could also play an important role in many clinical imaging scenarios. Established tissue segmentation approaches have however not been developed to cope with large anatomical changes resulting from pathology. The effect of the presence of brain lesions, for example, on their performance is thus currently uncontrolled and practically unpredictable. Contrastingly, with the advent of deep neural networks (DNNs), segmentation of brain lesions has matured significantly and is achieving performance levels making it of interest for clinical use. However, few existing approaches allow for jointly segmenting normal tissue and brain lesions. Developing a DNN for such joint task is currently hampered by the fact that annotated datasets typically address only one specific task and rely on a task-specific hetero-modal imaging protocol. In this work, we propose a novel approach to build a joint tissue and lesion segmentation model from task-specific hetero-modal and partially annotated datasets. Starting from a variational formulation of the joint problem, we show how the expected risk can be decomposed and optimised empirically. We exploit an upper-bound of the risk to deal with missing imaging modalities. For each task, our approach reaches comparable performance than task-specific and fully-supervised models.|http://arxiv.org/abs/1907.03327v1|Reuben Dorent,Wenqi Li,Jinendra Ekanayake,Sebastien Ourselin,Tom Vercauteren
1287|FBNETGEN: Task-aware GNN-based fMRI Analysis via Functional Brain Network Generation|Functional magnetic resonance imaging (fMRI) is one of the most common imaging modalities to investigate brain functions. Recent studies in neuroscience stress the great potential of functional brain networks constructed from fMRI data for clinical predictions. Traditional functional brain networks, however, are noisy and unaware of downstream prediction tasks, while also incompatible with the deep graph neural network (GNN) models. In order to fully unleash the power of GNNs in network-based fMRI analysis, we develop FBNETGEN, a task-aware and interpretable fMRI analysis framework via deep brain network generation. In particular, we formulate (1) prominent region of interest (ROI) features extraction, (2) brain networks generation, and (3) clinical predictions with GNNs, in an end-to-end trainable model under the guidance of particular prediction tasks. Along with the process, the key novel component is the graph generator which learns to transform raw time-series features into task-oriented brain networks. Our learnable graphs also provide unique interpretations by highlighting prediction-related brain regions. Comprehensive experiments on two datasets, i.e., the recently released and currently largest publicly available fMRI dataset Adolescent Brain Cognitive Development (ABCD), and the widely-used fMRI dataset PNC, prove the superior effectiveness and interpretability of FBNETGEN. The implementation is available at https://github.com/Wayfear/FBNETGEN.|http://arxiv.org/abs/2205.12465v2|Xuan Kan,Hejie Cui,Joshua Lukemire,Ying Guo,Carl Yang
1288|BrainNet: Epileptic Wave Detection from SEEG with Hierarchical Graph Diffusion Learning|Epilepsy is one of the most serious neurological diseases, affecting 1-2% of the world's population. The diagnosis of epilepsy depends heavily on the recognition of epileptic waves, i.e., disordered electrical brainwave activity in the patient's brain. Existing works have begun to employ machine learning models to detect epileptic waves via cortical electroencephalogram (EEG). However, the recently developed stereoelectrocorticography (SEEG) method provides information in stereo that is more precise than conventional EEG, and has been broadly applied in clinical practice. Therefore, we propose the first data-driven study to detect epileptic waves in a real-world SEEG dataset. While offering new opportunities, SEEG also poses several challenges. In clinical practice, epileptic wave activities are considered to propagate between different regions in the brain. These propagation paths, also known as the epileptogenic network, are deemed to be a key factor in the context of epilepsy surgery. However, the question of how to extract an exact epileptogenic network for each patient remains an open problem in the field of neuroscience. To address these challenges, we propose a novel model (BrainNet) that jointly learns the dynamic diffusion graphs and models the brain wave diffusion patterns. In addition, our model effectively aids in resisting label imbalance and severe noise by employing several self-supervised learning tasks and a hierarchical framework. By experimenting with the extensive real SEEG dataset obtained from multiple patients, we find that BrainNet outperforms several latest state-of-the-art baselines derived from time-series analysis.|http://arxiv.org/abs/2306.13101v1|Junru Chen,Yang Yang,Tao Yu,Yingying Fan,Xiaolong Mo,Carl Yang
1289|Hierarchical Event Descriptor library schema for EEG data annotation|Standardizing terminology to annotate electrophysiological events can improve both computational research and clinical care. Sharing data enriched with standard terms can facilitate data exploration, from case studies to mega-analyses. The machine readability of such electrophysiological event annotations is essential for performing analyses efficiently across software tools and packages. Hierarchical Event Descriptors (HED) provide a framework for describing events in neuroscience experiments. HED library schemas extend the standard HED schema vocabulary to include specialized vocabularies, such as standardized clinical terms for electrophysiological events. The Standardized Computer-based Organized Reporting of EEG (SCORE) defines terms for annotating EEG events, including artifacts. This study developed a HED library schema for SCORE, making the terms machine-readable. We demonstrate that the HED-SCORE library schema can be used to annotate events in EEG data stored in the Brain Imaging Data Structure (BIDS). Clinicians and researchers worldwide can now use the HED-SCORE library schema to annotate and compute on electrophysiological data obtained from the human brain.|http://arxiv.org/abs/2310.15173v2|Dora Hermes,Tal Pal Attia,Sndor Beniczky,Jorge Bosch-Bayard,Arnaud Delorme,Brian Nils Lundstrom,Christine Rogers,Stefan Rampp,Seyed Yahya Shirazi,Dung Truong,Pedro Valdes-Sosa,Greg Worrell,Scott Makeig,Kay Robbins
1290|Modeling long-term longitudinal HIV dynamics with application to an AIDS clinical study|A virologic marker, the number of HIV RNA copies or viral load, is currently used to evaluate antiretroviral (ARV) therapies in AIDS clinical trials. This marker can be used to assess the ARV potency of therapies, but is easily affected by drug exposures, drug resistance and other factors during the long-term treatment evaluation process. HIV dynamic studies have significantly contributed to the understanding of HIV pathogenesis and ARV treatment strategies. However, the models of these studies are used to quantify short-term HIV dynamics ($<$ 1 month), and are not applicable to describe long-term virological response to ARV treatment due to the difficulty of establishing a relationship of antiviral response with multiple treatment factors such as drug exposure and drug susceptibility during long-term treatment. Long-term therapy with ARV agents in HIV-infected patients often results in failure to suppress the viral load. Pharmacokinetics (PK), drug resistance and imperfect adherence to prescribed antiviral drugs are important factors explaining the resurgence of virus. To better understand the factors responsible for the virological failure, this paper develops the mechanism-based nonlinear differential equation models for characterizing long-term viral dynamics with ARV therapy. The models directly incorporate drug concentration, adherence and drug susceptibility into a function of treatment efficacy and, hence, fully integrate virologic, PK, drug adherence and resistance from an AIDS clinical trial into the analysis. A Bayesian nonlinear mixed-effects modeling approach in conjunction with the rescaled version of dynamic differential equations is investigated to estimate dynamic parameters and make inference. In addition, the correlations of baseline factors with estimated dynamic parameters are explored and some biologically meaningful correlation results are presented. Further, the estimated dynamic parameters in patients with virologic success were compared to those in patients with virologic failure and significantly important findings were summarized. These results suggest that viral dynamic parameters may play an important role in understanding HIV pathogenesis, designing new treatment strategies for long-term care of AIDS patients.|http://arxiv.org/abs/0901.3806v1|Yangxin Huang,Tao Lu
1291|Precision medicine as a control problem: Using simulation and deep reinforcement learning to discover adaptive, personalized multi-cytokine therapy for sepsis|Sepsis is a life-threatening condition affecting one million people per year in the US in which dysregulation of the body's own immune system causes damage to its tissues, resulting in a 28 - 50% mortality rate. Clinical trials for sepsis treatment over the last 20 years have failed to produce a single currently FDA approved drug treatment. In this study, we attempt to discover an effective cytokine mediation treatment strategy for sepsis using a previously developed agent-based model that simulates the innate immune response to infection: the Innate Immune Response agent-based model (IIRABM). Previous attempts at reducing mortality with multi-cytokine mediation using the IIRABM have failed to reduce mortality across all patient parameterizations and motivated us to investigate whether adaptive, personalized multi-cytokine mediation can control the trajectory of sepsis and lower patient mortality. We used the IIRABM to compute a treatment policy in which systemic patient measurements are used in a feedback loop to inform future treatment. Using deep reinforcement learning, we identified a policy that achieves 0% mortality on the patient parameterization on which it was trained. More importantly, this policy also achieves 0.8% mortality over 500 randomly selected patient parameterizations with baseline mortalities ranging from 1 - 99% (with an average of 49%) spanning the entire clinically plausible parameter space of the IIRABM. These results suggest that adaptive, personalized multi-cytokine mediation therapy could be a promising approach for treating sepsis. We hope that this work motivates researchers to consider such an approach as part of future clinical trials. To the best of our knowledge, this work is the first to consider adaptive, personalized multi-cytokine mediation therapy for sepsis, and is the first to exploit deep reinforcement learning on a biological simulation.|http://arxiv.org/abs/1802.10440v1|Brenden K. Petersen,Jiachen Yang,Will S. Grathwohl,Chase Cockrell,Claudio Santiago,Gary An,Daniel M. Faissol
1292|Regional Deep Atrophy: a Self-Supervised Learning Method to Automatically Identify Regions Associated With Alzheimer's Disease Progression From Longitudinal MRI|Longitudinal assessment of brain atrophy, particularly in the hippocampus, is a well-studied biomarker for neurodegenerative diseases, such as Alzheimer's disease (AD). In clinical trials, estimation of brain progressive rates can be applied to track therapeutic efficacy of disease modifying treatments. However, most state-of-the-art measurements calculate changes directly by segmentation and/or deformable registration of MRI images, and may misreport head motion or MRI artifacts as neurodegeneration, impacting their accuracy. In our previous study, we developed a deep learning method DeepAtrophy that uses a convolutional neural network to quantify differences between longitudinal MRI scan pairs that are associated with time. DeepAtrophy has high accuracy in inferring temporal information from longitudinal MRI scans, such as temporal order or relative inter-scan interval. DeepAtrophy also provides an overall atrophy score that was shown to perform well as a potential biomarker of disease progression and treatment efficacy. However, DeepAtrophy is not interpretable, and it is unclear what changes in the MRI contribute to progression measurements. In this paper, we propose Regional Deep Atrophy (RDA), which combines the temporal inference approach from DeepAtrophy with a deformable registration neural network and attention mechanism that highlights regions in the MRI image where longitudinal changes are contributing to temporal inference. RDA has similar prediction accuracy as DeepAtrophy, but its additional interpretability makes it more acceptable for use in clinical settings, and may lead to more sensitive biomarkers for disease monitoring in clinical trials of early AD.|http://arxiv.org/abs/2304.04673v1|Mengjin Dong,Long Xie,Sandhitsu R. Das,Jiancong Wang,Laura E. M. Wisse,Robin deFlores,David A. Wolk,Paul A. Yushkevich
1293|Bayesian Estimation of Two-Part Joint Models for a Longitudinal Semicontinuous Biomarker and a Terminal Event with R-INLA: Interests for Cancer Clinical Trial Evaluation|Two-part joint models for a longitudinal semicontinuous biomarker and a terminal event have been recently introduced based on frequentist estimation. The biomarker distribution is decomposed into a probability of positive value and the expected value among positive values. Shared random effects can represent the association structure between the biomarker and the terminal event. The computational burden increases compared to standard joint models with a single regression model for the biomarker. In this context, the frequentist estimation implemented in the R package frailtypack can be challenging for complex models (i.e., large number of parameters and dimension of the random effects). As an alternative, we propose a Bayesian estimation of two-part joint models based on the Integrated Nested Laplace Approximation (INLA) algorithm to alleviate the computational burden and fit more complex models. Our simulation studies confirm that INLA provides accurate approximation of posterior estimates and to reduced computation time and variability of estimates compared to frailtypack in the situations considered. We contrast the Bayesian and frequentist approaches in the analysis of two randomized cancer clinical trials (GERCOR and PRIME studies), where INLA has a reduced variability for the association between the biomarker and the risk of event. Moreover, the Bayesian approach was able to characterize subgroups of patients associated with different responses to treatment in the PRIME study. Our study suggests that the Bayesian approach using INLA algorithm enables to fit complex joint models that might be of interest in a wide range of clinical applications.|http://arxiv.org/abs/2010.13704v3|Denis Rustand,Janet van Niekerk,Hvard Rue,Christophe Tournigand,Virginie Rondeau,Laurent Briollais
1294|Pressure Ulcer Categorisation using Deep Learning: A Clinical Trial to Evaluate Model Performance|Pressure ulcers are a challenge for patients and healthcare professionals. In the UK, 700,000 people are affected by pressure ulcers each year. Treating them costs the National Health Service {\pounds}3.8 million every day. Their etiology is complex and multifactorial. However, evidence has shown a strong link between old age, disease-related sedentary lifestyles and unhealthy eating habits. Pressure ulcers are caused by direct skin contact with a bed or chair without frequent position changes. Urinary and faecal incontinence, diabetes, and injuries that restrict body position and nutrition are also known risk factors. Guidelines and treatments exist but their implementation and success vary across different healthcare settings. This is primarily because healthcare practitioners have a) minimal experience in dealing with pressure ulcers, and b) a general lack of understanding of pressure ulcer treatments. Poorly managed, pressure ulcers lead to severe pain, poor quality of life, and significant healthcare costs. In this paper, we report the findings of a clinical trial conducted by Mersey Care NHS Foundation Trust that evaluated the performance of a faster region-based convolutional neural network and mobile platform that categorised and documented pressure ulcers. The neural network classifies category I, II, III, and IV pressure ulcers, deep tissue injuries, and unstageable pressure ulcers. Photographs of pressure ulcers taken by district nurses are transmitted over 4/5G communications to an inferencing server for classification. Classified images are stored and reviewed to assess the model's predictions and relevance as a tool for clinical decision making and standardised reporting. The results from the study generated a mean average Precision=0.6796, Recall=0.6997, F1-Score=0.6786 with 45 false positives using an @.75 confidence score threshold.|http://arxiv.org/abs/2203.06248v1|Paul Fergus,Carl Chalmers,William Henderson,Danny Roberts,Atif Waraich
1295|Statistical Principles for Platform Trials|While within a clinical study there may be multiple doses and endpoints, across different studies each study will result in either an approval or a lack of approval of the drug compound studied. The term False Approval Rate (FAR) is the term this paper utilizes to represent the proportion of drug compounds that lack efficacy incorrectly approved by regulators. (In the U.S., compounds that have efficacy and are approved are not involved in the FAR consideration, according to our reading of the relevant U.S. Congressional statute).   While Tukey's (1953) Error Rate Familywise (ERFw) is meant to be applied within a clinical study, Tukey's (1953) Error Rate per Family (ERpF), defined along-side ERFw, is meant to be applied across studies. We show that controlling Error Rate Familywise (ERFw) within a clinical study at 5% in turn controls Error Rate per Family (ERpF) across studies at 5-per-100, regardless of whether the studies are correlated or not. Further, we show that ongoing regulatory practice, the additive multiplicity adjustment method of controlling ERpF, is controlling False Approval Rate (FAR) exactly (not conservatively) at 5-per-100 (even for Platform trials).   In contrast, if a regulatory agency chooses to control the False Discovery Rate (FDR) across studies at 5% instead, then this change in policy from ERpF control to FDR control will result in incorrectly approving drug compounds that lack efficacy at a rate higher than 5-per-100, because in essence it gives the industry additional rewards for successfully developing compounds that have efficacy and are approved. Seems to us the discussion of such a change in policy would be at a level higher than merely statistical, needing harmonizsation/harmonization (In the U.S., policy is set by the Congress).|http://arxiv.org/abs/2302.12728v4|Xinping Cui,Emily Ouyang,Yi Liu,Jingjing Schneider,Hong Tian,Bushi Wang,Jason C. Hsu
1296|Optimizing First-Line Therapeutics in Non-Small Cell Lung Cancer: Insights from Joint Modeling and Large-Scale Data Analysis|Non-small cell lung cancer (NSCLC) is often intrinsically resistant to several first- and second-line therapeutics and can rapidly acquire further resistance after a patient begins receiving treatment. Treatment outcomes are therefore significantly impacted by the optimization of therapeutic scheduling. Previous preclinical research has suggested scheduling bevacizumab in sequence with combination antiproliferatives could significantly improve clinical outcomes. Mathematical modeling is a well-suited tool for investigating this proposed scheduling modification. To address this critical need, individual patient tumor data from 11 clinical trials in NSCLC has been collated and used to develop a semi-mechanistic model of NSCLC growth and response to the various therapeutics represented in those trials. Precise estimates of clinical parameters fundamental to cancer modeling have been produced - such as the rate of acquired resistance to various pharmaceuticals, the relationship between drug concentration and cancer cell death, as well as the fine temporal dynamics of vascular remodeling in response to bevacizumab. In a reserved portion of the dataset, this model was used to predict the efficacy of individual treatment time courses with a mean error rate of 59.7% after a single tumor measurement and 11.7% after three successive tumor measurements. A delay of 9.6 hours between pemetrexed-cisplatin and bevacizumab administration is predicted to optimize the benefit of sequential administration. At this gap, approximately 93.5% of simulated patients benefited from a gap in sequential administration compared with concomitant administration. Of those simulated patients, the mean improvement in tumor reduction was 20.7%. This result suggests that scheduling a modest gap between the administration of bevacizumab and its partner antiproliferatives could meaningfully improve patient outcomes in NSCLC.|http://arxiv.org/abs/2410.16967v1|Benjamin K. Schneider,Sebastien Benzekry,Jonathan P. Mochel
1297|Investigating naturalistic hand movements by behavior mining in long-term video and neural recordings|Recent technological advances in brain recording and artificial intelligence are propelling a new paradigm in neuroscience beyond the traditional controlled experiment. Rather than focusing on cued, repeated trials, naturalistic neuroscience studies neural processes underlying spontaneous behaviors performed in unconstrained settings. However, analyzing such unstructured data lacking a priori experimental design remains a significant challenge, especially when the data is multi-modal and long-term. Here we describe an automated approach for analyzing simultaneously recorded long-term, naturalistic electrocorticography (ECoG) and naturalistic behavior video data. We take a behavior-first approach to analyzing the long-term recordings. Using a combination of computer vision, discrete latent-variable modeling, and string pattern-matching on the behavioral video data, we find and annotate spontaneous human upper-limb movement events. We show results from our approach applied to data collected for 12 human subjects over 7--9 days for each subject. Our pipeline discovers and annotates over 40,000 instances of naturalistic human upper-limb movement events in the behavioral videos. Analysis of the simultaneously recorded brain data reveals neural signatures of movement that corroborate prior findings from traditional controlled experiments. We also prototype a decoder for a movement initiation detection task to demonstrate the efficacy of our pipeline as a source of training data for brain-computer interfacing applications. Our work addresses the unique data analysis challenges in studying naturalistic human behaviors, and contributes methods that may generalize to other neural recording modalities beyond ECoG. We publicly release our curated dataset, providing a resource to study naturalistic neural and behavioral variability at a scale not previously available.|http://arxiv.org/abs/2001.08349v2|Satpreet H. Singh,Steven M. Peterson,Rajesh P. N. Rao,Bingni W. Brunton
1298|Systematic reviews in paediatric multiple sclerosis and Creutzfeldt-Jakob disease exemplify shortcomings in methods used to evaluate therapies in rare conditions|BACKGROUND: Randomized controlled trials (RCTs) are the gold standard design of clinical research to assess interventions. However, RCTs cannot always be applied for practical or ethical reasons. To investigate the current practices in rare diseases, we review evaluations of therapeutic interventions in paediatric multiple sclerosis (MS) and Creutzfeldt-Jakob disease (CJD). In particular, we shed light on the endpoints used, the study designs implemented and the statistical methodologies applied.   METHODS: We conducted literature searches to identify relevant primary studies. Data on study design, objectives, endpoints, patient characteristics, randomization and masking, type of intervention, control, withdrawals and statistical methodology were extracted from the selected studies. The risk of bias and the quality of the studies were assessed.   RESULTS: Twelve (seven) primary studies on paediatric MS (CJD) were included in the qualitative synthesis. No double-blind, randomized placebo-controlled trial for evaluating interventions in paediatric MS has been published yet. Evidence from one open-label RCT is available. The observational studies are before-after studies or controlled studies. Three of the seven selected studies on CJD are RCTs, of which two received the maximum mark on the Oxford Quality Scale. Four trials are controlled observational studies.   CONCLUSIONS: Evidence from double-blind RCTs on the efficacy of treatments appears to be variable between rare diseases. With regard to paediatric conditions it remains to be seen what impact regulators will have through e.g., paediatric investigation plans. Overall, there is space for improvement by using innovative trial designs and data analysis techniques.|http://arxiv.org/abs/1602.07207v1|Steffen Unkel,Christian Rver,Nigel Stallard,Norbert Benda,Martin Posch,Sarah Zohar,Tim Friede
1299|The design and statistical aspects of VIETNARMS: a strategic post-licensing trial of multiple oral direct acting antiviral Hepatitis C treatment strategies in Vietnam|Background Achieving hepatitis C elimination is hampered by the costs of treatment and the need to treat hard-to-reach populations. Treatment access could be widened by shortening treatment, but limited research means it is unclear which strategies could achieve sufficiently high cure rates to be acceptable. We present the statistical aspects of a multi-arm trial designed to test multiple strategies simultaneously with a monitoring mechanism to detect and stop those with unacceptably low cure rates quickly. Methods The VIETNARMS trial will factorially randomise patients to three randomisations. We will use Bayesian monitoring at interim analyses to detect and stop recruitment into unsuccessful strategies, defined as a >0.95 posterior probability of the true cure rate being <90%. Here, we tested the operating characteristics of the stopping guideline, planned the timing of the interim analyses and explored power at the final analysis. Results A beta(4.5, 0.5) prior for the true cure rate produces <0.05 probability of incorrectly stopping a group with true cure rate >90%. Groups with very low cure rates (<60%) are very likely (>0.9 probability) to stop after ~25% patients are recruited. Groups with moderately low cure rates (80%) are likely to stop (0.7 probability) before the end of recruitment. Interim analyses 7, 10, 13 and 18 months after recruitment commences provide good probabilities of stopping inferior groups. For an overall true cure rate of 95%, power is >90% to detect non-inferiority in the regimen and strategy comparisons using 5% and 10% margins respectively, regardless of the control cure rate, and to detect a 5% absolute difference in the ribavirin comparison. Conclusions The operating characteristics of the stopping guideline are appropriate and interim analyses can be timed to detect failing groups at various stages.|http://arxiv.org/abs/1911.02272v1|L. McCabe,I. R. White,N. V. Vinh Chau,E. Barnes,S. L. Pett,G. S. Cooke,A. S. Walker
1300|Hybrid sample size calculations for cluster randomised trials using assurance|Sample size determination for cluster randomised trials (CRTs) is challenging as it requires robust estimation of the intra-cluster correlation coefficient (ICC). Typically, the sample size is chosen to provide a certain level of power to reject the null hypothesis in a hypothesis test. This relies on the minimal clinically important difference (MCID) and estimates for the standard deviation, ICC and possibly the coefficient of variation of the cluster size. Varying these parameters can have a strong effect on the sample size. In particular, it is sensitive to small differences in the ICC. A relevant ICC estimate is often not available, or the available estimate is imprecise. If the ICC used is far from the unknown true value, this can lead to trials which are substantially over- or under-powered. We propose a hybrid approach using Bayesian assurance to find the sample size for a CRT with a frequentist analysis. Assurance is an alternative to power which incorporates uncertainty on parameters through a prior distribution. We suggest specifying prior distributions for the standard deviation, ICC and coefficient of variation of the cluster size, while still utilising the MCID. We illustrate the approach through the design of a CRT in post-stroke incontinence. We show assurance can be used to find a sample size based on an elicited prior distribution for the ICC, when a power calculation discards all information in the prior except a single point estimate. Results show that this approach can avoid misspecifying sample sizes when prior medians for the ICC are very similar but prior distributions exhibit quite different behaviour. Assurance provides an understanding of the probability of success of a trial given an MCID and can be used to produce sample sizes which are robust to parameter uncertainty. This is especially useful when there is difficulty obtaining reliable parameter estimates.|http://arxiv.org/abs/2308.11278v1|S. Faye Williamson,Svetlana V. Tishkovskaya,Kevin J. Wilson
1301|A tutorial on conducting sample size and power calculations for detecting treatment effect heterogeneity in cluster randomized trials|Cluster-randomized trials (CRTs) are a well-established class of designs for evaluating large-scale, community-based research questions. An essential task in planning these trials is determining the required number of clusters and cluster sizes to achieve sufficient statistical power for detecting a clinically relevant effect size. Compared to methods for evaluating the average treatment effect (ATE) for the entire study population, there is more recent development of sample size methods for testing the heterogeneity of treatment effects (HTEs), i.e., modification of treatment effects by subpopulation characteristics, in CRTs. For confirmatory analyses of HTEs in CRTs, effect modifiers must be pre-specified, and ideally, accompanied by sample size or power calculations to ensure the trial has adequate power for the planned analyses. Power analysis for HTE analyses is more complex than for ATEs due to the additional design parameters that must be specified. Power and sample size formulas for HTE analyses have been separately derived under several cluster-randomized designs, including single and multi-period parallel designs, crossover designs, and stepped-wedge designs, as well as under continuous and binary outcomes. This tutorial provides a consolidated reference guide for these methods and enhances their accessibility through the development of an online R Shiny calculator. We further discuss key considerations for researchers conducting sample size and power calculations for testing pre-specified HTE hypotheses in CRTs, including the essential role of advance estimates of intracluster correlation coefficients for both outcomes and covariates on power. The sample size methodology and calculator functionality are demonstrated through real CRT examples.|http://arxiv.org/abs/2501.18383v1|Mary Ryan Baumann,Monica Taljaard,Patrick J. Heagerty,Michael O. Harhay,Guangyu Tong,Rui Wang,Fan Li
1302|Monitoring fetal electroencephalogram intrapartum: a systematic literature review|Background: Studies about the feasibility of monitoring fetal electroencephalogram (fEEG) during labor began in the early 1940s. By the 1970s, clear diagnostic and prognostic benefits from intrapartum fEEG monitoring were reported, but until today, this monitoring technology has remained a curiosity.   Objectives: Our goal was to review the studies reporting the use of fEEG including the insights from interpreting fEEG patterns in response to uterine contractions during labor. We also used the most relevant information gathered from clinical studies to provide recommendations for enrollment in the unique environment of a labor and delivery unit.   Data sources: PubMed.   Eligibility criteria: The search strategy was: ("fetus"[MeSH Terms] OR "fetus"[All Fields] OR "fetal"[All Fields]) AND ("electroencephalography"[MeSH Terms] OR "electroencephalography"[All Fields] OR "eeg"[All Fields]) AND (Clinical Trial[ptyp] AND "humans"[MeSH Terms]). Because the landscape of fEEG research has been international, we included studies in English, French, German, and Russian.   Results: From 256 screened studies, 40 studies were ultimately included in the qualitative analysis. We summarize and report features of fEEG which clearly show its potential to act as a direct biomarker of fetal brain health during delivery, ancillary to fetal heart rate monitoring. However, clinical prospective studies are needed to further establish the utility of fEEG monitoring intrapartum. We identified clinical study designs likely to succeed in bringing this intrapartum monitoring modality to the bedside.   Limitations: Despite 80 years of studies in clinical cohorts and animal models, the field of research on intrapartum fEEG is still nascent and shows great promise to augment the currently practiced electronic fetal monitoring.|http://arxiv.org/abs/2005.13618v2|Aude Castel,Yael Frank,John Feltner,Floyd Karp,Catherine Albright,Martin G. Frasch
1303|CCS Explorer: Relevance Prediction, Extractive Summarization, and Named Entity Recognition from Clinical Cohort Studies|Clinical Cohort Studies (CCS), such as randomized clinical trials, are a great source of documented clinical research. Ideally, a clinical expert inspects these articles for exploratory analysis ranging from drug discovery for evaluating the efficacy of existing drugs in tackling emerging diseases to the first test of newly developed drugs. However, more than 100 articles are published daily on a single prevalent disease like COVID-19 in PubMed. As a result, it can take days for a physician to find articles and extract relevant information. Can we develop a system to sift through the long list of these articles faster and document the crucial takeaways from each of these articles? In this work, we propose CCS Explorer, an end-to-end system for relevance prediction of sentences, extractive summarization, and patient, outcome, and intervention entity detection from CCS. CCS Explorer is packaged in a web-based graphical user interface where the user can provide any disease name. CCS Explorer then extracts and aggregates all relevant information from articles on PubMed based on the results of an automatically generated query produced on the back-end. For each task, CCS Explorer fine-tunes pre-trained language representation models based on transformers with additional layers. The models are evaluated using two publicly available datasets. CCS Explorer obtains a recall of 80.2%, AUC-ROC of 0.843, and an accuracy of 88.3% on sentence relevance prediction using BioBERT and achieves an average Micro F1-Score of 77.8% on Patient, Intervention, Outcome detection (PIO) using PubMedBERT. Thus, CCS Explorer can reliably extract relevant information to summarize articles, saving time by $\sim \text{660}\times$.|http://arxiv.org/abs/2211.00201v2|Irfan Al-Hussaini,Davi Nakajima An,Albert J. Lee,Sarah Bi,Cassie S. Mitchell
1304|ReDWINE: A Clinical Datamart with Text Analytical Capabilities to Facilitate Rehabilitation Research|Rehabilitation research focuses on determining the components of a treatment intervention, the mechanism of how these components lead to recovery and rehabilitation, and ultimately the optimal intervention strategies to maximize patients' physical, psychologic, and social functioning. Traditional randomized clinical trials that study and establish new interventions face several challenges, such as high cost and time commitment. Observational studies that use existing clinical data to observe the effect of an intervention have shown several advantages over RCTs. Electronic Health Records (EHRs) have become an increasingly important resource for conducting observational studies. To support these studies, we developed a clinical research datamart, called ReDWINE (Rehabilitation Datamart With Informatics iNfrastructure for rEsearch), that transforms the rehabilitation-related EHR data collected from the UPMC health care system to the Observational Health Data Sciences and Informatics (OHDSI) Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) to facilitate rehabilitation research. The standardized EHR data stored in ReDWINE will further reduce the time and effort required by investigators to pool, harmonize, clean, and analyze data from multiple sources, leading to more robust and comprehensive research findings. ReDWINE also includes deployment of data visualization and data analytics tools to facilitate cohort definition and clinical data analysis. These include among others the Open Health Natural Language Processing (OHNLP) toolkit, a high-throughput NLP pipeline, to provide text analytical capabilities at scale in ReDWINE. Using this comprehensive representation of patient data in ReDWINE for rehabilitation research will facilitate real-world evidence for health interventions and outcomes.|http://arxiv.org/abs/2304.05929v1|David Oniani,Bambang Parmanto,Andi Saptono,Allyn Bove,Janet Freburger,Shyam Visweswaran Nickie Cappella,Brian McLay,Jonathan C. Silverstein,Michael J. Becich,Anthony Delitto,Elizabeth Skidmore,Yanshan Wang
1305|Making Study Populations Visible through Knowledge Graphs|Treatment recommendations within Clinical Practice Guidelines (CPGs) are largely based on findings from clinical trials and case studies, referred to here as research studies, that are often based on highly selective clinical populations, referred to here as study cohorts. When medical practitioners apply CPG recommendations, they need to understand how well their patient population matches the characteristics of those in the study cohort, and thus are confronted with the challenges of locating the study cohort information and making an analytic comparison. To address these challenges, we develop an ontology-enabled prototype system, which exposes the population descriptions in research studies in a declarative manner, with the ultimate goal of allowing medical practitioners to better understand the applicability and generalizability of treatment recommendations. We build a Study Cohort Ontology (SCO) to encode the vocabulary of study population descriptions, that are often reported in the first table in the published work, thus they are often referred to as Table 1. We leverage the well-used Semanticscience Integrated Ontology (SIO) for defining property associations between classes. Further, we model the key components of Table 1s, i.e., collections of study subjects, subject characteristics, and statistical measures in RDF knowledge graphs. We design scenarios for medical practitioners to perform population analysis, and generate cohort similarity visualizations to determine the applicability of a study population to the clinical population of interest. Our semantic approach to make study populations visible, by standardized representations of Table 1s, allows users to quickly derive clinically relevant inferences about study populations.|http://arxiv.org/abs/1907.04358v1|Shruthi Chari,Miao Qi,Nkcheniyere N. Agu,Oshani Seneviratne,James P. McCusker,Kristin P. Bennett,Amar K. Das,Deborah L. McGuinness
1306|Virtual vs. Reality: External Validation of COVID-19 Classifiers using XCAT Phantoms for Chest Computed Tomography|Research studies of artificial intelligence models in medical imaging have been hampered by poor generalization. This problem has been especially concerning over the last year with numerous applications of deep learning for COVID-19 diagnosis. Virtual imaging trials (VITs) could provide a solution for objective evaluation of these models. In this work utilizing the VITs, we created the CVIT-COVID dataset including 180 virtually imaged computed tomography (CT) images from simulated COVID-19 and normal phantom models under different COVID-19 morphology and imaging properties. We evaluated the performance of an open-source, deep-learning model from the University of Waterloo trained with multi-institutional data and an in-house model trained with the open clinical dataset called MosMed. We further validated the model's performance against open clinical data of 305 CT images to understand virtual vs. real clinical data performance. The open-source model was published with nearly perfect performance on the original Waterloo dataset but showed a consistent performance drop in external testing on another clinical dataset (AUC=0.77) and our simulated CVIT-COVID dataset (AUC=0.55). The in-house model achieved an AUC of 0.87 while testing on the internal test set (MosMed test set). However, performance dropped to an AUC of 0.65 and 0.69 when evaluated on clinical and our simulated CVIT-COVID dataset. The VIT framework offered control over imaging conditions, allowing us to show there was no change in performance as CT exposure was changed from 28.5 to 57 mAs. The VIT framework also provided voxel-level ground truth, revealing that performance of in-house model was much higher at AUC=0.87 for diffuse COVID-19 infection size >2.65% lung volume versus AUC=0.52 for focal disease with <2.65% volume. The virtual imaging framework enabled these uniquely rigorous analyses of model performance.|http://arxiv.org/abs/2203.03074v1|Fakrul Islam Tushar,Ehsan Abadi,Saman Sotoudeh-Paima,Rafael B. Fricks,Maciej A. Mazurowski,W. Paul Segars,Ehsan Samei,Joseph Y. Lo
1307|CARLA: Adjusted common average referencing for cortico-cortical evoked potential data|Human brain connectivity can be mapped by single pulse electrical stimulation during intracranial EEG measurements. The raw cortico-cortical evoked potentials (CCEP) are often contaminated by noise. Common average referencing (CAR) removes common noise and preserves response shapes but can introduce bias from responsive channels. We address this issue with an adjusted, adaptive CAR algorithm termed "CAR by Least Anticorrelation (CARLA)".   CARLA was tested on simulated CCEP data and real CCEP data collected from four human participants. In CARLA, the channels are ordered by increasing mean cross-trial covariance, and iteratively added to the common average until anticorrelation between any single channel and all re-referenced channels reaches a minimum, as a measure of shared noise.   We simulated CCEP data with true responses in 0 to 45 of 50 total channels. We quantified CARLA's error and found that it erroneously included 0 (median) truly responsive channels in the common average with less than or equal to 42 responsive channels, and erroneously excluded less than or equal to 2.5 (median) unresponsive channels at all responsiveness levels. On real CCEP data, signal quality was quantified with the mean R-squared between all pairs of channels, which represents inter-channel dependency and is low for well-referenced data. CARLA re-referencing produced significantly lower mean R-squared than standard CAR, CAR using a fixed bottom quartile of channels by covariance, and no re-referencing.   CARLA minimizes bias in re-referenced CCEP data by adaptively selecting the optimal subset of non-responsive channels. It showed high specificity and sensitivity on simulated CCEP data and lowered inter-channel dependency compared to CAR on real CCEP data.|http://arxiv.org/abs/2310.00185v1|Harvey Huang,Gabriela Ojeda Valencia,Nicholas M. Gregg,Gamaleldin M. Osman,Morgan N. Montoya,Gregory A. Worrell,Kai J. Miller,Dora Hermes
1308|Sensitivity of Quantitative Susceptibility Mapping in Clinical Brain Research|Background: Quantitative susceptibility mapping (QSM) of the brain is an advanced MRI technique for assessing tissue characteristics based on magnetic susceptibility, which varies with the composition of the tissue, such as iron, calcium, and myelin levels. QSM consists of multiple processing steps, with various choices for each step. Despite its increasing application in detecting and monitoring neurodegenerative diseases, the impact of algorithmic choices in QSM's workflow on clinical outcomes has not been thoroughly quantified.   Objective: This study aimed to evaluate how choices in background field removal (BFR), dipole inversion algorithms, and anatomical referencing impact the sensitivity and reproducibility error of QSM in detecting group-level and longitudinal changes in deep gray matter susceptibility in a clinical setting.   Methods: We compared 378 different QSM pipelines using a 10-year follow-up dataset of healthy adults. We analyzed the sensitivity of pipelines to detect known aging-related susceptibility changes in the DGM over time.   Results: We found high variability in the sensitivity of QSM pipelines to detect susceptibility changes. The study highlighted that while most pipelines could detect changes reliably, the choice of BFR algorithm and the referencing strategy substantially influenced the outcome reproducibility error and sensitivity. Notably, pipelines using RESHARP with AMP-PE, HEIDI or LSQR inversion showed the highest overall sensitivity.   Conclusions: The findings underscore the critical influence of algorithmic choices in QSM processing on the accuracy and reliability of detecting physiological changes in the brain. This has profound implications for clinical research and trials where QSM is used as a biomarker for disease progression, highlighting that careful consideration should be given to pipeline configuration to optimize clinical outcomes.|http://arxiv.org/abs/2501.17158v1|Fahad Salman,Abhisri Ramesh,Thomas Jochmann,Mirjam Prayer,Ademola Adegbemigun,Jack A. Reeves,Gregory E. Wilding,Junghun Cho,Dejan Jakimovski,Niels Bergsland,Michael G. Dwyer,Robert Zivadinov,Ferdinand Schweser
1309|Personalized Medical Treatments Using Novel Reinforcement Learning Algorithms|In both the fields of computer science and medicine there is very strong interest in developing personalized treatment policies for patients who have variable responses to treatments. In particular, I aim to find an optimal personalized treatment policy which is a non-deterministic function of the patient specific covariate data that maximizes the expected survival time or clinical outcome. I developed an algorithmic framework to solve multistage decision problem with a varying number of stages that are subject to censoring in which the "rewards" are expected survival times. In specific, I developed a novel Q-learning algorithm that dynamically adjusts for these parameters. Furthermore, I found finite upper bounds on the generalized error of the treatment paths constructed by this algorithm. I have also shown that when the optimal Q-function is an element of the approximation space, the anticipated survival times for the treatment regime constructed by the algorithm will converge to the optimal treatment path. I demonstrated the performance of the proposed algorithmic framework via simulation studies and through the analysis of chronic depression data and a hypothetical clinical trial. The censored Q-learning algorithm I developed is more effective than the state of the art clinical decision support systems and is able to operate in environments when many covariate parameters may be unobtainable or censored.|http://arxiv.org/abs/1406.3922v2|Yousuf M. Soliman
1310|A Conversation with Nancy Flournoy|Nancy Flournoy was born in Long Beach, California, on May 4, 1947. After graduating from Polytechnic School in Pasadena in 1965, she earned a B.S. (1969) and M.S. (1971) in biostatistics from UCLA. Between her bachelors and masters degrees, she worked as a Statistician I for Regional Medical Programs at UCLA. After receiving her master's degree, she spend three years at the Southwest Laboratory for Education Research and Development in Seal Beach, California. Flournoy joined the Seattle team pioneering bone marrow transplantation in 1973. She moved with the transplant team into the newly formed Fred Hutchinson Cancer Research Center in 1975 as Director of Clinical Statistics, where she supervised a group responsible for the design and analysis of about 80 simultaneous clinical trials. To support the Clinical Division, she supervised the development of an interdisciplinary shared data software system. She recruited Leonard B. Hearne to create this database management system in 1975 (and married him in 1978). While at the Cancer Center, she was also at the University of Washington, where she received her doctorate in biomathematics in 1982. She became the first female director of the program in statistics at the National Science Foundation (NSF) in 1986. She received service awards from the NSF in 1988 and the National Institute of Statistical Science in 2006 for facilitating interdisciplinary research. Flournoy joined the Department of Mathematics and Statistics at American University in 1988. She moved as department chair to the University of Missouri in 2002, where she became Curators' Distinguished Professor in 2012.|http://arxiv.org/abs/1504.03089v1|William F. Rosenberger
1311|An Overview of Melanoma Detection in Dermoscopy Images Using Image Processing and Machine Learning|The incidence of malignant melanoma continues to increase worldwide. This cancer can strike at any age; it is one of the leading causes of loss of life in young persons. Since this cancer is visible on the skin, it is potentially detectable at a very early stage when it is curable. New developments have converged to make fully automatic early melanoma detection a real possibility. First, the advent of dermoscopy has enabled a dramatic boost in clinical diagnostic ability to the point that melanoma can be detected in the clinic at the very earliest stages. The global adoption of this technology has allowed accumulation of large collections of dermoscopy images of melanomas and benign lesions validated by histopathology. The development of advanced technologies in the areas of image processing and machine learning have given us the ability to allow distinction of malignant melanoma from the many benign mimics that require no biopsy. These new technologies should allow not only earlier detection of melanoma, but also reduction of the large number of needless and costly biopsy procedures. Although some of the new systems reported for these technologies have shown promise in preliminary trials, widespread implementation must await further technical progress in accuracy and reproducibility. In this paper, we provide an overview of computerized detection of melanoma in dermoscopy images. First, we discuss the various aspects of lesion segmentation. Then, we provide a brief overview of clinical feature segmentation. Finally, we discuss the classification stage where machine learning algorithms are applied to the attributes generated from the segmented features to predict the existence of melanoma.|http://arxiv.org/abs/1601.07843v1|Nabin K. Mishra,M. Emre Celebi
1312|Estimate the Warfarin Dose by Ensemble of Machine Learning Algorithms|Warfarin dosing remains challenging due to narrow therapeutic index and highly individual variability. Incorrect warfarin dosing is associated with devastating adverse events. Remarkable efforts have been made to develop the machine learning based warfarin dosing algorithms incorporating clinical factors and genetic variants such as polymorphisms in CYP2C9 and VKORC1. The most widely validated pharmacogenetic algorithm is the IWPC algorithm based on multivariate linear regression (MLR). However, with only a single algorithm, the prediction performance may reach an upper limit even with optimal parameters. Here, we present novel algorithms using stacked generalization frameworks to estimate the warfarin dose, within which different types of machine learning algorithms function together through a meta-machine learning model to maximize the prediction accuracy. Compared to the IWPC-derived MLR algorithm, Stack 1 and 2 based on stacked generalization frameworks performed significantly better overall. Subgroup analysis revealed that the mean of the percentage of patients whose predicted dose of warfarin within 20% of the actual stable therapeutic dose (mean percentage within 20%) for Stack 1 was improved by 12.7% (from 42.47% to 47.86%) in Asians and by 13.5% (from 22.08% to 25.05%) in the low-dose group compared to that for MLR, respectively. These data suggest that our algorithms would especially benefit patients required low warfarin maintenance dose, as subtle changes in warfarin dose could lead to adverse clinical events (thrombosis or bleeding) in patients with low dose. Our study offers novel pharmacogenetic algorithms for clinical trials and practice.|http://arxiv.org/abs/1809.04069v2|Zhiyuan Ma,Ping Wang,Zehui Gao,Ruobing Wang,Koroush Khalighi
1313|The relative efficiency of time-to-progression and continuous measures of cognition in pre-symptomatic Alzheimer's|Pre-symptomatic (or Preclinical) Alzheimer's Disease is defined by biomarker evidence of fibrillar amyloid beta pathology in the absence of clinical symptoms. Clinical trials in this early phase of disease are challenging due to the slow rate of disease progression as measured by periodic cognitive performance tests or by transition to a diagnosis of Mild Cognitive Impairment. In a multisite study, experts provide diagnoses by central chart review without the benefit of in-person assessment. We use a simulation study to demonstrate that models of repeated cognitive assessments detect treatment effects more efficiently compared to models of time-to-progression to an endpoint such as change in diagnosis. Multivariate continuous data are simulated from a Bayesian joint mixed effects model fit to data from the Alzheimer's Disease Neuroimaging Initiative. Simulated progression events are algorithmically derived from the continuous assessments using a random forest model fit to the same data. We find that power is approximately doubled with models of repeated continuous outcomes compared to the time-to-progression analysis. The simulations also demonstrate that a plausible informative missing data pattern can induce a bias which inflates treatment effects, yet 5% Type I error is maintained.|http://arxiv.org/abs/1902.02026v1|Dan Li,Samuel Iddi,Paul S. Aisen,Wesley K. Thompson,Michael C. Donohue
1314|Artificial Intelligence for Pediatric Ophthalmology|PURPOSE OF REVIEW: Despite the impressive results of recent artificial intelligence (AI) applications to general ophthalmology, comparatively less progress has been made toward solving problems in pediatric ophthalmology using similar techniques. This article discusses the unique needs of pediatric ophthalmology patients and how AI techniques can address these challenges, surveys recent applications of AI to pediatric ophthalmology, and discusses future directions in the field.   RECENT FINDINGS: The most significant advances involve the automated detection of retinopathy of prematurity (ROP), yielding results that rival experts. Machine learning (ML) has also been successfully applied to the classification of pediatric cataracts, prediction of post-operative complications following cataract surgery, detection of strabismus and refractive error, prediction of future high myopia, and diagnosis of reading disability via eye tracking. In addition, ML techniques have been used for the study of visual development, vessel segmentation in pediatric fundus images, and ophthalmic image synthesis.   SUMMARY: AI applications could significantly benefit clinical care for pediatric ophthalmology patients by optimizing disease detection and grading, broadening access to care, furthering scientific discovery, and improving clinical efficiency. These methods need to match or surpass physician performance in clinical trials before deployment with patients. Due to widespread use of closed-access data sets and software implementations, it is difficult to directly compare the performance of these approaches, and reproducibility is poor. Open-access data sets and software implementations could alleviate these issues, and encourage further AI applications to pediatric ophthalmology.   KEYWORDS: pediatric ophthalmology, machine learning, artificial intelligence, deep learning|http://arxiv.org/abs/1904.08796v1|Julia E. Reid,Eric Eaton
1315|Bioconjugated oligonucleotides: recent developments and thera-eutic applications|Oligonucleotide-based agents have the potential to treat or cure almost any disease, and are one of the key therapeutic drug classes of the future. Bioconjugated oligonucleotides, a subset of this class, are emerging from basic research and being successfully translated to the clinic. In this review, we first briefly describe two approaches for inhibiting specific genes using oligonucleotides -- antisense DNA (ASO) and RNA interference (RNAi) -- followed by a discussion on delivery to cells. We then summarize and analyze recent developments in bioconjugated oligonucleotides including those possessing GalNAc, cell penetrating peptides, $\alpha$-tocopherol, aptamers, antibodies, cholesterol, squalene, fatty acids, or nucleolipids. These novel conjugates provide a means to enhance tissue targeting, cell internalization, endosomal escape, target binding specificity, resistance to nucleases, and more. We next describe those bioconjugated oligonucleotides approved for patient use or in clinical trials. Finally, we summarize the state of the field, describe current limitations, and discuss future prospects. Biocon-jugation chemistry is at the centerpiece of this therapeutic oligonucleotide revolution, and significant opportunities exist for development of new modification chemistries, for mechanistic studies at the chemical-biology interface, and for translating such agents to the clinic.|http://arxiv.org/abs/2002.11532v1|Sbastien Benizri,Arnaud Gissot,Andrew Martin,Brune Vialet,Mark Grinstaff,Philippe Barthlmy
1316|Wearable vibrotactile stimulation for upper extremity rehabilitation in chronic stroke: clinical feasibility trial using the VTS Glove|Objective: Evaluate the feasibility and potential impacts on hand function using a wearable stimulation device (the VTS Glove) which provides mechanical, vibratory input to the affected limb of chronic stroke survivors.   Methods: A double-blind, randomized, controlled feasibility study including sixteen chronic stroke survivors (mean age: 54; 1-13 years post-stroke) with diminished movement and tactile perception in their affected hand. Participants were given a wearable device to take home and asked to wear it for three hours daily over eight weeks. The device intervention was either (1) the VTS Glove, which provided vibrotactile stimulation to the hand, or (2) an identical glove with vibration disabled. Participants were equally randomly assigned to each condition. Hand and arm function were measured weekly at home and in local physical therapy clinics.   Results: Participants using the VTS Glove showed significantly improved Semmes-Weinstein monofilament exam, reduction in Modified Ashworth measures in the fingers, and some increased voluntary finger flexion, elbow and shoulder range of motion.   Conclusions: Vibrotactile stimulation applied to the disabled limb may impact tactile perception, tone and spasticity, and voluntary range of motion. Wearable devices allow extended application and study of stimulation methods outside of a clinical setting.|http://arxiv.org/abs/2007.09262v1|Caitlyn E. Seim,Steven L. Wolf,Thad E. Starner
1317|A novel method for Causal Structure Discovery from EHR data, a demonstration on type-2 diabetes mellitus|Introduction: The discovery of causal mechanisms underlying diseases enables better diagnosis, prognosis and treatment selection. Clinical trials have been the gold standard for determining causality, but they are resource intensive, sometimes infeasible or unethical. Electronic Health Records (EHR) contain a wealth of real-world data that holds promise for the discovery of disease mechanisms, yet the existing causal structure discovery (CSD) methods fall short on leveraging them due to the special characteristics of the EHR data. We propose a new data transformation method and a novel CSD algorithm to overcome the challenges posed by these characteristics. Materials and methods: We demonstrated the proposed methods on an application to type-2 diabetes mellitus. We used a large EHR data set from Mayo Clinic to internally evaluate the proposed transformation and CSD methods and used another large data set from an independent health system, Fairview Health Services, as external validation. We compared the performance of our proposed method to Fast Greedy Equivalence Search (FGES), a state-of-the-art CSD method in terms of correctness, stability and completeness. We tested the generalizability of the proposed algorithm through external validation. Results and conclusions: The proposed method improved over the existing methods by successfully incorporating study design considerations, was robust in face of unreliable EHR timestamps and inferred causal effect directions more correctly and reliably. The proposed data transformation successfully improved the clinical correctness of the discovered graph and the consistency of edge orientation across bootstrap samples. It resulted in superior accuracy, stability, and completeness.|http://arxiv.org/abs/2011.05489v1|Xinpeng Shen,Sisi Ma,Prashanthi Vemuri,M. Regina Castro,Pedro J. Caraballo,Gyorgy J. Simon
1318|A scoping review of causal methods enabling predictions under hypothetical interventions|Background and Aims: The methods with which prediction models are usually developed mean that neither the parameters nor the predictions should be interpreted causally. However, when prediction models are used to support decision making, there is often a need for predicting outcomes under hypothetical interventions. We aimed to identify published methods for developing and validating prediction models that enable risk estimation of outcomes under hypothetical interventions, utilizing causal inference: their main methodological approaches, underlying assumptions, targeted estimands, and potential pitfalls and challenges with using the method, and unresolved methodological challenges.   Methods: We systematically reviewed literature published by December 2019, considering papers in the health domain that used causal considerations to enable prediction models to be used for predictions under hypothetical interventions.   Results: We identified 4919 papers through database searches and a further 115 papers through manual searches, of which 13 were selected for inclusion, from both the statistical and the machine learning literature. Most of the identified methods for causal inference from observational data were based on marginal structural models and g-estimation.   Conclusions: There exist two broad methodological approaches for allowing prediction under hypothetical intervention into clinical prediction models: 1) enriching prediction models derived from observational studies with estimated causal effects from clinical trials and meta-analyses; and 2) estimating prediction models and causal effects directly from observational data. These methods require extending to dynamic treatment regimes, and consideration of multiple interventions to operationalise a clinical decision support system. Techniques for validating 'causal prediction models' are still in their infancy.|http://arxiv.org/abs/2011.09815v2|Lijing Lin,Matthew Sperrin,David A. Jenkins,Glen P. Martin,Niels Peek
1319|CASTELO: Clustered Atom Subtypes aidEd Lead Optimization -- a combined machine learning and molecular modeling method|Drug discovery is a multi-stage process that comprises two costly major steps: pre-clinical research and clinical trials. Among its stages, lead optimization easily consumes more than half of the pre-clinical budget. We propose a combined machine learning and molecular modeling approach that automates lead optimization workflow \textit{in silico}. The initial data collection is achieved with physics-based molecular dynamics (MD) simulation. Contact matrices are calculated as the preliminary features extracted from the simulations. To take advantage of the temporal information from the simulations, we enhanced contact matrices data with temporal dynamism representation, which are then modeled with unsupervised convolutional variational autoencoder (CVAE). Finally, conventional clustering method and CVAE-based clustering method are compared with metrics to rank the submolecular structures and propose potential candidates for lead optimization. With no need for extensive structure-activity relationship database, our method provides new hints for drug modification hotspots which can be used to improve drug efficacy. Our workflow can potentially reduce the lead optimization turnaround time from months/years to days compared with the conventional labor-intensive process and thus can potentially become a valuable tool for medical researchers.|http://arxiv.org/abs/2011.13788v1|Leili Zhang,Giacomo Domeniconi,Chih-Chieh Yang,Seung-gu Kang,Ruhong Zhou,Guojing Cong
1320|TreCap: A wearable device to measure and assess tremor data of visually guided hand movements in real time|The assessment and treatment of motor symptoms such as tremor in Parkinson's disease depends exclusively on the physician's visual observation of standardised movements (i.e. motor tasks). Wearable sensors such as accelerometers are able to detect some manifestations of these pathological signs in movement disorders. Sensor data from motor tasks, however, must be processed sequentially with annotated data from clinical experts. Hence, we designed TreCap, a custom-built wearable device with new software to capture and evaluate motor symptoms such as tremor in real time. Inertial sensor data is systematically processed, stored and tailored to each motor task by this software, including annotated data from clinical rating scores and deep brain stimulation parameters. For prototype testing, the wearable device was validated in a pilot study on subjects with physiological hand tremor. The processed data sets are suitable for machine learning to classify motor tasks. Results on healthy subjects demonstrate an accuracy of 95% with support vector machine algorithms. TreCap software is expandable and allows full access to the configuration of all sensors via Bluetooth. Finally, the functions of the entire device provide a platform to be apt for future clinical trials.|http://arxiv.org/abs/2108.01736v1|R. P. Bremm,A. Werle,C. Auer,F. Hertel,J. Gonalves,K. P. Koch
1321|Disability prediction in multiple sclerosis using performance outcome measures and demographic data|Literature on machine learning for multiple sclerosis has primarily focused on the use of neuroimaging data such as magnetic resonance imaging and clinical laboratory tests for disease identification. However, studies have shown that these modalities are not consistent with disease activity such as symptoms or disease progression. Furthermore, the cost of collecting data from these modalities is high, leading to scarce evaluations. In this work, we used multi-dimensional, affordable, physical and smartphone-based performance outcome measures (POM) in conjunction with demographic data to predict multiple sclerosis disease progression. We performed a rigorous benchmarking exercise on two datasets and present results across 13 clinically actionable prediction endpoints and 6 machine learning models. To the best of our knowledge, our results are the first to show that it is possible to predict disease progression using POMs and demographic data in the context of both clinical trials and smartphone-base studies by using two datasets. Moreover, we investigate our models to understand the impact of different POMs and demographics on model performance through feature ablation studies. We also show that model performance is similar across different demographic subgroups (based on age and sex). To enable this work, we developed an end-to-end reusable pre-processing and machine learning framework which allows quicker experimentation over disparate MS datasets.|http://arxiv.org/abs/2204.03969v1|Subhrajit Roy,Diana Mincu,Lev Proleev,Negar Rostamzadeh,Chintan Ghate,Natalie Harris,Christina Chen,Jessica Schrouff,Nenad Tomasev,Fletcher Lee Hartsell,Katherine Heller
1322|Intelligent Sight and Sound: A Chronic Cancer Pain Dataset|Cancer patients experience high rates of chronic pain throughout the treatment process. Assessing pain for this patient population is a vital component of psychological and functional well-being, as it can cause a rapid deterioration of quality of life. Existing work in facial pain detection often have deficiencies in labeling or methodology that prevent them from being clinically relevant. This paper introduces the first chronic cancer pain dataset, collected as part of the Intelligent Sight and Sound (ISS) clinical trial, guided by clinicians to help ensure that model findings yield clinically relevant results. The data collected to date consists of 29 patients, 509 smartphone videos, 189,999 frames, and self-reported affective and activity pain scores adopted from the Brief Pain Inventory (BPI). Using static images and multi-modal data to predict self-reported pain levels, early models show significant gaps between current methods available to predict pain today, with room for improvement. Due to the especially sensitive nature of the inherent Personally Identifiable Information (PII) of facial images, the dataset will be released under the guidance and control of the National Institutes of Health (NIH).|http://arxiv.org/abs/2204.04214v1|Catherine Ordun,Alexandra N. Cha,Edward Raff,Byron Gaskin,Alex Hanson,Mason Rule,Sanjay Purushotham,James L. Gulley
1323|A Decision-Theoretic Model for Using Scientific Data|Many Artificial Intelligence systems depend on the agent's updating its beliefs about the world on the basis of experience. Experiments constitute one type of experience, so scientific methodology offers a natural environment for examining the issues attendant to using this class of evidence. This paper presents a framework which structures the process of using scientific data from research reports for the purpose of making decisions, using decision analysis as the basis for the structure and using medical research as the general scientific domain. The structure extends the basic influence diagram for updating belief in an object domain parameter of interest by expanding the parameter into four parts: those of the patient, the population, the study sample, and the effective study sample. The structure uses biases to perform the transformation of one parameter into another, so that, for instance, selection biases, in concert with the population parameter, yield the study sample parameter. The influence diagram structure provides decision theoretic justification for practices of good clinical research such as randomized assignment and blindfolding of care providers. The model covers most research designs used in medicine: case-control studies, cohort studies, and controlled clinical trials, and provides an architecture to separate clearly between statistical knowledge and domain knowledge. The proposed general model can be the basis for clinical epidemiological advisory systems, when coupled with heuristic pruning of irrelevant biases; of statistical workstations, when the computational machinery for calculation of posterior distributions is added; and of meta-analytic reviews, when multiple studies may impact on a single population parameter.|http://arxiv.org/abs/1304.1514v1|Harold P. Lehmann
1324|Model-based optimal AML consolidation treatment|Neutropenia is an adverse event commonly arising during intensive chemotherapy of acute myeloid leukemia (AML). It is often associated with infectious complications. Mathematical modeling, simulation, and optimization of the treatment process would be a valuable tool to support clinical decision making, potentially resulting in less severe side effects and deeper remissions. However, until now, there has been no validated mathematical model available to simulate the effect of chemotherapy treatment on white blood cell (WBC) counts and leukemic cells simultaneously. We developed a population pharmacokinetic/pharmacodynamic (PK/PD) model combining a myelosuppression model considering endogenous granulocyte-colony stimulating factor (G-CSF), a PK model for cytarabine (Ara-C), a subcutaneous absorption model for exogenous G-CSF, and a two-compartment model for leukemic blasts. This model was fitted to data of 44 AML patients during consolidation therapy with a novel Ara-C plus G-CSF schedule from a phase II controlled clinical trial. Additionally, we were able to optimize treatment schedules with respect to disease progression, WBC nadirs, and the amount of Ara-C and G-CSF. The developed PK/PD model provided good prediction accuracies and an interpretation of the interaction between WBCs, G-CSF, and blasts. For 14 patients (those with available bone marrow blast counts), we achieved a median 4.2-fold higher WBC count at nadir, which is the most critical time during consolidation therapy. The simulation results showed that relative bone marrow blast counts remained below the clinically important threshold of 5%, with a median of 60% reduction in Ara-C.|http://arxiv.org/abs/1911.08980v2|Felix Jost,Enrico Schalk,Daniela Weber,Hartmut Doehner,Thomas Fischer,Sebastian Sager
1325|REFINE2: A tool to evaluate real-world performance of machine-learning based effect estimators for molecular and clinical studies|Data-adaptive (machine learning-based) effect estimators are increasingly popular to reduce bias in high-dimensional bioinformatic and clinical studies (e.g. real-world data, target trials, -omic discovery). Their relative statistical efficiency (high power) is particularly invaluable in these contexts since sample sizes are often limited due to practical and cost concerns. However, these methods are subject to technical limitations that are dataset specific and involve computational trade-offs. Thus, it is challenging for analysts to identify when such methods may offer benefits or select amongst statistical methods. We present extensive simulation studies of several cutting-edge estimators, evaluating both performance and computation time. Critically, rather than use arbitrary simulation data, we generate synthetic datasets mimicking the observed data structure (plasmode simulation) of a real molecular epidemiologic cohort. We find that machine learning approaches may not always be indicated in such data settings, but that performance is highly context dependent. We present a user-friendly Shiny app REFINE2 (Realistic Evaluations of Finite sample INference using Efficient Estimators) that enables analysts to simulate synthetic data from their own datasets and directly evaluate the performance of several cutting-edge algorithms in those settings. This tool may greatly facilitate the proper selection and implementation of machine-learning-based effect estimators in bioinformatic and clinical study contexts.|http://arxiv.org/abs/2105.13148v4|Xiang Meng,Jonathan Huang
1326|Lung Cancer Risk Estimation with Incomplete Data: A Joint Missing Imputation Perspective|Data from multi-modality provide complementary information in clinical prediction, but missing data in clinical cohorts limits the number of subjects in multi-modal learning context. Multi-modal missing imputation is challenging with existing methods when 1) the missing data span across heterogeneous modalities (e.g., image vs. non-image); or 2) one modality is largely missing. In this paper, we address imputation of missing data by modeling the joint distribution of multi-modal data. Motivated by partial bidirectional generative adversarial net (PBiGAN), we propose a new Conditional PBiGAN (C-PBiGAN) method that imputes one modality combining the conditional knowledge from another modality. Specifically, C-PBiGAN introduces a conditional latent space in a missing imputation framework that jointly encodes the available multi-modal data, along with a class regularization loss on imputed data to recover discriminative information. To our knowledge, it is the first generative adversarial model that addresses multi-modal missing imputation by modeling the joint distribution of image and non-image data. We validate our model with both the national lung screening trial (NLST) dataset and an external clinical validation cohort. The proposed C-PBiGAN achieves significant improvements in lung cancer risk estimation compared with representative imputation methods (e.g., AUC values increase in both NLST (+2.9\%) and in-house dataset (+4.3\%) compared with PBiGAN, p$<$0.05).|http://arxiv.org/abs/2107.11882v1|Riqiang Gao,Yucheng Tang,Kaiwen Xu,Ho Hin Lee,Steve Deppen,Kim Sandler,Pierre Massion,Thomas A. Lasko,Yuankai Huo,Bennett A. Landman
1327|Mathematical Modeling, In-Human Evaluation and Analysis of Volume Kinetics and Kidney Function after Burn Injury and Resuscitation|Existing burn resuscitation protocols exhibit large variability in treatment efficacy. Hence, they must be further optimized based on comprehensive knowledge of burn pathophysiology. A physics-based mathematical model that can replicate physiological responses in diverse burn patients can serve as an attractive basis to perform non-clinical testing of burn resuscitation protocols and to expand knowledge on burn pathophysiology. We intend to develop, optimize, validate, and analyze a mathematical model to replicate physiological responses in burn patients. Using clinical datasets collected from 233 burn patients receiving burn resuscitation, we developed and validated a mathematical model applicable to computer-aided in-human burn resuscitation trial and knowledge expansion. Using the validated mathematical model, we examined possible physiological mechanisms responsible for the cohort-dependent differences in burn pathophysiology between younger versus older patients, female versus male patients, and patients with versus without inhalational injury. We demonstrated that the mathematical model could replicate physiological responses in burn patients associated with wide demographic characteristics and injury severity and that an increased inflammatory response to injury may be a key contributing factor in increasing the mortality risk of older patients and patients with inhalation injury via an increase in the fluid retention. The mathematical model may provide an attractive platform to conduct non-clinical testing of burn resuscitation protocols and test new hypotheses on burn pathophysiology.|http://arxiv.org/abs/2110.13909v1|Ghazal ArabiDarrehDor,Ali Tivay,Chris Meador,George C. Kramer,Jin-Oh Hahn,Jose Salinas
1328|Definition and clinical validation of Pain Patient States from high-dimensional mobile data: application to a chronic pain cohort|The technical capacity to monitor patients with a mobile device has drastically expanded, but data produced from this approach are often difficult to interpret. We present a solution to produce a meaningful representation of patient status from large, complex data streams, leveraging both a data-driven approach, and use clinical knowledge to validate results. Data were collected from a clinical trial enrolling chronic pain patients, and included questionnaires, voice recordings, actigraphy, and standard health assessments. The data were reduced using a clustering analysis. In an initial exploratory analysis with only questionnaire data, we found up to 3 stable cluster solutions that grouped symptoms on a positive to negative spectrum. Objective features (actigraphy, speech) expanded the cluster solution granularity. Using a 5 state solution with questionnaire and actigraphy data, we found significant correlations between cluster properties and assessments of disability and quality-of-life. The correlation coefficient values showed an ordinal distinction, confirming the cluster ranking on a negative to positive spectrum. This suggests we captured novel, distinct Pain Patient States with this approach, even when multiple clusters were equated on pain magnitude. Relative to using complex time courses of many variables, Pain Patient States holds promise as an interpretable, useful, and actionable metric for a clinician or caregiver to simplify and provide timely delivery of care.|http://arxiv.org/abs/2301.00299v1|Jenna M. Reinen,Carla Agurto,Guillermo Cecchi,Jeffrey L. Rogers,NAVITAS,ENVISION Studies Physician Author Group,Boston Scientific Research Scientists Consortium
1329|Bayesian inference and role of astrocytes in amyloid-beta dynamics with modelling of Alzheimer's disease using clinical data|Alzheimer's disease (AD) is a prominent, worldwide, age-related neurodegenerative disease that currently has no systemic treatment. Strong evidence suggests that permeable amyloid-beta peptide (Abeta) oligomers, astrogliosis and reactive astrocytosis cause neuronal damage in AD. A large amount of Abeta is secreted by astrocytes, which contributes to the total Abeta deposition in the brain. This suggests that astrocytes may also play a role in AD, leading to increased attention to their dynamics and associated mechanisms. Therefore, in the present study, we developed and evaluated novel stochastic models for Abeta growth using ADNI data to predict the effect of astrocytes on AD progression in a clinical trial. In the AD case, accurate prediction is required for a successful clinical treatment plan. Given that AD studies are observational in nature and involve routine patient visits, stochastic models provide a suitable framework for modelling AD. Using the approximate Bayesian computation (ABC) approach, the AD etiology may be modelled as a multi-state disease process. As a result, we use this approach to examine the weak and strong influence of astrocytes at multiple disease progression stages using ADNI data from the baseline to 2-year visits for AD patients whose ages ranged from 50 to 90 years. Based on ADNI data, we discovered that the strong astrocyte effect (i.e., a higher concentration of astrocytes as compared to Abeta) could help to lower or clear the growth of Abeta, which is a key to slowing down AD progression.|http://arxiv.org/abs/2306.12520v1|Hina Shaheen,Roderick Melnik,The Alzheimer's Disease Neuroimaging Initiative
1330|Is attention all you need in medical image analysis? A review|Medical imaging is a key component in clinical diagnosis, treatment planning and clinical trial design, accounting for almost 90% of all healthcare data. CNNs achieved performance gains in medical image analysis (MIA) over the last years. CNNs can efficiently model local pixel interactions and be trained on small-scale MI data. The main disadvantage of typical CNN models is that they ignore global pixel relationships within images, which limits their generalisation ability to understand out-of-distribution data with different 'global' information. The recent progress of Artificial Intelligence gave rise to Transformers, which can learn global relationships from data. However, full Transformer models need to be trained on large-scale data and involve tremendous computational complexity. Attention and Transformer compartments (Transf/Attention) which can well maintain properties for modelling global relationships, have been proposed as lighter alternatives of full Transformers. Recently, there is an increasing trend to co-pollinate complementary local-global properties from CNN and Transf/Attention architectures, which led to a new era of hybrid models. The past years have witnessed substantial growth in hybrid CNN-Transf/Attention models across diverse MIA problems. In this systematic review, we survey existing hybrid CNN-Transf/Attention models, review and unravel key architectural designs, analyse breakthroughs, and evaluate current and future opportunities as well as challenges. We also introduced a comprehensive analysis framework on generalisation opportunities of scientific and clinical impact, based on which new data-driven domain generalisation and adaptation methods can be stimulated.|http://arxiv.org/abs/2307.12775v1|Giorgos Papanastasiou,Nikolaos Dikaios,Jiahao Huang,Chengjia Wang,Guang Yang
1331|Personalised Drug Identifier for Cancer Treatment with Transformers using Auxiliary Information|Cancer remains a global challenge due to its growing clinical and economic burden. Its uniquely personal manifestation, which makes treatment difficult, has fuelled the quest for personalized treatment strategies. Thus, genomic profiling is increasingly becoming part of clinical diagnostic panels. Effective use of such panels requires accurate drug response prediction (DRP) models, which are challenging to build due to limited labelled patient data. Previous methods to address this problem have used various forms of transfer learning. However, they do not explicitly model the variable length sequential structure of the list of mutations in such diagnostic panels. Further, they do not utilize auxiliary information (like patient survival) for model training. We address these limitations through a novel transformer based method, which surpasses the performance of state-of-the-art DRP models on benchmark data. We also present the design of a treatment recommendation system (TRS), which is currently deployed at the National University Hospital, Singapore and is being evaluated in a clinical trial.|http://arxiv.org/abs/2402.10551v1|Aishwarya Jayagopal,Hansheng Xue,Ziyang He,Robert J. Walsh,Krishna Kumar Hariprasannan,David Shao Peng Tan,Tuan Zea Tan,Jason J. Pitt,Anand D. Jeyasekharan,Vaibhav Rajan
1332|DeepCRE: Transforming Drug R&D via AI-Driven Cross-drug Response Evaluation|The fields of therapeutic application and drug research and development (R&D) both face substantial challenges, i.e., the therapeutic domain calls for more treatment alternatives, while numerous promising pre-clinical drugs have failed in clinical trials. One of the reasons is the inadequacy of Cross-drug Response Evaluation (CRE) during the late stages of drug R&D. Although in-silico CRE models bring a promising solution, existing methodologies are restricted to early stages of drug R&D, such as target and cell-line levels, offering limited improvement to clinical success rates. Herein, we introduce DeepCRE, a pioneering AI model designed to predict CRE effectively in the late stages of drug R&D. DeepCRE outperforms the existing best models by achieving an average performance improvement of 17.7% in patient-level CRE, and a 5-fold increase in indication-level CRE, facilitating more accurate personalized treatment predictions and better pharmaceutical value assessment for indications, respectively. Furthermore, DeepCRE has identified a set of six drug candidates that show significantly greater effectiveness than a comparator set of two approved drugs in 5/8 colorectal cancer organoids. This demonstrates the capability of DeepCRE to systematically uncover a spectrum of drug candidates with enhanced therapeutic effects, highlighting its potential to transform drug R&D.|http://arxiv.org/abs/2403.03768v3|Yushuai Wu,Ting Zhang,Hao Zhou,Hainan Wu,Hanwen Sunchu,Lei Hu,Xiaofang Chen,Suyuan Zhao,Gaochao Liu,Chao Sun,Jiahuan Zhang,Yizhen Luo,Peng Liu,Zaiqing Nie,Yushuai Wu
1333|A deep-learning model for one-shot transcranial ultrasound simulation and phase aberration correction|Transcranial ultrasound (TUS) has emerged as a promising tool in clinical and research settings due to its potential to modulate neuronal activity, open the blood-brain barrier, facilitate targeted drug delivery via nanoparticles, and perform thermal ablation, all non-invasively. By delivering focused ultrasound waves to precise regions anywhere in the brain, TUS enables targeted energy deposition and is being explored in over fifty clinical trials as a treatment for conditions such as opioid addiction, Alzheimer's disease, dementia, epilepsy, and glioblastoma. However, effective TUS treatment requires careful ultrasound parameter design and precise computation of the focal spot's location and pressure, as skull heterogeneity increases the risk of off-target sonication or insufficient energy delivery to neural tissue. In clinical settings, this phase aberration correction must be computed within seconds. To achieve this, commercial devices often rely on faster methods, such as ray tracing, to predict the focus location and pressure. While computationally efficient, these methods may not always provide the high level of accuracy needed for optimal TUS delivery. We present TUSNet, the first end-to-end deep learning approach to solve for both the pressure field and phase aberration corrections without being bound to the inherent trade-off between accuracy and efficiency. TUSNet computes the 2D transcranial ultrasound pressure field and phase corrections within 21 milliseconds (over $1200\times$ faster than k-Wave, a MATLAB-based acoustic simulation package), achieving $98.3\%$ accuracy in estimating peak pressure magnitude at the focal spot with a mean positioning error of only $0.18$ mm compared to ground truth from k-Wave.|http://arxiv.org/abs/2410.19995v1|Kasra Naftchi-Ardebili,Karanpartap Singh,Gerald R. Popelka,Kim Butts Pauly
1334|A Perspective for Adapting Generalist AI to Specialized Medical AI Applications and Their Challenges|The integration of Large Language Models (LLMs) into medical applications has sparked widespread interest across the healthcare industry, from drug discovery and development to clinical decision support, assisting telemedicine, medical devices, and healthcare insurance applications. This perspective paper aims to discuss the inner workings of building LLM-powered medical AI applications and introduces a comprehensive framework for their development. We review existing literature and outline the unique challenges of applying LLMs in specialized medical contexts. Additionally, we introduce a three-step framework to organize medical LLM research activities: 1) Modeling: breaking down complex medical workflows into manageable steps for developing medical-specific models; 2) Optimization: optimizing the model performance with crafted prompts and integrating external knowledge and tools, and 3) System engineering: decomposing complex tasks into subtasks and leveraging human expertise for building medical AI applications. Furthermore, we offer a detailed use case playbook that describes various LLM-powered medical AI applications, such as optimizing clinical trial design, enhancing clinical decision support, and advancing medical imaging analysis. Finally, we discuss various challenges and considerations for building medical AI applications with LLMs, such as handling hallucination issues, data ownership and compliance, privacy, intellectual property considerations, compute cost, sustainability issues, and responsible AI requirements.|http://arxiv.org/abs/2411.00024v3|Zifeng Wang,Hanyin Wang,Benjamin Danek,Ying Li,Christina Mack,Hoifung Poon,Yajuan Wang,Pranav Rajpurkar,Jimeng Sun
1335|How reproducible are data-driven subtypes of Alzheimer's disease atrophy?|Alzheimer's disease (AD) exhibits substantial clinical and biological heterogeneity, complicating efforts in treatment and intervention development. While new computational methods offer insights into AD progression, the reproducibility of these subtypes across datasets remains understudied, particularly concerning the robustness of subtype definitions when validated on diverse databases. This study evaluates the consistency of AD progression subtypes identified by the Subtype and Stage Inference (SuStaIn) algorithm using T1-weighted MRI data across 5,444 subjects from ANMerge, OASIS, and ADNI datasets, forming four independent cohorts. Each cohort was analyzed under two conditions: one using the full cohort, including cognitively normal controls, and another excluding controls to test subtype robustness. Results confirm the three primary atrophy subtypes identified in earlier studies: Typical, Cortical, and Subcortical, as well as the emergence of rare and atypical AD variants such as posterior cortical atrophy (PCA). Notably, each subtype displayed varying robustness to the inclusion of controls, with certain subtypes, like Subcortical, more influenced by cohort composition. This investigation underscores SuStaIn's reliability for defining stable AD subtypes and suggests its utility in clinical stratification for trials and diagnosis. However, our findings also highlight the need for improved dataset diversity, particularly in terms of ethnic representation, to enhance generalizability and support broader clinical application.|http://arxiv.org/abs/2412.00160v1|Emma Prevot,Cameron Shand,Neil Oxtoby,for Alzheimer's Disease Neuroimaging Initiative
1336|Computational modelling of biological systems now and then: revisiting tools and visions from the beginning of the century|Since the turn of the millennium, computational modelling of biological systems has evolved remarkably and sees matured use spanning basic and clinical research. While the topic of the peri-millennial debate about the virtues and limitations of 'reductionism and integrationism' seems less controversial today, a new apparent dichotomy dominates discussions: mechanistic vs. data-driven modelling. In light of this distinction, we provide an overview of recent achievements and new challenges with a focus on the cardiovascular system. Attention has shifted from generating a universal model of the human to either models of individual humans (digital twins) or entire cohorts of models representative of clinical populations to enable in silico clinical trials. Disease-specific parameterisation, inter-individual and intra-individual variability, uncertainty quantification as well as interoperable, standardised, and quality-controlled data are important issues today, which call for open tools, data and metadata standards, as well as strong community interactions. The quantitative, biophysical, and highly controlled approach provided by in silico methods has become an integral part of physiological and medical research. In silico methods have the potential to accelerate future progress also in the fields of integrated multi-physics modelling, multi-scale models, virtual cohort studies, and machine learning beyond what is feasible today. In fact, mechanistic and data-driven modelling can complement each other synergistically and fuel tomorrow's artificial intelligence applications to further our understanding of physiology and disease mechanisms, to generate new hypotheses and assess their plausibility, and thus to contribute to the evolution of preventive, diagnostic, and therapeutic approaches.|http://arxiv.org/abs/2501.13142v1|Axel Loewe,Peter J. Hunter,Peter Kohl
1337|Design and Implementation of a Psychiatry Resident Training System Based on Large Language Models|Mental disorders have become a significant global public health issue, while the shortage of psychiatrists and inefficient training systems severely hinder the accessibility of mental health services. This paper designs and implements an artificial intelligence-based training system for psychiatrists. By integrating technologies such as large language models, knowledge graphs, and expert systems, the system constructs an intelligent and standardized training platform. It includes six functional modules: case generation, consultation dialogue, examination prescription, diagnostic decision-making, integrated traditional Chinese and Western medicine prescription, and expert evaluation, providing comprehensive support from clinical skill training to professional level assessment.The system adopts a B/S architecture, developed using the Vue.js and Node.js technology stack, and innovatively applies deep learning algorithms for case generation and doctor-patient dialogue. In a clinical trial involving 60 psychiatrists at different levels, the system demonstrated excellent performance and training outcomes: system stability reached 99.95%, AI dialogue accuracy achieved 96.5%, diagnostic accuracy reached 92.5%, and user satisfaction scored 92.3%. Experimental data showed that doctors using the system improved their knowledge mastery, clinical thinking, and diagnostic skills by 35.6%, 28.4%, and 23.7%, respectively.The research results provide an innovative solution for improving the efficiency of psychiatrist training and hold significant importance for promoting the standardization and scalability of mental health professional development.|http://arxiv.org/abs/2501.14530v1|Zhenguang Zhong,Jia Tang
1338|Compressed Genotyping|Significant volumes of knowledge have been accumulated in recent years linking subtle genetic variations to a wide variety of medical disorders from Cystic Fibrosis to mental retardation. Nevertheless, there are still great challenges in applying this knowledge routinely in the clinic, largely due to the relatively tedious and expensive process of DNA sequencing. Since the genetic polymorphisms that underlie these disorders are relatively rare in the human population, the presence or absence of a disease-linked polymorphism can be thought of as a sparse signal. Using methods and ideas from compressed sensing and group testing, we have developed a cost-effective genotyping protocol. In particular, we have adapted our scheme to a recently developed class of high throughput DNA sequencing technologies, and assembled a mathematical framework that has some important distinctions from 'traditional' compressed sensing ideas in order to address different biological and technical constraints.|http://arxiv.org/abs/0909.3691v1|Yaniv Erlich,Assaf Gordon,Michael Brand,Gregory J. Hannon,Partha P. Mitra
1339|Portosystemic hepatic encephalopathy model shows reversal learning impairment and dysfunction of neural activity in the prefrontal cortex and regions involved in motivated behavior|Hepatic encephalopathy is a neurological complication that affects attention and memory. Experimental animal models have been used to study hepatic Encephalopathy, the most frequent being the portacaval shunt. In order to determine learning impairment and brain functional alterations in this model, we assessed reversal learning and neural metabolic activity in a PCS rat model. Portacaval shunt and sham-operated rats were tested for reversal learning in the Morris water maze. Brains were processed for cytochrome oxidase histochemistry. The portacaval shunt group presents reversal learning impairment and cytochrome oxidase activity reduction in prefrontal cortex, ventral tegmental area and accumbens shell nucleus. These results suggest that this model of portosystemic hepatic encephalopathy shows learning impairment that could be linked to dysfunction in neural activity in the prefrontal cortex and regions involved in motivated behavior.|http://arxiv.org/abs/1111.2957v1|M. Mndez,M. Mndez-Lpez,L. Lpez,M. A. Aller,J. Arias,J. L. Arias
1340|Towards dynamical network biomarkers in neuromodulation of episodic migraine|Computational methods have complemented experimental and clinical neursciences and led to improvements in our understanding of the nervous systems in health and disease. In parallel, neuromodulation in form of electric and magnetic stimulation is gaining increasing acceptance in chronic and intractable diseases. In this paper, we firstly explore the relevant state of the art in fusion of both developments towards translational computational neuroscience. Then, we propose a strategy to employ the new theoretical concept of dynamical network biomarkers (DNB) in episodic manifestations of chronic disorders. In particular, as a first example, we introduce the use of computational models in migraine and illustrate on the basis of this example the potential of DNB as early-warning signals for neuromodulation in episodic migraine.|http://arxiv.org/abs/1308.3552v1|Markus A. Dahlem,Sebastian Rode,Arne May,Naoya Fujiwara,Yoshito Hirata,Kazuyuki Aihara,Jrgen Kurths
1341|The Potential of the Human Connectome as a Biomarker of Brain Disease|The human connectome at the level of fiber tracts between brain regions has been shown to differ in patients with brain disorders compared to healthy control groups. Nonetheless, there is a potentially large number of different network organizations for individual patients that could lead to cognitive deficits prohibiting correct diagnosis. Therefore changes that can distinguish groups might not be sufficient to diagnose the disease that an individual patient suffers from and to indicate the best treatment option for that patient. We describe the challenges introduced by the large variability of connectomes within healthy subjects and patients and outline three common strategies to use connectomes as biomarkers of brain diseases. Finally, we propose a fourth option in using models of simulated brain activity (the dynamic connectome) based on structural connectivity rather than the structure (connectome) itself as a biomarker of disease. Dynamic connectomes, in addition to currently used structural, functional, or effective connectivity, could be an important future biomarker for clinical applications.|http://arxiv.org/abs/1310.4010v1|Marcus Kaiser
1342|Modeling effect of GABAergic current in a basal ganglia computational model|Electrical high frequency stimulation (HFS) of deep brain regions is a method shown to be clinically effective in different types of movement and neurological disorders. In order to shed light on its mode of action a computational model of the basal ganglia network coupled the HFS as injection current into the cells of the subthalamic nucleus (STN). Its overall increased activity rendered a faithful transmission of sensorimotor input through thalamo-cortical relay cells possible. Our contribution uses this model by Rubin and Terman (J Comput Neurosci, 16, 211-223, 2004) as a starting point and integrates recent findings on the importance of the extracellular concentrations of the inhibiting neurotransmitter GABA. We are able to show in this computational study that besides electrical stimulation a high concentration of GABA and its resulting conductivity in STN cells is able to re-establish faithful thalamocortical relaying, which otherwise broke down in the simulated parkinsonian state.|http://arxiv.org/abs/1312.7761v1|Felix Njap,Jens Christian Claussen,Andreas Moser,Ulrich G. Hofmann
1343|Understanding migraine using dynamical network biomarkers|Background: Mathematical modeling approaches are becoming ever more established in clinical neuroscience. They provide insight that is key to understand complex interactions of network phenomena, in general, and interactions within the migraine generator network, in particular.   Purpose: In this study, two recent modeling studies on migraine are set in the context of premonitory symptoms that are easy to confuse for trigger factors. This causality confusion is explained, if migraine attacks are initiated by a transition caused by a tipping point.   Conclusion: We need to characterize the involved neuronal and autonomic subnetworks and their connections during all parts of the migraine cycle if we are ever to understand migraine. We predict that mathematical models have the potential to dismantle large and correlated fluctuations in such subnetworks as a dynamical network biomarker of migraine.|http://arxiv.org/abs/1404.6126v1|Markus A. Dahlem,Jrgen Kurths,Michel D. Ferrari,Kazuyuki Aihara,Marten Scheffer,Arne May
1344|Bi-directioal Motion Detection: A Neural Intelligent Model For Perception of Cognitive Robots|In this paper, a new neuronal circuit, based on the spiking neuronal network model, is proposed in order to detect the movement direction of dynamic objects wandering around cognitive robots. Capability of our new approach in bi-directional movement detection is beholden to its symmetric configuration of the proposed circuit. With due attention to magnificence of handling of blocking problems in neuronal networks such as epilepsy, mounting both excitatory and inhibitory stimuli has been taken into account. Investigations upon applied implementation of aforementioned strategy on PIONEER cognitive robot reveals that the strategy leads to alleviation of potential level in the sensory networks. Furthermore, investigation on intrinsic delay of the circuit reveals not only the noticeable switching rate which could be acquired but the high-efficient coupling of the circuit with the other high-speed ones.|http://arxiv.org/abs/1407.1460v1|Matin Macktoobian
1345|Curved Trajectory Detection : A Novel Neurocognitive Perception Approach for Autonomous Smart Robots|Braitenberg vehicles could be mentioned as the seminal elements for cognitive studies in robotics fields especially neurorobotics to invent more smart robots. Motion detection of dynamic objects could be taken as one of the most inspiring abilities into account which can lead to evolve more intelligent Braitenberg vehicles. In this paper, a new neuronal circuit is established in order to detect curved movements of the objects wandering around Braitenberg vehicles. Modular structure of the novel circuit provides the opportunity to expand the model into huge sensory-biosystems. Furthermore, robust performance of the circuit against epileptic seizures is beholden to simultaneous utilization of excitatory and inhibitory stimuli in the circuit construction. Also, straight movements, as special case of curved movements could be tracked. PIONEER, with due attention to its suitable neurosensors, is used as a Braitenberg vehicle for empirical evaluations. Simulated results and practical experiments are applied to this vehicle in order to verify new achievements of the curved trajectory detector.|http://arxiv.org/abs/1407.1461v1|Matin Macktoobian
1346|Surrogate-assisted analysis of weighted functional brain networks|Graph-theoretical analyses of complex brain networks is a rapidly evolving field with a strong impact for neuroscientific and related clinical research. Due to a number of confounding variables, however, a reliable and meaningful characterization of particularly functional brain networks is a major challenge. Addressing this problem, we present an analysis approach for weighted networks that makes use of surrogate networks with preserved edge weights or vertex strengths. We first investigate whether characteristics of weighted networks are influenced by trivial properties of the edge weights or vertex strengths (e.g., their standard deviations). If so, these influences are then effectively segregated with an appropriate surrogate normalization of the respective network characteristic. We demonstrate this approach by re-examining, in a time-resolved manner, weighted functional brain networks of epilepsy patients and control subjects derived from simultaneous EEG/MEG recordings during different behavioral states. We show that this surrogate-assisted analysis approach reveals complementary information about these networks, can aid with their interpretation, and thus can prevent deriving inappropriate conclusions.|http://arxiv.org/abs/1408.6079v1|Gerrit Ansmann,Klaus Lehnertz
1347|Applied Neural Cross-Correlation into the Curved Trajectory Detection Process for Braitenberg Vehicles|Curved Trajectory Detection (CTD) process could be considered among high-level planned capabilities for cognitive agents, has which been acquired under aegis of embedded artificial spiking neuronal circuits. In this paper, hard-wired implementation of the cross-correlation, as the most common comparison-driven scheme for both natural and artificial bionic constructions named Depth Detection Module(DDM), has been taken into account. It is manifestation of efficient handling upon epileptic seizures due to application of both excitatory and inhibitory connections within the circuit structure. Presented traditional analytic approach of the cross-correlation computation with regard to our neural mapping technique and the acquired traced precision have been turned into account for coherent accomplishments of the aforementioned design in perspective of the desired accuracy upon high-level cognitive reactions. Furthermore, the proposed circuit could be fitted into the scalable neuronal network of the CTD, properly. Simulated denouements have been captured based on the computational model of PIONEER mobile robot to verify characteristics of the module, in detail.|http://arxiv.org/abs/1410.3199v1|Matin Macktoobian,Mohammad Jafari,Erfan Attarzadeh Gh
1348|Machine Learning for Neuroimaging with Scikit-Learn|Statistical machine learning methods are increasingly used for neuroimaging data analysis. Their main virtue is their ability to model high-dimensional datasets, e.g. multivariate analysis of activation images or resting-state time series. Supervised learning is typically used in decoding or encoding settings to relate brain images to behavioral or clinical observations, while unsupervised learning can uncover hidden structures in sets of images (e.g. resting state functional MRI) or find sub-populations in large cohorts. By considering different functional neuroimaging applications, we illustrate how scikit-learn, a Python machine learning library, can be used to perform some key analysis steps. Scikit-learn contains a very large set of statistical learning algorithms, both supervised and unsupervised, and its application to neuroimaging data provides a versatile tool to study the brain.|http://arxiv.org/abs/1412.3919v1|Alexandre Abraham,Fabian Pedregosa,Michael Eickenberg,Philippe Gervais,Andreas Muller,Jean Kossaifi,Alexandre Gramfort,Bertrand Thirion,Gel Varoquaux
1349|Functional Brain Imaging: A Comprehensive Survey|Functional brain imaging allows measuring dynamic functionality in all brain regions. It is broadly used in clinical cognitive neuroscience as, well as in research. It will allow the observation of neural activities in the brain simultaneously. From the beginning when functional brain imaging was initiated by the mapping of brain functions proposed by phrenologists, many scientists were asking why we need to image brain functionality since we have already structural information. Simply, their important question was including a great answer. Functional information of the human brain would definitely complement structural information, helping to have a better understanding of what is happening in the brain. This paper, which could be useful to those who have an interest in functional brain imaging, such as engineers, will present a quick review of modalities used in functional brain imaging. We will concentrate on the most used techniques in functional imaging which are functional magnetic resonance imaging (fMRI) and functional optical imaging, which is one of novelties in this area of study.|http://arxiv.org/abs/1602.02225v4|Saman Sarraf,Jian Sun
1350|The connected brain: Causality, models and intrinsic dynamics|Recently, there have been several concerted international efforts - the BRAIN initiative, European Human Brain Project and the Human Connectome Project, to name a few - that hope to revolutionize our understanding of the connected brain. Over the past two decades, functional neuroimaging has emerged as the predominant technique in systems neuroscience. This is foreshadowed by an ever increasing number of publications on functional connectivity, causal modeling, connectomics, and multivariate analyses of distributed patterns of brain responses. In this article, we summarize pedagogically the (deep) history of brain mapping. We will highlight the theoretical advances made in the (dynamic) causal modelling of brain function - that may have escaped the wider audience of this article - and provide a brief overview of recent developments and interesting clinical applications. We hope that this article will engage the signal processing community by showcasing the inherently multidisciplinary nature of this important topic and the intriguing questions that are being addressed.|http://arxiv.org/abs/1602.02945v1|Adeel Razi,Karl Friston
1351|On the Formal Development of Behavioral Reactive Agents: A Systematic Braitenberg-Vehicle Approach|In this paper, a novel process has been developed to realize high-level complex cognitive behaviors into reactive agents, efficiently. This method paves the way for deducting high-level reactive behaviors from low-level perceptive information by autonomous robots. The aforementioned process lets us actualize different generations of Braitenberg vehicles, are which able to mimic desired behaviors to survive in complex environments with high degrees of flexibility in perception and emergence of high-level cognitive actions. The approach has been used to engineer a Braitenberg vehicle with a wide range of perception-action capabilities. Verification would be realized within this framework, due to the efficient traceability between each sequential pair of process phases. The applied simulations demonstrate the efficiency of the established development process, based on the Braitenberg vehicle's behavior.|http://arxiv.org/abs/1612.03979v1|Matin Macktoobian,Ahmad T Khataminejad
1352|Computational Psychiatry in Borderline Personality Disorder|Purpose of review: We review the literature on the use and potential use of computational psychiatry methods in Borderline Personality Disorder.   Recent findings: Computational approaches have been used in psychiatry to increase our understanding of the molecular, circuit, and behavioral basis of mental illness. This is of particular interest in BPD, where the collection of ecologically valid data, especially in interpersonal settings, is becoming more common and more often subject to quantification. Methods that test learning and memory in social contexts, collect data from real-world settings, and relate behavior to molecular and circuit networks are yielding data of particular interest.   Summary: Research in BPD should focus on collaborative efforts to design and interpret experiments with direct relevance to core BPD symptoms and potential for translation to the clinic.|http://arxiv.org/abs/1707.03354v2|Sarah K Fineberg,Dylan Stahl,Philip Corlett
1353|Extracranial estimation of neural mass model parameters using the Unscented Kalman Filter|Data assimilation, defined as the fusion of data with preexisting knowledge, is particularly suited to elucidating underlying phenomena from noisy/insufficient observations. Although this approach has been widely used in diverse fields, only recently have efforts been directed to problems in neuroscience, using mainly intracranial data and thus limiting its applicability to invasive measurements involving electrode implants. Here we intend to apply data assimilation to non-invasive electroencephalography (EEG) measurements to infer brain states and their characteristics. For this purpose, we use Kalman filtering to combine synthetic EEG data with a coupled neural-mass model together with Ary's model of the head, which projects intracranial signals onto the scalp. Our results show that using several extracranial electrodes allows to successfully estimate the state and parameters of the neural masses and their interactions, whereas one single electrode provides only a very partial and insufficient view of the system. The superiority of using multiple extracranial electrodes over using only one, be it intra- or extracranial, is shown over a wide variety of dynamical behaviours. Our results show potential towards future clinical applications of the method.|http://arxiv.org/abs/1708.05282v1|Lara Escuain-Poole,Jordi Garcia-Ojalvo,Antonio J. Pons
1354|Interpreting weight maps in terms of cognitive or clinical neuroscience: nonsense?|Since machine learning models have been applied to neuroimaging data, researchers have drawn conclusions from the derived weight maps. In particular, weight maps of classifiers between two conditions are often described as a proxy for the underlying signal differences between the conditions. Recent studies have however suggested that such weight maps could not reliably recover the source of the neural signals and even led to false positives (FP). In this work, we used semi-simulated data from ElectroCorticoGraphy (ECoG) to investigate how the signal-to-noise ratio and sparsity of the neural signal affect the similarity between signal and weights. We show that not all cases produce FP and that it is unlikely for FP features to have a high weight in most cases.|http://arxiv.org/abs/1804.11259v1|Jessica Schrouff,Janaina Mourao-Miranda
1355|Bringing in the outliers: A sparse subspace clustering approach to learn a dictionary of mouse ultrasonic vocalizations|Mice vocalize in the ultrasonic range during social interactions. These vocalizations are used in neuroscience and clinical studies to tap into complex behaviors and states. The analysis of these ultrasonic vocalizations (USVs) has been traditionally a manual process, which is prone to errors and human bias, and is not scalable to large scale analysis. We propose a new method to automatically create a dictionary of USVs based on a two-step spectral clustering approach, where we split the set of USVs into inlier and outlier data sets. This approach is motivated by the known degrading performance of sparse subspace clustering with outliers. We apply spectral clustering to the inlier data set and later find the clusters for the outliers. We propose quantitative and qualitative performance measures to evaluate our method in this setting, where there is no ground truth. Our approach outperforms two baselines based on k-means and spectral clustering in all of the proposed performance measures, showing greater distances between clusters and more variability between clusters.|http://arxiv.org/abs/2003.05897v1|Jiaxi Wang,Karel Mundnich,Allison T. Knoll,Pat Levitt,Shrikanth Narayanan
1356|Completing the puzzle: why studies in non-human primates are needed to better understand the effects of non-invasive brain stimulation|Brain stimulation is a core method in neuroscience. Numerous non-invasive brain stimulation (NIBS) techniques are currently in use in basic and clinical research, and recent advances promise the ability to non-invasively access deep brain structures. While encouraging, there is a surprising gap in our understanding of precisely how NIBS perturbs neural activity throughout an interconnected network, and how such perturbed neural activity ultimately links to behaviour. In this review, we will consider why non-human primate (NHP) models of NIBS are ideally situated to address this gap in knowledge, and will consider why the oculomotor network that moves our line of sight offers a particularly valuable platform in which to empirically test hypothesis regarding NIBS-induced changes in brain and behaviour. NHP models of NIBS will enable investigation of the complex, dynamic effects of brain stimulation across multiple hierarchically interconnected brain areas, networks, and effectors. By establishing such links between brain and behavioural output, work in NHPs can help optimize experimental and therapeutic approaches, improve NIBS efficacy, and reduce side-effects of NIBS.|http://arxiv.org/abs/2104.11844v2|Sebastian J Lehmann,Brian D Corneil
1357|Topological biomarkers for real-time detection of epileptic seizures|Real time seizure detection is a fundamental problem in computational neuroscience towards diagnosis and treatment's improvement of epileptic disease. We propose a real-time computational method for tracking and detection of epileptic seizures from raw neurophysiological recordings. Our mechanism is based on the topological analysis of the sliding-window embedding of the time series derived from simultaneously recorded channels. We extract topological biomarkers from the signals via the computation of the persistent homology of time-evolving topological spaces. Remarkably, the proposed biomarkers robustly captures the change in the brain dynamics during the ictal state. We apply our methods in different types of signals including scalp and intracranial electroencephalograms and magnetoencephalograms, in patients during interictal and ictal states, showing high accuracy in a range of clinical situations.|http://arxiv.org/abs/2211.02523v2|Ximena Fernndez,Diego Mateos
1358|Introduction: Toward an interdisciplinary science of spontaneous thought|Enormous questions still loom for the emerging science of spontaneous thought: what, exactly, is spontaneous thought? Why does our brain engage in spontaneous forms of thinking, and when is this most likely to occur? And perhaps the question most interesting and accessible from a scientific perspective: how does the brain generate, elaborate, and evaluate its own spontaneous creations? The central aim of this volume is to bring together views from neuroscience, psychology, philosophy, phenomenology, history, education, contemplative traditions, and clinical practice in order to begin to address the ubiquitous but poorly understood mental phenomena that we collectively call 'spontaneous thought.' Perhaps no other mental experience is so familiar to us in daily life, and yet so difficult to understand and explain scientifically. The present volume represents the first effort to bring such highly diverse perspectives to bear on answering the what, when, why, and how of spontaneous thought.|http://arxiv.org/abs/1704.06014v1|Kieran C. R. Fox,Kalina Christoff
1359|Computational Pipeline to probe NaV1.7 gain-of-functions variants in neuropathic painful syndromes|Applications of machine learning and graph theory techniques to neuroscience have witnessed an increased interest in the last decade due to the large data availability and unprecedented technology developments. Their employment to investigate the effect of mutational changes in genes encoding for proteins modulating the membrane of excitable cells, whose biological correlates are assessed at electrophysiological level, could provide useful predictive clues. We apply this concept to the analysis of variants in sodium channel NaV1.7 subunit found in patients with chronic painful syndromes, by the implementation of a dedicated computational pipeline empowering different and complementary techniques including homology modeling, network theory, and machine learning. By testing three templates of different origin and sequence identities, we provide an optimal condition for its use. Our findings reveal the usefulness of our computational pipeline in supporting the selection of candidates for cell electrophysiology assay and with potential clinical applications.|http://arxiv.org/abs/2010.01607v1|Alberto Toffano,Giacomo Chiarot,Stefano Zamuner,Margherita Marchi,Erika Salvi,Stephen G. Waxman,Catharina G. Faber,Giuseppe Lauria,Achille Giacometti,Marta Simeoni
1360|Few-shot Decoding of Brain Activation Maps|Few-shot learning addresses problems for which a limited number of training examples are available. So far, the field has been mostly driven by applications in computer vision. Here, we are interested in adapting recently introduced few-shot methods to solve problems dealing with neuroimaging data, a promising application field. To this end, we create a neuroimaging benchmark dataset for few-shot learning and compare multiple learning paradigms, including meta-learning, as well as various backbone networks. Our experiments show that few-shot methods are able to efficiently decode brain signals using few examples, which paves the way for a number of applications in clinical and cognitive neuroscience, such as identifying biomarkers from brain scans or understanding the generalization of brain representations across a wide range of cognitive tasks.|http://arxiv.org/abs/2010.12500v3|Myriam Bontonou,Giulia Lioi,Nicolas Farrugia,Vincent Gripon
1361|A Blueprint for the Study of the Brain's Spatiotemporal Patterns|The functioning of an organ such as the brain emerges from interactions between its constituent parts. Further, this interaction is not immutable in time but rather unfolds in a succession of patterns, thereby allowing the brain to adapt to constantly changing exterior and interior milieus. This calls for a framework able to study patterned spatiotemporal interactions between components of the brain. A theoretical and methodological framework is developed to study the brain's coordination dynamics. Here we present a toolset designed to decipher the continuous dynamics of electrophysiological data and its relation to (dys-) function. Understanding the spatiotemporal organization of brain patterns and their association with behavioral, cognitive and clinically-relevant variables is an important challenge for the fields of neuroscience and biologically-inspired engineering. It is hoped that such a comprehensive framework will shed light not only on human behavior and the human mind but also help in understanding the growing number of pathologies that are linked to disorders of brain connectivity.|http://arxiv.org/abs/2106.00637v1|Emmanuelle Tognoli,Daniela Benites,J. A. Scott Kelso
1362|Statistical learning methods for neuroimaging data analysis with applications|The aim of this paper is to provide a comprehensive review of statistical challenges in neuroimaging data analysis from neuroimaging techniques to large-scale neuroimaging studies to statistical learning methods. We briefly review eight popular neuroimaging techniques and their potential applications in neuroscience research and clinical translation. We delineate the four common themes of neuroimaging data and review major image processing analysis methods for processing neuroimaging data at the individual level. We briefly review four large-scale neuroimaging-related studies and a consortium on imaging genomics and discuss four common themes of neuroimaging data analysis at the population level. We review nine major population-based statistical analysis methods and their associated statistical challenges and present recent progress in statistical methodology to address these challenges.|http://arxiv.org/abs/2210.09217v1|Hongtu Zhu,Tengfei Li,Bingxin Zhao
1363|Dynamic Functional Connectivity|Most generally, dynamic functional connectivity (FC) refers to the non-instantaneous couplings across timeseries from a set of brain areas, here as measured by fMRI. This is in contrast to static FC, which is defined as purely instantaneous relations. In this chapter, we provide a hands-on description of a non-exhaustive selection of different methods used to estimate dynamic FC (such as sliding windows, clustering approaches, Hidden Markov Models, and multivariate autoregressive models), and we explain, using practical examples, how data should be prepared for dynamic FC analyses and how models of dynamic FC can be evaluated. We also discuss current developments in the dynamic FC research field, including challenges of reliability and reproducibility, and perspectives of using dynamic FC for prediction.|http://arxiv.org/abs/2301.03408v1|Christine Ahrends,Diego Vidaurre
1364|PTGB: Pre-Train Graph Neural Networks for Brain Network Analysis|The human brain is the central hub of the neurobiological system, controlling behavior and cognition in complex ways. Recent advances in neuroscience and neuroimaging analysis have shown a growing interest in the interactions between brain regions of interest (ROIs) and their impact on neural development and disorder diagnosis. As a powerful deep model for analyzing graph-structured data, Graph Neural Networks (GNNs) have been applied for brain network analysis. However, training deep models requires large amounts of labeled data, which is often scarce in brain network datasets due to the complexities of data acquisition and sharing restrictions. To make the most out of available training data, we propose PTGB, a GNN pre-training framework that captures intrinsic brain network structures, regardless of clinical outcomes, and is easily adaptable to various downstream tasks. PTGB comprises two key components: (1) an unsupervised pre-training technique designed specifically for brain networks, which enables learning from large-scale datasets without task-specific labels; (2) a data-driven parcellation atlas mapping pipeline that facilitates knowledge transfer across datasets with different ROI systems. Extensive evaluations using various GNN models have demonstrated the robust and superior performance of PTGB compared to baseline methods.|http://arxiv.org/abs/2305.14376v1|Yi Yang,Hejie Cui,Carl Yang
1365|Bayesian Time-Series Classifier for Decoding Simple Visual Stimuli from Intracranial Neural Activity|Understanding how external stimuli are encoded in distributed neural activity is of significant interest in clinical and basic neuroscience. To address this need, it is essential to develop analytical tools capable of handling limited data and the intrinsic stochasticity present in neural data. In this study, we propose a straightforward Bayesian time series classifier (BTsC) model that tackles these challenges whilst maintaining a high level of interpretability. We demonstrate the classification capabilities of this approach by utilizing neural data to decode colors in a visual task. The model exhibits consistent and reliable average performance of 75.55% on 4 patients' dataset, improving upon state-of-the-art machine learning techniques by about 3.0 percent. In addition to its high classification accuracy, the proposed BTsC model provides interpretable results, making the technique a valuable tool to study neural activity in various tasks and categories. The proposed solution can be applied to neural data recorded in various tasks, where there is a need for interpretable results and accurate classification accuracy.|http://arxiv.org/abs/2307.15672v1|Navid Ziaei,Reza Saadatifard,Ali Yousefi,Behzad Nazari,Sydney S. Cash,Angelique C. Paulk
1366|Mapping tissue microstructure of brain white matter in vivo in health and disease using diffusion MRI|Diffusion magnetic resonance imaging offers unique in vivo sensitivity to tissue microstructure in brain white matter, which undergoes significant changes during development and is compromised in virtually every neurological disorder. Yet, the challenge is to develop biomarkers that are specific to micrometer-scale cellular features in a human MRI scan of a few minutes. Here we quantify the sensitivity and specificity of a multicompartment diffusion modeling framework to the density, orientation and integrity of axons. We demonstrate that using a machine learning based estimator, our biophysical model captures the morphological changes of axons in early development, acute ischemia and multiple sclerosis (total N=821). The methodology of microstructure mapping is widely applicable in clinical settings and in large imaging consortium data to study development, aging and pathology.|http://arxiv.org/abs/2307.16386v2|Ying Liao,Santiago Coelho,Jenny Chen,Benjamin Ades-Aron,Michelle Pang,Ricardo Osorio,Timothy Shepherd,Yvonne W. Lui,Dmitry S. Novikov,Els Fieremans
1367|Dual Meta-Learning with Longitudinally Generalized Regularization for One-Shot Brain Tissue Segmentation Across the Human Lifespan|Brain tissue segmentation is essential for neuroscience and clinical studies. However, segmentation on longitudinal data is challenging due to dynamic brain changes across the lifespan. Previous researches mainly focus on self-supervision with regularizations and will lose longitudinal generalization when fine-tuning on a specific age group. In this paper, we propose a dual meta-learning paradigm to learn longitudinally consistent representations and persist when fine-tuning. Specifically, we learn a plug-and-play feature extractor to extract longitudinal-consistent anatomical representations by meta-feature learning and a well-initialized task head for fine-tuning by meta-initialization learning. Besides, two class-aware regularizations are proposed to encourage longitudinal consistency. Experimental results on the iSeg2019 and ADNI datasets demonstrate the effectiveness of our method. Our code is available at https://github.com/ladderlab-xjtu/DuMeta.|http://arxiv.org/abs/2308.06774v1|Yongheng Sun,Fan Wang,Jun Shu,Haifeng Wang,Li Wang. Deyu Meng,Chunfeng Lian
1368|Linking fast and slow: the case for generative models|A pervasive challenge in neuroscience is testing whether neuronal connectivity changes over time due to specific causes, such as stimuli, events, or clinical interventions. Recent hardware innovations and falling data storage costs enable longer, more naturalistic neuronal recordings. The implicit opportunity for understanding the self-organised brain calls for new analysis methods that link temporal scales: from the order of milliseconds over which neuronal dynamics evolve, to the order of minutes, days or even years over which experimental observations unfold. This review article demonstrates how hierarchical generative models and Bayesian inference help to characterise neuronal activity across different time scales. Crucially, these methods go beyond describing statistical associations among observations and enable inference about underlying mechanisms. We offer an overview of fundamental concepts in state-space modeling and suggest a taxonomy for these methods. Additionally, we introduce key mathematical principles that underscore a separation of temporal scales, such as the slaving principle, and review Bayesian methods that are being used to test hypotheses about the brain with multi-scale data. We hope that this review will serve as a useful primer for experimental and computational neuroscientists on the state of the art and current directions of travel in the complex systems modelling literature.|http://arxiv.org/abs/2308.10618v1|Johan Medrano,Karl J. Friston,Peter Zeidman
1369|Toward Semantic Publishing in Non-Invasive Brain Stimulation: A Comprehensive Analysis of rTMS Studies|Noninvasive brain stimulation (NIBS) encompasses transcranial stimulation techniques that can influence brain excitability. These techniques have the potential to treat conditions like depression, anxiety, and chronic pain, and to provide insights into brain function. However, a lack of standardized reporting practices limits its reproducibility and full clinical potential. This paper aims to foster interinterdisciplinarity toward adopting Computer Science Semantic reporting methods for the standardized documentation of Neuroscience NIBS studies making them explicitly Findable, Accessible, Interoperable, and Reusable (FAIR).   In a large-scale systematic review of 600 repetitive transcranial magnetic stimulation (rTMS), a subarea of NIBS, dosages, we describe key properties that allow for structured descriptions and comparisons of the studies. This paper showcases the semantic publishing of NIBS in the ecosphere of knowledge-graph-based next-generation scholarly digital libraries. Specifically, the FAIR Semantic Web resource(s)-based publishing paradigm is implemented for the 600 reviewed rTMS studies in the Open Research Knowledge Graph.|http://arxiv.org/abs/2310.06517v1|Swathi Anil,Jennifer D'Souza
1370|The explanatory power of activity flow models of brain function|Tremendous neuroscientific progress has recently been made by mapping brain connectivity, complementing extensive knowledge of task-evoked brain activation patterns. However, despite evidence that they are related, these connectivity and activity lines of research have mostly progressed separately. Here I review the notable productivity and future promise of combining connectivity and task-evoked activity estimates into activity flow models. These data-driven computational models simulate the generation of task-evoked activations (including those linked to behavior), producing empirically-supported explanations of the origin of neurocognitive functions based on the flow of task-evoked activity over empirical brain connections. Critically, by incorporating causal principles and extensive empirical constraints from brain data, this approach can provide more mechanistic accounts of neurocognitive phenomena than purely predictive (as opposed to explanatory) models or models optimized primarily for task performance (e.g., standard artificial neural networks). The variety of activity-flow-based explanations reported so far are covered here along with important methodological and theoretical considerations when discovering new activity-flow-based explanations. Together, these considerations illustrate the promise of activity flow modeling for the future of neuroscience and ultimately for the development of novel clinical treatments (e.g., using brain stimulation) for brain disorders.|http://arxiv.org/abs/2402.02191v1|Michael W. Cole
1371|Robust Inference of Dynamic Covariance Using Wishart Processes and Sequential Monte Carlo|Several disciplines, such as econometrics, neuroscience, and computational psychology, study the dynamic interactions between variables over time. A Bayesian nonparametric model known as the Wishart process has been shown to be effective in this situation, but its inference remains highly challenging. In this work, we introduce a Sequential Monte Carlo (SMC) sampler for the Wishart process, and show how it compares to conventional inference approaches, namely MCMC and variational inference. Using simulations we show that SMC sampling results in the most robust estimates and out-of-sample predictions of dynamic covariance. SMC especially outperforms the alternative approaches when using composite covariance functions with correlated parameters. We demonstrate the practical applicability of our proposed approach on a dataset of clinical depression (n=1), and show how using an accurate representation of the posterior distribution can be used to test for dynamics on covariance|http://arxiv.org/abs/2406.04796v1|Hester Huijsdens,David Leeftink,Linda Geerligs,Max Hinne
1372|Individual brain parcellation: Review of methods, validations and applications|Individual brains vary greatly in morphology, connectivity and organization. The applicability of group-level parcellations is limited by the rapid development of precision medicine today because they do not take into account the variation of parcels at the individual level. Accurate mapping of brain functional regions at the individual level is pivotal for a comprehensive understanding of the variations in brain function and behaviors, early and precise identification of brain abnormalities, as well as personalized treatments for neuropsychiatric disorders. With the development of neuroimaging and machine learning techniques, studies on individual brain parcellation are booming. In this paper, we offer an overview of recent advances in the methodologies of individual brain parcellation, including optimization- and learning-based methods. Comprehensive evaluation metrics to validate individual brain mapping have been introduced. We also review the studies of how individual brain mapping promotes neuroscience research and clinical medicine. Finally, we summarize the major challenges and important future directions of individualized brain parcellation. Collectively, we intend to offer a thorough overview of individual brain parcellation methods, validations, and applications, along with highlighting the current challenges that call for an urgent demand for integrated platforms that integrate datasets, methods, and validations.|http://arxiv.org/abs/2407.00984v1|Chengyi Li,Shan Yu,Yue Cui
1373|EEG-SSM: Leveraging State-Space Model for Dementia Detection|State-space models (SSMs) have garnered attention for effectively processing long data sequences, reducing the need to segment time series into shorter intervals for model training and inference. Traditionally, SSMs capture only the temporal dynamics of time series data, omitting the equally critical spectral features. This study introduces EEG-SSM, a novel state-space model-based approach for dementia classification using EEG data. Our model features two primary innovations: EEG-SSM temporal and EEG-SSM spectral components. The temporal component is designed to efficiently process EEG sequences of varying lengths, while the spectral component enhances the model by integrating frequency-domain information from EEG signals. The synergy of these components allows EEG-SSM to adeptly manage the complexities of multivariate EEG data, significantly improving accuracy and stability across different temporal resolutions. Demonstrating a remarkable 91.0 percent accuracy in classifying Healthy Control (HC), Frontotemporal Dementia (FTD), and Alzheimer's Disease (AD) groups, EEG-SSM outperforms existing models on the same dataset. The development of EEG-SSM represents an improvement in the use of state-space models for screening dementia, offering more precise and cost-effective tools for clinical neuroscience.|http://arxiv.org/abs/2407.17801v1|Xuan-The Tran,Linh Le,Quoc Toan Nguyen,Thomas Do,Chin-Teng Lin
1374|In which fields can ChatGPT detect journal article quality? An evaluation of REF2021 results|Time spent by academics on research quality assessment might be reduced if automated approaches can help. Whilst citation-based indicators have been extensively developed and evaluated for this, they have substantial limitations and Large Language Models (LLMs) like ChatGPT provide an alternative approach. This article assesses whether ChatGPT 4o-mini can be used to estimate the quality of journal articles across academia. It samples up to 200 articles from all 34 Units of Assessment (UoAs) in the UK's Research Excellence Framework (REF) 2021, comparing ChatGPT scores with departmental average scores. There was an almost universally positive Spearman correlation between ChatGPT scores and departmental averages, varying between 0.08 (Philosophy) and 0.78 (Psychology, Psychiatry and Neuroscience), except for Clinical Medicine (rho=-0.12). Although other explanations are possible, especially because REF score profiles are public, the results suggest that LLMs can provide reasonable research quality estimates in most areas of science, and particularly the physical and health sciences and engineering, even before citation data is available. Nevertheless, ChatGPT assessments seem to be more positive for most health and physical sciences than for other fields, a concern for multidisciplinary assessments, and the ChatGPT scores are only based on titles and abstracts, so cannot be research evaluations.|http://arxiv.org/abs/2409.16695v1|Mike Thelwall,Abdallah Yaghi
1375|CwA-T: A Channelwise AutoEncoder with Transformer for EEG Abnormality Detection|Electroencephalogram (EEG) signals are critical for detecting abnormal brain activity, but their high dimensionality and complexity pose significant challenges for effective analysis. In this paper, we propose CwA-T, a novel framework that combines a channelwise CNN-based autoencoder with a single-head transformer classifier for efficient EEG abnormality detection. The channelwise autoencoder compresses raw EEG signals while preserving channel independence, reducing computational costs and retaining biologically meaningful features. The compressed representations are then fed into the transformer-based classifier, which efficiently models long-term dependencies to distinguish between normal and abnormal signals. Evaluated on the TUH Abnormal EEG Corpus, the proposed model achieves 85.0% accuracy, 76.2% sensitivity, and 91.2% specificity at the per-case level, outperforming baseline models such as EEGNet, Deep4Conv, and FusionCNN. Furthermore, CwA-T requires only 202M FLOPs and 2.9M parameters, making it significantly more efficient than transformer-based alternatives. The framework retains interpretability through its channelwise design, demonstrating great potential for future applications in neuroscience research and clinical practice. The source code is available at https://github.com/YossiZhao/CAE-T.|http://arxiv.org/abs/2412.14522v2|Youshen Zhao,Keiji Iramina
1376|$DPF^*$: improved Depth Potential Function for scale-invariant sulcal depth estimation|The shape of human brain is complex and highly variable, with interactions between brain size, cortical folding, and age well-documented in the literature. However, few studies have explored how global brain size influences geometric features of the cortical surface derived from anatomical MRI. In this work, we focus on sulcal depth, an imaging phenotype that has gained significant attention in both basic research and clinical applications. We make key contributions to the field by: 1) providing the first quantitative analysis of how brain size affects sulcal depth measurements; 2) introducing a novel, scale-invariant method for sulcal depth estimation based on an original formalization of the problem; 3) presenting a validation framework and sharing our code and benchmark data with the community; and 4) demonstrating the biological relevance of our new sulcal depth measure using a large sample of 1,987 subjects spanning the developmental period from 26 weeks post-conception to adulthood.|http://arxiv.org/abs/2501.05436v1|Maxime Dieudonn,Guillaume Auzias,Julien Lefvre
1377|An Effective Membrane Model of the Immunological Synapse|The immunological synapse is a patterned collection of different types of receptors and ligands that forms in the intercellular junction between T Cells and antigen presenting cells (APCs) during recognition. The synapse is implicated in information transfer between cells, and is characterized by different spatial patterns of receptors at different stages in the life cycle of T cells. We obtain a minimalist model that captures this experimentally observed phenomenology. A functional RG analysis provides further insights.|http://arxiv.org/abs/cond-mat/0304202v1|Subhadip Raychaudhuri,Arup K. Chakraborty,Mehran Kardar
1378|SIMMUNE, a tool for simulating and analyzing immune system behavior|We present a new approach to the simulation and analysis of immune system behavior. The simulations that can be done with our software package called SIMMUNE are based on immunological data that describe the behavior of immune system agents (cells, molecules) on a microscopial (i.e. agent-agent interaction) scale by defining cellular stimulus-response mechanisms. Since the behavior of the agents in SIMMUNE can be very flexibly configured, its application is not limited to immune system simulations. We outline the principles of SIMMUNE's multiscale analysis of emergent structure within the simulated immune system that allow the identification of immunological contexts using minimal a priori assumptions about the higher level organization of the immune system.|http://arxiv.org/abs/cs/9903017v1|M. Meier-Schellersheim,G. Mack
1379|Using Immunology Principles for Anomaly Detection in Electrical Systems|The immune system is a cognitive system of complexity comparable to the brain and its computational algorithms suggest new solutions to engineering problems or new ways of looking at these problems. Using immunological principles, a two (or three-) module algorithm is developed which is capable of launching a specific response to an anomalous situation. Applications are being developed for electromechanical drives and network power transformers. Experimental results illustrate an application to fault detection in squirrel-cage electric motors.|http://arxiv.org/abs/nlin/0106027v1|P. J. Costa Branco,J. A. Dente,R. Vilela Mendes
1380|Agent-Based Modeling of Host-Pathogen Systems: The Successes and Challenges|Agent-based models have been employed to describe numerous processes in immunology. Simulations based on these types of models have been used to enhance our understanding of immunology and disease pathology. We review various agent-based models relevant to host-pathogen systems and discuss their contributions to our understanding of biological processes. We then point out some limitations and challenges of agent-based models and encourage efforts towards reproducibility and model validation.|http://arxiv.org/abs/0807.0247v1|Amy L. Bauer,Catherine A. A. Beauchemin,Alan S. Perelson
1381|Shuyi, A Name After Dendritic Cell-mediated Immunological Memory|Immunological memory is a fundamental theory of modern immunology, which is traditionally believed to be mediated only by B and T lymphocytes that recognize antigen epitopes in a receptor-restricted manner. During the last decade data accumulated to show that monocytes and macrophages, the two main initiators of innate immune response, also built up a "memory" to antigens they encountered, though in most concerned publications a different wording (i.e. "train" or"educate") was utilized to describe this feature. More recently, Hole et al demonstrated a "memory-like" response of dendritic cells (DCs). In brief, if fungal-challenged mice could develop a protective immune response, DCs immediately (in 3 weeks) isolated from those mice would manifest a pro-inflammatory phenotype. Even after the mice were allowed to rest for 10 weeks, DCs from them still exhibited an enhanced immune activation profile in their transcriptome and cytokine productions upon re-challenge with same pathogens. Lastly, Hole showed that the "training" or memory-building in DCs was achieved by histone modification. All above findings obtained in monocytes, macrophages or DCs emphasized the necessity for rechecking the questions whether antigen presenting cells (APCs) as a whole could be classified the third class of cells that would mediate immunological memory. In this essay, the author described the effort he made in late 1990s to identify dendtitic cell-mediated memory, and how he named his daughter SHUYI to memorize that hypothesis.|http://arxiv.org/abs/1908.03686v2|Yiqiang Wang
1382|The use of quantum dots to amplify antigen detection|Proposal to develop an Improved immunological assay employing primary IgG antibodies and secondary IgM antibodies labeled with quantum dots to amplify antigen detection.|http://arxiv.org/abs/1704.08419v1|Earl Bloch,Lou Massa,Kilian Dill
1383|Towards a Mathematical Foundation of Immunology and Amino Acid Chains|We attempt to set a mathematical foundation of immunology and amino acid chains. To measure the similarities of these chains, a kernel on strings is defined using only the sequence of the chains and a good amino acid substitution matrix (e.g. BLOSUM62). The kernel is used in learning machines to predict binding affinities of peptides to human leukocyte antigens DR (HLA-DR) molecules. On both fixed allele (Nielsen and Lund 2009) and pan-allele (Nielsen et.al. 2010) benchmark databases, our algorithm achieves the state-of-the-art performance. The kernel is also used to define a distance on an HLA-DR allele set based on which a clustering analysis precisely recovers the serotype classifications assigned by WHO (Nielsen and Lund 2009, and Marsh et.al. 2010). These results suggest that our kernel relates well the chain structure of both peptides and HLA-DR molecules to their biological functions, and that it offers a simple, powerful and promising methodology to immunology and amino acid chain studies.|http://arxiv.org/abs/1205.6031v2|Wen-Jun Shen,Hau-San Wong,Quan-Wu Xiao,Xin Guo,Stephen Smale
1384|Supervised Learning and Anti-learning of Colorectal Cancer Classes and Survival Rates from Cellular Biology Parameters|In this paper, we describe a dataset relating to cellular and physical conditions of patients who are operated upon to remove colorectal tumours. This data provides a unique insight into immunological status at the point of tumour removal, tumour classification and post-operative survival. Attempts are made to learn relationships between attributes (physical and immunological) and the resulting tumour stage and survival. Results for conventional machine learning approaches can be considered poor, especially for predicting tumour stages for the most important types of cancer. This poor performance is further investigated and compared with a synthetic, dataset based on the logical exclusive-OR function and it is shown that there is a significant level of 'anti-learning' present in all supervised methods used and this can be explained by the highly dimensional, complex and sparsely representative dataset. For predicting the stage of cancer from the immunological attributes, anti-learning approaches outperform a range of popular algorithms.|http://arxiv.org/abs/1307.1599v1|Chris Roadknight,Uwe Aickelin,Guoping Qiu,John Scholefield,Lindy Durrant
1385|A Generation Method of Immunological Memory in Clonal Selection Algorithm by using Restricted Boltzmann Machines|Recently, a high technique of image processing is required to extract the image features in real time. In our research, the tourist subject data are collected from the Mobile Phone based Participatory Sensing (MPPS) system. Each record consists of image files with GPS, geographic location name, user's numerical evaluation, and comments written in natural language at sightseeing spots where a user really visits. In our previous research, the famous landmarks in sightseeing spot can be detected by Clonal Selection Algorithm with Immunological Memory Cell (CSAIM). However, some landmarks was not detected correctly by the previous method because they didn't have enough amount of information for the feature extraction. In order to improve the weakness, we propose the generation method of immunological memory by Restricted Boltzmann Machines. To verify the effectiveness of the method, some experiments for classification of the subjective data are executed by using machine learning tools for Deep Learning.|http://arxiv.org/abs/1804.02816v1|Shin Kamada,Takumi Ichimura
1386|An Immunology-Inspired Network Security Architecture|The coming 5G networks have been enabling the creation of a wide variety of new services and applications which demand a new network security architecture. Immunology is the study of the immune system in vertebrates (including humans) which protects us from infection through various lines of defence. By studying the resemblance between the immune system and network security system, we acquire some inspirations from immunology and distill some guidelines for the design of network security architecture. We present a philosophical design principle, that is maintaining the balance between security and availability. Then, we derive two methodological principles: 1) achieving situation-awareness and fast response through community cooperation among heterogeneous nodes, and 2) Enhancing defense capability through consistently contesting with invaders in a real environment and actively mutating/evolving attack strategies. We also present a reference architecture designed based on the principles.|http://arxiv.org/abs/2001.09273v1|Quan Yu,Jing Ren,Jiyan Zhang,Siyang Liu,Yinjin Fu,Ying Li,Linru Ma,Jian Jing,Wei Zhang
1387|A Guideline for the Statistical Analysis of Compositional Data in Immunology|The study of immune cellular composition has been of great scientific interest in immunology because of the generation of multiple large-scale data. From the statistical point of view, such immune cellular data should be treated as compositional. In compositional data, each element is positive, and all the elements sum to a constant, which can be set to one in general. Standard statistical methods are not directly applicable for the analysis of compositional data because they do not appropriately handle correlations between the compositional elements. In this paper, we review statistical methods for compositional data analysis and illustrate them in the context of immunology. Specifically, we focus on regression analyses using log-ratio transformations and the generalized linear model with Dirichlet distribution, discuss their theoretical foundations, and illustrate their applications with immune cellular fraction data generated from colorectal cancer patients.|http://arxiv.org/abs/2201.07945v2|Jinkyung Yoo,Zequn Sun,Michael Greenacre,Qin Ma,Dongjun Chung,Young Min Kim
1388|Stimuli reduce the dimensionality of cortical activity|The activity of ensembles of simultaneously recorded neurons can be represented as a set of points in the space of firing rates. Even though the dimension of this space is equal to the ensemble size, neural activity can be effectively localized on smaller subspaces. The dimensionality of the neural space is an important determinant of the computational tasks supported by the neural activity. Here, we investigate the dimensionality of neural ensembles from the sensory cortex of alert rats during period of ongoing (inter-trial) and stimulus-evoked activity. We find that dimensionality grows linearly with ensemble size, and grows significantly faster during ongoing activity compared to evoked activity. We explain these results using a spiking network model based on a clustered architecture. The model captures the difference in growth rate between ongoing and evoked activity and predicts a characteristic scaling with ensemble size that could be tested in high-density multi-electrode recordings. Moreover, the model predicts the existence of an upper bound on dimensionality. This upper bound is inversely proportional to the amount of pair-wise correlations and, compared to a homogeneous network without clusters, it is larger by a factor equal to the number of clusters. The empirical estimation of such bounds depends on the number and duration of trials. Together, these results provide a framework to analyze neural dimensionality in alert animals, its behavior under stimulus presentation, and its theoretical dependence on ensemble size, number of clusters, and pair-wise correlations in spiking network models.|http://arxiv.org/abs/1509.03621v3|Luca Mazzucato,Alfredo Fontanini,Giancarlo La Camera
1389|Video object detection for privacy-preserving patient monitoring in intensive care|Patient monitoring in intensive care units, although assisted by biosensors, needs continuous supervision of staff. To reduce the burden on staff members, IT infrastructures are built to record monitoring data and develop clinical decision support systems. These systems, however, are vulnerable to artifacts (e.g. muscle movement due to ongoing treatment), which are often indistinguishable from real and potentially dangerous signals. Video recordings could facilitate the reliable classification of biosignals using object detection (OD) methods to find sources of unwanted artifacts. Due to privacy restrictions, only blurred videos can be stored, which severely impairs the possibility to detect clinically relevant events such as interventions or changes in patient status with standard OD methods. Hence, new kinds of approaches are necessary that exploit every kind of available information due to the reduced information content of blurred footage and that are at the same time easily implementable within the IT infrastructure of a normal hospital. In this paper, we propose a new method for exploiting information in the temporal succession of video frames. To be efficiently implementable using off-the-shelf object detectors that comply with given hardware constraints, we repurpose the image color channels to account for temporal consistency, leading to an improved detection rate of the object classes. Our method outperforms a standard YOLOv5 baseline model by +1.7% mAP@.5 while also training over ten times faster on our proprietary dataset. We conclude that this approach has shown effectiveness in the preliminary experiments and holds potential for more general video OD in the future.|http://arxiv.org/abs/2306.14620v1|Raphael Emberger,Jens Michael Boss,Daniel Baumann,Marko Seric,Shufan Huo,Lukas Tuggener,Emanuela Keller,Thilo Stadelmann
1390|Expectation-Propagation for Likelihood-Free Inference|Many models of interest in the natural and social sciences have no closed-form likelihood function, which means that they cannot be treated using the usual techniques of statistical inference. In the case where such models can be efficiently simulated, Bayesian inference is still possible thanks to the Approximate Bayesian Computation (ABC) algorithm. Although many refinements have been suggested, ABC inference is still far from routine. ABC is often excruciatingly slow due to very low acceptance rates. In addition, ABC requires introducing a vector of "summary statistics", the choice of which is relatively arbitrary, and often require some trial and error, making the whole process quite laborious for the user.   We introduce in this work the EP-ABC algorithm, which is an adaptation to the likelihood-free context of the variational approximation algorithm known as Expectation Propagation (Minka, 2001). The main advantage of EP-ABC is that it is faster by a few orders of magnitude than standard algorithms, while producing an overall approximation error which is typically negligible. A second advantage of EP-ABC is that it replaces the usual global ABC constraint on the vector of summary statistics computed on the whole dataset, by n local constraints of the form that apply separately to each data-point. As a consequence, it is often possible to do away with summary statistics entirely. In that case, EP-ABC approximates directly the evidence (marginal likelihood) of the model.   Comparisons are performed in three real-world applications which are typical of likelihood-free inference, including one application in neuroscience which is novel, and possibly too challenging for standard ABC techniques.|http://arxiv.org/abs/1107.5959v2|Simon Barthelm,Nicolas Chopin
1391|Phase Diagram of Spiking Neural Networks|In computer simulations of spiking neural networks, often it is assumed that every two neurons of the network are connected by a probability of 2\%, 20\% of neurons are inhibitory and 80\% are excitatory. These common values are based on experiments, observations, and trials and errors, but here, I take a different perspective, inspired by evolution, I systematically simulate many networks, each with a different set of parameters, and then I try to figure out what makes the common values desirable. I stimulate networks with pulses and then measure their: dynamic range, dominant frequency of population activities, total duration of activities, maximum rate of population and the occurrence time of maximum rate. The results are organized in phase diagram. This phase diagram gives an insight into the space of parameters -- excitatory to inhibitory ratio, sparseness of connections and synaptic weights. This phase diagram can be used to decide the parameters of a model. The phase diagrams show that networks which are configured according to the common values, have a good dynamic range in response to an impulse and their dynamic range is robust in respect to synaptic weights, and for some synaptic weights they oscillate in $\alpha$ or $\beta$ frequencies, even in absence of external stimuli.|http://arxiv.org/abs/1312.0125v2|Hamed Seyed-allaei
1392|Bayesian Multi--Dipole Modeling in the Frequency Domain|Background: Magneto- and Electro-encephalography record the electromagnetic field generated by neural currents with high temporal frequency and good spatial resolution, and are therefore well suited for source localization in the time and in the frequency domain. In particular, localization of the generators of neural oscillations is very important in the study of cognitive processes in the healthy and in the pathological brain.   New method: We introduce the use of a Bayesian multi-dipole localization method in the frequency domain. Given the Fourier Transform of the data at one or multiple frequencies and/or trials, the algorithm approximates numerically the posterior distribution with Monte Carlo techniques.   Results: We use synthetic data to show that the proposed method behaves well under a wide range of experimental conditions, including low signal-to-noise ratios and correlated sources. We use dipole clusters to mimic the effect of extended sources. In addition, we test the algorithm on real MEG data to confirm its feasibility.   Comparison with existing method(s): Throughout the whole study, DICS (Dynamic Imaging of Coherent Sources) is used systematically as a benchmark. The two methods provide similar general pictures; the posterior distributions of the Bayesian approach contain much richer information at the price of a higher computational cost.   Conclusions: The Bayesian method described in this paper represents a reliable approach for localization of multiple dipoles in the frequency domain.|http://arxiv.org/abs/1808.08086v2|Gianvittorio Luria,Dunja Duran,Elisa Visani,Sara Sommariva,Fabio Rotondi,Davide Rossi Sebastiano,Ferruccio Panzica,Michele Piana,Alberto Sorrentino
1393|Factorized Neural Processes for Neural Processes: $K$-Shot Prediction of Neural Responses|In recent years, artificial neural networks have achieved state-of-the-art performance for predicting the responses of neurons in the visual cortex to natural stimuli. However, they require a time consuming parameter optimization process for accurately modeling the tuning function of newly observed neurons, which prohibits many applications including real-time, closed-loop experiments. We overcome this limitation by formulating the problem as $K$-shot prediction to directly infer a neuron's tuning function from a small set of stimulus-response pairs using a Neural Process. This required us to developed a Factorized Neural Process, which embeds the observed set into a latent space partitioned into the receptive field location and the tuning function properties. We show on simulated responses that the predictions and reconstructed receptive fields from the Factorized Neural Process approach ground truth with increasing number of trials. Critically, the latent representation that summarizes the tuning function of a neuron is inferred in a quick, single forward pass through the network. Finally, we validate this approach on real neural data from visual cortex and find that the predictive accuracy is comparable to -- and for small $K$ even greater than -- optimization based approaches, while being substantially faster. We believe this novel deep learning systems identification framework will facilitate better real-time integration of artificial neural network modeling into neuroscience experiments.|http://arxiv.org/abs/2010.11810v1|R. James Cotton,Fabian H. Sinz,Andreas S. Tolias
1394|A neural network model for timing control with reinforcement|How do humans and animals perform trial-and-error learning when the space of possibilities is infinite? In a previous study, we used an interval timing production task and discovered an updating strategy in which the agent adjusted the behavioral and neuronal noise for exploration. In the experiment, human subjects proactively generated a series of timed motor outputs. We found that the sequential motor timing varied at two temporal scales: long-term correlation around the target interval due to memory drifts and short-term adjustments of timing variability according to feedback. We have previously described these features of timing variability with an augmented Gaussian process, termed reward sensitive Gaussian process (RSGP). Here we provide a mechanistic model and simulate the process by borrowing the architecture of recurrent neural networks. While recurrent connection provided the long-term serial correlation in motor timing, to facilitate reward-driven short-term variations, we introduced reward-dependent variability in the network connectivity, inspired by the stochastic nature of synaptic transmission in the brain. Our model was able to recursively generate an output sequence incorporating the internal variability and external reinforcement in a Bayesian framework. We show that the model can learn the key features of human behavior. Unlike other neural network models that search for unique network connectivity for the best match between the model prediction and observation, this model can estimate the uncertainty associated with each outcome and thus did a better job in teasing apart adjustable task-relevant variability from unexplained variability. The proposed artificial neural network model parallels the mechanisms of information processing in neural systems and can extend the framework of brain-inspired reinforcement learning in continuous state control.|http://arxiv.org/abs/2205.04347v1|Jing Wang,Yousuf El-Jayyousi,Ilker Ozden
1395|Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics Alignment with Diffusion Models|In the field of behavior-related brain computation, it is necessary to align raw neural signals against the drastic domain shift among them. A foundational framework within neuroscience research posits that trial-based neural population activities rely on low-dimensional latent dynamics, thus focusing on the latter greatly facilitates the alignment procedure. Despite this field's progress, existing methods ignore the intrinsic spatio-temporal structure during the alignment phase. Hence, their solutions usually lead to poor quality in latent dynamics structures and overall performance. To tackle this problem, we propose an alignment method ERDiff, which leverages the expressivity of the diffusion model to preserve the spatio-temporal structure of latent dynamics. Specifically, the latent dynamics structures of the source domain are first extracted by a diffusion model. Then, under the guidance of this diffusion model, such structures are well-recovered through a maximum likelihood alignment procedure in the target domain. We first demonstrate the effectiveness of our proposed method on a synthetic dataset. Then, when applied to neural recordings from the non-human primate motor cortex, under both cross-day and inter-subject settings, our method consistently manifests its capability of preserving the spatiotemporal structure of latent dynamics and outperforms existing approaches in alignment goodness-of-fit and neural decoding performance.|http://arxiv.org/abs/2306.06138v2|Yule Wang,Zijing Wu,Chengrui Li,Anqi Wu
1396|Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory|Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. We also include human behavioral benchmarks as an upper bound for comparison. Our results suggest that AI models replicate some characteristics of WM in the brain, most notably primacy and recency effects, and neural clusters and correlates specialized for different domains and functionalities of WM. In the experiments, we also reveal some limitations in existing models to approximate human behavior. This dataset serves as a valuable resource for communities in cognitive psychology, neuroscience, and AI, offering a standardized framework to compare and enhance WM models, investigate WM's neural underpinnings, and develop WM models with human-like capabilities. Our source code and data are available at https://github.com/ZhangLab-DeepNeuroCogLab/WorM.|http://arxiv.org/abs/2307.10768v2|Ankur Sikarwar,Mengmi Zhang
1397|Persistent-Transient Duality: A Multi-mechanism Approach for Modeling Human-Object Interaction|Humans are highly adaptable, swiftly switching between different modes to progressively handle different tasks, situations and contexts. In Human-object interaction (HOI) activities, these modes can be attributed to two mechanisms: (1) the large-scale consistent plan for the whole activity and (2) the small-scale children interactive actions that start and end along the timeline. While neuroscience and cognitive science have confirmed this multi-mechanism nature of human behavior, machine modeling approaches for human motion are trailing behind. While attempted to use gradually morphing structures (e.g., graph attention networks) to model the dynamic HOI patterns, they miss the expeditious and discrete mode-switching nature of the human motion. To bridge that gap, this work proposes to model two concurrent mechanisms that jointly control human motion: the Persistent process that runs continually on the global scale, and the Transient sub-processes that operate intermittently on the local context of the human while interacting with objects. These two mechanisms form an interactive Persistent-Transient Duality that synergistically governs the activity sequences. We model this conceptual duality by a parent-child neural network of Persistent and Transient channels with a dedicated neural module for dynamic mechanism switching. The framework is trialed on HOI motion forecasting. On two rich datasets and a wide variety of settings, the model consistently delivers superior performances, proving its suitability for the challenge.|http://arxiv.org/abs/2307.12729v1|Hung Tran,Vuong Le,Svetha Venkatesh,Truyen Tran
1398|NeuroAssist: Enhancing Cognitive-Computer Synergy with Adaptive AI and Advanced Neural Decoding for Efficient EEG Signal Classification|Traditional methods of controlling prosthetics frequently encounter difficulties regarding flexibility and responsiveness, which can substantially impact people with varying cognitive and physical abilities. Advancements in computational neuroscience and machine learning (ML) have recently led to the development of highly advanced brain-computer interface (BCI) systems that may be customized to meet individual requirements. To address these issues, we propose NeuroAssist, a sophisticated method for analyzing EEG data that merges state-of-the-art BCI technology with adaptable artificial intelligence (AI) algorithms. NeuroAssist's hybrid neural network design efficiently overcomes the constraints of conventional EEG data processing. Our methodology combines a Natural Language Processing (NLP) BERT model to extract complex features from numerical EEG data and utilizes LSTM networks to handle temporal dynamics. In addition, we integrate spiking neural networks (SNNs) and deep Q-networks (DQN) to improve decision-making and flexibility. Our preprocessing method classifies motor imagery (MI) one-versus-the-rest using a common spatial pattern (CSP) while preserving EEG temporal characteristics. The hybrid architecture of NeuroAssist serves as the DQN's Q-network, enabling continuous feedback-based improvement and adaptability. This enables it to acquire optimal actions through trial and error. This experimental analysis has been conducted on the GigaScience and BCI-competition-IV-2a datasets, which have shown exceptional effectiveness in categorizing MI-EEG signals, obtaining an impressive classification accuracy of 99.17%. NeuroAssist offers a crucial approach to current assistive technology by potentially enhancing the speed and versatility of BCI systems.|http://arxiv.org/abs/2406.01600v1|Eeshan G. Dandamudi
1399|An Image-Guided Robotic System for Transcranial Magnetic Stimulation: System Development and Experimental Evaluation|Transcranial magnetic stimulation (TMS) is a noninvasive medical procedure that can modulate brain activity, and it is widely used in neuroscience and neurology research. Compared to manual operators, robots may improve the outcome of TMS due to their superior accuracy and repeatability. However, there has not been a widely accepted standard protocol for performing robotic TMS using fine-segmented brain images, resulting in arbitrary planned angles with respect to the true boundaries of the modulated cortex. Given that the recent study in TMS simulation suggests a noticeable difference in outcomes when using different anatomical details, cortical shape should play a more significant role in deciding the optimal TMS coil pose. In this work, we introduce an image-guided robotic system for TMS that focuses on (1) establishing standardized planning methods and heuristics to define a reference (true zero) for the coil poses and (2) solving the issue that the manual coil placement requires expert hand-eye coordination which often leading to low repeatability of the experiments. To validate the design of our robotic system, a phantom study and a preliminary human subject study were performed. Our results show that the robotic method can half the positional error and improve the rotational accuracy by up to two orders of magnitude. The accuracy is proven to be repeatable because the standard deviation of multiple trials is lowered by an order of magnitude. The improved actuation accuracy successfully translates to the TMS application, with a higher and more stable induced voltage in magnetic field sensors.|http://arxiv.org/abs/2410.15243v1|Yihao Liu,Jiaming Zhang,Letian Ai,Jing Tian,Shahriar Sefati,Huan Liu,Alejandro Martin-Gomez,Amir Kheradmand,Mehran Armand
1400|Predicting the outcomes of treatment to eradicate the latent reservoir for HIV-1|Massive research efforts are now underway to develop a cure for HIV infection, allowing patients to discontinue lifelong combination antiretroviral therapy (ART). New latency-reversing agents (LRAs) may be able to purge the persistent reservoir of latent virus in resting memory CD4+ T cells, but the degree of reservoir reduction needed for cure remains unknown. Here we use a stochastic model of infection dynamics to estimate the efficacy of LRA needed to prevent viral rebound after ART interruption. We incorporate clinical data to estimate population-level parameter distributions and outcomes. Our findings suggest that approximately 2,000-fold reductions are required to permit a majority of patients to interrupt ART for one year without rebound and that rebound may occur suddenly after multiple years. Greater than 10,000-fold reductions may be required to prevent rebound altogether. Our results predict large variation in rebound times following LRA therapy, which will complicate clinical management. This model provides benchmarks for moving LRAs from the lab to the clinic and can aid in the design and interpretation of clinical trials. These results also apply to other interventions to reduce the latent reservoir and can explain the observed return of viremia after months of apparent cure in recent bone marrow transplant recipients and an immediately-treated neonate.|http://arxiv.org/abs/1403.4196v2|Alison L. Hill,Daniel I. S. Rosenbloom,Feng Fu,Martin A. Nowak,Robert F. Siliciano
1401|A New Approach to Privacy-Preserving Clinical Decision Support Systems|Background: Clinical decision support systems (CDSS) are a category of health information technologies that can assist clinicians to choose optimal treatments. These support systems are based on clinical trials and expert knowledge; however, the amount of data available to these systems is limited. For this reason, CDSSs could be significantly improved by using the knowledge obtained by treating patients. This knowledge is mainly contained in patient records, whose usage is restricted due to privacy and confidentiality constraints.   Methods: A treatment effectiveness measure, containing valuable information for treatment prescription, was defined and a method to extract this measure from patient records was developed. This method uses an advanced cryptographic technology, known as secure Multiparty Computation (henceforth referred to as MPC), to preserve the privacy of the patient records and the confidentiality of the clinicians' decisions.   Results: Our solution enables to compute the effectiveness measure of a treatment based on patient records, while preserving privacy. Moreover, clinicians are not burdened with the computational and communication costs introduced by the privacy-preserving techniques that are used. Our system is able to compute the effectiveness of 100 treatments for a specific patient in less than 24 minutes, querying a database containing 20,000 patient records.   Conclusion: This paper presents a novel and efficient clinical decision support system, that harnesses the potential and insights acquired from treatment data, while preserving the privacy of patient records and the confidentiality of clinician decisions.|http://arxiv.org/abs/1810.01107v2|Thomas Attema,Emiliano Mancini,Gabriele Spini,Mark Abspoel,Jan de Gier,Serge Fehr,Thijs Veugen,Maran van Heesch,Danil Worm,Andrea De Luca,Ronald Cramer,Peter M. A. Sloot
1402|A Bayesian approach to tissue-fraction estimation for oncological PET segmentation|Tumor segmentation in oncological PET is challenging, a major reason being the partial-volume effects that arise due to low system resolution and finite voxel size. The latter results in tissue-fraction effects, i.e. voxels contain a mixture of tissue classes. Conventional segmentation methods are typically designed to assign each voxel in the image as belonging to a certain tissue class. Thus, these methods are inherently limited in modeling tissue-fraction effects. To address the challenge of accounting for partial-volume effects, and in particular, tissue-fraction effects, we propose a Bayesian approach to tissue-fraction estimation for oncological PET segmentation. Specifically, this Bayesian approach estimates the posterior mean of fractional volume that the tumor occupies within each voxel of the image. The proposed method, implemented using a deep-learning-based technique, was first evaluated using clinically realistic 2-D simulation studies with known ground truth, in the context of segmenting the primary tumor in PET images of patients with lung cancer. The evaluation studies demonstrated that the method accurately estimated the tumor-fraction areas and significantly outperformed widely used conventional PET segmentation methods, including a U-net-based method, on the task of segmenting the tumor. In addition, the proposed method was relatively insensitive to partial-volume effects and yielded reliable tumor segmentation for different clinical-scanner configurations. The method was then evaluated using clinical images of patients with stage IIB/III non-small cell lung cancer from ACRIN 6668/RTOG 0235 multi-center clinical trial. Here, the results showed that the proposed method significantly outperformed all other considered methods and yielded accurate tumor segmentation on patient images with Dice similarity coefficient (DSC) of 0.82 (95 % CI: [0.78, 0.86]).|http://arxiv.org/abs/2003.00317v3|Ziping Liu,Joyce C. Mhlanga,Richard Laforest,Paul-Robert Derenoncourt,Barry A. Siegel,Abhinav K. Jha
1403|Longitudinal modeling of MS patient trajectories improves predictions of disability progression|Research in Multiple Sclerosis (MS) has recently focused on extracting knowledge from real-world clinical data sources. This type of data is more abundant than data produced during clinical trials and potentially more informative about real-world clinical practice. However, this comes at the cost of less curated and controlled data sets. In this work, we address the task of optimally extracting information from longitudinal patient data in the real-world setting with a special focus on the sporadic sampling problem. Using the MSBase registry, we show that with machine learning methods suited for patient trajectories modeling, such as recurrent neural networks and tensor factorization, we can predict disability progression of patients in a two-year horizon with an ROC-AUC of 0.86, which represents a 33% decrease in the ranking pair error (1-AUC) compared to reference methods using static clinical features. Compared to the models available in the literature, this work uses the most complete patient history for MS disease progression prediction.|http://arxiv.org/abs/2011.04749v1|Edward De Brouwer,Thijs Becker,Yves Moreau,Eva Kubala Havrdova,Maria Trojano,Sara Eichau,Serkan Ozakbas,Marco Onofrj,Pierre Grammond,Jens Kuhle,Ludwig Kappos,Patrizia Sola,Elisabetta Cartechini,Jeannette Lechner-Scott,Raed Alroughani,Oliver Gerlach,Tomas Kalincik,Franco Granella,Francois GrandMaison,Roberto Bergamaschi,Maria Jose Sa,Bart Van Wijmeersch,Aysun Soysal,Jose Luis Sanchez-Menoyo,Claudio Solaro,Cavit Boz,Gerardo Iuliano,Katherine Buzzard,Eduardo Aguera-Morales,Murat Terzi,Tamara Castillo Trivio,Daniele Spitaleri,Vincent Van Pesch,Vahid Shaygannej,Fraser Moore,Celia Oreja Guevara,Davide Maimone,Riadh Gouider,Tunde Csepany,Cristina Ramo-Tello,Liesbet Peeters
1404|Joint modelling of longitudinal and multi-state processes: application to clinical progressions in prostate cancer|Joint modelling of longitudinal and survival data is increasingly used in clinical trials on cancer. In prostate cancer for example, these models permit to account for the link between longitudinal measures of prostate-specific antigen (PSA) and the time of clinical recurrence when studying the risk of relapse. In practice, multiple types of relapse may occur successively. Distinguishing these transitions between health states would allow to evaluate, for example, how PSA trajectory and classical covariates impact the risk of dying after a distant recurrence post-radiotherapy, or to predict the risk of one specific type of clinical recurrence post-radiotherapy, from the PSA history. In this context, we present a joint model for a longitudinal process and a multi-state process which is divided into two sub-models: a linear mixed sub-model for longitudinal data, and a multi-state sub-model with proportional hazards for transition times, both linked by shared random effects. Parameters of this joint multi-state model are estimated within the maximum likelihood framework using an EM algorithm coupled to a quasi-Newton algorithm in case of slow convergence. It is implemented under R, by combining and extending the mstate and JM packages. The estimation program is validated by simulations and applied on pooled data from two cohorts of men with localized prostate cancer and treated by radiotherapy. Thanks to the classical covariates available at baseline and the PSA measurements collected repeatedly during the follow-up, we are able to assess the biomarker's trajectory, define the risks of transitions between health states, and quantify the impact of the PSA dynamics on each transition intensity.|http://arxiv.org/abs/1506.07496v3|Loc Ferrer,Virginie Rondeau,James J. Dignam,Tom Pickles,Hlne Jacqmin-Gadda,Ccile Proust-Lima
1405|A Self-Supervised Learning-based Approach to Clustering Multivariate Time-Series Data with Missing Values (SLAC-Time): An Application to TBI Phenotyping|Self-supervised learning approaches provide a promising direction for clustering multivariate time-series data. However, real-world time-series data often include missing values, and the existing approaches require imputing missing values before clustering, which may cause extensive computations and noise and result in invalid interpretations. To address these challenges, we present a Self-supervised Learning-based Approach to Clustering multivariate Time-series data with missing values (SLAC-Time). SLAC-Time is a Transformer-based clustering method that uses time-series forecasting as a proxy task for leveraging unlabeled data and learning more robust time-series representations. This method jointly learns the neural network parameters and the cluster assignments of the learned representations. It iteratively clusters the learned representations with the K-means method and then utilizes the subsequent cluster assignments as pseudo-labels to update the model parameters. To evaluate our proposed approach, we applied it to clustering and phenotyping Traumatic Brain Injury (TBI) patients in the Transforming Research and Clinical Knowledge in Traumatic Brain Injury (TRACK-TBI) study. Our experiments demonstrate that SLAC-Time outperforms the baseline K-means clustering algorithm in terms of silhouette coefficient, Calinski Harabasz index, Dunn index, and Davies Bouldin index. We identified three TBI phenotypes that are distinct from one another in terms of clinically significant variables as well as clinical outcomes, including the Extended Glasgow Outcome Scale (GOSE) score, Intensive Care Unit (ICU) length of stay, and mortality rate. The experiments show that the TBI phenotypes identified by SLAC-Time can be potentially used for developing targeted clinical trials and therapeutic strategies.|http://arxiv.org/abs/2302.13457v2|Hamid Ghaderi,Brandon Foreman,Amin Nayebi,Sindhu Tipirneni,Chandan K. Reddy,Vignesh Subbian
1406|Need for Objective Task-based Evaluation of Deep Learning-Based Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT|Artificial intelligence-based methods have generated substantial interest in nuclear medicine. An area of significant interest has been using deep-learning (DL)-based approaches for denoising images acquired with lower doses, shorter acquisition times, or both. Objective evaluation of these approaches is essential for clinical application. DL-based approaches for denoising nuclear-medicine images have typically been evaluated using fidelity-based figures of merit (FoMs) such as RMSE and SSIM. However, these images are acquired for clinical tasks and thus should be evaluated based on their performance in these tasks. Our objectives were to (1) investigate whether evaluation with these FoMs is consistent with objective clinical-task-based evaluation; (2) provide a theoretical analysis for determining the impact of denoising on signal-detection tasks; (3) demonstrate the utility of virtual clinical trials (VCTs) to evaluate DL-based methods. A VCT to evaluate a DL-based method for denoising myocardial perfusion SPECT (MPS) images was conducted. The impact of DL-based denoising was evaluated using fidelity-based FoMs and AUC, which quantified performance on detecting perfusion defects in MPS images as obtained using a model observer with anthropomorphic channels. Based on fidelity-based FoMs, denoising using the considered DL-based method led to significantly superior performance. However, based on ROC analysis, denoising did not improve, and in fact, often degraded detection-task performance. The results motivate the need for objective task-based evaluation of DL-based denoising approaches. Further, this study shows how VCTs provide a mechanism to conduct such evaluations using VCTs. Finally, our theoretical treatment reveals insights into the reasons for the limited performance of the denoising approach.|http://arxiv.org/abs/2303.02110v5|Zitong Yu,Md Ashequr Rahman,Richard Laforest,Thomas H. Schindler,Robert J. Gropler,Richard L. Wahl,Barry A. Siegel,Abhinav K. Jha
1407|Next-generation MRD assays: do we have the tools to evaluate them properly?|Circulating tumour DNA (ctDNA) detection of molecular residual disease (MRD) in solid tumours correlates strongly with patient outcomes and is being adopted as a new clinical standard. ctDNA levels are known to correlate with tumor volume, and although the absolute levels vary across indication and histology, its analysis is driving the adoption of MRD. MRD assays must detect tumor when imaging cannot and, as such, require very high sensitivity to detect the low levels of ctDNA found after curative intent therapy. The minimum threshold is 0.01% Tumour Fraction but current methods like Archer and Signatera are limited by detection sensitivity resulting in some patients receiving a false negative call thereby missing out on earlier therapeutic intervention. Multiple vendors are increasing the number of somatic variants tracked in tumour-informed and personalized NGS assays, from tens to thousands of variants. Most recently, assays using other biological features of ctDNA, e.g methylation or fragmentome, have been developed at the LOD required for clinical utility. These uniformed, or tumour-naive and non-personalised assays may be more easily, and therefore more rapidly, adopted in the clinic. However, this rapid development in MRD assay technology results in significant challenges in benchmarking these new technologies for use in clinical trials. This is further complicated by the fact that previous reference materials have focused on somatic variants, and do not retain all of the epigenomic features assessed by newer technologies. In this Comments and Controversy paper, we detail what is known and what remains to be determined for optimal reference materials of MRD methods and provide opinions generated during three-years of MRD technology benchmarking in AstraZeneca Translational Medicine to help guide the community conversation.|http://arxiv.org/abs/2311.00015v1|Dan Stetson,Paul Labrousse,Hugh Russell,David Shera,Chris Abbosh,Brian Dougherty,J. Carl Barrett,Darren Hodgson,James Hadfield
1408|Screen Them All: High-Throughput Pan-Cancer Genetic and Phenotypic Biomarker Screening from H&E Whole Slide Images|Many molecular alterations serve as clinically prognostic or therapy-predictive biomarkers, typically detected using single or multi-gene molecular assays. However, these assays are expensive, tissue destructive and often take weeks to complete. Using AI on routine H&E WSIs offers a fast and economical approach to screen for multiple molecular biomarkers. We present a high-throughput AI-based system leveraging Virchow2, a foundation model pre-trained on 3 million slides, to interrogate genomic features previously determined by an next-generation sequencing (NGS) assay, using 47,960 scanned hematoxylin and eosin (H&E) whole slide images (WSIs) from 38,984 cancer patients. Unlike traditional methods that train individual models for each biomarker or cancer type, our system employs a unified model to simultaneously predict a wide range of clinically relevant molecular biomarkers across cancer types. By training the network to replicate the MSK-IMPACT targeted biomarker panel of 505 genes, it identified 80 high performing biomarkers with a mean AU-ROC of 0.89 in 15 most common cancer types. In addition, 40 biomarkers demonstrated strong associations with specific cancer histologic subtypes. Furthermore, 58 biomarkers were associated with targets frequently assayed clinically for therapy selection and response prediction. The model can also predict the activity of five canonical signaling pathways, identify defects in DNA repair mechanisms, and predict genomic instability measured by tumor mutation burden, microsatellite instability (MSI), and chromosomal instability (CIN). The proposed model can offer potential to guide therapy selection, improve treatment efficacy, accelerate patient screening for clinical trials and provoke the interrogation of new therapeutic targets.|http://arxiv.org/abs/2408.09554v2|Yi Kan Wang,Ludmila Tydlitatova,Jeremy D. Kunz,Gerard Oakley,Ran A. Godrich,Matthew C. H. Lee,Chad Vanderbilt,Razik Yousfi,Thomas Fuchs,David S. Klimstra,Siqi Liu
1409|Influence of Medical Foreign Bodies on Dark-Field Chest Radiographs: First experiences|Objectives: Evaluating the effects and artifacts introduced by medical foreign bodies in clinical dark-field chest radiographs and assessing their influence on the evaluation of pulmonary tissue, compared to conventional radiographs.   Material & Methods: This retrospective study analyzed data from subjects enrolled in clinical trials conducted between 2018 and 2021, focusing on chronic obstructive pulmonary disease (COPD) and COVID-19 patients. All patients obtained a radiograph using an in-house developed clinical prototype for grating-based dark-field chest radiography. The prototype simultaneously delivers a conventional and dark-field radiograph. Two radiologists independently assessed the clinical studies to identify patients with foreign bodies. Subsequently, an analysis was conducted on the effects and artifacts attributed to distinct foreign bodies and their impact on the assessment of pulmonary tissue.   Results: Overall, 30 subjects with foreign bodies were included in this study (mean age, 64 years +/- 11 [standard deviation]; 15 men). Foreign bodies composed of materials lacking microstructure exhibited a diminished dark-field signal or no discernible signal. Foreign bodies with a microstructure, in our investigations the cementation of the kyphoplasty, produce a positive dark-field signal. Since most foreign bodies lack microstructural features, dark-field imaging revealed fewer signals and artifacts by foreign bodies compared to conventional radiographs.   Conclusion: Dark-field radiography enhances the assessment of pulmonary tissue with overlaying foreign bodies compared to conventional radiography. Reduced interfering signals result in fewer overlapping radiopaque artifacts within the investigated regions. This mitigates the impact on image quality and interpretability of the radiographs and the projection-related limitations of radiography compared to CT.|http://arxiv.org/abs/2408.10855v1|Lennard Kaster,Henriette Klein,Alexander W. Marka,Theresa Urban,Sandra Karl,Florian T. Gassert,Lisa Steinhelfer,Marcus R. Makowski,Daniela Pfeiffer,Franz Pfeiffer
1410|Superhuman performance of a large language model on the reasoning tasks of a physician|Performance of large language models (LLMs) on medical tasks has traditionally been evaluated using multiple choice question benchmarks. However, such benchmarks are highly constrained, saturated with repeated impressive performance by LLMs, and have an unclear relationship to performance in real clinical scenarios. Clinical reasoning, the process by which physicians employ critical thinking to gather and synthesize clinical data to diagnose and manage medical problems, remains an attractive benchmark for model performance. Prior LLMs have shown promise in outperforming clinicians in routine and complex diagnostic scenarios. We sought to evaluate OpenAI's o1-preview model, a model developed to increase run-time via chain of thought processes prior to generating a response. We characterize the performance of o1-preview with five experiments including differential diagnosis generation, display of diagnostic reasoning, triage differential diagnosis, probabilistic reasoning, and management reasoning, adjudicated by physician experts with validated psychometrics. Our primary outcome was comparison of the o1-preview output to identical prior experiments that have historical human controls and benchmarks of previous LLMs. Significant improvements were observed with differential diagnosis generation and quality of diagnostic and management reasoning. No improvements were observed with probabilistic reasoning or triage differential diagnosis. This study highlights o1-preview's ability to perform strongly on tasks that require complex critical thinking such as diagnosis and management while its performance on probabilistic reasoning tasks was similar to past models. New robust benchmarks and scalable evaluation of LLM capabilities compared to human physicians are needed along with trials evaluating AI in real clinical settings.|http://arxiv.org/abs/2412.10849v1|Peter G. Brodeur,Thomas A. Buckley,Zahir Kanjee,Ethan Goh,Evelyn Bin Ling,Priyank Jain,Stephanie Cabral,Raja-Elie Abdulnour,Adrian Haimovich,Jason A. Freed,Andrew Olson,Daniel J. Morgan,Jason Hom,Robert Gallo,Eric Horvitz,Jonathan Chen,Arjun K. Manrai,Adam Rodman
1411|Acquisition-Independent Deep Learning for Quantitative MRI Parameter Estimation using Neural Controlled Differential Equations|Deep learning has proven to be a suitable alternative to least-squares (LSQ) fitting for parameter estimation in various quantitative MRI (QMRI) models. However, current deep learning implementations are not robust to changes in MR acquisition protocols. In practice, QMRI acquisition protocols differ substantially between different studies and clinical settings. The lack of generalizability and adoptability of current deep learning approaches for QMRI parameter estimation impedes the implementation of these algorithms in clinical trials and clinical practice. Neural Controlled Differential Equations (NCDEs) allow for the sampling of incomplete and irregularly sampled data with variable length, making them ideal for use in QMRI parameter estimation. In this study, we show that NCDEs can function as a generic tool for the accurate prediction of QMRI parameters, regardless of QMRI sequence length, configuration of independent variables and QMRI forward model (variable flip angle T1-mapping, intravoxel incoherent motion MRI, dynamic contrast-enhanced MRI). NCDEs achieved lower mean squared error than LSQ fitting in low-SNR simulations and in vivo in challenging anatomical regions like the abdomen and leg, but this improvement was no longer evident at high SNR. NCDEs reduce estimation error interquartile range without increasing bias, particularly under conditions of high uncertainty. These findings suggest that NCDEs offer a robust approach for reliable QMRI parameter estimation, especially in scenarios with high uncertainty or low image quality. We believe that with NCDEs, we have solved one of the main challenges for using deep learning for QMRI parameter estimation in a broader clinical and research setting.|http://arxiv.org/abs/2412.20844v1|Daan Kuppens,Sebastiano Barbieri,Daisy van den Berg,Pepijn Schouten,Harriet C. Thoeny,Myrte Wennen,Oliver J. Gurney-Champion
1412|The Brain Tumor Segmentation in Pediatrics (BraTS-PEDs) Challenge: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs)|Pediatric tumors of the central nervous system are the most common cause of cancer-related death in children. The five-year survival rate for high-grade gliomas in children is less than 20%. Due to their rarity, the diagnosis of these entities is often delayed, their treatment is mainly based on historic treatment concepts, and clinical trials require multi-institutional collaborations. Here we present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs challenge, focused on pediatric brain tumors with data acquired across multiple international consortia dedicated to pediatric neuro-oncology and clinical trials. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs challenge brings together clinicians and AI/imaging scientists to lead to faster development of automated segmentation techniques that could benefit clinical trials, and ultimately the care of children with brain tumors.|http://arxiv.org/abs/2404.15009v4|Anahita Fathi Kazerooni,Nastaran Khalili,Xinyang Liu,Deep Gandhi,Zhifan Jiang,Syed Muhammed Anwar,Jake Albrecht,Maruf Adewole,Udunna Anazodo,Hannah Anderson,Ujjwal Baid,Timothy Bergquist,Austin J. Borja,Evan Calabrese,Verena Chung,Gian-Marco Conte,Farouk Dako,James Eddy,Ivan Ezhov,Ariana Familiar,Keyvan Farahani,Andrea Franson,Anurag Gottipati,Shuvanjan Haldar,Juan Eugenio Iglesias,Anastasia Janas,Elaine Johansen,Blaise V Jones,Neda Khalili,Florian Kofler,Dominic LaBella,Hollie Anne Lai,Koen Van Leemput,Hongwei Bran Li,Nazanin Maleki,Aaron S McAllister,Zeke Meier,Bjoern Menze,Ahmed W Moawad,Khanak K Nandolia,Julija Pavaine,Marie Piraud,Tina Poussaint,Sanjay P Prabhu,Zachary Reitman,Jeffrey D Rudie,Mariana Sanchez-Montano,Ibraheem Salman Shaikh,Nakul Sheth,Wenxin Tu,Chunhao Wang,Jeffrey B Ware,Benedikt Wiestler,Anna Zapaishchykova,Miriam Bornhorst,Michelle Deutsch,Maryam Fouladi,Margot Lazow,Leonie Mikael,Trent Hummel,Benjamin Kann,Peter de Blank,Lindsey Hoffman,Mariam Aboian,Ali Nabavizadeh,Roger Packer,Spyridon Bakas,Adam Resnick,Brian Rood,Arastoo Vossough,Marius George Linguraru
1413|BraTS-PEDs: Results of the Multi-Consortium International Pediatric Brain Tumor Segmentation Challenge 2023|Pediatric central nervous system tumors are the leading cause of cancer-related deaths in children. The five-year survival rate for high-grade glioma in children is less than 20%. The development of new treatments is dependent upon multi-institutional collaborative clinical trials requiring reproducible and accurate centralized response assessment. We present the results of the BraTS-PEDs 2023 challenge, the first Brain Tumor Segmentation (BraTS) challenge focused on pediatric brain tumors. This challenge utilized data acquired from multiple international consortia dedicated to pediatric neuro-oncology and clinical trials. BraTS-PEDs 2023 aimed to evaluate volumetric segmentation algorithms for pediatric brain gliomas from magnetic resonance imaging using standardized quantitative performance evaluation metrics employed across the BraTS 2023 challenges. The top-performing AI approaches for pediatric tumor analysis included ensembles of nnU-Net and Swin UNETR, Auto3DSeg, or nnU-Net with a self-supervised framework. The BraTSPEDs 2023 challenge fostered collaboration between clinicians (neuro-oncologists, neuroradiologists) and AI/imaging scientists, promoting faster data sharing and the development of automated volumetric analysis techniques. These advancements could significantly benefit clinical trials and improve the care of children with brain tumors.|http://arxiv.org/abs/2407.08855v2|Anahita Fathi Kazerooni,Nastaran Khalili,Xinyang Liu,Debanjan Haldar,Zhifan Jiang,Anna Zapaishchykova,Julija Pavaine,Lubdha M. Shah,Blaise V. Jones,Nakul Sheth,Sanjay P. Prabhu,Aaron S. McAllister,Wenxin Tu,Khanak K. Nandolia,Andres F. Rodriguez,Ibraheem Salman Shaikh,Mariana Sanchez Montano,Hollie Anne Lai,Maruf Adewole,Jake Albrecht,Udunna Anazodo,Hannah Anderson,Syed Muhammed Anwar,Alejandro Aristizabal,Sina Bagheri,Ujjwal Baid,Timothy Bergquist,Austin J. Borja,Evan Calabrese,Verena Chung,Gian-Marco Conte,James Eddy,Ivan Ezhov,Ariana M. Familiar,Keyvan Farahani,Deep Gandhi,Anurag Gottipati,Shuvanjan Haldar,Juan Eugenio Iglesias,Anastasia Janas,Elaine Elaine,Alexandros Karargyris,Hasan Kassem,Neda Khalili,Florian Kofler,Dominic LaBella,Koen Van Leemput,Hongwei B. Li,Nazanin Maleki,Zeke Meier,Bjoern Menze,Ahmed W. Moawad,Sarthak Pati,Marie Piraud,Tina Poussaint,Zachary J. Reitman,Jeffrey D. Rudie,Rachit Saluja,MIcah Sheller,Russell Takeshi Shinohara,Karthik Viswanathan,Chunhao Wang,Benedikt Wiestler,Walter F. Wiggins,Christos Davatzikos,Phillip B. Storm,Miriam Bornhorst,Roger Packer,Trent Hummel,Peter de Blank,Lindsey Hoffman,Mariam Aboian,Ali Nabavizadeh,Jeffrey B. Ware,Benjamin H. Kann,Brian Rood,Adam Resnick,Spyridon Bakas,Arastoo Vossough,Marius George Linguraru
1414|Extension of the SAEM algorithm for nonlinear mixed models with two levels of random effects|This article focuses on parameter estimation of multi-levels nonlinear mixed effects models (MNLMEMs). These models are used to analyze data presenting multiple hierarchical levels of grouping (cluster data, clinical trials with several observation periods,...). The variability of the individual parameters of the regression function is thus decomposed as a between-sub ject variability and higher levels of variability (for example within-sub ject variability). We propose maximum likelihood estimates of parameters of those MNLMEMs with two levels of random effects, using an extension of the SAEM-MCMC algorithm. The extended SAEM algorithm is split into an explicit direct EM algorithm and a stochastic EM part. Compared to the original algorithm, additional sufficient statistics have to be approximated by relying on the conditional distribution of the second level of random effects. This estimation method is evaluated on pharmacokinetic cross-over simulated trials, mimicking theophyllin concentration data. Results obtained on those datasets with either the SAEM algorithm or the FOCE algorithm (implemented in the nlme function of R software) are compared: biases and RMSEs of almost all the SAEM estimates are smaller than the FOCE ones. Finally, we apply the extended SAEM algorithm to analyze the pharmacokinetic interaction of tenofovir on atazanavir, a novel protease inhibitor, from the ANRS 107-Puzzle 2 study. A significant decrease of the area under the curve of atazanavir is found in patients receiving both treatments.|http://arxiv.org/abs/0803.4437v1|Xavire Panhard,Adeline Samson
1415|Are adaptive allocation designs beneficial for improving power in binary response trials?|We consider the classical problem of selecting the best of two treatments in clinical trials with binary response. The target is to find the design that maximizes the power of the relevant test. Many papers use a normal approximation to the power function and claim that Neyman allocation that assigns subjects to treatment groups according to the ratio of the responses' standard deviations, should be used. As the standard deviations are unknown, an adaptive design is often recommended. The asymptotic justification of this approach is arguable, since it uses the normal approximation in tails where the error in the approximation is larger than the estimated quantity. We consider two different approaches for optimality of designs that are related to Pitman and Bahadur definitions of relative efficiency of tests. We prove that the optimal allocation according to the Pitman criterion is the balanced allocation and that the optimal allocation according to the Bahadur approach depends on the unknown parameters. Exact calculations reveal that the optimal allocation according to Bahadur is often close to the balanced design, and the powers of both are comparable to the Neyman allocation for small sample sizes and are generally better for large experiments. Our findings have important implications to the design of experiments, as the balanced design is proved to be optimal or close to optimal and the need for the complications involved in following an adaptive design for the purpose of increasing the power of tests is therefore questionable.|http://arxiv.org/abs/1102.4915v3|David Azriel,Micha Mandel,Yosef Rinott
1416|To what extent does not wearing shoes affect the local dynamic stability of the gait? Effect size and intra-session repeatability|Local dynamic stability (LDS) quantifies how a system responds to small perturbations. Several experimental and clinical findings have highlighted the association between gait LDS and fall risk. Walking without shoes is known to slightly modify gait parameters. Barefoot walking (BW) may cause unusual sensory feedback to individuals accustomed to shod walking (SW), and this may impact on LDS. The objective of this study was therefore to compare the LDS of SW and BW in healthy individuals and to analyze the intrasession repeatability. Forty participants traversed a 70 m indoor corridor wearing normal shoes in one trial and walking barefoot in a second trial. Trunk accelerations were recorded with a 3D-accelerometer attached to the lower back. The LDS was computed using the finite-time maximal Lyapunov exponent method. Absolute agreement between the forward and backward paths was estimated with the intraclass correlation coefficient (ICC). BW did not significantly modify the LDS as compared to SW (average standardized effect size: +0.12). The intrasession repeatability was high in SW (ICC: 0.73-0.79) and slightly higher in BW (ICC: 0.82-0.88). Therefore, it seems that BW can be used to evaluate LDS without introducing bias as compared to SW, and with a sufficient reliability.|http://arxiv.org/abs/1212.5510v4|Philippe Terrier,Fabienne Reynard
1417|Dynamic models for estimating the effect of HAART on CD4 in observational studies: application to the Aquitaine Cohort study and the Swiss HIV Cohort Study|Highly active antiretroviral therapy (HAART) has proved efficient in increasing CD4 counts in many randomized clinical trials. Because randomized trials have some limitations (e.g., short duration, highly selected subjects), it is interesting to assess it using observational studies. This is challenging because treatment is started preferentially in subjects with severe conditions, in particular in subjects with low CD4 counts. This general problem had been treated using Marginal Structural Models (MSM) relying on the counterfactual formulation. Another approach to causality is based on dynamical models. First, we present three discrete-time dynamic models based on linear increments (LIM): the simplest model is described by one difference equation for CD4 counts; the second has an equilibrium point; the third model is based on a system of two difference equations which allows jointly modeling CD4 counts and viral load. Then we consider continuous time models based on ordinary differential equations with random effects (ODE-NLME). These mechanistic models allow incorporating biological knowledge when available, which leads to increased power for detecting treatment effect. Inference in ODE-NLME models, however, is challenging from a numerical point of view, and requires specific methods and softwares. LIMs are a valuable intermediary option in terms of consistency, precision and complexity. The different approaches are compared in simulation and applied to HIV cohorts (the ANRS CO3 Aquitaine Cohort and the Swiss HIV Cohort Study).|http://arxiv.org/abs/1503.08658v3|M. Prague,D. Commenges,J. M. Gran,B. Ledergerber,J. young,H. Furrer,R. Thibaut
1418|On the impact of Masking and Blocking Hypotheses for measuring efficacy of new tuberculosis vaccines|Over the past 60 years, the Mycobacterium bovis bacille Calmette-Gu\'erin (BCG) has been used worldwide to prevent tuberculosis (TB). However, BCG has shown a very variable efficacy in different trials, showing a wide range of protection in adults against pulmonary TB. Previous studies indicate that this failure is related to pre-existing immune response to antigens that are common to environmental sources of mycobacterial antigens and Mycobacterium tuberculosis. Specifically, two different mechanisms have been hypothesized: the masking, (previous sensitization confers some level of protection against TB), and the blocking (previous immune response prevent vaccine taking of a new TB vaccine), effects. In this work we introduce a series of models to discriminate between masking and blocking mechanisms and address their relative likelihood. The application of our models to interpret the results coming from the BCG-REVAC clinical trials, specifically designed for the study of sources of efficacy variability yields estimates that are consistent with high levels of blocking (41% in Manaus -95% C.I. [14%-68%]- and 96% in Salvador -95% C.I. [52%-100%]-), and no support for masking to play any relevant role in modifying vaccine efficacy either alone or aside blocking. The quantification of these effects around a plausible model constitutes a relevant step towards impact evaluation of novel anti-tuberculosis vaccines, which are susceptible of being affected by similar effects if applied on individuals previously exposed to mycobacterial antigens.|http://arxiv.org/abs/1509.06778v1|Sergio Arregui,Joaqun Sanz,Dessislava Marinova,Carlos Martn,Yamir Moreno
1419|Assessing the Treatment Effect Heterogeneity with a Latent Variable|The average treatment effect (ATE) is popularly used to assess the treatment effect. However, the ATE implicitly assumes a homogenous treatment effect even amongst individuals with different characteristics. In this paper, we mainly focus on assessing the treatment effect heterogeneity, which has important implications in designing the optimal individual treatment regimens and in policy making. The treatment benefit rate (TBR) and treatment harm rate (THR) have been defined to characterize the magnitude of heterogeneity for binary outcomes. When the outcomes are continuous, we extend the definitions of the TBR and THR to compare the difference between potential outcomes with a pre-specified level c. Unlike the ATE, these rates involve the joint distribution of the potential outcomes and can not be identified without further assumptions even in randomized clinical trials. In this article, we assume the potential outcomes are independent conditional on the observed covariates and an unmeasured latent variable. Under this assumption, we prove the identification of the TBR and THR in non-separable (generalized) linear models for both continuous and binary outcomes. We then propose estimators and derive their asymptotic distributions. In the simulation studies, we implement our proposed methods to assess the performance of our estimators and carry out a sensitive analysis for different underlying distribution for the latent variable. Finally, we illustrate the proposed methods in two randomized controlled trials.|http://arxiv.org/abs/1603.02712v1|Yunjian Yin,Lan Liu,Zhi Geng
1420|Complex tensor factorisation with PARAFAC2 for the estimation of brain connectivity from the EEG|Objective: The coupling between neuronal populations and its magnitude have been shown to be informative for various clinical applications. One method to estimate brain connectivity is with electroencephalography (EEG) from which the cross-spectrum between different sensor locations is derived. We wish to test the efficacy of tensor factorisation in the estimation of brain connectivity. Methods: Complex tensor factorisation based on PARAFAC2 is used to decompose the EEG into scalp components described by the spatial, spectral, and complex trial profiles. An EEG model in the complex domain was derived that shows the suitability of PARAFAC2. A connectivity metric was also derived on the complex trial profiles of the extracted components. Results: Results on a benchmark EEG dataset confirmed that PARAFAC2 can estimate connectivity better than traditional tensor analysis such as PARAFAC within a range of signal-to-noise ratios. The analysis of EEG from patients with mild cognitive impairment or Alzheimer's disease showed that PARAFAC2 identifies loss of brain connectivity better than traditional approaches and agreeing with prior pathological knowledge. Conclusion: The complex PARAFAC2 algorithm is suitable for EEG connectivity estimation since it allows to extract meaningful coupled sources and provides better estimates than complex PARAFAC. Significance: A new paradigm that employs complex tensor factorisation has demonstrated to be successful in identifying brain connectivity and the location of couples sources for both a benchmark and a real-world EEG dataset. This can enable future applications and has the potential to solve some the issues that deteriorate the performance of traditional connectivity metrics.|http://arxiv.org/abs/1705.02019v1|Loukianos Spyrou,Mario Parra,Javier Escudero
1421|Individualized Treatment Effects with Censored Data via Fully Nonparametric Bayesian Accelerated Failure Time Models|Individuals often respond differently to identical treatments, and characterizing such variability in treatment response is an important aim in the practice of personalized medicine. In this article, we describe a non-parametric accelerated failure time model that can be used to analyze heterogeneous treatment effects (HTE) when patient outcomes are time-to-event. By utilizing Bayesian additive regression trees and a mean-constrained Dirichlet process mixture model, our approach offers a flexible model for the regression function while placing few restrictions on the baseline hazard. Our non-parametric method leads to natural estimates of individual treatment effect and has the flexibility to address many major goals of HTE assessment. Moreover, our method requires little user input in terms of tuning parameter selection or subgroup specification. We illustrate the merits of our proposed approach with a detailed analysis of two large clinical trials for the prevention and treatment of congestive heart failure using an angiotensin-converting enzyme inhibitor. The analysis revealed considerable evidence for the presence of HTE in both trials as demonstrated by substantial estimated variation in treatment effect and by high proportions of patients exhibiting strong evidence of having treatment effects which differ from the overall treatment effect.|http://arxiv.org/abs/1706.06611v1|Nicholas C. Henderson,Thomas A. Louis,Gary L. Rosner,Ravi Varadhan
1422|Heart Rate Monitoring During Different Lung Volume Phases Using Seismocardiography|Seismocardiography (SCG) is a non-invasive method that can be used for cardiac activity monitoring. This paper presents a new electrocardiogram (ECG) independent approach for estimating heart rate (HR) during low and high lung volume (LLV and HLV, respectively) phases using SCG signals. In this study, SCG, ECG, and respiratory flow rate (RFR) signals were measured simultaneously in 7 healthy subjects. The lung volume information was calculated from the RFR and was used to group the SCG events into low and high lung-volume groups. LLV and HLV SCG events were then used to estimate the subjects HR as well as the HR during LLV and HLV in 3 different postural positions, namely supine, 45 degree heads-up, and sitting. The performance of the proposed algorithm was tested against the standard ECG measurements. Results showed that the HR estimations from the SCG and ECG signals were in a good agreement (bias of 0.08 bpm). All subjects were found to have a higher HR during HLV (HR$_\text{HLV}$) compared to LLV (HR$_\text{LLV}$) at all postural positions. The HR$_\text{HLV}$/HR$_\text{LLV}$ ratio was 1.11$\pm$0.07, 1.08$\pm$0.05, 1.09$\pm$0.04, and 1.09$\pm$0.04 (mean$\pm$SD) for supine, 45 degree-first trial, 45 degree-second trial, and sitting positions, respectively. This heart rate variability may be due, at least in part, to the well-known respiratory sinus arrhythmia. HR monitoring from SCG signals might be used in different clinical applications including wearable cardiac monitoring systems.|http://arxiv.org/abs/1803.10346v2|Amirtaha Taebi,Andrew J Bomar,Richard H Sandler,Hansen A Mansy
1423|Prediction of Autism Treatment Response from Baseline fMRI using Random Forests and Tree Bagging|Treating children with autism spectrum disorders (ASD) with behavioral interventions, such as Pivotal Response Treatment (PRT), has shown promise in recent studies. However, deciding which therapy is best for a given patient is largely by trial and error, and choosing an ineffective intervention results in loss of valuable treatment time. We propose predicting patient response to PRT from baseline task-based fMRI by the novel application of a random forest and tree bagging strategy. Our proposed learning pipeline uses random forest regression to determine candidate brain voxels that may be informative in predicting treatment response. The candidate voxels are then tested stepwise for inclusion in a bagged tree ensemble. After the predictive model is constructed, bias correction is performed to further increase prediction accuracy. Using data from 19 ASD children who underwent a 16 week trial of PRT and a leave-one-out cross-validation framework, the presented learning pipeline was tested against several standard methods and variations of the pipeline and resulted in the highest prediction accuracy.|http://arxiv.org/abs/1805.09799v1|Nicha C. Dvornek,Daniel Yang,Archana Venkataraman,Pamela Ventola,Lawrence H. Staib,Kevin A. Pelphrey,James S. Duncan
1424|Post-randomization Biomarker Effect Modification in an HIV Vaccine Clinical Trial|While the HVTN 505 trial showed no overall efficacy of the tested vaccine to prevent HIV infection over placebo, previous studies, biological theories, and the finding that immune response markers strongly correlated with infection in vaccine recipients generated the hypothesis that a qualitative interaction occurred. This hypothesis can be assessed with statistical methods for studying treatment effect modification by an intermediate response variable (i.e., principal stratification effect modification (PSEM) methods). However, available PSEM methods make untestable structural risk assumptions, such that assumption-lean versions of PSEM methods are needed in order to surpass the high bar of evidence to demonstrate a qualitative interaction. Fortunately, the survivor average causal effect (SACE) literature is replete with assumption-lean methods that can be readily adapted to the PSEM application for the special case of a binary intermediate response variable. We map this adaptation, opening up a host of new PSEM methods for a binary intermediate variable measured via two-phase sampling, for a dichotomous or failure time final outcome and including or excluding the SACE monotonicity assumption. The new methods support that the vaccine partially protected vaccine recipients with a high polyfunctional CD8+ T cell response, an important new insight for the HIV vaccine field.|http://arxiv.org/abs/1811.03930v1|Peter B. Gilbert,Bryan S. Blette,Bryan E. Shepherd,Michael G. Hudgens
1425|Employing latent variable models to improve efficiency in composite endpoint analysis|Composite endpoints that combine multiple outcomes on different scales are common in clinical trials, particularly in chronic conditions. In many of these cases, patients will have to cross a predefined responder threshold in each of the outcomes to be classed as a responder overall. One instance of this occurs in systemic lupus erythematosus (SLE), where the responder endpoint combines two continuous, one ordinal and one binary measure. The overall binary responder endpoint is typically analysed using logistic regression, resulting in a substantial loss of information. We propose a latent variable model for the SLE endpoint, which assumes that the discrete outcomes are manifestations of latent continuous measures and can proceed to jointly model the components of the composite. We perform a simulation study and find the method to offer large efficiency gains over the standard analysis. We find that the magnitude of the precision gains are highly dependent on which components are driving response. Bias is introduced when joint normality assumptions are not satisfied, which we correct for using a bootstrap procedure. The method is applied to the Phase IIb MUSE trial in patients with moderate to severe SLE. We show that it estimates the treatment effect 2.5 times more precisely, offering a 60% reduction in required sample size.|http://arxiv.org/abs/1902.07037v1|Martina McMenamin,Jessica K. Barrett,Anna Berglind,James M. S. Wason
1426|Nonparametric analysis of nonhomogeneous multi-state processes based on clustered observations|Frequently, clinical trials and observational studies involve complex event history data with multiple events. When the observations are independent, the analysis of such studies can be based on standard methods for multi-state models. However, the independence assumption is often violated, such as in multicenter studies, which makes the use of standard methods improper. In this work we address the issue of nonparametric estimation and two-sample testing for the population-averaged transition and state occupation probabilities under general multi-state models based on right-censored, left-truncated, and clustered observations. The proposed methods do not impose assumptions regarding the within-cluster dependence, allow for informative cluster size, and are applicable to both Markov and non-Markov processes. Using empirical process theory, the estimators are shown to be uniformly consistent and to converge weakly to tight Gaussian processes. Closed-form variance estimators are derived, rigorous methodology for the calculation of simultaneous confidence bands is proposed, and the asymptotic properties of the nonparametric tests are established. Furthermore, we provide theoretical arguments for the validity of the nonparametric cluster bootstrap, which can be readily implemented in practice regardless of how complex the underlying multi-state model is. Simulation studies show that the performance of the proposed methods is good, and that methods that ignore the within-cluster dependence can lead to invalid inferences. Finally, the methods are applied to data from a multicenter randomized controlled trial.|http://arxiv.org/abs/1912.00487v3|Giorgos Bakoyannis
1427|Estimation and Validation of Ratio-based Conditional Average Treatment Effects Using Observational Data|While sample sizes in randomized clinical trials are large enough to estimate the average treatment effect well, they are often insufficient for estimation of treatment-covariate interactions critical to studying data-driven precision medicine. Observational data from real world practice may play an important role in alleviating this problem. One common approach in trials is to predict the outcome of interest with separate regression models in each treatment arm, and estimate the treatment effect based on the contrast of the predictions. Unfortunately, this simple approach may induce spurious treatment-covariate interaction in observational studies when the regression model is misspecified. Motivated by the need of modeling the number of relapses in multiple sclerosis patients, where the ratio of relapse rates is a natural choice of the treatment effect, we propose to estimate the conditional average treatment effect (CATE) as the ratio of expected potential outcomes, and derive a doubly robust estimator of this CATE in a semiparametric model of treatment-covariate interactions. We also provide a validation procedure to check the quality of the estimator on an independent sample. We conduct simulations to demonstrate the finite sample performance of the proposed methods, and illustrate their advantages on real data by examining the treatment effect of dimethyl fumarate compared to teriflunomide in multiple sclerosis patients.|http://arxiv.org/abs/1912.06977v2|Steve Yadlowsky,Fabio Pellegrini,Federica Lionetto,Stefan Braune,Lu Tian
1428|A deep learning-facilitated radiomics solution for the prediction of lung lesion shrinkage in non-small cell lung cancer trials|Herein we propose a deep learning-based approach for the prediction of lung lesion response based on radiomic features extracted from clinical CT scans of patients in non-small cell lung cancer trials. The approach starts with the classification of lung lesions from the set of primary and metastatic lesions at various anatomic locations. Focusing on the lung lesions, we perform automatic segmentation to extract their 3D volumes. Radiomic features are then extracted from the lesion on the pre-treatment scan and the first follow-up scan to predict which lesions will shrink at least 30% in diameter during treatment (either Pembrolizumab or combinations of chemotherapy and Pembrolizumab), which is defined as a partial response by the Response Evaluation Criteria In Solid Tumors (RECIST) guidelines. A 5-fold cross validation on the training set led to an AUC of 0.84 +/- 0.03, and the prediction on the testing dataset reached AUC of 0.73 +/- 0.02 for the outcome of 30% diameter shrinkage.|http://arxiv.org/abs/2003.02943v1|Antong Chen,Jennifer Saouaf,Bo Zhou,Randolph Crawford,Jianda Yuan,Junshui Ma,Richard Baumgartner,Shubing Wang,Gregory Goldmacher
1429|Do-search -- a tool for causal inference and study design with multiple data sources|Epidemiological evidence is based on multiple data sources including clinical trials, cohort studies, surveys, registries and expert opinions. Merging information from different sources opens up new possibilities for the estimation of causal effects. We show how causal effects can be identified and estimated by combining experiments and observations in real and realistic scenarios. As a new tool, we present do-search, a recently developed algorithmic approach that can determine the identifiability of a causal effect. The approach is based on do-calculus, and it can utilize data with non-trivial missing data and selection bias mechanisms. When the effect is identifiable, do-search outputs an identifying formula on which numerical estimation can be based. When the effect is not identifiable, we can use do-search to recognize additional data sources and assumptions that would make the effect identifiable. Throughout the paper, we consider the effect of salt-adding behavior on blood pressure mediated by the salt intake as an example. The identifiability of this effect is resolved in various scenarios with different assumptions on confounding. There are scenarios where the causal effect is identifiable from a chain of experiments but not from survey data, as well as scenarios where the opposite is true. As an illustration, we use survey data from NHANES 2013--2016 and the results from a meta-analysis of randomized controlled trials and estimate the reduction in average systolic blood pressure under an intervention where the use of table salt is discontinued.|http://arxiv.org/abs/2007.08189v1|Juha Karvanen,Santtu Tikka,Antti Hyttinen
1430|Bayesian sample size determination using commensurate priors to leverage pre-experimental data|This paper develops Bayesian sample size formulae for experiments comparing two groups. We assume the experimental data will be analysed in the Bayesian framework, where pre-experimental information from multiple sources can be represented into robust priors. In particular, such robust priors account for preliminary belief about the pairwise commensurability between parameters that underpin the historical and new experiments, to permit flexible borrowing of information. Averaged over the probability space of the new experimental data, appropriate sample sizes are found according to criteria that control certain aspects of the posterior distribution, such as the coverage probability or length of a defined density region. Our Bayesian methodology can be applied to circumstances where the common variance in the new experiment is known or unknown. Exact solutions are available based on most of the criteria considered for Bayesian sample size determination, while a search procedure is described in cases for which there are no closed-form expressions. We illustrate the application of our Bayesian sample size formulae in the setting of designing a clinical trial. Hypothetical data examples, motivated by a rare-disease trial with elicitation of expert prior opinion, and a comprehensive performance evaluation of the proposed methodology are presented.|http://arxiv.org/abs/2011.01106v1|Haiyan Zheng,Thomas Jaki,James M. S. Wason
1431|A Comparison of Model-Free Phase I Dose Escalation Designs for Dual-Agent Combination Therapies|It is increasingly common for therapies in oncology to be given in combination. In some cases, patients can benefit from the interaction between two drugs, although often at the risk of higher toxicity. A large number of designs to conduct phase I trials in this setting are available, where the objective is to select the maximum tolerated dose combination (MTC). Recently, a number of model-free (also called model-assisted) designs have provoked interest, providing several practical advantages over the more conventional approaches of rule-based or model-based designs. In this paper, we demonstrate a novel calibration procedure for model-free designs to determine their most desirable parameters. Under the calibration procedure, we compare the behaviour of model-free designs to a model-based approach in a comprehensive simulation study, covering a number of clinically plausible scenarios. It is found that model-free designs are competitive with the model-based design in terms of the proportion of correct selections of the MTC. However, there are a number of scenarios in which model-free designs offer a safer alternative. This is also illustrated in the application of the designs to a case study using data from a phase I oncology trial.|http://arxiv.org/abs/2104.14923v1|Helen Yvette Barnett,Matthew George,Donia Skanji,Gaelle Saint-Hilary,Thomas Jaki,Pavel Mozgunov
1432|Extent of Safety Database in Pediatric Drug Development: Types of Assessment, Analytical Precision, and Pathway for Extrapolation through On-Target Effects|Pediatric patients should have access to medicines that have been appropriately evaluated for safety and efficacy. Given this goal of revised labelling, the adequacy of the pediatric clinical development plan and resulting safety database must inform a favorable benefit-risk assessment for the intended use of the medicinal product. While extrapolation from adults can be used to support efficacy of drugs in children, there may be a reluctance to use the same approach in safety assessments, wiping out potential gains in trial efficiency through a reduction of sample size. To address this reluctance, we explore safety review in pediatric trials, including factors affecting these data, specific types of safety assessments, and precision on the estimation of event rates for specific adverse events (AEs) that can be achieved. In addition, we discuss the assessments which can provide a benchmark for the use of extrapolation of safety that focuses on on-target effects. Finally, we explore a unified approach for understanding precision using Bayesian approaches as the most appropriate methodology to describe/ascertain risk in probabilistic terms for the estimate of the event rate of specific AEs.|http://arxiv.org/abs/2211.13329v1|Margaret Gamalo,Yihua Zhao,Aijun Gao,Jingjing Ye,Ralph DeMasi,Eiji Eshida,YJ Choi,Robert Nelson
1433|Bayesian Semiparametric Model for Sequential Treatment Decisions with Informative Timing|We develop a Bayesian semi-parametric model for the estimating the impact of dynamic treatment rules on survival among patients diagnosed with pediatric acute myeloid leukemia (AML). The data consist of a subset of patients enrolled in the phase III AAML1031 clinical trial in which patients move through a sequence of four treatment courses. At each course, they undergo treatment that may or may not include anthracyclines (ACT). While ACT is known to be effective at treating AML, it is also cardiotoxic and can lead to early death for some patients. Our task is to estimate the potential survival probability under hypothetical dynamic ACT treatment strategies, but there are several impediments. First, since ACT was not randomized in the trial, its effect on survival is confounded over time. Second, subjects initiate the next course depending on when they recover from the previous course, making timing potentially informative of subsequent treatment and survival. Third, patients may die or drop out before ever completing the full treatment sequence. We develop a generative Bayesian semi-parametric model based on Gamma Process priors to address these complexities. At each treatment course, the model captures subjects' transition to subsequent treatment or death in continuous time under a given rule. A g-computation procedure is used to compute a posterior over potential survival probability that is adjusted for time-varying confounding. Using this approach, we conduct posterior inference for the efficacy of hypothetical treatment rules that dynamically modify ACT based on evolving cardiac function.|http://arxiv.org/abs/2211.16393v1|Arman Oganisian,Kelly D. Getz,Todd A. Alonzo,Richard Aplenc,Jason A. Roy
1434|Estimating Marginal Treatment Effect in Cluster Randomized Trials with Multi-level Missing Outcomes|Analyses of cluster randomized trials (CRTs) can be complicated by informative missing outcome data. Methods such as inverse probability weighted generalized estimating equations have been proposed to account for informative missingness by weighting the observed individual outcome data in each cluster. These existing methods have focused on settings where missingness occurs at the individual level and each cluster has partially or fully observed individual outcomes. In the presence of missing clusters, e.g., all outcomes from a cluster are missing due to drop-out of the cluster, these approaches effectively ignore this cluster-level missingness and can lead to biased inference if the cluster-level missingness is informative. Informative missingness at multiple levels can also occur in CRTs with a multi-level structure where study participants are nested in subclusters such as health care providers, and the subclusters are nested in clusters such as clinics. In this paper, we propose new estimators for estimating the marginal treatment effect in CRTs accounting for missing outcome data at multiple levels based on weighted generalized estimating equations. We show that the proposed multi-level multiply robust estimator is consistent and asymptotically normally distributed provided that one set of the propensity score models is correctly specified. We evaluate the performance of the proposed method through extensive simulation and illustrate its use with a CRT evaluating a Malaria risk-reduction intervention in rural Madagascar.|http://arxiv.org/abs/2304.05583v1|Chia-Rui Chang,Rui Wang
1435|The effect of estimating prevalences on the population-wise error rate|The population-wise error rate (PWER) is a type I error rate for clinical trials with multiple target populations. In such trials, a treatment is tested for its efficacy in each population. The PWER is defined as the probability that a randomly selected, future patient will be exposed to an inefficient treatment based on the study results. It can be understood and computed as an average of strata-specific family wise error rates and involves the prevalences of these strata. A major issue of this concept is that the prevalences are usually unknown in practice, so that the PWER cannot be directly controlled. Instead, one could use an estimator based on the given sample, like their maximum-likelihood estimator under a multinomial distribution. In this article, we demonstrate through simulations that this does not substantially inflate the true PWER. We differentiate between the expected PWER, which is almost perfectly controlled, and study-specific values of the PWER which are conditioned on all subgroup sample sizes and vary within a narrow range. Thereby, we consider up to eight different overlapping populations and moderate to large sample sizes. In these settings, we also consider the maximum strata-wise family wise error rate, which is found to be, on average, at least bounded by twice the significance level used for PWER control.|http://arxiv.org/abs/2304.09988v3|Remi Luschei,Werner Brannath
1436|Predictively Consistent Prior Effective Sample Sizes|Determining the sample size of an experiment can be challenging, even more so when incorporating external information via a prior distribution. Such information is increasingly used to reduce the size of the control group in randomized clinical trials. Knowing the amount of prior information, expressed as an equivalent prior effective sample size (ESS), clearly facilitates trial designs. Various methods to obtain a prior's ESS have been proposed recently. They have been justified by the fact that they give the standard ESS for one-parameter exponential families. However, despite being based on similar information-based metrics, they may lead to surprisingly different ESS for non-conjugate settings, which complicates many designs with prior information. We show that current methods fail a basic predictive consistency criterion, which requires the expected posterior-predictive ESS for a sample of size $N$ to be the sum of the prior ESS and $N$. The expected local-information-ratio ESS is introduced and shown to be predictively consistent. It corrects the ESS of current methods, as shown for normally distributed data with a heavy-tailed Student-t prior and exponential data with a generalized Gamma prior. Finally, two applications are discussed: the prior ESS for the control group derived from historical data, and the posterior ESS for hierarchical subgroup analyses.|http://arxiv.org/abs/1907.04185v1|Beat Neuenschwander,Sebastian Weber,Heinz Schmidli,Anthony O'Hagan
1437|Will This Idea Spread Beyond Academia? Understanding Knowledge Transfer of Scientific Concepts across Text Corpora|What kind of basic research ideas are more likely to get applied in practice? There is a long line of research investigating patterns of knowledge transfer, but it generally focuses on documents as the unit of analysis and follow their transfer into practice for a specific scientific domain. Here we study translational research at the level of scientific concepts for all scientific fields. We do this through text mining and predictive modeling using three corpora: 38.6 million paper abstracts, 4 million patent documents, and 0.28 million clinical trials. We extract scientific concepts (i.e., phrases) from corpora as instantiations of "research ideas", create concept-level features as motivated by literature, and then follow the trajectories of over 450,000 new concepts (emerged from 1995-2014) to identify factors that lead only a small proportion of these ideas to be used in inventions and drug trials. Results from our analysis suggest several mechanisms that distinguish which scientific concept will be adopted in practice, and which will not. We also demonstrate that our derived features can be used to explain and predict knowledge transfer with high accuracy. Our work provides greater understanding of knowledge transfer for researchers, practitioners, and government agencies interested in encouraging translational research.|http://arxiv.org/abs/2010.06657v1|Hancheng Cao,Mengjie Cheng,Zhepeng Cen,Daniel A. McFarland,Xiang Ren
1438|Weighted Approach for Estimating Effects in Principal Strata with Missing Data for a Categorical Post-Baseline Variable in Randomized Controlled Trials|This research was motivated by studying anti-drug antibody (ADA) formation and its potential impact on long-term benefit of a biologic treatment in a randomized controlled trial, in which ADA status was not only unobserved in the control arm but also in a subset of patients from the experimental treatment arm. Recent literature considers the principal stratum estimand strategy to estimate treatment effect in groups of patients defined by an intercurrent status, i.e. in groups defined by a post-randomization variable only observed in one arm and potentially associated with the outcome. However, status information might be missing even for a non-negligible number of patients in the experimental arm. For this setting, a novel weighted principal stratum approach is presented: Data from patients with missing intercurrent event status were re-weighted based on baseline covariates and additional longitudinal information. A theoretical justification of the proposed approach is provided for different types of outcomes, and assumptions allowing for causal conclusions on treatment effect are specified and investigated. Simulations demonstrated that the proposed method yielded valid inference and was robust against certain violations of assumptions. The method was shown to perform well in a clinical study with ADA status as an intercurrent event.|http://arxiv.org/abs/2101.04263v1|Shengchun Kong,Dominik Heinzmann,Sabine Lauer,Tian Lu
1439|Causal Inference in medicine and in health policy, a summary|A data science task can be deemed as making sense of the data or testing a hypothesis about it. The conclusions inferred from data can greatly guide us to make informative decisions. Big data has enabled us to carry out countless prediction tasks in conjunction with machine learning, such as identifying high risk patients suffering from a certain disease and taking preventable measures. However, healthcare practitioners are not content with mere predictions - they are also interested in the cause-effect relation between input features and clinical outcomes. Understanding such relations will help doctors treat patients and reduce the risk effectively. Causality is typically identified by randomized controlled trials. Often such trials are not feasible when scientists and researchers turn to observational studies and attempt to draw inferences. However, observational studies may also be affected by selection and/or confounding biases that can result in wrong causal conclusions. In this chapter, we will try to highlight some of the drawbacks that may arise in traditional machine learning and statistical approaches to analyze the observational data, particularly in the healthcare data analytics domain. We will discuss causal inference and ways to discover the cause-effect from observational studies in healthcare domain. Moreover, we will demonstrate the applications of causal inference in tackling some common machine learning issues such as missing data and model transportability. Finally, we will discuss the possibility of integrating reinforcement learning with causality as a way to counter confounding bias.|http://arxiv.org/abs/2105.04655v4|Wenhao Zhang,Ramin Ramezani,Arash Naeim
1440|A Bayesian adaptive design for dual-agent phase I-II oncology trials integrating efficacy data across stages|Combination of several anti-cancer treatments has typically been presumed to have enhanced drug activity. Motivated by a real clinical trial, this paper considers phase I-II dose finding designs for dual-agent combinations, where one main objective is to characterize both the toxicity and efficacy profiles. We propose a two-stage Bayesian adaptive design that accommodates a change of patient population in-between. In stage I, we estimate a maximum tolerated dose combination using the escalation with overdose control (EWOC) principle. This is followed by a stage II, conducted in a new yet relevant patient population, to find the most efficacious dose combination. We implement a robust Bayesian hierarchical random-effects model to allow sharing of information on the efficacy across stages, assuming that the related parameters are either exchangeable or nonexchangeable. Under the assumption of exchangeability, a random-effects distribution is specified for the main effects parameters to capture uncertainty about the between-stage differences. The inclusion of nonexchangeability assumption further enables that the stage-specific efficacy parameters have their own priors. The proposed methodology is assessed with an extensive simulation study. Our results suggest a general improvement of the operating characteristics for the efficacy assessment, under a conservative assumption about the exchangeability of the parameters \textit{a priori}|http://arxiv.org/abs/2106.08277v3|Jos L. Jimnez,Haiyan Zheng
1441|ALL-IN meta-analysis: breathing life into living systematic reviews|Science is idolized as a cumulative process ("standing on the shoulders of giants"), yet scientific knowledge is typically built on a patchwork of research contributions without much coordination. This lack of efficiency has specifically been addressed in clinical research by recommendations for living systematic reviews and against research waste. We propose to further those recommendations with ALL-IN meta-analysis: Anytime Live and Leading INterim meta-analysis. ALL-IN provides statistical methodology for a meta-analysis that can be updated at any time -- reanalyzing after each new observation while retaining type-I error guarantees, live -- no need to prespecify the looks, and leading -- in the decisions on whether individual studies should be initiated, stopped or expanded, the meta-analysis can be the leading source of information. We illustrate the method for time-to-event data, showing how synthesizing data at interim stages of studies can increase efficiency when studies are slow in themselves to provide the necessary number of events for completion. The meta-analysis can be performed on interim data, but does not have to. The analysis design requires no information about the number of patients in trials or the number of trials eventually included. So it can breathe life into living systematic reviews, through better and simpler statistics, efficiency, collaboration and communication.|http://arxiv.org/abs/2109.12141v1|Judith ter Schure,Peter Grnwald
1442|Automated tabulation of clinical trial results: A joint entity and relation extraction approach with transformer-based language representations|Evidence-based medicine, the practice in which healthcare professionals refer to the best available evidence when making decisions, forms the foundation of modern healthcare. However, it relies on labour-intensive systematic reviews, where domain specialists must aggregate and extract information from thousands of publications, primarily of randomised controlled trial (RCT) results, into evidence tables. This paper investigates automating evidence table generation by decomposing the problem across two language processing tasks: \textit{named entity recognition}, which identifies key entities within text, such as drug names, and \textit{relation extraction}, which maps their relationships for separating them into ordered tuples. We focus on the automatic tabulation of sentences from published RCT abstracts that report the results of the study outcomes. Two deep neural net models were developed as part of a joint extraction pipeline, using the principles of transfer learning and transformer-based language representations. To train and test these models, a new gold-standard corpus was developed, comprising almost 600 result sentences from six disease areas. This approach demonstrated significant advantages, with our system performing well across multiple natural language processing tasks and disease areas, as well as in generalising to disease domains unseen during training. Furthermore, we show these results were achievable through training our models on as few as 200 example sentences. The final system is a proof of concept that the generation of evidence tables can be semi-automated, representing a step towards fully automating systematic reviews.|http://arxiv.org/abs/2112.05596v1|Jetsun Whitton,Anthony Hunter
1443|Estimating average causal effects from patient trajectories|In medical practice, treatments are selected based on the expected causal effects on patient outcomes. Here, the gold standard for estimating causal effects are randomized controlled trials; however, such trials are costly and sometimes even unethical. Instead, medical practice is increasingly interested in estimating causal effects among patient (sub)groups from electronic health records, that is, observational data. In this paper, we aim at estimating the average causal effect (ACE) from observational data (patient trajectories) that are collected over time. For this, we propose DeepACE: an end-to-end deep learning model. DeepACE leverages the iterative G-computation formula to adjust for the bias induced by time-varying confounders. Moreover, we develop a novel sequential targeting procedure which ensures that DeepACE has favorable theoretical properties, i.e., is doubly robust and asymptotically efficient. To the best of our knowledge, this is the first work that proposes an end-to-end deep learning model tailored for estimating time-varying ACEs. We compare DeepACE in an extensive number of experiments, confirming that it achieves state-of-the-art performance. We further provide a case study for patients suffering from low back pain to demonstrate that DeepACE generates important and meaningful findings for clinical practice. Our work enables practitioners to develop effective treatment recommendations based on population effects.|http://arxiv.org/abs/2203.01228v2|Dennis Frauen,Tobias Hatt,Valentyn Melnychuk,Stefan Feuerriegel
1444|SCouT: Synthetic Counterfactuals via Spatiotemporal Transformers for Actionable Healthcare|The Synthetic Control method has pioneered a class of powerful data-driven techniques to estimate the counterfactual reality of a unit from donor units. At its core, the technique involves a linear model fitted on the pre-intervention period that combines donor outcomes to yield the counterfactual. However, linearly combining spatial information at each time instance using time-agnostic weights fails to capture important inter-unit and intra-unit temporal contexts and complex nonlinear dynamics of real data. We instead propose an approach to use local spatiotemporal information before the onset of the intervention as a promising way to estimate the counterfactual sequence. To this end, we suggest a Transformer model that leverages particular positional embeddings, a modified decoder attention mask, and a novel pre-training task to perform spatiotemporal sequence-to-sequence modeling. Our experiments on synthetic data demonstrate the efficacy of our method in the typical small donor pool setting and its robustness against noise. We also generate actionable healthcare insights at the population and patient levels by simulating a state-wide public health policy to evaluate its effectiveness, an in silico trial for asthma medications to support randomized controlled trials, and a medical intervention for patients with Friedreich's ataxia to improve clinical decision-making and promote personalized therapy.|http://arxiv.org/abs/2207.04208v2|Bhishma Dedhia,Roshini Balasubramanian,Niraj K. Jha
1445|Evaluating tests for cluster-randomized trials with few clusters under generalized linear mixed models with covariate adjustment: a simulation study|Generalized linear mixed models (GLMM) are commonly used to analyze clustered data, but when the number of clusters is small to moderate, standard statistical tests may produce elevated type I error rates. Small-sample corrections have been proposed for continuous or binary outcomes without covariate adjustment. However, appropriate tests to use for count outcomes or under covariate-adjusted models remains unknown. An important setting in which this issue arises is in cluster-randomized trials (CRTs). Because many CRTs have just a few clusters (e.g., clinics or health systems), covariate adjustment is particularly critical to address potential chance imbalance and/or low power (e.g., adjustment following stratified randomization or for the baseline value of the outcome). We conducted simulations to evaluate GLMM-based tests of the treatment effect that account for the small (10) or moderate (20) number of clusters under a parallel-group CRT setting across scenarios of covariate adjustment (including adjustment for one or more person-level or cluster-level covariates) for both binary and count outcomes. We find that when the intraclass correlation is non-negligible ($\geq 0.01$) and the number of covariates is small ($\leq 2$), likelihood ratio tests with a between-within denominator degree of freedom have type I error rates close to the nominal level. When the number of covariates is moderate ($\geq 5$), across our simulation scenarios, the relative performance of the tests varied considerably and no method performed uniformly well. Therefore, we recommend adjusting for no more than a few covariates and using likelihood ratio tests with a between-within denominator degree of freedom.|http://arxiv.org/abs/2209.04364v2|Hongxiang Qiu,Andrea J. Cook,Jennifer F. Bobb
1446|Automated Fidelity Assessment for Strategy Training in Inpatient Rehabilitation using Natural Language Processing|Strategy training is a multidisciplinary rehabilitation approach that teaches skills to reduce disability among those with cognitive impairments following a stroke. Strategy training has been shown in randomized, controlled clinical trials to be a more feasible and efficacious intervention for promoting independence than traditional rehabilitation approaches. A standardized fidelity assessment is used to measure adherence to treatment principles by examining guided and directed verbal cues in video recordings of rehabilitation sessions. Although the fidelity assessment for detecting guided and directed verbal cues is valid and feasible for single-site studies, it can become labor intensive, time consuming, and expensive in large, multi-site pragmatic trials. To address this challenge to widespread strategy training implementation, we leveraged natural language processing (NLP) techniques to automate the strategy training fidelity assessment, i.e., to automatically identify guided and directed verbal cues from video recordings of rehabilitation sessions. We developed a rule-based NLP algorithm, a long-short term memory (LSTM) model, and a bidirectional encoder representation from transformers (BERT) model for this task. The best performance was achieved by the BERT model with a 0.8075 F1-score. This BERT model was verified on an external validation dataset collected from a separate major regional health system and achieved an F1 score of 0.8259, which shows that the BERT model generalizes well. The findings from this study hold widespread promise in psychology and rehabilitation intervention research and practice.|http://arxiv.org/abs/2209.06727v2|Hunter Osterhoudt,Courtney E. Schneider,Haneef A Mohammad,Minmei Shih,Alexandra E. Harper,Leming Zhou,Elizabeth R Skidmore,Yanshan Wang
1447|Using a Surrogate with Heterogeneous Utility to Test for a Treatment Effect|The primary benefit of identifying a valid surrogate marker is the ability to use it in a future trial to test for a treatment effect with shorter follow-up time or less cost. However, previous work has demonstrated potential heterogeneity in the utility of a surrogate marker. When such heterogeneity exists, existing methods that use the surrogate to test for a treatment effect while ignoring this heterogeneity may lead to inaccurate conclusions about the treatment effect, particularly when the patient population in the new study has a different mix of characteristics than the study used to evaluate the utility of the surrogate marker. In this paper, we develop a novel test for a treatment effect using surrogate marker information that accounts for heterogeneity in the utility of the surrogate. We compare our testing procedure to a test that uses primary outcome information (gold standard) and a test that uses surrogate marker information, but ignores heterogeneity. We demonstrate the validity of our approach and derive the asymptotic properties of our estimator and variance estimates. Simulation studies examine the finite sample properties of our testing procedure and demonstrate when our proposed approach can outperform the testing approach that ignores heterogeneity. We illustrate our methods using data from an AIDS clinical trial to test for a treatment effect using CD4 count as a surrogate marker for RNA.|http://arxiv.org/abs/2209.08315v1|Layla Parast,Tianxi Cai,Lu Tian
1448|Bias of the additive hazard model in the presence of causal effect heterogeneity|Hazard ratios are prone to selection bias, compromising their use as causal estimands. On the other hand, the hazard difference has been shown to remain unaffected by the selection of frailty factors over time. Therefore, observed hazard differences can be used as an unbiased estimator for the causal hazard differences in the absence of confounding. However, in the presence of effect (on the hazard) heterogeneity, the hazard difference is also affected by selection. In this work, we formalize how the observed hazard difference (from a randomized controlled trial) evolves by selecting favourable levels of effect modifiers in the exposed group and thus deviates from the causal hazard difference of interest. Such selection may result in a non-linear integrated hazard difference curve even when the individual causal effects are time-invariant. Therefore, a homogeneous time-varying causal additive effect on the hazard can not be distinguished from a constant but heterogeneous causal effect. We illustrate this causal issue by studying the effect of chemotherapy on the survival time of patients suffering from carcinoma of the oropharynx using data from a clinical trial. The hazard difference can thus not be used as an appropriate measure of the causal effect without making untestable assumptions.|http://arxiv.org/abs/2210.16555v1|Richard Post,Edwin van den Heuvel,Hein Putter
1449|Do Multi-Document Summarization Models Synthesize?|Multi-document summarization entails producing concise synopses of collections of inputs. For some applications, the synopsis should accurately synthesize inputs with respect to a key aspect, e.g., a synopsis of film reviews written about a particular movie should reflect the average critic consensus. As a more consequential example, narrative summaries that accompany biomedical systematic reviews of clinical trial results should accurately summarize the potentially conflicting results from individual trials. In this paper we ask: To what extent do modern multi-document summarization models implicitly perform this sort of synthesis? We run experiments over opinion and evidence synthesis datasets using a suite of summarization models, from fine-tuned transformers to GPT-4. We find that existing models partially perform synthesis, but imperfectly: even the best performing models are over-sensitive to changes in input ordering and under-sensitive to changes in input compositions (e.g., ratio of positive to negative reviews). We propose a simple, general, effective method for improving model synthesis capabilities by generating an explicitly diverse set of candidate outputs, and then selecting from these the string best aligned with the expected aggregate measure for the inputs, or abstaining when the model produces no good candidate.|http://arxiv.org/abs/2301.13844v2|Jay DeYoung,Stephanie C. Martinez,Iain J. Marshall,Byron C. Wallace
1450|Asymptotic Properties of Multi-Treatment Covariate Adaptive Randomization Procedures for Balancing Observed and Unobserved Covariates|Applications of CAR for balancing continuous covariates remain comparatively rare, especially in multi-treatment clinical trials, and the theoretical properties of multi-treatment CAR have remained largely elusive for decades. In this paper, we consider a general framework of CAR procedures for multi-treatment clinal trials which can balance general covariate features, such as quadratic and interaction terms which can be discrete, continuous, and mixing. We show that under widely satisfied conditions the proposed procedures have superior balancing properties; in particular, the convergence rate of imbalance vectors can attain the best rate $O_P(1)$ for discrete covariates, continuous covariates, or combinations of both discrete and continuous covariates, and at the same time, the convergence rate of the imbalance of unobserved covariates is $O_P(\sqrt n)$, where $n$ is the sample size. The general framework unifies many existing methods and related theories, introduces a much broader class of new and useful CAR procedures, and provides new insights and a complete picture of the properties of CAR procedures. The favorable balancing properties lead to the precision of the treatment effect test in the presence of a heteroscedastic linear model with dependent covariate features. As an application, the properties of the test of treatment effect with unobserved covariates are studied under the CAR procedures, and consistent tests are proposed so that the test has an asymptotic precise type I error even if the working model is wrong and covariates are unobserved in the analysis.|http://arxiv.org/abs/2305.13842v1|Li-Xin Zhang
1451|Empirical prior distributions for Bayesian meta-analyses of binary and time to event outcomes|Bayesian model-averaged meta-analysis allows quantification of evidence for both treatment effectiveness $\mu$ and across-study heterogeneity $\tau$. We use the Cochrane Database of Systematic Reviews to develop discipline-wide empirical prior distributions for $\mu$ and $\tau$ for meta-analyses of binary and time-to-event clinical trial outcomes. First, we use 50% of the database to estimate parameters of different required parametric families. Second, we use the remaining 50% of the database to select the best-performing parametric families and explore essential assumptions about the presence or absence of the treatment effectiveness and across-study heterogeneity in real data. We find that most meta-analyses of binary outcomes are more consistent with the absence of the meta-analytic effect or heterogeneity while meta-analyses of time-to-event outcomes are more consistent with the presence of the meta-analytic effect or heterogeneity. Finally, we use the complete database - with close to half a million trial outcomes - to propose specific empirical prior distributions, both for the field in general and for specific medical subdisciplines. An example from acute respiratory infections demonstrates how the proposed prior distributions can be used to conduct a Bayesian model-averaged meta-analysis in the open-source software R and JASP.|http://arxiv.org/abs/2306.11468v1|Frantiek Barto,Willem M. Otte,Quentin F. Gronau,Bram Timmers,Alexander Ly,Eric-Jan Wagenmakers
1452|The effect of data augmentation and 3D-CNN depth on Alzheimer's Disease detection|Machine Learning (ML) has emerged as a promising approach in healthcare, outperforming traditional statistical techniques. However, to establish ML as a reliable tool in clinical practice, adherence to best practices regarding data handling, experimental design, and model evaluation is crucial. This work summarizes and strictly observes such practices to ensure reproducible and reliable ML. Specifically, we focus on Alzheimer's Disease (AD) detection, which serves as a paradigmatic example of challenging problem in healthcare. We investigate the impact of different data augmentation techniques and model complexity on the overall performance. We consider MRI data from ADNI dataset to address a classification problem employing 3D Convolutional Neural Network (CNN). The experiments are designed to compensate for data scarcity and initial random parameters by utilizing cross-validation and multiple training trials. Within this framework, we train 15 predictive models, considering three different data augmentation strategies and five distinct 3D CNN architectures, each varying in the number of convolutional layers. Specifically, the augmentation strategies are based on affine transformations, such as zoom, shift, and rotation, applied concurrently or separately. The combined effect of data augmentation and model complexity leads to a variation in prediction performance up to 10% of accuracy. When affine transformation are applied separately, the model is more accurate, independently from the adopted architecture. For all strategies, the model accuracy followed a concave behavior at increasing number of convolutional layers, peaking at an intermediate value of layers. The best model (8 CL, (B)) is the most stable across cross-validation folds and training trials, reaching excellent performance both on the testing set and on an external test set.|http://arxiv.org/abs/2309.07192v1|Rosanna Turrisi,Alessandro Verri,Annalisa Barla
1453|Adaptive Experimental Design for Intrusion Data Collection|Intrusion research frequently collects data on attack techniques currently employed and their potential symptoms. This includes deploying honeypots, logging events from existing devices, employing a red team for a sample attack campaign, or simulating system activity. However, these observational studies do not clearly discern the cause-and-effect relationships between the design of the environment and the data recorded. Neglecting such relationships increases the chance of drawing biased conclusions due to unconsidered factors, such as spurious correlations between features and errors in measurement or classification. In this paper, we present the theory and empirical data on methods that aim to discover such causal relationships efficiently. Our adaptive design (AD) is inspired by the clinical trial community: a variant of a randomized control trial (RCT) to measure how a particular ``treatment'' affects a population. To contrast our method with observational studies and RCT, we run the first controlled and adaptive honeypot deployment study, identifying the causal relationship between an ssh vulnerability and the rate of server exploitation. We demonstrate that our AD method decreases the total time needed to run the deployment by at least 33%, while still confidently stating the impact of our change in the environment. Compared to an analogous honeypot study with a control group, our AD requests 17% fewer honeypots while collecting 19% more attack recordings than an analogous honeypot study with a control group.|http://arxiv.org/abs/2310.13224v1|Kate Highnam,Zach Hanif,Ellie Van Vogt,Sonali Parbhoo,Sergio Maffeis,Nicholas R. Jennings
1454|PKBOIN-12: A Bayesian optimal interval Phase I/II design incorporating pharmacokinetics outcomes to find the optimal biological dose|Immunotherapies and targeted therapies have gained popularity due to their promising therapeutic effects across multiple treatment areas. The focus of early phase dose-finding clinical trials has shifted from finding the maximum tolerated dose (MTD) to identifying the optimal biological dose (OBD), which aims to balance the toxicity and efficacy outcomes, thereby optimizing the risk-benefit trade-off. These trials often collect multiple pharmacokinetics (PK) outcomes to assess drug exposure, which has shown correlations with toxicity and efficacy outcomes but has not been utilized in the current dose-finding designs for OBD selection. Moreover, PK outcomes are usually available within days after initial treatment, much faster than toxicity and efficacy outcomes. To bridge this gap, we introduce the innovative model-assisted PKBOIN-12 design, which enhances the BOIN12 design by integrating PK information into both the dose-finding algorithm and the final OBD determination process. We further extend PKBOIN-12 to the TITE-PKBOIN-12 design to address the challenges of late-onset toxicity and efficacy outcomes. Simulation results demonstrate that the PKBOIN-12 design more effectively identifies the OBD and allocates a greater number of patients to it than BOIN12. Additionally, PKBOIN-12 decreases the probability of selecting inefficacious doses as the OBD by excluding those with low drug exposure. Comprehensive simulation studies and sensitivity analysis confirm the robustness of both PKBOIN-12 and TITE-PKBOIN-12 designs in various scenarios.|http://arxiv.org/abs/2312.15396v1|Hao Sun,Jieqi Tu
1455|System and Method to Determine ME/CFS and Long COVID Disease Severity Using a Wearable Sensor|Objective: We present a simple parameter, calculated from a single wearable sensor, that can be used to objectively measure disease severity in people with myalgic encephalomyelitis/chronic fatigue syndrome (ME/CFS) or Long COVID. We call this parameter UpTime. Methods: Prior research has shown that the amount of time a person spends upright, defined as lower legs vertical with feet on the floor, correlates strongly with ME/CFS disease severity. We use a single commercial inertial measurement unit (IMU) attached to the ankle to calculate the percentage of time each day that a person spends upright (i.e., UpTime) and number of Steps/Day. As Long COVID shares symptoms with ME/CFS, we also apply this method to determine Long COVID disease severity. We performed a trial with 55 subjects broken into three cohorts, healthy controls, ME/CFS, and Long COVID. Subjects wore the IMU on their ankle for a period of 7 days. UpTime and Steps/Day were calculated each day and results compared between cohorts. Results: UpTime effectively distinguishes between healthy controls and subjects diagnosed with ME/CFS ($\mathbf{p = 0.00004}$) and between healthy controls and subjects diagnosed with Long COVID ($\mathbf{p = 0.01185}$). Steps/Day did distinguish between controls and subjects with ME/CFS ($\mathbf{p = 0.01}$) but did not distinguish between controls and subjects with Long COVID ($\mathbf{p = 0.3}$). Conclusion: UpTime is an objective measure of ME/CFS and Long COVID severity. UpTime can be used as an objective outcome measure in clinical research and treatment trials. Significance: Objective assessment of ME/CFS and Long COVID disease severity using UpTime could spur development of treatments by enabling the effect of those treatments to be easily measured.|http://arxiv.org/abs/2404.04345v1|Yifei Sun,Suzanne D. Vernon,Shad Roundy
1456|End-To-End Causal Effect Estimation from Unstructured Natural Language Data|Knowing the effect of an intervention is critical for human decision-making, but current approaches for causal effect estimation rely on manual data collection and structuring, regardless of the causal assumptions. This increases both the cost and time-to-completion for studies. We show how large, diverse observational text data can be mined with large language models (LLMs) to produce inexpensive causal effect estimates under appropriate causal assumptions. We introduce NATURAL, a novel family of causal effect estimators built with LLMs that operate over datasets of unstructured text. Our estimators use LLM conditional distributions (over variables of interest, given the text data) to assist in the computation of classical estimators of causal effect. We overcome a number of technical challenges to realize this idea, such as automating data curation and using LLMs to impute missing information. We prepare six (two synthetic and four real) observational datasets, paired with corresponding ground truth in the form of randomized trials, which we used to systematically evaluate each step of our pipeline. NATURAL estimators demonstrate remarkable performance, yielding causal effect estimates that fall within 3 percentage points of their ground truth counterparts, including on real-world Phase 3/4 clinical trials. Our results suggest that unstructured text data is a rich source of causal effect information, and NATURAL is a first step towards an automated pipeline to tap this resource.|http://arxiv.org/abs/2407.07018v3|Nikita Dhawan,Leonardo Cotta,Karen Ullrich,Rahul G. Krishnan,Chris J. Maddison
1457|Adaptive weight selection for time-to-event data under non-proportional hazards|When planning a clinical trial for a time-to-event endpoint, we require an estimated effect size and need to consider the type of effect. Usually, an effect of proportional hazards is assumed with the hazard ratio as the corresponding effect measure. Thus, the standard procedure for survival data is generally based on a single-stage log-rank test. Knowing that the assumption of proportional hazards is often violated and sufficient knowledge to derive reasonable effect sizes is usually unavailable, such an approach is relatively rigid. We introduce a more flexible procedure by combining two methods designed to be more robust in case we have little to no prior knowledge. First, we employ a more flexible adaptive multi-stage design instead of a single-stage design. Second, we apply combination-type tests in the first stage of our suggested procedure to benefit from their robustness under uncertainty about the deviation pattern. We can then use the data collected during this period to choose a more specific single-weighted log-rank test for the subsequent stages. In this step, we employ Royston-Parmar spline models to extrapolate the survival curves to make a reasonable decision. Based on a real-world data example, we show that our approach can save a trial that would otherwise end with an inconclusive result. Additionally, our simulation studies demonstrate a sufficient power performance while maintaining more flexibility.|http://arxiv.org/abs/2409.15145v1|Moritz Fabian Danzer,Ina Dormuth
1458|Nonparametric Bayesian approach for dynamic borrowing of historical control data|When incorporating historical control data into the analysis of current randomized controlled trial data, it is critical to account for differences between the datasets. When the cause of the difference is an unmeasured factor and adjustment for observed covariates only is insufficient, it is desirable to use a dynamic borrowing method that reduces the impact of heterogeneous historical controls. We propose a nonparametric Bayesian approach for borrowing historical controls that are homogeneous with the current control. Additionally, to emphasize the resolution of conflicts between the historical controls and current control, we introduce a method based on the dependent Dirichlet process mixture. The proposed methods can be implemented using the same procedure, regardless of whether the outcome data comprise aggregated study-level data or individual participant data. We also develop a novel index of similarity between the historical and current control data, based on the posterior distribution of the parameter of interest. We conduct a simulation study and analyze clinical trial examples to evaluate the performance of the proposed methods compared to existing methods. The proposed method based on the dependent Dirichlet process mixture can more accurately borrow from homogeneous historical controls while reducing the impact of heterogeneous historical controls compared to the typical Dirichlet process mixture. The proposed methods outperform existing methods in scenarios with heterogeneous historical controls, in which the meta-analytic approach is ineffective.|http://arxiv.org/abs/2411.11675v1|Tomohiro Ohigashi,Kazushi Maruo,Takashi Sozu,Masahiko Gosho
1459|Dynamic Indicators of Adherence and Retention in Digital Health Studies: Insights from the Brighten Study|Background: Effective use of mobile health technologies requires high participant adherence and retention. However, remote digital health studies often face high attrition and low adherence, potentially introducing bias and limiting generalizability. Objective: This study aims to identify longitudinal indicators of participant retention and adherence to develop strategies for improving data collection in digital health studies and understanding how cohorts are shaped by participant withdrawal and non-adherence. Methods: We conducted analyses on the Brighten study, a smartphone-based randomized controlled trial evaluating apps for depression treatment. Participants were asked to complete seven digital questionnaires regularly. Outcomes included adherence (questionnaire completion), engagement (post-baseline participation), and retention (continued participation over time). We analyzed relationships between these outcomes, static factors (e.g., demographics, average questionnaire scores) and dynamic factors (e.g., questionnaire score changes over time). Results: Of 2,201 participants, 1,093 completed at least one non-baseline questionnaire (median completion rate: 37.6%). Adherence was higher among participants with lower average depression severity (P<.001) and those perceiving improvement (P=.001). Demographic factors significantly influenced adherence and engagement. Participants with greater baseline depressive symptoms were more likely to withdraw before completing non-baseline questionnaires (t=-2.53, P=.01). However, symptom improvement was linked to better adherence (U=127,084; P<.001) and retention (HR=0.78, P=.002). Conclusion: Clinical trajectories and perceived improvements in depressive symptoms are key indicators of engagement, adherence, and retention. These findings may enhance data interpretation and inform strategies to boost retention and adherence in future trials.|http://arxiv.org/abs/2412.00942v1|Dylan Hamitouche,Youcef Barkat,Deven Parekh,Eva Hammer,David Benrimoh
1460|On the two-step hybrid design for augmenting randomized trials using real-world data|Hybrid clinical trials, that borrow real-world data (RWD), are gaining interest, especially for rare diseases. They assume RWD and randomized control arm be exchangeable, but violations can bias results, inflate type I error, or reduce power. A two-step hybrid design first tests exchangeability, reducing inappropriate borrowing but potentially inflating type I error (Yuan et al., 2019). We propose four methods to better control type I error. Approach 1 estimates the variance of test statistics, rejecting the null hypothesis based on large sample normal approximation. Approach 2 uses a numerical approach for exact critical value determination. Approach 3 splits type I error rates by equivalence test outcome. Approach 4 adjusts the critical value only when equivalence is established. Simulation studies using a hypothetical ALS scenario, evaluate type I error and power under various conditions, compared to the Bayesian power prior approach (Ibrahim et al., 2015). Our methods and the Bayesian power prior control type I error, whereas Yuan et al. (2019) increases it under exchangeability. If exchangeability doesn't hold, all methods fail to control type I error. Our methods show type I error inflation of 6%-8%, compared to 10% for Yuan et al. (2019) and 16% for the Bayesian power prior.|http://arxiv.org/abs/2501.12453v1|Jiapeng Xu,Ruben P. A. van Eijk,Alicia Ellis,Tianyu Pan,Lorene M. Nelson,Kit C. B. Roes,Marc van Dijk,Maria Sarno,Leonard H. van den Berg,Lu Tian,Ying Lu
1461|Targeted Data Fusion for Causal Survival Analysis Under Distribution Shift|Causal inference across multiple data sources has the potential to improve the generalizability, transportability, and replicability of scientific findings. However, data integration methods for time-to-event outcomes -- common in medical contexts such as clinical trials -- remain underdeveloped. Existing data fusion methods focus on binary or continuous outcomes, neglecting the distinct challenges of survival analysis, including right-censoring and the unification of discrete and continuous time frameworks. To address these gaps, we propose two novel approaches for multi-source causal survival analysis. First, considering a target site-specific causal effect, we introduce a semiparametric efficient estimator for scenarios where data-sharing is feasible. Second, we develop a federated learning framework tailored to privacy-constrained environments. This framework dynamically adjusts source site-specific contributions, downweighting biased sources and upweighting less biased ones relative to the target population. Both approaches incorporate nonparametric machine learning models to enhance robustness and efficiency, with theoretical guarantees applicable to both continuous and discrete time-to-event outcomes. We demonstrate the practical utility of our methods through extensive simulations and an application to two randomized trials of a monoclonal neutralizing antibody for HIV-1 prevention: HVTN 704/HPTN 085 (cisgender men and transgender persons in the Americas and Switzerland) and HVTN 703/HPTN 081 (women in sub-Saharan Africa). The results highlight the potential of our approaches to efficiently estimate causal effects while addressing heterogeneity across data sources and adhering to privacy and robustness constraints.|http://arxiv.org/abs/2501.18798v1|Yi Liu,Alexander W. Levis,Ke Zhu,Shu Yang,Peter B. Gilbert,Larry Han
1462|Definition of Cybernetical Neuroscience|A new scientific field is introduced and discussed, named cybernetical neuroscience, which studies mathematical models adopted in computational neuroscience by methods of cybernetics -- the science of control and communication in a living organism, machine and society. It also considers the practical application of the results obtained when studying mathematical models. The main tasks and methods, as well as some results of cybernetic neuroscience are considered.|http://arxiv.org/abs/2409.16314v1|Alexander Fradkov
1463|Analyzing Incomplete Discrete Longitudinal Clinical Trial Data|Commonly used methods to analyze incomplete longitudinal clinical trial data include complete case analysis (CC) and last observation carried forward (LOCF). However, such methods rest on strong assumptions, including missing completely at random (MCAR) for CC and unchanging profile after dropout for LOCF. Such assumptions are too strong to generally hold. Over the last decades, a number of full longitudinal data analysis methods have become available, such as the linear mixed model for Gaussian outcomes, that are valid under the much weaker missing at random (MAR) assumption. Such a method is useful, even if the scientific question is in terms of a single time point, for example, the last planned measurement occasion, and it is generally consistent with the intention-to-treat principle. The validity of such a method rests on the use of maximum likelihood, under which the missing data mechanism is ignorable as soon as it is MAR. In this paper, we will focus on non-Gaussian outcomes, such as binary, categorical or count data. This setting is less straightforward since there is no unambiguous counterpart to the linear mixed model. We first provide an overview of the various modeling frameworks for non-Gaussian longitudinal data, and subsequently focus on generalized linear mixed-effects models, on the one hand, of which the parameters can be estimated using full likelihood, and on generalized estimating equations, on the other hand, which is a nonlikelihood method and hence requires a modification to be valid under MAR. We briefly comment on the position of models that assume missingness not at random and argue they are most useful to perform sensitivity analysis. Our developments are underscored using data from two studies. While the case studies feature binary outcomes, the methodology applies equally well to other discrete-data settings, hence the qualifier ``discrete'' in the title.|http://arxiv.org/abs/math/0606497v1|Ivy Jansen,Caroline Beunckens,Geert Molenberghs,Geert Verbeke,Craig Mallinckrodt
1464|Using phase II data for the analysis of phase III studies: an application in rare diseases|Clinical research and drug development in orphan diseases is challenging, since large-scale randomized studies are difficult to conduct. Formally synthesizing the evidence is therefore of great value, yet this is rarely done in the drug approval process. Phase III designs that make better use of phase II data can facilitate drug development in orphan diseases.   A Bayesian meta-analytic approach is used to inform the phase III study with phase II data. It is particularly attractive, since uncertainty of between-trial heterogeneity can be dealt with probabilistically, which is critical if the number of studies is small. Furthermore, it allows quantifying and discounting the phase II data through the predictive distribution relevant for phase III. A phase III design is proposed which uses the phase II data and considers approval based on a phase III interim analysis. The design is illustrated with a non-inferiority case study from an FDA approval in herpetic keratitis (an orphan disease). Design operating characteristics are compared to those of a traditional design, which ignores the phase II data.   An analysis of the phase II data reveals good but insufficient evidence for non-inferiority, highlighting the need for a phase III study. For the phase III study supported by phase II data, the interim analysis is based on half of the patients. For this design, the meta-analytic interim results are conclusive and would justify approval. In contrast, based on the phase III data only, interim results are inconclusive and would require further evidence.   To accelerate drug development for orphan diseases, innovative study designs and appropriate methodology are needed. Taking advantage of randomized phase II data when analyzing phase III studies looks promising because the evidence from phase II supports informed decision making. The implementation of the Bayesian design is straightforward.|http://arxiv.org/abs/1609.03367v1|Simon Wandel,Beat Neuenschwander,Tim Friede,Christian Rver
1465|On estimands and the analysis of adverse events in the presence of varying follow-up times within the benefit assessment of therapies|The analysis of adverse events (AEs) is a key component in the assessment of a drug's safety profile. Inappropriate analysis methods may result in misleading conclusions about a therapy's safety and consequently its benefit-risk ratio. The statistical analysis of AEs is complicated by the fact that the follow-up times can vary between the patients included in a clinical trial. This paper takes as its focus the analysis of AE data in the presence of varying follow-up times within the benefit assessment of therapeutic interventions. Instead of approaching this issue directly and solely from an analysis point of view, we first discuss what should be estimated in the context of safety data, leading to the concept of estimands. Although the current discussion on estimands is mainly related to efficacy evaluation, the concept is applicable to safety endpoints as well. Within the framework of estimands, we present statistical methods for analysing AEs with the focus being on the time to the occurrence of the first AE of a specific type. We give recommendations which estimators should be used for the estimands described. Furthermore, we state practical implications of the analysis of AEs in clinical trials and give an overview of examples across different indications. We also provide a review of current practices of health technology assessment (HTA) agencies with respect to the evaluation of safety data. Finally, we describe problems with meta-analyses of AE data and sketch possible solutions.|http://arxiv.org/abs/1805.01834v2|Steffen Unkel,Marjan Amiri,Norbert Benda,Jan Beyersmann,Dietrich Knoerzer,Katrin Kupas,Frank Langer,Friedhelm Leverkus,Anja Loos,Claudia Ose,Tanja Proctor,Claudia Schmoor,Carsten Schwenke,Guido Skipka,Kristina Unnebrink,Florian Voss,Tim Friede
1466|The side effect profile of Clozapine in real world data of three large mental hospitals|Objective: Mining the data contained within Electronic Health Records (EHRs) can potentially generate a greater understanding of medication effects in the real world, complementing what we know from Randomised control trials (RCTs). We Propose a text mining approach to detect adverse events and medication episodes from the clinical text to enhance our understanding of adverse effects related to Clozapine, the most effective antipsychotic drug for the management of treatment-resistant schizophrenia, but underutilised due to concerns over its side effects. Material and Methods: We used data from de-identified EHRs of three mental health trusts in the UK (>50 million documents, over 500,000 patients, 2835 of which were prescribed Clozapine). We explored the prevalence of 33 adverse effects by age, gender, ethnicity, smoking status and admission type three months before and after the patients started Clozapine treatment. We compared the prevalence of adverse effects with those reported in the Side Effects Resource (SIDER) where possible. Results: Sedation, fatigue, agitation, dizziness, hypersalivation, weight gain, tachycardia, headache, constipation and confusion were amongst the highest recorded Clozapine adverse effect in the three months following the start of treatment. Higher percentages of all adverse effects were found in the first month of Clozapine therapy. Using a significance level of (p< 0.05) out chi-square tests show a significant association between most of the ADRs in smoking status and hospital admissions and some in gender and age groups. Further, the data was combined from three trusts, and chi-square tests were applied to estimate the average effect of ADRs in each monthly interval. Conclusion: A better understanding of how the drug works in the real world can complement clinical trials and precision medicine.|http://arxiv.org/abs/2001.09698v1|Ehtesham Iqbal,Risha Govind,Alvin Romero,Olubanke Dzahini,Matthew Broadbent,Robert Stewart,Tanya Smith,Chi-Hun Kim,Nomi Werbeloff,Richard Dobson,Zina Ibrahim
1467|A Machine Learning alternative to placebo-controlled clinical trials upon new diseases: A primer|The appearance of a new dangerous and contagious disease requires the development of a drug therapy faster than what is foreseen by usual mechanisms. Many drug therapy developments consist in investigating through different clinical trials the effects of different specific drug combinations by delivering it into a test group of ill patients, meanwhile a placebo treatment is delivered to the remaining ill patients, known as the control group. We compare the above technique to a new technique in which all patients receive a different and reasonable combination of drugs and use this outcome to feed a Neural Network. By averaging out fluctuations and recognizing different patient features, the Neural Network learns the pattern that connects the patients initial state to the outcome of the treatments and therefore can predict the best drug therapy better than the above method. In contrast to many available works, we do not study any detail of drugs composition nor interaction, but instead pose and solve the problem from a phenomenological point of view, which allows us to compare both methods. Although the conclusion is reached through mathematical modeling and is stable upon any reasonable model, this is a proof-of-concept that should be studied within other expertises before confronting a real scenario. All calculations, tools and scripts have been made open source for the community to test, modify or expand it. Finally it should be mentioned that, although the results presented here are in the context of a new disease in medical sciences, these are useful for any field that requires a experimental technique with a control group.|http://arxiv.org/abs/2003.12454v1|Ezequiel Alvarez,Federico Lamagna,Manuel Szewc
1468|3D Augmented Reality-Assisted CT-Guided Interventions: System Design and Preclinical Trial on an Abdominal Phantom using HoloLens 2|Background: Out-of-plane lesions pose challenges for CT-guided interventions. Augmented reality (AR) headset devices have evolved and are readily capable to provide virtual 3D guidance to improve CT-guided targeting.   Purpose: To describe the design of a three-dimensional (3D) AR-assisted navigation system using HoloLens 2 and evaluate its performance through CT-guided simulations.   Materials and Methods: A prospective trial was performed assessing CT-guided needle targeting on an abdominal phantom with and without AR guidance. A total of 8 operators with varying clinical experience were enrolled and performed a total of 86 needle passes. Procedure efficiency, radiation dose, and complication rates were compared with and without AR guidance. Vector analysis of the first needle pass was also performed.   Results: Average total number of needle passes to reach the target reduced from 7.4 passes without AR to 3.4 passes with AR (54.2% decrease, p=0.011). Average dose-length product (DLP) decreased from 538 mGy-cm without AR to 318 mGy-cm with AR (41.0% decrease, p=0.009). Complication rate of hitting a non-targeted lesion decreased from 11.9% without AR (7/59 needle passes) to 0% with AR (0/27 needle passes). First needle passes were more nearly aligned with the ideal target trajectory with AR versus without AR (4.6{\deg} vs 8.0{\deg} offset, respectively, p=0.018). Medical students, residents, and attendings all performed at the same level with AR guidance.   Conclusions: 3D AR guidance can provide significant improvements in procedural efficiency and radiation dose savings for targeting challenging, out-of-plane lesions. AR guidance elevated the performance of all operators to the same level irrespective of prior clinical experience.|http://arxiv.org/abs/2005.09146v1|Brian J. Park,Stephen J. Hunt,Gregory J. Nadolski,Terence P. Gade
1469|Interoception Underlies The Therapeutic Effects of Mindfulness Meditation for Post-Traumatic Stress Disorder: A Randomized Clinical Trial|Mindfulness-based interventions have proven its efficacy in treating post-traumatic stress disorder (PTSD), but the underlying neurobiological mechanism is unknown. To determine the neurobiological mechanism of action of mindfulness-based stress reduction (MBSR) treating PTSD, we conducted a randomized clinical trial (RCT) in which 98 veterans with PTSD were randomly assigned to receive MBSR therapy (n = 47) or present-centered group therapy (PCGT; n = 51; an active-control condition). Pre- and post-intervention measures of PTSD symptom severity (PTSD Checklist) and brain activity measures of electroencephalography (EEG) were assessed, including spectral power of spontaneous neural oscillatory activities during resting and meditation periods, time-frequency (TF) power of cognitive task-related brain responses, and TF power of heartbeat-evoked brain responses (HEBR) that reflect cardiac interoceptive brain responses during resting and meditation. Compared to controls, the MBSR group had greater improvements in PTSD symptoms, spontaneous EEG alpha (8-13 Hz) power in posterior sites, task-related frontal theta power (4-7 Hz in 140-220 ms post-stimulus), and frontal theta HEBR (3-5 Hz and 265-328 ms post-R-peak). Latent difference score modeling found that only the changes in the frontal theta HEBR mediated the MBSR treatment effect. Brain source-level analysis found that the theta HEBR changes in the anterior cingulate cortex, anterior insular cortex, and the lateral prefrontal cortex predicted PTSD symptom improvements. These results indicated that mindfulness meditation improves spontaneous brain activities reflecting internally oriented relaxation and brain functions of attentional control. However, interoceptive brain capacity enhanced by MBSR appears to be the primary cerebral mechanism that regulates emotional disturbances and improves anxiety symptoms of PTSD.|http://arxiv.org/abs/2010.06078v1|Seung Suk Kang,Ph. D.,Scott R. Sponheim,Ph. D.,Kelvin O. Lim,M. D
1470|Investigating ADR mechanisms with knowledge graph mining and explainable AI|Adverse Drug Reactions (ADRs) are characterized within randomized clinical trials and postmarketing pharmacovigilance, but their molecular mechanism remains unknown in most cases. Aside from clinical trials, many elements of knowledge about drug ingredients are available in open-access knowledge graphs. In addition, drug classifications that label drugs as either causative or not for several ADRs, have been established. We propose to mine knowledge graphs for identifying biomolecular features that may enable reproducing automatically expert classifications that distinguish drug causative or not for a given type of ADR. In an explainable AI perspective, we explore simple classification techniques such as Decision Trees and Classification Rules because they provide human-readable models, which explain the classification itself, but may also provide elements of explanation for molecular mechanisms behind ADRs. In summary, we mine a knowledge graph for features; we train classifiers at distinguishing, drugs associated or not with ADRs; we isolate features that are both efficient in reproducing expert classifications and interpretable by experts (i.e., Gene Ontology terms, drug targets, or pathway names); and we manually evaluate how they may be explanatory. Extracted features reproduce with a good fidelity classifications of drugs causative or not for DILI and SCAR. Experts fully agreed that 73% and 38% of the most discriminative features are possibly explanatory for DILI and SCAR, respectively; and partially agreed (2/3) for 90% and 77% of them. Knowledge graphs provide diverse features to enable simple and explainable models to distinguish between drugs that are causative or not for ADRs. In addition to explaining classifications, most discriminative features appear to be good candidates for investigating ADR mechanisms further.|http://arxiv.org/abs/2012.09077v1|Emmanuel Bresso,Pierre Monnin,Cdric Bousquet,Franois-Elie Calvier,Ndeye-Coumba Ndiaye,Nadine Petitpain,Malika Smal-Tabbone,Adrien Coulet
1471|An in silico drug repurposing pipeline to identify drugs with the potential to inhibit SARS-CoV-2 replication|Drug repurposing provides an opportunity to redeploy drugs, which ideally are already approved for use in humans, for the treatment of other diseases. For example, the repurposing of dexamethasone and baricitinib has played a crucial role in saving patient lives during the ongoing SARS-CoV-2 pandemic. There remains a need to expand therapeutic approaches to prevent life-threatening complications in patients with COVID-19. Using an in silico approach based on structural similarity to drugs already in clinical trials for COVID-19, potential drugs were predicted for repurposing. For a subset of identified drugs with different targets to their corresponding COVID-19 clinical trial drug, a mechanism of action analysis was applied to establish whether they might have a role in inhibiting the replication of SARS-CoV-2. Of sixty drugs predicted in this study, two with the potential to inhibit SARS-CoV-2 replication were identified using mechanism of action analysis. Triamcinolone is a corticosteroid that is structurally similar to dexamethasone; gallopamil is a calcium channel blocker that is structurally similar to verapamil. In silico approaches indicate possible mechanisms of action for both drugs in inhibiting SARS-CoV-2 replication. The identification of these drugs as potentially useful for patients with COVID-19 who are at a higher risk of developing severe disease supports the use of in silico approaches to facilitate quick and cost-effective drug repurposing. Such drugs could expand the number of treatments available to patients who are not protected by vaccination.|http://arxiv.org/abs/2107.02905v2|Mabh MacMahon,Woochang Hwang,Soorin Yim,Eoghan MacMahon,Alexandre Abraham,Justin Barton,Mukunthan Tharmakulasingam,Paul Bilokon,Vasanthi Priyadarshini Gaddi,Namshik Han
1472|Evaluating causal effects on time-to-event outcomes in an RCT in Oncology with treatment discontinuation|In clinical trials, patients may discontinue treatments prematurely, breaking the initial randomization and, thus, challenging inference. Stakeholders in drug development are generally interested in going beyond the Intention-To-Treat (ITT) analysis, which provides valid causal estimates of the effect of treatment assignment but does not inform on the effect of the actual treatment receipt. Our study is motivated by an RCT in oncology, where patients assigned the investigational treatment may discontinue it due to adverse events. We propose adopting a principal stratum strategy and decomposing the overall ITT effect into principal causal effects for groups of patients defined by their potential discontinuation behavior. We first show how to implement a principal stratum strategy to assess causal effects on a survival outcome in the presence of continuous time treatment discontinuation, its advantages, and the conclusions one can draw. Our strategy deals with the time-to-event intermediate variable that may not be defined for patients who would not discontinue; moreover, discontinuation time and the primary endpoint are subject to censoring. We employ a flexible model-based Bayesian approach to tackle these complexities, providing easily interpretable results. We apply this Bayesian principal stratification framework to analyze synthetic data of the motivating oncology trial. We simulate data under different assumptions that reflect real scenarios where patients' behavior depends on critical baseline covariates. Supported by a simulation study, we shed light on the role of covariates in this framework: beyond making structural and parametric assumptions more credible, they lead to more precise inference and can be used to characterize patients' discontinuation behavior, which could help inform clinical practice and future protocols.|http://arxiv.org/abs/2310.06653v2|Veronica Ballerini,Bjrn Bornkamp,Alessandra Mattei,Fabrizia Mealli,Craig Wang,Yufen Zhang
1473|Bayesian modelling of response to therapy and drug-sensitivity in acute lymphoblastic leukemia|Acute lymphoblastic leukemia (ALL) is a heterogeneous hematologic malignancy involving the abnormal proliferation of immature lymphocytes, accounting for most pediatric cancer cases. ALL management in children has seen great improvement in the last decades thanks to better understanding of the disease leading to improved treatment strategies evidenced through clinical trials. Commonly a first course of chemotherapy (induction phase) is administered, followed by treatment with a combination of anti-leukemia drugs. A measure of the efficacy early in the course of therapy is minimal residual disease (MRD). MRD quantifies residual tumor cells and indicates the effectiveness of the treatment over the course of therapy. MRD positivity is defined for values of MRD greater than 0.01%, yielding left-censored observations. We propose a Bayesian model to study the relationship between patient features and MRD observed at two time points during the induction phase. Specifically, we model the observed MRD values via an auto-regressive model, accounting for left-censoring of the data and for the fact that some patients are already in remission after the induction phase. Patient characteristics are included in the model via linear regression terms. In particular, patient-specific drug sensitivity based on ex-vivo assays of patient samples is exploited to identify groups of subjects with similar profiles. We include this information as a covariate in the model for MRD. We adopt horseshoe priors for the regression coefficients to perform variable selection to identify important covariates. We fit the proposed approach to data from three prospective pediatric ALL clinical trials carried out at the St. Jude Children's Research Hospital. Our results highlight that drug sensitivity profiles and leukemic subtypes play an important role in the response to induction therapy as measured by serial MRD measures.|http://arxiv.org/abs/2311.04408v1|Andrea Cremaschi,Wenjian Yang,Maria De Iorio,William E. Evans,Jun J. Yang,Gary L. Rosner
1474|Characterizing the limits of human stability during motion: perturbative experiment validates a model-based approach for the Sit-to-Stand task|Falls affect a growing number of the population each year. Clinical methods to identify those at greatest risk for falls usually evaluate individuals while they perform specific motions such as balancing or Sit-to-Stand (STS). Unfortunately these techniques have been shown to have poor predictive power and are unable to identify the magnitude, direction, and timing of perturbations that can cause an individual to lose stability during motion. To address this limitation, the recently proposed Stability Basin (SB) aims to characterize the set of perturbations that will cause an individual to fall under a specific motor control strategy. The SB is defined as the set of configurations that do not lead to failure for an individual under their chosen control strategy. This paper presents a novel method to compute the SB and the first experimental validation of the SB with an 11-person perturbative STS experiment involving forwards or backwards pulls from a motor-driven cable. The individually-constructed SBs are used to identify when a trial fails, i.e., when an individual must switch control strategies (indicated by a step or sit) to recover from a perturbation. The constructed SBs correctly predict the outcome of trials where failure was observed with over 90% accuracy, and correctly predict the outcome of successful trials with over 95% accuracy. The SB was compared to three other methods and was found to estimate the stable region with over 45% more accuracy in all cases. This study demonstrates that SBs offer a novel model-based approach for quantifying stability during motion, which could be used in physical therapy for individuals at risk of falling.|http://arxiv.org/abs/1908.01876v1|Patrick D. Holmes,Shannon M. Danforth,Xiao-Yu Fu,Talia Y. Moore,Ram Vasudevan
1475|Constructing Artificial Data for Fine-tuning for Low-Resource Biomedical Text Tagging with Applications in PICO Annotation|Biomedical text tagging systems are plagued by the dearth of labeled training data. There have been recent attempts at using pre-trained encoders to deal with this issue. Pre-trained encoder provides representation of the input text which is then fed to task-specific layers for classification. The entire network is fine-tuned on the labeled data from the target task. Unfortunately, a low-resource biomedical task often has too few labeled instances for satisfactory fine-tuning. Also, if the label space is large, it contains few or no labeled instances for majority of the labels. Most biomedical tagging systems treat labels as indexes, ignoring the fact that these labels are often concepts expressed in natural language e.g. `Appearance of lesion on brain imaging'. To address these issues, we propose constructing extra labeled instances using label-text (i.e. label's name) as input for the corresponding label-index (i.e. label's index). In fact, we propose a number of strategies for manufacturing multiple artificial labeled instances from a single label. The network is then fine-tuned on a combination of real and these newly constructed artificial labeled instances. We evaluate the proposed approach on an important low-resource biomedical task called \textit{PICO annotation}, which requires tagging raw text describing clinical trials with labels corresponding to different aspects of the trial i.e. PICO (Population, Intervention/Control, Outcome) characteristics of the trial. Our empirical results show that the proposed method achieves a new state-of-the-art performance for PICO annotation with very significant improvements over competitive baselines.|http://arxiv.org/abs/1910.09255v3|Gaurav Singh,Zahra Sabet,John Shawe-Taylor,James Thomas
1476|Multiple Imputation Approaches for Epoch-level Accelerometer data in Trials|Clinical trials that investigate interventions on physical activity often use accelerometers to measure step count at a very granular level, often in 5-second epochs. Participants typically wear the accelerometer for a week-long period at baseline, and for one or more week-long follow-up periods after the intervention. The data is usually aggregated to provide daily or weekly step counts for the primary analysis. Missing data are common as participants may not wear the device as per protocol. Approaches to handling missing data in the literature have largely defined missingness on the day level using a threshold on daily wear time, which leads to loss of information on the time of day when data are missing. We propose an approach to identifying and classifying missingness at the finer epoch-level, and then present two approaches to handling missingness. Firstly, we present a parametric approach which takes into account the number of missing epochs per day. Secondly, we describe a non-parametric approach to Multiple Imputation (MI) where missing periods during the day are replaced by donor data from the same person where possible, or data from a different person who is matched on demographic and physical activity-related variables. Our simulation studies comparing these approaches in a number of settings show that the non-parametric approach leads to estimates of the effect of treatment that are least biased while maintaining small standard errors. We illustrate the application of these different MI strategies to the analysis of the 2017 PACE-UP Trial. The proposed framework of classifying missingness and applying MI at the epoch-level is likely to be applicable to a number of different outcomes and data from other wearable devices.|http://arxiv.org/abs/2303.17331v1|Mia S. Tackney,Elizabeth Williamson,Derek G. Cook,Elizabeth Limb,Tess Harris,James Carpenter
1477|Do machine learning methods lead to similar individualized treatment rules? A comparison study on real data|Identifying patients who benefit from a treatment is a key aspect of personalized medicine, which allows the development of individualized treatment rules (ITRs). Many machine learning methods have been proposed to create such rules. However, to what extent the methods lead to similar ITRs, i.e., recommending the same treatment for the same individuals is unclear. In this work, we compared 22 of the most common approaches in two randomized control trials. Two classes of methods can be distinguished. The first class of methods relies on predicting individualized treatment effects from which an ITR is derived by recommending the treatment evaluated to the individuals with a predicted benefit. In the second class, methods directly estimate the ITR without estimating individualized treatment effects. For each trial, the performance of ITRs was assessed by various metrics, and the pairwise agreement between all ITRs was also calculated. Results showed that the ITRs obtained via the different methods generally had considerable disagreements regarding the patients to be treated. A better concordance was found among akin methods. Overall, when evaluating the performance of ITRs in a validation sample, all methods produced ITRs with limited performance, suggesting a high potential for optimism. For non-parametric methods, this optimism was likely due to overfitting. The different methods do not lead to similar ITRs and are therefore not interchangeable. The choice of the method strongly influences for which patients a certain treatment is recommended, drawing some concerns about their practical use.|http://arxiv.org/abs/2308.03398v2|Florie Bouvier,Etienne Peyrot,Alan Balendran,Corentin Sgalas,Ian Roberts,Franois Petit,Raphal Porcher
1478|Comparison of reconstruction algorithms for digital breast tomosynthesis|Digital breast tomosynthesis (DBT) is an emerging modality for breast imaging. A typical tomosynthesis image is reconstructed from projection data acquired at a limited number of views over a limited angular range. In general, the quantitative accuracy of the image can be significantly compromised by severe artifacts and non-isotropic resolution resulting from the incomplete data. Nevertheless, it has been demonstrated that DBT may yield useful information for detection/classification tasks and thus is considered a promising breast imaging modality currently undergoing pre-clinical evaluation trials. The purpose of this work is to conduct a preliminary, but systematic, investigation and evaluation of the properties of reconstruction algorithms that have been proposed for DBT. We use a breast phantom designed for DBT evaluation to generate analytic projection data for a typical DBT configuration, which is currently undergoing pre-clinical evaluation. The reconstruction algorithms under comparison include (i) filtered backprojection (FBP), (ii) expectation maximization (EM), and (iii) TV-minimization algorithms. Results of our studies indicate that FBP reconstructed images are generally noisier and demonstrate lower in-depth resolution than those obtained through iterative reconstruction and that the TV-minimization reconstruction yield images with reduced artifacts as compared to that obtained with other algorithms under study.|http://arxiv.org/abs/0908.2610v1|I. Reiser,J. Bian,R. M. Nishikawa,E. Y. Sidky,X. Pan
1479|A Partitioning Deletion/Substitution/Addition Algorithm for Creating Survival Risk Groups|Accurately assessing a patient's risk of a given event is essential in making informed treatment decisions. One approach is to stratify patients into two or more distinct risk groups with respect to a specific outcome using both clinical and demographic variables. Outcomes may be categorical or continuous in nature; important examples in cancer studies might include level of toxicity or time to recurrence. Recursive partitioning methods are ideal for building such risk groups. Two such methods are Classification and Regression Trees (CART) and a more recent competitor known as the partitioning Deletion/Substitution/Addition (partDSA) algorithm, both which also utilize loss functions (e.g. squared error for a continuous outcome) as the basis for building, selecting and assessing predictors but differ in the manner by which regression trees are constructed.   Recently, we have shown that partDSA often outperforms CART in so-called "full data" (e.g., uncensored) settings. However, when confronted with censored outcome data, the loss functions used by both procedures must be modified. There have been several attempts to adapt CART for right-censored data. This article describes two such extensions for \emph{partDSA} that make use of observed data (i.e. possibly censored) loss functions. These observed data loss functions, constructed using inverse probability of censoring weights, are consistent estimates of their uncensored counterparts provided that the corresponding censoring model is correctly specified. The relative performance of these new methods is evaluated via simulation studies and illustrated through an analysis of clinical trial data on brain cancer patients.|http://arxiv.org/abs/1101.4331v2|Karen Lostritto,Robert Strawderman,Annette Molinaro
1480|ggRandomForests: Exploring Random Forest Survival|Random forest (Leo Breiman 2001a) (RF) is a non-parametric statistical method requiring no distributional assumptions on covariate relation to the response. RF is a robust, nonlinear technique that optimizes predictive accuracy by fitting an ensemble of trees to stabilize model estimates. Random survival forests (RSF) (Ishwaran and Kogalur 2007; Ishwaran et al. 2008) are an extension of Breimans RF techniques allowing efficient nonparametric analysis of time to event data. The randomForestSRC package (Ishwaran and Kogalur 2014) is a unified treatment of Breimans random forest for survival, regression and classification problems.   Predictive accuracy makes RF an attractive alternative to parametric models, though complexity and interpretability of the forest hinder wider application of the method. We introduce the ggRandomForests package, tools for visually understand random forest models grown in R (R Core Team 2014) with the randomForestSRC package. The ggRandomForests package is structured to extract intermediate data objects from randomForestSRC objects and generate figures using the ggplot2 (Wickham 2009) graphics package.   This document is structured as a tutorial for building random forest for survival with the randomForestSRC package and using the ggRandomForests package for investigating how the forest is constructed. We analyse the Primary Biliary Cirrhosis of the liver data from a clinical trial at the Mayo Clinic (Fleming and Harrington 1991). Our aim is to demonstrate the strength of using Random Forest methods for both prediction and information retrieval, specifically in time to event data settings.|http://arxiv.org/abs/1612.08974v1|John Ehrlinger
1481|A machine learning approach to drug repositioning based on drug expression profiles: Applications to schizophrenia and depression/anxiety disorders|Development of new medications is a very lengthy and costly process. Finding novel indications for existing drugs, or drug repositioning, can serve as a useful strategy to shorten the development cycle. In this study, we present an approach to drug discovery or repositioning by predicting indication for a particular disease based on expression profiles of drugs, with a focus on applications in psychiatry. Drugs that are not originally indicated for the disease but with high predicted probabilities serve as good candidates for repurposing. This framework is widely applicable to any chemicals or drugs with expression profiles measured, even if the drug targets are unknown. It is also highly flexible as virtually any supervised learning algorithms can be used. We applied this approach to identify repositioning opportunities for schizophrenia as well as depression and anxiety disorders. We applied various state-of-the-art machine learning (ML) approaches for prediction, including deep neural networks, support vector machines (SVM), elastic net, random forest and gradient boosted machines. The performance of the five approaches did not differ substantially, with SVM slightly outperformed the others. However, methods with lower predictive accuracy can still reveal literature-supported candidates that are of different mechanisms of actions. As a further validation, we showed that the repositioning hits are enriched for psychiatric medications considered in clinical trials. Notably, many top repositioning hits are supported by previous preclinical or clinical studies. Finally, we propose that ML approaches may provide a new avenue to explore drug mechanisms via examining the variable importance of gene features.|http://arxiv.org/abs/1706.03014v2|Kai Zhao,Hon-Cheong So
1482|Genetic variation in human drug-related genes|Variability in drug efficacy and adverse effects are observed in clinical practice. While the extent of genetic variability in classical pharmacokinetic genes is rather well understood, the role of genetic variation in drug targets is typically less studied. Based on 60,706 human exomes from the ExAC dataset, we performed an in-depth computational analysis of the prevalence of functional-variants in in 806 drug-related genes, including 628 known drug targets. We find that most genetic variants in these genes are very rare (f < 0.1%) and thus likely not observed in clinical trials. Overall, however, four in five patients are likely to carry a functional-variant in a target for commonly prescribed drugs and many of these might alter drug efficacy. We further computed the likelihood of 1,236 FDA approved drugs to be affected by functional-variants in their targets and show that the patient-risk varies for many drugs with respect to geographic ancestry. A focused analysis of oncological drug targets indicates that the probability of a patient carrying germline variants in oncological drug targets is with 44% high enough to suggest that not only somatic alterations, but also germline variants carried over into the tumor genome should be included in therapeutic decision-making.|http://arxiv.org/abs/1706.08238v1|Charlotta P. I. Schrfe,Roman Tremmel,Matthias Schwab,Oliver Kohlbacher,Debora S. Marks
1483|Using marginal structural models to adjust for treatment drop-in when developing clinical prediction models|Objectives: Clinical prediction models (CPMs) can inform decision-making concerning treatment initiation. Here, one requires predicted risks assuming that no treatment is given. This is challenging since CPMs are often derived in datasets where patients receive treatment; moreover, treatment can commence post-baseline - treatment drop-ins. This study presents a novel approach of using marginal structural models (MSMs) to adjust for treatment drop-in.   Study Design and Setting: We illustrate the use of MSMs in the CPM framework through simulation studies, representing randomised controlled trials and observational data. The simulations include a binary treatment and a covariate, each recorded at two timepoints and having a prognostic effect on a binary outcome. The bias in predicted risk was examined in a model ignoring treatment, a model fitted on treatment na\"ive patients (at baseline), a model including baseline treatment, and the MSM.   Results: In all simulation scenarios, all models except the MSM under-estimated the risk of outcome given absence of treatment. Consequently, CPMs that do not acknowledge treatment drop-in can lead to under-allocation of treatment.   Conclusion: When developing CPMs to predict treatment-na\"ive risk, authors should consider using MSMs to adjust for treatment drop-in. MSMs also allow estimation of individual treatment effects.|http://arxiv.org/abs/1709.06859v1|Matthew Sperrin,Glen Martin,Tjeerd Van Staa,Niels Peek,Iain Buchan
1484|A Bayesian Mark Interaction Model for Analysis of Tumor Pathology Images|With the advance of imaging technology, digital pathology imaging of tumor tissue slides is becoming a routine clinical procedure for cancer diagnosis. This process produces massive imaging data that capture histological details in high resolution. Recent developments in deep-learning methods have enabled us to identify and classify individual cells from digital pathology images at large scale. The randomly distributed cells can be considered from a marked point process, where each point is defined by its position and cell type. Reliable statistical approaches to model such marked spatial point patterns can provide new insight into tumor progression and shed light on the biological mechanisms of cancer. In this paper, we consider the problem of modeling spatial correlations among three commonly seen cells (i.e. lymphocyte, stromal, and tumor) observed in tumor pathology images. A novel marking model of marked point processes, with interpretable underlying parameters (some of which are clinically meaningful), is proposed in a Bayesian framework. We use Markov chain Monte Carlo (MCMC) sampling techniques, combined with the double Metropolis-Hastings (DMH) algorithm, to sample from the posterior distribution with an intractable normalizing constant. On the benchmark datasets, we demonstrate how this model-based analysis can lead to sharper inferences than ordinary exploratory analyses. Lastly, we conduct a case study on the pathology images of 188 lung cancer patients from the National Lung Screening Trial. The results show that the spatial correlation between tumor and stromal cells predicts patient prognosis. This statistical methodology not only presents a new model for characterizing spatial correlations in a multi-type spatial point pattern, but also provides a new perspective for understanding the role of cell-cell interactions in cancer progression.|http://arxiv.org/abs/1802.08308v1|Qiwei Li,Xinlei Wang,Faming Liang,Guanghua Xiao
1485|Generating retinal flow maps from structural optical coherence tomography with artificial intelligence|Despite significant advances in artificial intelligence (AI) for computer vision, its application in medical imaging has been limited by the burden and limits of expert-generated labels. We used images from optical coherence tomography angiography (OCTA), a relatively new imaging modality that measures perfusion of the retinal vasculature, to train an AI algorithm to generate vasculature maps from standard structural optical coherence tomography (OCT) images of the same retinae, both exceeding the ability and bypassing the need for expert labeling. Deep learning was able to infer perfusion of microvasculature from structural OCT images with similar fidelity to OCTA and significantly better than expert clinicians (P < 0.00001). OCTA suffers from need of specialized hardware, laborious acquisition protocols, and motion artifacts; whereas our model works directly from standard OCT which are ubiquitous and quick to obtain, and allows unlocking of large volumes of previously collected standard OCT data both in existing clinical trials and clinical practice. This finding demonstrates a novel application of AI to medical imaging, whereby subtle regularities between different modalities are used to image the same body part and AI is used to generate detailed and accurate inferences of tissue function from structure imaging.|http://arxiv.org/abs/1802.08925v1|Cecilia S. Lee,Ariel J. Tyring,Yue Wu,Sa Xiao,Ariel S. Rokem,Nicolaas P. Deruyter,Qinqin Zhang,Adnan Tufail,Ruikang K. Wang,Aaron Y. Lee
1486|Natural Language Processing for EHR-Based Computational Phenotyping|This article reviews recent advances in applying natural language processing (NLP) to Electronic Health Records (EHRs) for computational phenotyping. NLP-based computational phenotyping has numerous applications including diagnosis categorization, novel phenotype discovery, clinical trial screening, pharmacogenomics, drug-drug interaction (DDI) and adverse drug event (ADE) detection, as well as genome-wide and phenome-wide association studies. Significant progress has been made in algorithm development and resource construction for computational phenotyping. Among the surveyed methods, well-designed keyword search and rule-based systems often achieve good performance. However, the construction of keyword and rule lists requires significant manual effort, which is difficult to scale. Supervised machine learning models have been favored because they are capable of acquiring both classification patterns and structures from data. Recently, deep learning and unsupervised learning have received growing attention, with the former favored for its performance and the latter for its ability to find novel phenotypes. Integrating heterogeneous data sources have become increasingly important and have shown promise in improving model performance. Often better performance is achieved by combining multiple modalities of information. Despite these many advances, challenges and opportunities remain for NLP-based computational phenotyping, including better model interpretability and generalizability, and proper characterization of feature relations in clinical narratives|http://arxiv.org/abs/1806.04820v2|Zexian Zeng,Yu Deng,Xiaoyu Li,Tristan Naumann,Yuan Luo
1487|Quantifying and attenuating pathologic tremor in virtual reality|We present a virtual reality (VR) experience that creates a research-grade benchmark in assessing patients with active upper-limb tremor, while simultaneously offering the opportunity for patients to engage with VR experiences without their pathologic tremor. Accurate and precise use of handheld motion controllers in VR gaming applications may be limited for patients with upper limb tremor. In parallel, objective tools measuring tremor are not in widespread, routine clinical use. We used a commercially available VR system and designed a challenging virtual-balloon-popping test mimicking a common nose-to-target pointing task used by medical practitioners to subjectively evaluate tremor in the exam room. Within our VR experience, we offer a software mode which uses a low-pass filter to adjust hand position and pointing orientation over a series of past data points. This digital filter creates a smoothing function for hand movement which effectively removes the patient's tremor in the VR representation. While the patient completes trials of the reaching task, quantitative data on the pathologic tremor is digitally recorded. With speed, accuracy, and the tremor components computed across three axes of movement, patients can be evaluated for their tremor amplitudes in a quantitative, replicable, and enjoyable manner. Removal of tremor in digital space may allow patients having significant upper limb tremor to have both an objective clinical measurement of symptoms while providing patients positive feedback and interaction.|http://arxiv.org/abs/1809.05970v1|Brian A. Cohn,Dilan D. Shah,Ali Marjaninejad,Martin Shapiro,Serhan Ulkumen,Christopher M. Laine,Francisco J. Valero-Cuevas,Kenneth H. Hayashida,Sarah Ingersoll
1488|High-Throughput, Semi-Autonomous Measurement of Cavitation-Mediated Material Breakage|Engineered microbubbles can be acoustically driven to cavitate against a substrate to produce localized erosion and fragmentation. This mechanical action has therapeutic applications in the treatment of biomineralizations, such as in urinary stone disease. However, current methods for quantifying the mechanical action of cavitation on a substrate are slow or imprecise. In this paper, we describe the design of a device that applies calibrated pressures to microbubbles engineered to target a calcium-containing hydroxyapatite substrate under physiological conditions and quantifies the result via an automated submerged mass measurement with high precision and low drift. Measurements of microbubble-mediated mass loss were observed to be linear with time, with variance that was comparable to the resolution of the instrument. The rate of mass loss with microbubbles present was 5.5-fold greater than in the absence of microbubbles. This research instrument captures the essential mechanical and physiological features of in vivo microbubble-mediated erosion and fragmentation of urinary stones and has been used to optimize the parameters of this treatment in a clinical trial for a promising new approach to the treatment of nephrolithiasis. In addition to clinically relevant therapeutic applications, this approach will contribute to broader understanding of acoustic cavitation against a substrate.|http://arxiv.org/abs/1812.05576v1|David G. Bell,Matthew A. Hopcroft,William M. Behnke-Parks
1489|A deep learning model for early prediction of Alzheimer's disease dementia based on hippocampal MRI|Introduction: It is challenging at baseline to predict when and which individuals who meet criteria for mild cognitive impairment (MCI) will ultimately progress to Alzheimer's disease (AD) dementia. Methods: A deep learning method is developed and validated based on MRI scans of 2146 subjects (803 for training and 1343 for validation) to predict MCI subjects' progression to AD dementia in a time-to-event analysis setting. Results: The deep learning time-to-event model predicted individual subjects' progression to AD dementia with a concordance index (C-index) of 0.762 on 439 ADNI testing MCI subjects with follow-up duration from 6 to 78 months (quartiles: [24, 42, 54]) and a C-index of 0.781 on 40 AIBL testing MCI subjects with follow-up duration from 18-54 months (quartiles: [18, 36,54]). The predicted progression risk also clustered individual subjects into subgroups with significant differences in their progression time to AD dementia (p<0.0002). Improved performance for predicting progression to AD dementia (C-index=0.864) was obtained when the deep learning based progression risk was combined with baseline clinical measures. Conclusion: Our method provides a cost effective and accurate means for prognosis and potentially to facilitate enrollment in clinical trials with individuals likely to progress within a specific temporal period.|http://arxiv.org/abs/1904.07282v1|Hongming Li,Mohamad Habes,David A. Wolk,Yong Fan
1490|Landmark Proportional Subdistribution Hazards Models for Dynamic Prediction of Cumulative Incidence Functions|An individualized risk prediction model that dynamically updates the probability of a clinical event from a specific cause is valuable for physicians to be able to optimize personalized treatment strategies in real-time by incorporating all available information collected over the follow-up. However, this is more complex and challenging when competing risks are present, because it requires simultaneously updating the overall survival and the cumulative incidence functions (CIFs) while adjusting for the time-dependent covariates and time-varying covariate effects. In this study, we developed a landmark proportional subdistribution hazards (PSH) model and a more comprehensive supermodel by extending the landmark method to the Fine-Gray model. The performance of our models was assessed via simulations and through analysis of data from a multicenter clinical trial for breast cancer patients. Our proposed models have appealing advantages over other dynamic prediction models for data with competing risks. First, our models are robust against violations of the PSH assumption and can directly predict the conditional CIFs bypassing the estimation of overall survival and greatly simplify the prediction procedure. Second, our landmark PSH supermodel enables researchers to make predictions at a set of landmark points in one step. Third, the proposed models can easily incorporate various types of time-dependent information using existing standard software without computational burden.|http://arxiv.org/abs/1904.09002v1|Qing Liu,Gong Tang,Joseph P. Costantino,Chung-Chou H. Chang
1491|Radiopathomics: Integration of radiographic and histologic characteristics for prognostication in glioblastoma|Both radiographic (Rad) imaging, such as multi-parametric magnetic resonance imaging, and digital pathology (Path) images captured from tissue samples are currently acquired as standard clinical practice for glioblastoma tumors. Both these data streams have been separately used for diagnosis and treatment planning, despite the fact that they provide complementary information. In this research work, we aimed to assess the potential of both Rad and Path images in combination and comparison. An extensive set of engineered features was extracted from delineated tumor regions in Rad images, comprising T1, T1-Gd, T2, T2-FLAIR, and 100 random patches extracted from Path images. Specifically, the features comprised descriptors of intensity, histogram, and texture, mainly quantified via gray-level-co-occurrence matrix and gray-level-run-length matrices. Features extracted from images of 107 glioblastoma patients, downloaded from The Cancer Imaging Archive, were run through support vector machine for classification using leave-one-out cross-validation mechanism, and through support vector regression for prediction of continuous survival outcome. The Pearson correlation coefficient was estimated to be 0.75, 0.74, and 0.78 for Rad, Path and RadPath data. The area-under the receiver operating characteristic curve was estimated to be 0.74, 0.76 and 0.80 for Rad, Path and RadPath data, when patients were discretized into long- and short-survival groups based on average survival cutoff. Our results support the notion that synergistically using Rad and Path images may lead to better prognosis at the initial presentation of the disease, thereby facilitating the targeted enrollment of patients into clinical trials.|http://arxiv.org/abs/1909.07581v2|Saima Rathore,Muhammad A. Iftikhar,Metin N. Gurcan,Zissimos Mourelatos
1492|Old Drugs for Newly Emerging Viral Disease, COVID-19: Bioinformatic Prospective|Coronavirus (COVID-19) outbreak in late 2019 and 2020 comprises a serious and more likely a pandemic threat worldwide. Given that the disease has not approved vaccines or drugs up to now, any efforts for drug design and or clinical trails of old drugs based on their mechanism of action are worthy and creditable in such circumstances. Experienced docking experiments using the newly released coordinate structure for COVID-19 protease as a receptor and thoughtfully selected chemicals among antiviral and antibiotics drugs as ligands may be leading in this context. We selected nine drugs from HIV-1 protease inhibitors and twenty-one candidates from anti bronchitis drugs based on their chemical structures and enrolled them in blind and active site-directed dockings in different modes and in native-like conditions of interactions. Our findings suggest the binding capacity and the inhibitory potency of candidates are as follows Tipranavir>Indinavir>Atazanavir>Darunavir>Ritonavir>Amprenavir for HIV-1 protease inhibitors and Cefditoren>Cefixime>Erythromycin>Clarithromycin for anti bronchitis medicines. The drugs bioavailability, their hydrophobicity and the hydrophobic properties of their binding sites and also the rates of their metabolisms and deactivations in the human body are the next determinants for their overall effects on viral infections, the net results that should survey by clinical trials to assess their therapeutic usefulness for coronavirus infections.|http://arxiv.org/abs/2003.04524v1|Mohammad Reza Dayer
1493|A two-stage prediction model for heterogeneous effects of many treatment options: application to drugs for Multiple Sclerosis|Treatment effects vary across different patients and estimation of this variability is important for clinical decisions. The aim is to develop a model to estimate the benefit of alternative treatment options for individual patients. Hence, we developed a two-stage prediction model for heterogeneous treatment effects, by combining prognosis research and network meta-analysis methods when individual patient data is available. In a first stage, we develop a prognostic model and we predict the baseline risk of the outcome. In the second stage, we use this baseline risk score from the first stage as a single prognostic factor and effect modifier in a network meta-regression model. We apply the approach to a network meta-analysis of three randomized clinical trials comparing the relapse rate in Natalizumab, Glatiramer Acetate and Dimethyl Fumarate including 3590 patients diagnosed with relapsing-remitting multiple sclerosis. We find that the baseline risk score modifies the relative and absolute treatment effects. Several patient characteristics such as age and disability status impact on the baseline risk of relapse, and this in turn moderates the benefit that may be expected for each of the treatments. For high-risk patients, the treatment that minimizes the risk to relapse in two years is Natalizumab, whereas for low-risk patients Dimethyl Fumarate Fumarate might be a better option. Our approach can be easily extended to all outcomes of interest and has the potential to inform a personalised treatment approach.|http://arxiv.org/abs/2004.13464v3|Konstantina Chalkou,Ewout Steyerberg,Matthias Egger,Andrea Manca,Fabio Pellegrini,Georgia Salanti
1494|Learning Individualized Treatment Rules with Estimated Translated Inverse Propensity Score|Randomized controlled trials typically analyze the effectiveness of treatments with the goal of making treatment recommendations for patient subgroups. With the advance of electronic health records, a great variety of data has been collected in clinical practice, enabling the evaluation of treatments and treatment policies based on observational data. In this paper, we focus on learning individualized treatment rules (ITRs) to derive a treatment policy that is expected to generate a better outcome for an individual patient. In our framework, we cast ITRs learning as a contextual bandit problem and minimize the expected risk of the treatment policy. We conduct experiments with the proposed framework both in a simulation study and based on a real-world dataset. In the latter case, we apply our proposed method to learn the optimal ITRs for the administration of intravenous (IV) fluids and vasopressors (VP). Based on various offline evaluation methods, we could show that the policy derived in our framework demonstrates better performance compared to both the physicians and other baselines, including a simple treatment prediction approach. As a long-term goal, our derived policy might eventually lead to better clinical guidelines for the administration of IV and VP.|http://arxiv.org/abs/2007.01083v1|Zhiliang Wu,Yinchong Yang,Yunpu Ma,Yushan Liu,Rui Zhao,Michael Moor,Volker Tresp
1495|Disentangling brain heterogeneity via semi-supervised deep-learning and MRI: dimensional representations of Alzheimer's Disease|Heterogeneity of brain diseases is a challenge for precision diagnosis/prognosis. We describe and validate Smile-GAN (SeMI-supervised cLustEring-Generative Adversarial Network), a novel semi-supervised deep-clustering method, which dissects neuroanatomical heterogeneity, enabling identification of disease subtypes via their imaging signatures relative to controls. When applied to MRIs (2 studies; 2,832 participants; 8,146 scans) including cognitively normal individuals and those with cognitive impairment and dementia, Smile-GAN identified 4 neurodegenerative patterns/axes: P1, normal anatomy and highest cognitive performance; P2, mild/diffuse atrophy and more prominent executive dysfunction; P3, focal medial temporal atrophy and relatively greater memory impairment; P4, advanced neurodegeneration. Further application to longitudinal data revealed two distinct progression pathways: P1$\rightarrow$P2$\rightarrow$P4 and P1$\rightarrow$P3$\rightarrow$P4. Baseline expression of these patterns predicted the pathway and rate of future neurodegeneration. Pattern expression offered better yet complementary performance in predicting clinical progression, compared to amyloid/tau. These deep-learning derived biomarkers offer promise for precision diagnostics and targeted clinical trial recruitment.|http://arxiv.org/abs/2102.12582v1|Zhijian Yang,Ilya M. Nasrallah,Haochang Shou,Junhao Wen,Jimit Doshi,Mohamad Habes,Guray Erus,Ahmed Abdulkadir,Susan M. Resnick,David Wolk,Christos Davatzikos
1496|A network-based analysis of disease modules from a taxonomic perspective|Objective: Human-curated disease ontologies are widely used for diagnostic evaluation, treatment and data comparisons over time, and clinical decision support. The classification principles underlying these ontologies are guided by the analysis of observable pathological similarities between disorders, often based on anatomical or histological principles. Although, thanks to recent advances in molecular biology, disease ontologies are slowly changing to integrate the etiological and genetic origins of diseases, nosology still reflects this "reductionist" perspective. Proximity relationships of disease modules (hereafter DMs) in the human interactome network are now increasingly used in diagnostics, to identify pathobiologically similar diseases and to support drug repurposing and discovery. On the other hand, similarity relations induced from structural proximity of DMs also have several limitations, such as incomplete knowledge of disease-gene relationships and reliability of clinical trials to assess their validity. The purpose of the study described in this paper is to shed more light on disease similarities by analyzing the relationship between categorical proximity of diseases in human-curated ontologies and structural proximity of the related DM in the interactome. Method: We propose a methodology (and related algorithms) to automatically induce a hierarchical structure from proximity relations between DMs, and to compare this structure with a human-curated disease taxonomy. Results: We demonstrate that the proposed methodology allows to systematically analyze commonalities and differences among structural and categorical similarity of human diseases, help refine and extend human disease classification systems, and identify promising network areas where new disease-gene interactions can be discovered.|http://arxiv.org/abs/2104.00386v1|Giorgio Grani,Lorenzo Madeddu,Paola Velardi
1497|Cohort Bias Adaptation in Aggregated Datasets for Lesion Segmentation|Many automatic machine learning models developed for focal pathology (e.g. lesions, tumours) detection and segmentation perform well, but do not generalize as well to new patient cohorts, impeding their widespread adoption into real clinical contexts. One strategy to create a more diverse, generalizable training set is to naively pool datasets from different cohorts. Surprisingly, training on this \it{big data} does not necessarily increase, and may even reduce, overall performance and model generalizability, due to the existence of cohort biases that affect label distributions. In this paper, we propose a generalized affine conditioning framework to learn and account for cohort biases across multi-source datasets, which we call Source-Conditioned Instance Normalization (SCIN). Through extensive experimentation on three different, large scale, multi-scanner, multi-centre Multiple Sclerosis (MS) clinical trial MRI datasets, we show that our cohort bias adaptation method (1) improves performance of the network on pooled datasets relative to naively pooling datasets and (2) can quickly adapt to a new cohort by fine-tuning the instance normalization parameters, thus learning the new cohort bias with only 10 labelled samples.|http://arxiv.org/abs/2108.00713v2|Brennan Nichyporuk,Jillian Cardinell,Justin Szeto,Raghav Mehta,Sotirios Tsaftaris,Douglas L. Arnold,Tal Arbel
1498|Personalized Prediction of Future Lesion Activity and Treatment Effect in Multiple Sclerosis from Baseline MRI|Precision medicine for chronic diseases such as multiple sclerosis (MS) involves choosing a treatment which best balances efficacy and side effects/preferences for individual patients. Making this choice as early as possible is important, as delays in finding an effective therapy can lead to irreversible disability accrual. To this end, we present the first deep neural network model for individualized treatment decisions from baseline magnetic resonance imaging (MRI) (with clinical information if available) for MS patients. Our model (a) predicts future new and enlarging T2 weighted (NE-T2) lesion counts on follow-up MRI on multiple treatments and (b) estimates the conditional average treatment effect (CATE), as defined by the predicted future suppression of NE-T2 lesions, between different treatment options relative to placebo. Our model is validated on a proprietary federated dataset of 1817 multi-sequence MRIs acquired from MS patients during four multi-centre randomized clinical trials. Our framework achieves high average precision in the binarized regression of future NE-T2 lesions on five different treatments, identifies heterogeneous treatment effects, and provides a personalized treatment recommendation that accounts for treatment-associated risk (e.g. side effects, patient preference, administration difficulties).|http://arxiv.org/abs/2204.01702v4|Joshua Durso-Finley,Jean-Pierre R. Falet,Brennan Nichyporuk,Douglas L. Arnold,Tal Arbel
1499|Comparison of Baseline Covariate Adjustment Methods for Restricted Mean Survival Time|The restricted mean survival time is a clinically easy-to-interpret measure that does not require any assumption of proportional hazards. We focus on two ways to directly model the survival time and adjust the covariates. One is to calculate the pseudo-survival time for each subject using leave-one-out, and then perform a model analysis using all pseudo-values to adjust for covariates. The pseudo-survival time is used to reflect information of censored subjects in the model analysis. The other method adjusts for covariates using subjects for whom the time-to-event was observed while adjusting for the censored subjects using the inverse probability of censoring weighting (IPCW). This paper evaluates the performance of these two methods in terms of the power to detect group differences through a simple example dataset and computer simulations. The simple example illustrates the intuitive behavior of the two methods. With the method using pseudo-survival times, it is difficult to interpret the pseudo-values. We confirm that the pseudo-survival times are different from the actual data obtained in a primary biliary cholangitis clinical trial because of the many censored data. In the simulations, the method using IPCW is found to be more powerful. Even in the case of group differences with respect to the censor incidence rates and covariates, the method using IPCW maintains a nominal significance level for the type-1 error rate. We conclude that the IPCW method should be used to estimate the restricted mean survival time when adjusting the covariates.|http://arxiv.org/abs/2211.00784v1|Keisuke Hanada,Junji Moriya,Masahiro Kojima
1500|Harnessing electronic health records for real-world evidence|While randomized controlled trials (RCTs) are the gold-standard for establishing the efficacy and safety of a medical treatment, real-world evidence (RWE) generated from real-world data (RWD) has been vital in post-approval monitoring and is being promoted for the regulatory process of experimental therapies. An emerging source of RWD is electronic health records (EHRs), which contain detailed information on patient care in both structured (e. g., diagnosis codes) and unstructured (e. g., clinical notes, images) form. Despite the granularity of the data available in EHRs, critical variables required to reliably assess the relationship between a treatment and clinical outcome can be challenging to extract. We provide an integrated data curation and modeling pipeline leveraging recent advances in natural language processing, computational phenotyping, modeling techniques with noisy data to address this fundamental challenge and accelerate the reliable use of EHRs for RWE, as well as the creation of digital twins. The proposed pipeline is highly automated for the task and includes guidance for deployment. Examples are also drawn from existing literature on EHR emulation of RCT and accompanied by our own studies with Mass General Brigham (MGB) EHR.|http://arxiv.org/abs/2211.16609v1|Jue Hou,Rachel Zhao,Jessica Gronsbell,Brett K. Beaulieu-Jones,Griffin Webber,Thomas Jemielita,Shuyan Wan,Chuan Hong,Yucong Lin,Tianrun Cai,Jun Wen,Vidul A. Panickan,Clara-Lea Bonzel,Kai-Li Liaw,Katherine P. Liao,Tianxi Cai
1501|A Calibration Approach to Transportability and Data-Fusion with Observational Data|Two important considerations in clinical research studies are proper evaluations of internal and external validity. While randomized clinical trials can overcome several threats to internal validity, they may be prone to poor external validity. Conversely, large prospective observational studies sampled from a broadly generalizable population may be externally valid, yet susceptible to threats to internal validity, particularly confounding. Thus, methods that address confounding and enhance transportability of study results across populations are essential for internally and externally valid causal inference, respectively. These issues persist for another problem closely related to transportability known as data-fusion. We develop a calibration method to generate balancing weights that address confounding and sampling bias, thereby enabling valid estimation of the target population average treatment effect. We compare the calibration approach to two additional doubly-robust methods that estimate the effect of an intervention on an outcome within a second, possibly unrelated target population. The proposed methodologies can be extended to resolve data-fusion problems that seek to evaluate the effects of an intervention using data from two related studies sampled from different populations. A simulation study is conducted to demonstrate the advantages and similarities of the different techniques. We also test the performance of the calibration approach in a motivating real data example comparing whether the effect of biguanides versus sulfonylureas - the two most common oral diabetes medication classes for initial treatment - on all-cause mortality described in a historical cohort applies to a contemporary cohort of US Veterans with diabetes.|http://arxiv.org/abs/2008.06615v4|Kevin P. Josey,Fan Yang,Debashis Ghosh,Sridharan Raghavan
1502|Ethical Implementation of Artificial Intelligence to Select Embryos in In Vitro Fertilization|AI has the potential to revolutionize many areas of healthcare. Radiology, dermatology, and ophthalmology are some of the areas most likely to be impacted in the near future, and they have received significant attention from the broader research community. But AI techniques are now also starting to be used in in vitro fertilization (IVF), in particular for selecting which embryos to transfer to the woman. The contribution of AI to IVF is potentially significant, but must be done carefully and transparently, as the ethical issues are significant, in part because this field involves creating new people. We first give a brief introduction to IVF and review the use of AI for embryo selection. We discuss concerns with the interpretation of the reported results from scientific and practical perspectives. We then consider the broader ethical issues involved. We discuss in detail the problems that result from the use of black-box methods in this context and advocate strongly for the use of interpretable models. Importantly, there have been no published trials of clinical effectiveness, a problem in both the AI and IVF communities, and we therefore argue that clinical implementation at this point would be premature. Finally, we discuss ways for the broader AI community to become involved to ensure scientifically sound and ethically responsible development of AI in IVF.|http://arxiv.org/abs/2105.00060v1|Michael Anis Mihdi Afnan,Cynthia Rudin,Vincent Conitzer,Julian Savulescu,Abhishek Mishra,Yanhe Liu,Masoud Afnan
1503|Using machine learning techniques to predict hospital admission at the emergency department|Introduction: One of the most important tasks in the Emergency Department (ED) is to promptly identify the patients who will benefit from hospital admission. Machine Learning (ML) techniques show promise as diagnostic aids in healthcare. Material and methods: We investigated the following features seeking to investigate their performance in predicting hospital admission: serum levels of Urea, Creatinine, Lactate Dehydrogenase, Creatine Kinase, C-Reactive Protein, Complete Blood Count with differential, Activated Partial Thromboplastin Time, D Dimer, International Normalized Ratio, age, gender, triage disposition to ED unit and ambulance utilization. A total of 3,204 ED visits were analyzed. Results: The proposed algorithms generated models which demonstrated acceptable performance in predicting hospital admission of ED patients. The range of F-measure and ROC Area values of all eight evaluated algorithms were [0.679-0.708] and [0.734-0.774], respectively. Discussion: The main advantages of this tool include easy access, availability, yes/no result, and low cost. The clinical implications of our approach might facilitate a shift from traditional clinical decision-making to a more sophisticated model. Conclusion: Developing robust prognostic models with the utilization of common biomarkers is a project that might shape the future of emergency medicine. Our findings warrant confirmation with implementation in pragmatic ED trials.|http://arxiv.org/abs/2106.12921v2|Georgios Feretzakis,George Karlis,Evangelos Loupelis,Dimitris Kalles,Rea Chatzikyriakou,Nikolaos Trakas,Eugenia Karakou,Aikaterini Sakagianni,Lazaros Tzelves,Stavroula Petropoulou,Aikaterini Tika,Ilias Dalainas,Vasileios Kaldis
1504|GeoECG: Data Augmentation via Wasserstein Geodesic Perturbation for Robust Electrocardiogram Prediction|There has been an increased interest in applying deep neural networks to automatically interpret and analyze the 12-lead electrocardiogram (ECG). The current paradigms with machine learning methods are often limited by the amount of labeled data. This phenomenon is particularly problematic for clinically-relevant data, where labeling at scale can be time-consuming and costly in terms of the specialized expertise and human effort required. Moreover, deep learning classifiers may be vulnerable to adversarial examples and perturbations, which could have catastrophic consequences, for example, when applied in the context of medical treatment, clinical trials, or insurance claims. In this paper, we propose a physiologically-inspired data augmentation method to improve performance and increase the robustness of heart disease detection based on ECG signals. We obtain augmented samples by perturbing the data distribution towards other classes along the geodesic in Wasserstein space. To better utilize domain-specific knowledge, we design a ground metric that recognizes the difference between ECG signals based on physiologically determined features. Learning from 12-lead ECG signals, our model is able to distinguish five categories of cardiac conditions. Our results demonstrate improvements in accuracy and robustness, reflecting the effectiveness of our data augmentation method.|http://arxiv.org/abs/2208.01220v2|Jiacheng Zhu,Jielin Qiu,Zhuolin Yang,Douglas Weber,Michael A. Rosenberg,Emerson Liu,Bo Li,Ding Zhao
1505|Counterfactual Image Synthesis for Discovery of Personalized Predictive Image Markers|The discovery of patient-specific imaging markers that are predictive of future disease outcomes can help us better understand individual-level heterogeneity of disease evolution. In fact, deep learning models that can provide data-driven personalized markers are much more likely to be adopted in medical practice. In this work, we demonstrate that data-driven biomarker discovery can be achieved through a counterfactual synthesis process. We show how a deep conditional generative model can be used to perturb local imaging features in baseline images that are pertinent to subject-specific future disease evolution and result in a counterfactual image that is expected to have a different future outcome. Candidate biomarkers, therefore, result from examining the set of features that are perturbed in this process. Through several experiments on a large-scale, multi-scanner, multi-center multiple sclerosis (MS) clinical trial magnetic resonance imaging (MRI) dataset of relapsing-remitting (RRMS) patients, we demonstrate that our model produces counterfactuals with changes in imaging features that reflect established clinical markers predictive of future MRI lesional activity at the population level. Additional qualitative results illustrate that our model has the potential to discover novel and subject-specific predictive markers of future activity.|http://arxiv.org/abs/2208.02311v1|Amar Kumar,Anjun Hu,Brennan Nichyporuk,Jean-Pierre R. Falet,Douglas L. Arnold,Sotirios Tsaftaris,Tal Arbel
1506|Causally-interpretable meta-analysis: clearly-defined causal effects and two case studies|Meta-analysis is commonly used to combine results from multiple clinical trials, but traditional meta-analysis methods do not refer explicitly to a population of individuals to whom the results apply and it is not clear how to use their results to assess a treatment's effect for a population of interest. We describe recently-introduced causally-interpretable meta-analysis methods and apply their treatment effect estimators to two individual-participant data sets. These estimators transport estimated treatment effects from studies in the meta-analysis to a specified target population using individuals' potentially effect-modifying covariates. We consider different regression and weighting methods within this approach and compare the results to traditional aggregated-data meta-analysis methods. In our applications, certain versions of the causally-interpretable methods performed somewhat better than the traditional methods, but the latter generally did well. The causally-interpretable methods offer the most promise when covariates modify treatment effects and our results suggest that traditional methods work well when there is little effect heterogeneity. The causally-interpretable approach gives meta-analysis an appealing theoretical framework by relating an estimator directly to a specific population and lays a solid foundation for future developments.|http://arxiv.org/abs/2302.07840v1|Kollin W. Rott,Gert Bronfort,Haitao Chu,Jared D. Huling,Brent Leininger,Mohammad Hassan Murad,Zhen Wang,James S. Hodges
1507|Multimodal and multicontrast image fusion via deep generative models|Recently, it has become progressively more evident that classic diagnostic labels are unable to reliably describe the complexity and variability of several clinical phenotypes. This is particularly true for a broad range of neuropsychiatric illnesses (e.g., depression, anxiety disorders, behavioral phenotypes). Patient heterogeneity can be better described by grouping individuals into novel categories based on empirically derived sections of intersecting continua that span across and beyond traditional categorical borders. In this context, neuroimaging data carry a wealth of spatiotemporally resolved information about each patient's brain. However, they are usually heavily collapsed a priori through procedures which are not learned as part of model training, and consequently not optimized for the downstream prediction task. This is because every individual participant usually comes with multiple whole-brain 3D imaging modalities often accompanied by a deep genotypic and phenotypic characterization, hence posing formidable computational challenges. In this paper we design a deep learning architecture based on generative models rooted in a modular approach and separable convolutional blocks to a) fuse multiple 3D neuroimaging modalities on a voxel-wise level, b) convert them into informative latent embeddings through heavy dimensionality reduction, c) maintain good generalizability and minimal information loss. As proof of concept, we test our architecture on the well characterized Human Connectome Project database demonstrating that our latent embeddings can be clustered into easily separable subject strata which, in turn, map to different phenotypical information which was not included in the embedding creation process. This may be of aid in predicting disease evolution as well as drug response, hence supporting mechanistic disease understanding and empowering clinical trials.|http://arxiv.org/abs/2303.15963v2|Giovanna Maria Dimitri,Simeon Spasov,Andrea Duggento,Luca Passamonti,Pietro Li`o,Nicola Toschi
1508|Comprehensive evaluation of deep and graph learning on drug-drug interactions prediction|Recent advances and achievements of artificial intelligence (AI) as well as deep and graph learning models have established their usefulness in biomedical applications, especially in drug-drug interactions (DDIs). DDIs refer to a change in the effect of one drug to the presence of another drug in the human body, which plays an essential role in drug discovery and clinical research. DDIs prediction through traditional clinical trials and experiments is an expensive and time-consuming process. To correctly apply the advanced AI and deep learning, the developer and user meet various challenges such as the availability and encoding of data resources, and the design of computational methods. This review summarizes chemical structure based, network based, NLP based and hybrid methods, providing an updated and accessible guide to the broad researchers and development community with different domain knowledge. We introduce widely-used molecular representation and describe the theoretical frameworks of graph neural network models for representing molecular structures. We present the advantages and disadvantages of deep and graph learning methods by performing comparative experiments. We discuss the potential technical challenges and highlight future directions of deep and graph learning models for accelerating DDIs prediction.|http://arxiv.org/abs/2306.05257v1|Xuan Lin,Lichang Dai,Yafang Zhou,Zu-Guo Yu,Wen Zhang,Jian-Yu Shi,Dong-Sheng Cao,Li Zeng,Haowen Chen,Bosheng Song,Philip S. Yu,Xiangxiang Zeng
1509|A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy|The generation of virtual populations (VPs) of anatomy is essential for conducting in silico trials of medical devices. Typically, the generated VP should capture sufficient variability while remaining plausible and should reflect the specific characteristics and demographics of the patients observed in real populations. In several applications, it is desirable to synthesise virtual populations in a \textit{controlled} manner, where relevant covariates are used to conditionally synthesise virtual populations that fit a specific target population/characteristics. We propose to equip a conditional variational autoencoder (cVAE) with normalising flows to boost the flexibility and complexity of the approximate posterior learnt, leading to enhanced flexibility for controllable synthesis of VPs of anatomical structures. We demonstrate the performance of our conditional flow VAE using a data set of cardiac left ventricles acquired from 2360 patients, with associated demographic information and clinical measurements (used as covariates/conditional information). The results obtained indicate the superiority of the proposed method for conditional synthesis of virtual populations of cardiac left ventricles relative to a cVAE. Conditional synthesis performance was evaluated in terms of generalisation and specificity errors and in terms of the ability to preserve clinically relevant biomarkers in synthesised VPs, that is, the left ventricular blood pool and myocardial volume, relative to the real observed population.|http://arxiv.org/abs/2306.14680v2|Haoran Dou,Nishant Ravikumar,Alejandro F. Frangi
1510|Improving Opioid Use Disorder Risk Modelling through Behavioral and Genetic Feature Integration|Opioids are an effective analgesic for acute and chronic pain, but also carry a considerable risk of addiction leading to millions of opioid use disorder (OUD) cases and tens of thousands of premature deaths in the United States yearly. Estimating OUD risk prior to prescription could improve the efficacy of treatment regimens, monitoring programs, and intervention strategies, but risk estimation is typically based on self-reported data or questionnaires. We develop an experimental design and computational methods that combine genetic variants associated with OUD with behavioral features extracted from GPS and Wi-Fi spatiotemporal coordinates to assess OUD risk. Since both OUD mobility and genetic data do not exist for the same cohort, we develop algorithms to (1) generate mobility features from empirical distributions and (2) synthesize mobility and genetic samples assuming an expected level of disease co-occurrence. We show that integrating genetic and mobility modalities improves risk modelling using classification accuracy, area under the precision-recall and receiver operator characteristic curves, and $F_1$ score. Interpreting the fitted models suggests that mobility features have more influence on OUD risk, although the genetic contribution was significant, particularly in linear models. While there exist concerns with respect to privacy, security, bias, and generalizability that must be evaluated in clinical trials before being implemented in practice, our framework provides preliminary evidence that behavioral and genetic features may improve OUD risk estimation to assist with personalized clinical decision-making.|http://arxiv.org/abs/2309.10837v2|Sybille Lgitime,Kaustubh Prabhu,Devin McConnell,Bing Wang,Dipak K. Dey,Derek Aguiar
1511|High-content stimulated Raman histology of human breast cancer|Histological examination is crucial for cancer diagnosis, including hematoxylin and eosin (H&E) staining for mapping morphology and immunohistochemistry (IHC) staining for revealing chemical information. Recently developed two-color stimulated Raman histology could bypass the complex tissue processing to mimic H&E-like morphology. Yet, the underlying chemical features are not revealed, compromising the effectiveness of prognostic stratification. Here, we present a high-content stimulated Raman histology (HC-SRH) platform that provides both morphological and chemical information for cancer diagnosis based on un-stained breast tissues. Through spectral unmixing in the C-H vibration window, HC-SRH can map unsaturated lipids, cellular protein, extracellular matrix, saturated lipid, and water in breast tissue. In this way, HC-SRH provides excellent contrast for various tissue components. Considering rapidness is important in clinical trials, we implemented spectral selective sampling to boost the speed of HC-SRH by one order. We also successfully demonstrated the HC-SRH in a clinical-compatible fiber laser-based SRS microscopy. With the widely rapid tuning capability of the advanced fiber laser, a clear chemical contrast of nucleic acid and solid-state ester is shown in the fingerprint result.|http://arxiv.org/abs/2309.11642v1|Hongli Ni,Chinmayee Prabhu Dessai,Haonan Lin,Wei Wang,Shaoxiong Chen,Yuhao Yuan,Xiaowei Ge,Jianpeng Ao,Nolan Vild,Ji-Xin Cheng
1512|Assessment and treatment of visuospatial neglect using active learning with Gaussian processes regression|Visuospatial neglect is a disorder characterised by impaired awareness for visual stimuli located in regions of space and frames of reference. It is often associated with stroke. Patients can struggle with all aspects of daily living and community participation. Assessment methods are limited and show several shortcomings, considering they are mainly performed on paper and do not implement the complexity of daily life. Similarly, treatment options are sparse and often show only small improvements. We present an artificial intelligence solution designed to accurately assess a patient's visuospatial neglect in a three-dimensional setting. We implement an active learning method based on Gaussian process regression to reduce the effort it takes a patient to undergo an assessment. Furthermore, we describe how this model can be utilised in patient oriented treatment and how this opens the way to gamification, tele-rehabilitation and personalised healthcare, providing a promising avenue for improving patient engagement and rehabilitation outcomes. To validate our assessment module, we conducted clinical trials involving patients in a real-world setting. We compared the results obtained using our AI-based assessment with the widely used conventional visuospatial neglect tests currently employed in clinical practice. The validation process serves to establish the accuracy and reliability of our model, confirming its potential as a valuable tool for diagnosing and monitoring visuospatial neglect. Our VR application proves to be more sensitive, while intra-rater reliability remains high.|http://arxiv.org/abs/2310.13701v1|Ivan De Boi,Elissa Embrechts,Quirine Schatteman,Rudi Penne,Steven Truijen,Wim Saeys
1513|Plug-and-Play Stability for Intracortical Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication|Intracortical brain-computer interfaces (iBCIs) have shown promise for restoring rapid communication to people with neurological disorders such as amyotrophic lateral sclerosis (ALS). However, to maintain high performance over time, iBCIs typically need frequent recalibration to combat changes in the neural recordings that accrue over days. This requires iBCI users to stop using the iBCI and engage in supervised data collection, making the iBCI system hard to use. In this paper, we propose a method that enables self-recalibration of communication iBCIs without interrupting the user. Our method leverages large language models (LMs) to automatically correct errors in iBCI outputs. The self-recalibration process uses these corrected outputs ("pseudo-labels") to continually update the iBCI decoder online. Over a period of more than one year (403 days), we evaluated our Continual Online Recalibration with Pseudo-labels (CORP) framework with one clinical trial participant. CORP achieved a stable decoding accuracy of 93.84% in an online handwriting iBCI task, significantly outperforming other baseline methods. Notably, this is the longest-running iBCI stability demonstration involving a human participant. Our results provide the first evidence for long-term stabilization of a plug-and-play, high-performance communication iBCI, addressing a major barrier for the clinical translation of iBCIs.|http://arxiv.org/abs/2311.03611v1|Chaofei Fan,Nick Hahn,Foram Kamdar,Donald Avansino,Guy H. Wilson,Leigh Hochberg,Krishna V. Shenoy,Jaimie M. Henderson,Francis R. Willett
1514|SP-DiffDose: A Conditional Diffusion Model for Radiation Dose Prediction Based on Multi-Scale Fusion of Anatomical Structures, Guided by SwinTransformer and Projector|Radiation therapy serves as an effective and standard method for cancer treatment. Excellent radiation therapy plans always rely on high-quality dose distribution maps obtained through repeated trial and error by experienced experts. However, due to individual differences and complex clinical situations, even seasoned expert teams may need help to achieve the best treatment plan every time quickly. Many automatic dose distribution prediction methods have been proposed recently to accelerate the radiation therapy planning process and have achieved good results. However, these results suffer from over-smoothing issues, with the obtained dose distribution maps needing more high-frequency details, limiting their clinical application. To address these limitations, we propose a dose prediction diffusion model based on SwinTransformer and a projector, SP-DiffDose. To capture the direct correlation between anatomical structure and dose distribution maps, SP-DiffDose uses a structural encoder to extract features from anatomical images, then employs a conditional diffusion process to blend noise and anatomical images at multiple scales and gradually map them to dose distribution maps. To enhance the dose prediction distribution for organs at risk, SP-DiffDose utilizes SwinTransformer in the deeper layers of the network to capture features at different scales in the image. To learn good representations from the fused features, SP-DiffDose passes the fused features through a designed projector, improving dose prediction accuracy. Finally, we evaluate SP-DiffDose on an internal dataset. The results show that SP-DiffDose outperforms existing methods on multiple evaluation metrics, demonstrating the superiority and generalizability of our method.|http://arxiv.org/abs/2312.06187v1|Linjie Fu,Xia Li,Xiuding Cai,Yingkai Wang,Xueyao Wang,Yu Yao,Yali Shen
1515|Combining propensity score methods with variational autoencoders for generating synthetic data in presence of latent sub-groups|In settings requiring synthetic data generation based on a clinical cohort, e.g., due to data protection regulations, heterogeneity across individuals might be a nuisance that we need to control or faithfully preserve. The sources of such heterogeneity might be known, e.g., as indicated by sub-groups labels, or might be unknown and thus reflected only in properties of distributions, such as bimodality or skewness. We investigate how such heterogeneity can be preserved and controlled when obtaining synthetic data from variational autoencoders (VAEs), i.e., a generative deep learning technique that utilizes a low-dimensional latent representation. To faithfully reproduce unknown heterogeneity reflected in marginal distributions, we propose to combine VAEs with pre-transformations. For dealing with known heterogeneity due to sub-groups, we complement VAEs with models for group membership, specifically from propensity score regression. The evaluation is performed with a realistic simulation design that features sub-groups and challenging marginal distributions. The proposed approach faithfully recovers the latter, compared to synthetic data approaches that focus purely on marginal distributions. Propensity scores add complementary information, e.g., when visualized in the latent space, and enable sampling of synthetic data with or without sub-group specific characteristics. We also illustrate the proposed approach with real data from an international stroke trial that exhibits considerable distribution differences between study sites, in addition to bimodality. These results indicate that describing heterogeneity by statistical approaches, such as propensity score regression, might be more generally useful for complementing generative deep learning for obtaining synthetic data that faithfully reflects structure from clinical cohorts.|http://arxiv.org/abs/2312.07781v1|Kiana Farhadyar,Federico Bonofiglio,Maren Hackenberg,Daniela Zoeller,Harald Binder
1516|Hierarchical Classification System for Breast Cancer Specimen Report (HCSBC) -- an end-to-end model for characterizing severity and diagnosis|Automated classification of cancer pathology reports can extract information from unstructured reports and categorize each report into structured diagnosis and severity categories. Thus, such system can reduce the burden for populating tumor registries, help registration for clinical trial as well as developing large dataset for deep learning model development using true pathologic ground truth. However, the content of breast pathology reports can be difficult for categorize due to the high linguistic variability in content and wide variety of potential diagnoses >50. Existing NLP models are primarily focused on developing classifier for primary breast cancer types (e.g. IDC, DCIS, ILC) and tumor characteristics, and ignore the rare diagnosis of cancer subtypes. We then developed a hierarchical hybrid transformer-based pipeline (59 labels) - Hierarchical Classification System for Breast Cancer Specimen Report (HCSBC), which utilizes the potential of the transformer context-preserving NLP technique and compared our model to several state of the art ML and DL models. We trained the model on the EUH data and evaluated our model's performance on two external datasets - MGH and Mayo Clinic. We publicly release the code and a live application under Huggingface spaces repository|http://arxiv.org/abs/2312.12442v1|Thiago Santos,Harish Kamath,Christopher R. McAdams,Mary S. Newell,Marina Mosunjac,Gabriela Oprea-Ilies,Geoffrey Smith,Constance Lehman,Judy Gichoya,Imon Banerjee,Hari Trivedi
1517|Detecting QT prolongation From a Single-lead ECG With Deep Learning|For a number of antiarrhythmics, drug loading requires a 3 day hospitalization with monitoring for QT prolongation. Automated QT monitoring with wearable ECG monitors would facilitate out-of-hospital care. We develop a deep learning model that infers QT intervals from ECG lead-I - the lead most often acquired from ambulatory ECG monitors - and to use this model to detect clinically meaningful QT-prolongation episodes during Dofetilide drug loading. Using 4.22 million 12-lead ECG recordings from 903.6 thousand patients at the Massachusetts General Hospital, we develop a deep learning model, QTNet, that infers QT intervals from lead-I. Over 3 million ECGs from 653 thousand patients are used to train the model and an internal-test set containing 633 thousand ECGs from 135 thousand patients was used for testing. QTNet is further evaluated on an external-validation set containing 3.1 million ECGs from 667 thousand patients at another institution. QTNet was used to detect Dofetilide-induced QT prolongation in a publicly available database (ECGRDVQ-dataset) containing ECGs from subjects enrolled in a clinical trial evaluating the effects of antiarrhythmic drugs. QTNet achieves mean absolute errors of 12.63ms (internal-test) and 12.30ms (external-validation) for estimating absolute QT intervals. The associated Pearson correlation coefficients are 0.91 (internal-test) and 0.92 (external-validation). For the ECGRDVQ-dataset, QTNet detects Dofetilide-induced QTc prolongation with 87% sensitivity and 77% specificity. The negative predictive value of the model is greater than 95% when the pre-test probability of drug-induced QTc prolongation is below 25%. Drug-induced QT prolongation risk can be tracked from ECG lead-I using deep learning.|http://arxiv.org/abs/2401.05378v1|Ridwan Alam,Aaron Aguirre,Collin Stultz
1518|Cardiac Digital Twin Pipeline for Virtual Therapy Evaluation|Cardiac digital twins are computational tools capturing key functional and anatomical characteristics of patient hearts for investigating disease phenotypes and predicting responses to therapy. When paired with large-scale computational resources and large clinical datasets, digital twin technology can enable virtual clinical trials on virtual cohorts to fast-track therapy development. Here, we present an automated pipeline for personalising ventricular anatomy and electrophysiological function based on routinely acquired cardiac magnetic resonance (CMR) imaging data and the standard 12-lead electrocardiogram (ECG). Using CMR-based anatomical models, a sequential Monte-Carlo approximate Bayesian computational inference method is extended to infer electrical activation and repolarisation characteristics from the ECG. Fast simulations are conducted with a reaction-Eikonal model, including the Purkinje network and biophysically-detailed subcellular ionic current dynamics for repolarisation. For each patient, parameter uncertainty is represented by inferring a population of ventricular models rather than a single one, which means that parameter uncertainty can be propagated to therapy evaluation. Furthermore, we have developed techniques for translating from reaction-Eikonal to monodomain simulations, which allows more realistic simulations of cardiac electrophysiology. The pipeline is demonstrated in a healthy female subject, where our inferred reaction-Eikonal models reproduced the patient's ECG with a Pearson's correlation coefficient of 0.93, and the translated monodomain simulations have a correlation coefficient of 0.89. We then apply the effect of Dofetilide to the monodomain population of models for this subject and show dose-dependent QT and T-peak to T-end prolongations that are in keeping with large population drug response data.|http://arxiv.org/abs/2401.10029v1|Julia Camps,Zhinuo Jenny Wang,Ruben Doste,Maxx Holmes,Brodie Lawson,Jakub Tomek,Kevin Burrage,Alfonso Bueno-Orovio,Blanca Rodriguez
1519|Learning Optimal Dynamic Treatment Regimes from Longitudinal Data|Studies often report estimates of the average treatment effect. While the ATE summarizes the effect of a treatment on average, it does not provide any information about the effect of treatment within any individual. A treatment strategy that uses an individual's information to tailor treatment to maximize benefit is known as an optimal dynamic treatment rule. Treatment, however, is typically not limited to a single point in time; consequently, learning an optimal rule for a time-varying treatment may involve not just learning the extent to which the comparative treatments' benefits vary across the characteristics of individuals, but also learning the extent to which the comparative treatments' benefits vary as relevant circumstances evolve within an individual. The goal of this paper is to provide a tutorial for estimating ODTR from longitudinal observational and clinical trial data for applied researchers. We describe an approach that uses a doubly-robust unbiased transformation of the conditional average treatment effect. We then learn a time-varying ODTR for when to increase buprenorphine-naloxone dose to minimize return-to-regular-opioid-use among patients with opioid use disorder. Our analysis highlights the utility of ODTRs in the context of sequential decision making: the learned ODTR outperforms a clinically defined strategy.|http://arxiv.org/abs/2401.10867v1|Nicholas T. Williams,Katherine L. Hoffman Ivn Daz,Kara E. Rudolph
1520|Understanding data analysis aspects of TMS-EEG in clinical study: a mini review and a case study with open dataset|Concurrency of transcranial magnetic stimulation with electroencephalography (TMS-EEG) technique is a powerful and challenging methodology for basic research and clinical applications. Aspects considered in experiments for effective TMS-EEG recordings and analysis, including artifact management, data analysis and interpretation and protocols. mini review offers an extensive insight of TMS-EEG methodology in experimental and computational procedures. Case study aims to leverage an openly available, high-quality EEG dataset to delve into the alterations in cortical activity. By applying Intermittent theta-burst stimulation (iTBS) and continuous theta-burst stimulation (cTBS) to the left dorsolateral prefrontal cortex (DLPFC) in healthy individuals, we observe changes in oscillatory patterns within the EEG data. The dataset includes meticulously extracted resting-state EEG recordings, TMS-evoked potential data, and MRI scans. To process these data, we utilized Brainstorm, an open-source Matlab application, which facilitated noise reduction through independent component analysis and signal-space projection techniques. It allowed us to identify, visualize, and analyze TMS-evoked potentials (TEPs) and TMS-induced oscillations (TIOs). In addition, the study presents detailed plots of resting-state EEG power, local mean field power (LMFP), TMS-related spectral perturbation (TSRP), and inter-trial phase clustering (ITPC). Paired t-tests and cluster-based permutation tests have been performed for statistical analysis. The wealth and quality of this dataset make it ideal for examining the neuromodulatory impact of TBS on the prefrontal cortex. Brainstorm's extensive feature set greatly supports the exploration of such neurological data. Future research directions could concentrate on conducting source localization analyses and comparative group studies.|http://arxiv.org/abs/2403.09707v1|Hua Cheng
1521|Evaluating Physician-AI Interaction for Cancer Management: Paving the Path towards Precision Oncology|We evaluated how clinicians approach clinical decision-making when given findings from both randomized controlled trials (RCTs) and machine learning (ML) models. To do so, we designed a clinical decision support system (CDSS) that displays survival curves and adverse event information from a synthetic RCT and ML model for 12 patients with multiple myeloma. We conducted an interventional study in a simulated setting to evaluate how clinicians synthesized the available data to make treatment decisions. Participants were invited to participate in a follow-up interview to discuss their choices in an open-ended format. When ML model results were concordant with RCT results, physicians had increased confidence in treatment choice compared to when they were given RCT results alone. When ML model results were discordant with RCT results, the majority of physicians followed the ML model recommendation in their treatment selection. Perceived reliability of the ML model was consistently higher after physicians were provided with data on how it was trained and validated. Follow-up interviews revealed four major themes: (1) variability in what variables participants used for decision-making, (2) perceived advantages to an ML model over RCT data, (3) uncertainty around decision-making when the ML model quality was poor, and (4) perception that this type of study is an important thought exercise for clinicians. Overall, ML-based CDSSs have the potential to change treatment decisions in cancer management. However, meticulous development and validation of these systems as well as clinician training are required before deployment.|http://arxiv.org/abs/2404.15187v1|Zeshan Hussain,Barbara D. Lam,Fernando A. Acosta-Perez,Irbaz Bin Riaz,Maia Jacobs,Andrew J. Yee,David Sontag
1522|Digital Twin Generators for Disease Modeling|A patient's digital twin is a computational model that describes the evolution of their health over time. Digital twins have the potential to revolutionize medicine by enabling individual-level computer simulations of human health, which can be used to conduct more efficient clinical trials or to recommend personalized treatment options. Due to the overwhelming complexity of human biology, machine learning approaches that leverage large datasets of historical patients' longitudinal health records to generate patients' digital twins are more tractable than potential mechanistic models. In this manuscript, we describe a neural network architecture that can learn conditional generative models of clinical trajectories, which we call Digital Twin Generators (DTGs), that can create digital twins of individual patients. We show that the same neural network architecture can be trained to generate accurate digital twins for patients across 13 different indications simply by changing the training set and tuning hyperparameters. By introducing a general purpose architecture, we aim to unlock the ability to scale machine learning approaches to larger datasets and across more indications so that a digital twin could be created for any patient in the world.|http://arxiv.org/abs/2405.01488v1|Nameyeh Alam,Jake Basilico,Daniele Bertolini,Satish Casie Chetty,Heather D'Angelo,Ryan Douglas,Charles K. Fisher,Franklin Fuller,Melissa Gomes,Rishabh Gupta,Alex Lang,Anton Loukianov,Rachel Mak-McCully,Cary Murray,Hanalei Pham,Susanna Qiao,Elena Ryapolova-Webb,Aaron Smith,Dimitri Theoharatos,Anil Tolwani,Eric W. Tramel,Anna Vidovszky,Judy Viduya,Jonathan R. Walsh
1523|Deep Learning Models to Automate the Scoring of Hand Radiographs for Rheumatoid Arthritis|The van der Heijde modification of the Sharp (SvdH) score is a widely used radiographic scoring method to quantify damage in Rheumatoid Arthritis (RA) in clinical trials. However, its complexity with a necessity to score each individual joint, and the expertise required limit its application in clinical practice, especially in disease progression measurement. In this work, we addressed this limitation by developing a bespoke, automated pipeline that is capable of predicting the SvdH score and RA severity from hand radiographs without the need to localise the joints first. Using hand radiographs from RA and suspected RA patients, we first investigated the performance of the state-of-the-art architectures in predicting the total SvdH score for hands and wrists and its corresponding severity class. Secondly, we leveraged publicly available data sets to perform transfer learning with different finetuning schemes and ensemble learning, which resulted in substantial improvement in model performance being on par with an experienced human reader. The best model for RA scoring achieved a Pearson's correlation coefficient (PCC) of 0.925 and root mean squared error (RMSE) of 18.02, while the best model for RA severity classification achieved an accuracy of 0.358 and PCC of 0.859. Our score prediction model attained almost comparable accuracy with experienced radiologists (PCC = 0.97, RMSE = 18.75). Finally, using Grad-CAM, we showed that our models could focus on the anatomical structures in hands and wrists which clinicians deemed as relevant to RA progression in the majority of cases.|http://arxiv.org/abs/2406.09980v1|Zhiyan Bo,Laura C. Coates,Bartlomiej W. Papiez
1524|Quantifying the Impact of Biobanks and Cohort Studies|Biobanks advance biomedical and clinical research by collecting and offering data and biological samples for numerous studies. However, the impact of these repositories varies greatly due to differences in their purpose, scope, governance, and data collected. Here, we computationally identified 2,663 biobanks and their textual mentions in 228,761 scientific articles, 16,210 grants, 15,469 patents, 1,769 clinical trials, and 9,468 public policy documents, helping characterize the academic communities that utilize and support them. We found a strong concentration of biobank-related research on a few diseases, where 20\% of publications focus on obesity, Alzheimer's disease, breast cancer, and diabetes. Moreover, collaboration, rather than citation count, shapes the community's recognition of a biobank. We show that, on average, 41.1\% of articles miss to reference any of the biobank's reference papers and 59.6\% include a biobank member as a co-author. Using a generalized linear model, we identified the key factors that contribute to the impact of a biobank, finding that an impactful biobank tends to be more open to external researchers, and that quality data -- especially linked medical records -- as opposed to large data, correlates with a higher impact in science, innovation, and disease. The collected data and findings are accessible through an open-access web application intended to inform strategies to expand access and maximize the value of these valuable resources.|http://arxiv.org/abs/2407.01248v1|Rodrigo Dorantes-Gilardi,Kerry Ivey,Lauren Costa,Rachael Matty,Kelly Cho,John Michael Gaziano,Albert-Lszl Barabsi
1525|ReCAP: Recursive Cross Attention Network for Pseudo-Label Generation in Robotic Surgical Skill Assessment|In surgical skill assessment, the Objective Structured Assessments of Technical Skills (OSATS) and Global Rating Scale (GRS) are well-established tools for evaluating surgeons during training. These metrics, along with performance feedback, help surgeons improve and reach practice standards. Recent research on the open-source JIGSAWS dataset, which includes both GRS and OSATS labels, has focused on regressing GRS scores from kinematic data, video, or their combination. However, we argue that regressing GRS alone is limiting, as it aggregates OSATS scores and overlooks clinically meaningful variations during a surgical trial. To address this, we developed a recurrent transformer model that tracks a surgeon's performance throughout a session by mapping hidden states to six OSATS, derived from kinematic data, using a clinically motivated objective function. These OSATS scores are averaged to predict GRS, allowing us to compare our model's performance against state-of-the-art (SOTA) methods. We report Spearman's Correlation Coefficients (SCC) demonstrating that our model outperforms SOTA using kinematic data (SCC 0.83-0.88), and matches performance with video-based models. Our model also surpasses SOTA in most tasks for average OSATS predictions (SCC 0.46-0.70) and specific OSATS (SCC 0.56-0.95). The generation of pseudo-labels at the segment level translates quantitative predictions into qualitative feedback, vital for automated surgical skill assessment pipelines. A senior surgeon validated our model's outputs, agreeing with 77% of the weakly-supervised predictions (p=0.006).|http://arxiv.org/abs/2407.05180v3|Julien Quarez,Marc Modat,Sebastien Ourselin,Jonathan Shapey,Alejandro Granados
1526|Realistic wave-optics simulation of X-ray dark-field imaging at a human scale|Background: X-ray dark-field imaging (XDFI) has been explored to provide superior performance over the conventional X-ray imaging for the diagnosis of many pathologic conditions. A simulation tool to reliably predict clinical XDFI images at a human scale, however, is currently missing. Purpose: In this paper, we demonstrate XDFI simulation at a human scale for the first time to the best of our knowledge. Using the developed simulation tool, we demonstrate the strengths and limitations of XDFI for the diagnosis of emphysema, fibrosis, atelectasis, edema, and pneumonia.   Methods: We augment the XCAT phantom with Voronoi grids to simulate alveolar substructure, responsible for the dark-field signal from lungs, assign material properties to each tissue type, and simulate X-ray wave propagation through the augmented XCAT phantom using a multi-layer wave-optics propagation. Altering the density and thickness of the Voronoi grids as well as the material properties, we simulate XDFI images of normal and diseased lungs.   Results: Our simulation framework can generate realistic XDFI images of a human chest with normal or diseased lungs. The simulation confirms that the normal, emphysematous, and fibrotic lungs show clearly distinct dark-field signals. It also shows that alveolar fluid accumulation in pneumonia, wall thickening in interstitial edema, and deflation in atelectasis result in a similar reduction in dark-field signal.   Conclusions: It is feasible to augment XCAT with pulmonary substructure and generate realistic XDFI images using multi-layer wave optics. By providing the most realistic XDFI images of lung pathologies, the developed simulation framework will enable in-silico clinical trials and the optimization of both hardware and software for XDFI.|http://arxiv.org/abs/2407.12664v1|Yongjin Sung,Brandon Nelson,Rajiv Gupta
1527|Design and inference for multi-arm clinical trials with informational borrowing: the interacting urns design|This paper deals with a new design methodology for stratified comparative experiments based on interacting reinforced urn systems. The key idea is to model the interaction between urns for borrowing information across strata and to use it in the design phase in order to i) enhance the information exchange at the beginning of the study, when only few subjects have been enrolled and the stratum-specific information on treatments' efficacy could be scarce, ii) let the information sharing adaptively evolves via a reinforcement mechanism based on the observed outcomes, for skewing at each step the allocations towards the stratum-specific most promising treatment and iii) make the contribution of the strata with different treatment efficacy vanishing as the stratum information grows. In particular, we introduce the Interacting Urns Design, namely a new Covariate-Adjusted Response-Adaptive procedure, that randomizes the treatment allocations according to the evolution of the urn system. The theoretical properties of this proposal are described and the corresponding asymptotic inference is provided. Moreover, by a functional central limit theorem, we obtain the asymptotic joint distribution of the Wald-type sequential test statistics, which allows to sequentially monitor the suggested design in the clinical practice.|http://arxiv.org/abs/2407.20819v1|Giacomo Aletti,Alessandro Baldi Antognini,Irene Crimaldi,Rosamarie Frieri,Andrea Ghiglietti
1528|Model Ensemble for Brain Tumor Segmentation in Magnetic Resonance Imaging|Segmenting brain tumors in multi-parametric magnetic resonance imaging enables performing quantitative analysis in support of clinical trials and personalized patient care. This analysis provides the potential to impact clinical decision-making processes, including diagnosis and prognosis. In 2023, the well-established Brain Tumor Segmentation (BraTS) challenge presented a substantial expansion with eight tasks and 4,500 brain tumor cases. In this paper, we present a deep learning-based ensemble strategy that is evaluated for newly included tumor cases in three tasks: pediatric brain tumors (PED), intracranial meningioma (MEN), and brain metastases (MET). In particular, we ensemble outputs from state-of-the-art nnU-Net and Swin UNETR models on a region-wise basis. Furthermore, we implemented a targeted post-processing strategy based on a cross-validated threshold search to improve the segmentation results for tumor sub-regions. The evaluation of our proposed method on unseen test cases for the three tasks resulted in lesion-wise Dice scores for PED: 0.653, 0.809, 0.826; MEN: 0.876, 0.867, 0.849; and MET: 0.555, 0.6, 0.58; for the enhancing tumor, tumor core, and whole tumor, respectively. Our method was ranked first for PED, third for MEN, and fourth for MET, respectively.|http://arxiv.org/abs/2409.08232v1|Daniel Capelln-Martn,Zhifan Jiang,Abhijeet Parida,Xinyang Liu,Van Lam,Hareem Nisar,Austin Tapp,Sarah Elsharkawi,Maria J. Ledesma-Carbayo,Syed Muhammad Anwar,Marius George Linguraru
1529|Clinical research and methodology What usage and what hierarchical order for secondary endpoints?|In a randomised clinical trial, when the result of the primary endpoint shows a significant benefit, the secondary endpoints are scrutinised to identify additional effects of the treatment. However, this approach entails a risk of concluding that there is a benefit for one of these endpoints when such benefit does not exist (inflation of type I error risk). There are mainly two methods used to control the risk of drawing erroneous conclusions for secondary endpoints. The first method consists of distributing the risk over several co-primary endpoints, so as to maintain an overall risk of 5%. The second is the hierarchical test procedure, which consists of first establishing a hierarchy of the endpoints, then evaluating each endpoint in succession according to this hierarchy while the endpoints continue to show statistical significance. This simple method makes it possible to show the additional advantages of treatments and to identify the factors that differentiate them.|http://arxiv.org/abs/2409.14770v1|Silvy Laporte,Marine Divin,Danile Girault,Pierre Boutouyrie,Olivier Chassany,Michel Cucherat,Herv de Trogoff,Sophie Dubois,Cecile Fouret,Natalie Hoog-Labouret,Pascale Jolliet,Patrick Mismetti,Raphal Porcher,Ccile Rey-Coquais,Eric Vicaut
1530|Challenges and Possible Strategies to Address Them in Rare Disease Drug Development: A Statistical Perspective|Developing drugs for rare diseases presents unique challenges from a statistical perspective. These challenges may include slowly progressive diseases with unmet medical needs, poorly understood natural history, small population size, diversified phenotypes and geneotypes within a disorder, and lack of appropriate surrogate endpoints to measure clinical benefits. The Real-World Evidence (RWE) Scientific Working Group of the American Statistical Association Biopharmaceutical Section has assembled a research team to assess the landscape including challenges and possible strategies to address these challenges and the role of real-world data (RWD) and RWE in rare disease drug development. This paper first reviews the current regulations by regulatory agencies worldwide and then discusses in more details the challenges from a statistical perspective in the design, conduct, and analysis of rare disease clinical trials. After outlining an overall development pathway for rare disease drugs, corresponding strategies to address the aforementioned challenges are presented. Other considerations are also discussed for generating relevant evidence for regulatory decision-making on drugs for rare diseases. The accompanying paper discusses how RWD and RWE can be used to improve the efficiency of rare disease drug development.|http://arxiv.org/abs/2410.06585v1|Jie Chen,Lei Nie,Shiowjen Lee,Haitao Chu,Haijun Tian,Yan Wang,Weili He,Thomas Jemielita,Susan Gruber,Yang Song,Roy Tamura,Lu Tian,Yihua Zhao,Yong Chen,Mark van der Laan,Hana Lee
1531|Target Aggregate Data Adjustment Method for Transportability Analysis Utilizing Summary-Level Data from the Target Population|Transportability analysis is a causal inference framework used to evaluate the external validity of randomized clinical trials (RCTs) or observational studies. Most existing transportability analysis methods require individual patient-level data (IPD) for both the source and the target population, narrowing its applicability when only target aggregate-level data (AgD) is available. Besides, accounting for censoring is essential to reduce bias in longitudinal data, yet AgD-based transportability methods in the presence of censoring remain underexplored. Here, we propose a two-stage weighting framework named "Target Aggregate Data Adjustment" (TADA) to address the mentioned challenges simultaneously. TADA is designed as a two-stage weighting scheme to simultaneously adjust for both censoring bias and distributional imbalances of effect modifiers (EM), where the final weights are the product of the inverse probability of censoring weights and participation weights derived using the method of moments. We have conducted an extensive simulation study to evaluate TADA's performance. Our results indicate that TADA can effectively control the bias resulting from censoring within a non-extreme range suitable for most practical scenarios, and enhance the application and clinical interpretability of transportability analyses in settings with limited data availability.|http://arxiv.org/abs/2412.12335v1|Yichen Yan,Quang Vuong,Rebecca K Metcalfe,Tianyu Guan,Haolun Shi,Jay JH Park
1532|Semi-Supervised Learning from Small Annotated Data and Large Unlabeled Data for Fine-grained PICO Entity Recognition|Objective: Extracting PICO elements -- Participants, Intervention, Comparison, and Outcomes -- from clinical trial literature is essential for clinical evidence retrieval, appraisal, and synthesis. Existing approaches do not distinguish the attributes of PICO entities. This study aims to develop a named entity recognition (NER) model to extract PICO entities with fine granularities.   Materials and Methods: Using a corpus of 2,511 abstracts with PICO mentions from 4 public datasets, we developed a semi-supervised method to facilitate the training of a NER model, FinePICO, by combining limited annotated data of PICO entities and abundant unlabeled data. For evaluation, we divided the entire dataset into two subsets: a smaller group with annotations and a larger group without annotations. We then established the theoretical lower and upper performance bounds based on the performance of supervised learning models trained solely on the small, annotated subset and on the entire set with complete annotations, respectively. Finally, we evaluated FinePICO on both the smaller annotated subset and the larger, initially unannotated subset. We measured the performance of FinePICO using precision, recall, and F1.   Results: Our method achieved precision/recall/F1 of 0.567/0.636/0.60, respectively, using a small set of annotated samples, outperforming the baseline model (F1: 0.437) by more than 16\%. The model demonstrates generalizability to a different PICO framework and to another corpus, which consistently outperforms the benchmark in diverse experimental settings (p-value \textless0.001).   Conclusion: This study contributes a generalizable and effective semi-supervised approach to named entity recognition leveraging large unlabeled data together with small, annotated data. It also initially supports fine-grained PICO extraction.|http://arxiv.org/abs/2412.19346v1|Fangyi Chen,Gongbo Zhang,Yilu Fang,Yifan Peng,Chunhua Weng
1533|PSO-Net: Development of an automated psoriasis assessment system using attention-based interpretable deep neural networks|Psoriasis is a chronic skin condition that requires long-term treatment and monitoring. Although, the Psoriasis Area and Severity Index (PASI) is utilized as a standard measurement to assess psoriasis severity in clinical trials, it has many drawbacks such as (1) patient burden for in-person clinic visits for assessment of psoriasis, (2) time required for investigator scoring and (3) variability of inter- and intra-rater scoring. To address these drawbacks, we propose a novel and interpretable deep learning architecture called PSO-Net, which maps digital images from different anatomical regions to derive attention-based scores. Regional scores are further combined to estimate an absolute PASI score. Moreover, we devise a novel regression activation map for interpretability through ranking attention scores. Using this approach, we achieved inter-class correlation scores of 82.2% [95% CI: 77- 87%] and 87.8% [95% CI: 84-91%] with two different clinician raters, respectively.|http://arxiv.org/abs/2501.18782v1|Sharif A. Kamran,Molly V. Lucas,Brendon Lutnick,Chaitanya Parmar,Basudha Pal,Asha Patel Shah,David Apfel,Steven Fakharzadeh,Lloyd Miller,Stephen Yip,Kristopher Standish,Gabriela Oana Cula
1534|Delayed treatment with nimesulide reduces measures of oxidative stress following global ischemic brain injury in gerbils|Metabolism of arachidonic acid by cyclooxygenase is one of the primary sources of reactive oxygen species in the ischemic brain. Neuronal overexpression of cyclooxygenase-2 has recently been shown to contribute to neurodegeneration following ischemic injury. In the present study, we examined the possibility that the neuroprotective effects of the cyclooxygenase-2 inhibitor nimesulide would depend upon reduction of oxidative stress following cerebral ischemia. Gerbils were subjected to 5 min of transient global cerebral ischemia followed by 48 h of reperfusion and markers of oxidative stress were measured in hippocampus of gerbils receiving vehicle or nimesulide treatment at three different clinically relevant doses (3, 6 or 12 mg/kg). Compared with vehicle, nimesulide significantly (P<0.05) reduced hippocampal glutathione depletion and lipid peroxidation, as assessed by the levels of malondialdehyde (MDA), 4-hydroxy-alkenals (4-HDA) and lipid hydroperoxides levels, even when the treatment was delayed until 6 h after ischemia. Biochemical evidences of nimesulide neuroprotection were supported by histofluorescence findings using the novel marker of neuronal degeneration Fluoro-Jade B. Few Fluoro-Jade B positive cells were seen in CA1 region of hippocampus in ischemic animals treated with nimesulide compared with vehicle. These results suggest that nimesulide may protect neurons by attenuating oxidative stress and reperfusion injury following the ischemic insult with a wide therapeutic window of protection.|http://arxiv.org/abs/0708.0559v1|E. Candelario-Jalil,D. Alvarez,N. Merino,O. S. Leon
1535|Brain Computer Interface Technologies in the Coming Decades|As the proliferation of technology dramatically infiltrates all aspects of modern life, in many ways the world is becoming so dynamic and complex that technological capabilities are overwhelming human capabilities to optimally interact with and leverage those technologies. Fortunately, these technological advancements have also driven an explosion of neuroscience research over the past several decades, presenting engineers with a remarkable opportunity to design and develop flexible and adaptive brain-based neurotechnologies that integrate with and capitalize on human capabilities and limitations to improve human-system interactions. Major forerunners of this conception are brain-computer interfaces (BCIs), which to this point have been largely focused on improving the quality of life for particular clinical populations and include, for example, applications for advanced communications with paralyzed or locked in patients as well as the direct control of prostheses and wheelchairs. Near-term applications are envisioned that are primarily task oriented and are targeted to avoid the most difficult obstacles to development. In the farther term, a holistic approach to BCIs will enable a broad range of task-oriented and opportunistic applications by leveraging pervasive technologies and advanced analytical approaches to sense and merge critical brain, behavioral, task, and environmental information. Communications and other applications that are envisioned to be broadly impacted by BCIs are highlighted; however, these represent just a small sample of the potential of these technologies.|http://arxiv.org/abs/1211.0886v1|Brent J. Lance,Scott E. Kerick,Anthony J. Ries,Kelvin S. Oie,Kaleb McDowell
1536|Percolation under Noise: Detecting Explosive Percolation Using the Second Largest Component|We consider the problem of distinguishing classical (Erd\H{o}s-R\'{e}nyi) percolation from explosive (Achlioptas) percolation, under noise. A statistical model of percolation is constructed allowing for the birth and death of edges as well as the presence of noise in the observations. This graph-valued stochastic process is composed of a latent and an observed non-stationary process, where the observed graph process is corrupted by Type I and Type II errors. This produces a hidden Markov graph model. We show that for certain choices of parameters controlling the noise, the classical (ER) percolation is visually indistinguishable from the explosive (Achlioptas) percolation model. In this setting, we compare two different criteria for discriminating between these two percolation models, based on a quantile difference (QD) of the first component's size and on the maximal size of the second largest component. We show through data simulations that this second criterion outperforms the QD of the first component's size, in terms of discriminatory power. The maximal size of the second component therefore provides a useful statistic for distinguishing between the ER and Achlioptas models of percolation, under physically motivated conditions for the birth and death of edges, and under noise. The potential application of the proposed criteria for percolation detection in clinical neuroscience is also discussed.|http://arxiv.org/abs/1401.3518v1|Wes Viles,Cedric E. Ginestet,Ariana Tang,Mark A. Kramer,Eric D. Kolaczyk
1537|Spatial Neural Networks and their Functional Samples: Similarities and Differences|Models of neural networks have proven their utility in the development of learning algorithms in computer science and in the theoretical study of brain dynamics in computational neuroscience. We propose in this paper a spatial neural network model to analyze the important class of functional networks, which are commonly employed in computational studies of clinical brain imaging time series. We developed a simulation framework inspired by multichannel brain surface recordings (more specifically, EEG -- electroencephalogram) in order to link the mesoscopic network dynamics (represented by sampled functional networks) and the microscopic network structure (represented by an integrate-and-fire neural network located in a 3D space -- hence the term spatial neural network). Functional networks are obtained by computing pairwise correlations between time-series of mesoscopic electric potential dynamics, which allows the construction of a graph where each node represents one time-series. The spatial neural network model is central in this study in the sense that it allowed us to characterize sampled functional networks in terms of what features they are able to reproduce from the underlying spatial network. Our modeling approach shows that, in specific conditions of sample size and edge density, it is possible to precisely estimate several network measurements of spatial networks by just observing functional samples.|http://arxiv.org/abs/1405.0573v1|Lucas Antiqueira,Liang Zhao
1538|Accurately Predicting Functional Connectivity from Diffusion Imaging|Understanding the relationship between the dynamics of neural processes and the anatomical substrate of the brain is a central question in neuroscience. On the one hand, modern neuroimaging technologies, such as diffusion tensor imaging, can be used to construct structural graphs representing the architecture of white matter streamlines linking cortical and subcortical structures. On the other hand, temporal patterns of neural activity can be used to construct functional graphs representing temporal correlations between brain regions. Although some studies provide evidence that whole-brain functional connectivity is shaped by the underlying anatomy, the observed relationship between function and structure is weak, and the rules by which anatomy constrains brain dynamics remain elusive. In this article, we introduce a methodology to predict with high accuracy the functional connectivity of a subject at rest from his or her structural graph. Using our methodology, we are able to systematically unveil the role of structural paths in the formation of functional correlations. Furthermore, in our empirical evaluations, we observe that the eigen-modes of the predicted functional connectivity are aligned with activity patterns associated with different cognitive systems. Our work offers the potential to infer properties of brain dynamics in clinical or developmental populations with low tolerance for functional neuroimaging.|http://arxiv.org/abs/1512.02602v3|Cassiano O. Becker,Sergio Pequito,George J. Pappas,Michael B. Miller,Scott T. Grafton,Danielle S. Bassett,Victor M. Preciado
1539|Decoded fMRI neurofeedback can induce bidirectional behavioral changes within single participants|Studies using real-time functional magnetic resonance imaging (rt-fMRI) have recently incorporated the decoding approach, allowing for fMRI to be used as a tool for manipulation of fine-grained neural activity. Because of the tremendous potential for clinical applications, certain questions regarding decoded neurofeedback (DecNef) must be addressed. Neurofeedback effects can last for months, but the short- to mid-term dynamics are not known. Specifically, can the same subjects learn to induce neural patterns in two opposite directions in different sessions? This leads to a further question, whether learning to reverse a neural pattern may be less effective after training to induce it in a previous session. Here we employed a within-subjects' design, with subjects undergoing DecNef training sequentially in opposite directions (up or down regulation of confidence judgements in a perceptual task), with the order counterbalanced across subjects. Behavioral results indicated that the manipulation was strongly influenced by the order and direction of neurofeedback. We therefore applied nonlinear mathematical modeling to parametrize four main consequences of DecNef: main effect of change in behavior, strength of down-regulation effect relative to up-regulation, maintenance of learning over sessions, and anterograde learning interference. Modeling results revealed that DecNef successfully induced bidirectional behavioral changes in different sessions. Furthermore, up-regulation was more sizable, and the effect was largely preserved even after an interval of one-week. Lastly, the second week effect was diminished as compared to the first week effect, indicating strong anterograde learning interference. These results suggest reinforcement learning characteristics of DecNef, and provide important constraints on its application to basic neuroscience, occupational and sports trainings, and therapies.|http://arxiv.org/abs/1603.03162v1|Aurelio Cortese,Kaoru Amano,Ai Koizumi,Hakwan Lau,Mitsuo Kawato
1540|Reverse-engineering biological networks from large data sets|Much of contemporary systems biology owes its success to the abstraction of a network, the idea that diverse kinds of molecular, cellular, and organismal species and interactions can be modeled as relational nodes and edges in a graph of dependencies. Since the advent of high-throughput data acquisition technologies in fields such as genomics, metabolomics, and neuroscience, the automated inference and reconstruction of such interaction networks directly from large sets of activation data, commonly known as reverse-engineering, has become a routine procedure. Whereas early attempts at network reverse-engineering focused predominantly on producing maps of system architectures with minimal predictive modeling, reconstructions now play instrumental roles in answering questions about the statistics and dynamics of the underlying systems they represent. Many of these predictions have clinical relevance, suggesting novel paradigms for drug discovery and disease treatment. While other reviews focus predominantly on the details and effectiveness of individual network inference algorithms, here we examine the emerging field as a whole. We first summarize several key application areas in which inferred networks have made successful predictions. We then outline the two major classes of reverse-engineering methodologies, emphasizing that the type of prediction that one aims to make dictates the algorithms one should employ. We conclude by discussing whether recent breakthroughs justify the computational costs of large-scale reverse-engineering sufficiently to admit it as a mainstay in the quantitative analysis of living systems.|http://arxiv.org/abs/1705.06370v2|Joseph L. Natale,David Hofmann,Damian G. Hernndez,Ilya Nemenman
1541|The Morphospace of Consciousness|We construct a complexity-based morphospace to study systems-level properties of conscious & intelligent systems. The axes of this space label 3 complexity types: autonomous, cognitive & social. Given recent proposals to synthesize consciousness, a generic complexity-based conceptualization provides a useful framework for identifying defining features of conscious & synthetic systems. Based on current clinical scales of consciousness that measure cognitive awareness and wakefulness, we take a perspective on how contemporary artificially intelligent machines & synthetically engineered life forms measure on these scales. It turns out that awareness & wakefulness can be associated to computational & autonomous complexity respectively. Subsequently, building on insights from cognitive robotics, we examine the function that consciousness serves, & argue the role of consciousness as an evolutionary game-theoretic strategy. This makes the case for a third type of complexity for describing consciousness: social complexity. Having identified these complexity types, allows for a representation of both, biological & synthetic systems in a common morphospace. A consequence of this classification is a taxonomy of possible conscious machines. We identify four types of consciousness, based on embodiment: (i) biological consciousness, (ii) synthetic consciousness, (iii) group consciousness (resulting from group interactions), & (iv) simulated consciousness (embodied by virtual agents within a simulated reality). This taxonomy helps in the investigation of comparative signatures of consciousness across domains, in order to highlight design principles necessary to engineer conscious machines. This is particularly relevant in the light of recent developments at the crossroads of cognitive neuroscience, biomedical engineering, artificial intelligence & biomimetics.|http://arxiv.org/abs/1705.11190v3|Xerxes D. Arsiwalla,Ricard Sole,Clement Moulin-Frier,Ivan Herreros,Marti Sanchez-Fibla,Paul Verschure
1542|Hybrid Wavelet and EMD/ICA Approach for Artifact Suppression in Pervasive EEG|Electroencephalogram (EEG) signals are often corrupted with unintended artifacts which need to be removed for extracting meaningful clinical information from them. Typically a priori knowledge of the nature of the artifacts is needed for such purpose. Artifact contamination of EEG is even more prominent for pervasive EEG systems where the subjects are free to move and thereby introducing a wide variety of motion-related artifacts. This makes hard to get a priori knowledge about their characteristics rendering conventional artifact removal techniques often ineffective. In this paper, we explore the performance of two hybrid artifact removal algorithms: Wavelet packet transform followed by Independent Component Analysis (WPTICA) and Wavelet Packet Transform followed by Empirical Mode Decomposition (WPTEMD) in pervasive EEG recording scenario, assuming existence of no a priori knowledge about the artifacts and compare their performance with two existing artifact removal algorithms. Artifact cleaning performance has been measured using Root Mean Square Error (RMSE) and Artifact to Signal Ratio (ASR) - an index similar to traditional Signal to Noise Ratio (SNR), and also by observing normalized power distribution topography over the scalp. Comparison has been made first using semi-simulated signals and then with real experimentally acquired EEG data with commercially available 19-channel pervasive EEG system Enobio corrupted by eight types of artifact. Our explorations show that WPTEMD consistently gives best artifact cleaning performance not only in semi-simulated scenario but also in the case of real EEG data containing artifacts.|http://arxiv.org/abs/1803.00053v1|Valentina Bono,Saptarshi Das,Wasifa Jamal,Koushik Maharatna
1543|HINT: A Hierarchical Independent Component Analysis Toolbox for Investigating Brain Functional Networks using Neuroimaging Data|Independent component analysis (ICA) is a popular tool for investigating brain organization in neuroscience research. In fMRI studies, an important goal is to study how brain networks are modulated by subjects' clinical and demographic variables. Existing ICA methods and toolboxes don't incorporate subjects' covariates effects in ICA estimation of brain networks, which potentially leads to loss in accuracy and statistical power in detecting brain network differences between subjects' groups. We introduce a Matlab toolbox, HINT (Hierarchical INdependent component analysis Toolbox), that provides a hierarchical covariate-adjusted ICA (hc-ICA) for modeling and testing covariate effects and generates model-based estimates of brain networks on both the population- and individual-level. HINT provides a user-friendly Matlab GUI that allows users to easily load images, specify covariate effects, monitor model estimation via an EM algorithm, specify hypothesis tests, and visualize results. HINT also has a command line interface which allows users to conveniently run and reproduce the analysis with a script. HINT implements a new multi-level probabilistic ICA model for group ICA. It provides a statistically principled ICA modeling framework for investigating covariate effects on brain networks. HINT can also generate and visualize model-based network estimates for user-specified subject groups, which greatly facilitates group comparisons.|http://arxiv.org/abs/1803.07587v5|Joshua Lukemire,Yikai Wang,Amit Verma,Ying Guo
1544|Spherical Harmonic Residual Network for Diffusion Signal Harmonization|Diffusion imaging is an important method in the field of neuroscience, as it is sensitive to changes within the tissue microstructure of the human brain. However, a major challenge when using MRI to derive quantitative measures is that the use of different scanners, as used in multi-site group studies, introduces measurement variability. This can lead to an increased variance in quantitative metrics, even if the same brain is scanned.   Contrary to the assumption that these characteristics are comparable and similar, small changes in these values are observed in many clinical studies, hence harmonization of the signals is essential.   In this paper, we present a method that does not require additional preprocessing, such as segmentation or registration, and harmonizes the signal based on a deep learning residual network. For this purpose, a training database is required, which consist of the same subjects, scanned on different scanners.   The results show that harmonized signals are significantly more similar to the ground truth signal compared to no harmonization, but also improve in comparison to another deep learning method. The same effect is also demonstrated in commonly used metrics derived from the diffusion MRI signal.|http://arxiv.org/abs/1808.01595v1|Simon Koppers,Luke Bloy,Jeffrey I. Berman,Chantal M. W. Tax,J. Christopher Edgar,Dorit Merhof
1545|Motor Control Insights on Walking Planner and its Stability|The application of biomechanic and motor control models in the control of bidedal robots (humanoids, and exoskeletons) has revealed limitations of our understanding of human locomotion. A recently proposed model uses the potential energy for bipedal structures to model the bipedal dynamics, and it allows to predict the system dynamics from its kinematics. This work proposes a task-space planner for human-like straight locomotion that target application of in rehabilitation robotics and computational neuroscience. The proposed architecture is based on the potential energy model and employs locomotor strategies from human data as a reference for human behaviour. The model generates Centre of Mass (CoM) trajectories, foot swing trajectories and the Base of Support (BoS) over time. The data show that the proposed architecture can generate behaviour in line with human walking strategies for both the CoM and the foot swing. Despite the CoM vertical trajectory being not as smooth as a human trajectory, yet the proposed model significantly reduces the error in the estimation of the CoM vertical trajectory compared to the inverted pendulum models. The proposed model is also able to asses the stability based on the body kinematics embedding in currently used in the clinical practice. However, the model also implies a shift in the interpretation of the spatiotemporal parameters of the gait, which are now determined by the conditions for the equilibrium and not \textit{vice versa}. In other words, locomotion is a dynamic reaching where the motor primitives are also determined by gravity.|http://arxiv.org/abs/1808.10799v4|Carlo Tiseo,Kalyana C Veluvolu,Wei Tech Ang
1546|Disrupted core-periphery structure of multimodal brain networks in Alzheimer's Disease|In Alzheimer's disease (AD), the progressive atrophy leads to aberrant network reconfigurations both at structural and functional levels. In such network reorganization, the core and peripheral nodes appear to be crucial for the prediction of clinical outcome due to their ability to influence large-scale functional integration. However, the role of the different types of brain connectivity in such prediction still remains unclear. Using a multiplex network approach we integrated information from DWI, fMRI and MEG brain connectivity to extract an enriched description of the core-periphery structure in a group of AD patients and age-matched controls. Globally, the regional coreness - i.e., the probability of a region to be in the multiplex core - significantly decreased in AD patients as a result of the randomization process initiated by the neurodegeneration. Locally, the most impacted areas were in the core of the network - including temporal, parietal and occipital areas - while we reported compensatory increments for the peripheral regions in the sensorimotor system. Furthermore, these network changes significantly predicted the cognitive and memory impairment of patients. Taken together these results indicate that a more accurate description of neurodegenerative diseases can be obtained from the multimodal integration of neuroimaging-derived network data.|http://arxiv.org/abs/1811.11688v2|Jeremy Guillon,Mario Chavez,Federico Battiston,Yohan Attal,Valentina La Corte,Michel Thiebaut de Schotten,Bruno Dubois,Denis Schwartz,Olivier Colliot,Fabrizio De Vico Fallani
1547|A tutorial on group effective connectivity analysis, part 1: first level analysis with DCM for fMRI|Dynamic Causal Modelling (DCM) is the predominant method for inferring effective connectivity from neuroimaging data. In the 15 years since its introduction, the neural models and statistical routines in DCM have developed in parallel, driven by the needs of researchers in cognitive and clinical neuroscience. In this tutorial, we step through an exemplar fMRI analysis in detail, reviewing the current implementation of DCM and demonstrating recent developments in group-level connectivity analysis. In the first part of the tutorial (current paper), we focus on issues specific to DCM for fMRI, unpacking the relevant theory and highlighting practical considerations. In particular, we clarify the assumptions (i.e., priors) used in DCM for fMRI and how to interpret the model parameters. This tutorial is accompanied by all the necessary data and instructions to reproduce the analyses using the SPM software. In the second part (in a companion paper), we move from subject-level to group-level modelling using the Parametric Empirical Bayes framework, and illustrate how to test for commonalities and differences in effective connectivity across subjects, based on imaging data from any modality.|http://arxiv.org/abs/1902.10597v1|Peter Zeidman,Amirhossein Jafarian,Nadge Corbin,Mohamed L. Seghier,Adeel Razi,Cathy J. Price,Karl J. Friston
1548|Intuitive Neuromyoelectric Control of a Dexterous Bionic Arm Using a Modified Kalman Filter|Background: Multi-articulate prostheses are capable of performing dexterous hand movements. However, clinically available control strategies fail to provide users with intuitive, independent and proportional control over multiple degrees of freedom (DOFs) in real-time. New Method: We detail the use of a modified Kalman filter (MKF) to provide intuitive, independent and proportional control over six-DOF prostheses such as the DEKA "LUKE" Arm. Input features include neural firing rates recorded from Utah Slanted Electrode Arrays and mean absolute value of intramuscular electromyographic (EMG) recordings. Ad-hoc modifications include thresholds and non-unity gains on the output of a Kalman filter. Results: We demonstrate that both neural and EMG data can be combined effectively. We also highlight that modifications can be optimized to significantly improve performance relative to an unmodified Kalman filter. Thresholds significantly reduced unintended movement and promoted more independent control of the different DOFs. Gain were significantly greater than one and served to amplify participant effort. Optimal modifications can be determined quickly offline and translate to functional improvements online. Using a portable take-home system, participants performed various activities of daily living. Comparison with Existing Methods: In contrast to pattern recognition, the MKF allows users to continuously modulate their force output, which is critical for fine dexterity. The MKF is also computationally efficient and can be trained in less than five minutes. Conclusions: The MKF can be used to explore the functional and psychological benefits associated with long-term, at-home control of dexterous prosthetic hands.|http://arxiv.org/abs/1908.10522v2|Jacob A. George,Tyler S. Davis,Mark R. Brinton,Gregory A. Clark
1549|Efficient T2 mapping with Blip-up/down EPI and gSlider-SMS (T2-BUDA-gSlider)|Purpose: To rapidly obtain high isotropic-resolution T2 maps with whole-brain coverage and high geometric fidelity.   Methods: A T2 blip-up/down echo planar imaging (EPI) acquisition with generalized Slice-dithered enhanced resolution (T2-BUDA-gSlider) is proposed. A radiofrequency (RF)-encoded multi-slab spin-echo EPI acquisition with multiple echo times (TEs) was developed to obtain high SNR efficiency with reduced repetition time (TR). This was combined with an interleaved 2-shot EPI acquisition using blip-up/down phase encoding. An estimated field map was incorporated into the joint multi-shot EPI reconstruction with a structured low rank constraint to achieve distortion-free and robust reconstruction for each slab without navigation. A Bloch simulated subspace model was integrated into gSlider reconstruction and utilized for T2 quantification.   Results: In vivo results demonstrated that the T2 values estimated by the proposed method were consistent with gold standard spin-echo acquisition. Compared to the reference 3D fast spin echo (FSE) images, distortion caused by off-resonance and eddy current effects were effectively mitigated.   Conclusion: BUDA-gSlider SE-EPI acquisition and gSlider-subspace joint reconstruction enabled distortion-free whole-brain T2 mapping in 2 min at ~1 mm3 isotropic resolution, which could bring significant benefits to related clinical and neuroscience applications.|http://arxiv.org/abs/1909.12999v2|Xiaozhi Cao,Congyu Liao,Zijing Zhang,Siddharth Srinivasan Iyer,Kang Wang,Hongjian He,Huafeng Liu,Kawin Setsompop,Jianhui Zhong,Berkin Bilgic
1550|Inhibition and Set-Shifting Tasks in Central Executive Function of Working Memory: An Event-Related Potential (ERP) Study|Understanding of neuro-dynamics of a complex higher cognitive process, Working Memory (WM) is challenging. In WM, information processing occurs through four subsystems: phonological loop, visual sketch pad, memory buffer and central executive function (CEF). CEF plays a principal role in WM. In this study, our objective was to understand the neurospatial correlates of CEF during inhibition and set-shifting processes. Thirty healthy educated subjects were selected. Event-Related Potential (ERP) related to visual inhibition and set-shifting task was collected using 32 channel EEG system. Activation of those ERPs components was analyzed using amplitudes of positive and negative peaks. Experiment was controlled using certain parametric constraints to judge behavior, based on average responses in order to establish relationship between ERP and local area of brain activation and represented using standardized low resolution brain electromagnetic tomography. The average score of correct responses was higher for inhibition task (87.5%) as compared to set-shifting task (59.5%). The peak amplitude of neuronal activity for inhibition task was lower compared to set-shifting task in fronto-parieto-central regions. Hence this proposed paradigm and technique can be used to measure inhibition and set-shifting neuronal processes in understanding pathological central executive functioning in patients with neuro-psychiatric disorders.|http://arxiv.org/abs/2003.05900v1|Pankaj,Jamuna Rajeswaran,Divya Sadana
1551|A Bayesian brain model of adaptive behavior: An application to the Wisconsin Card Sorting Task|Adaptive behavior emerges through a dynamic interaction between cognitive agents and changing environmental demands. The investigation of information processing underlying adaptive behavior relies on controlled experimental settings in which individuals are asked to accomplish demanding tasks whereby a hidden state or an abstract rule has to be learned dynamically. Although performance in such tasks is regularly considered as a proxy for measuring high-level cognitive processes, the standard approach consists in summarizing response patterns by simple heuristic scoring measures. With this work, we propose and validate a new computational Bayesian model accounting for individual performance in the established Wisconsin Card Sorting Test. We embed the new model within the mathematical framework of Bayesian Brain Theory, according to which beliefs about the hidden environmental states are dynamically updated following the logic of Bayesian inference. Our computational model maps distinct cognitive processes into separable, neurobiologically plausible, information-theoretic constructs underlying observed response patterns. We assess model identification and expressiveness in accounting for meaningful human performance through extensive simulation studies. We further apply the model to real behavioral data in order to highlight the utility of the proposed model in recovering cognitive dynamics at an individual level. Practical and theoretical implications of our computational modeling approach for clinical and cognitive neuroscience research are finally discussed, as well as potential future improvements.|http://arxiv.org/abs/2003.07394v2|Marco D'Alessandro,Stefan T. Radev,Andreas Voss,Luigi Lombardi
1552|Review on Biophysical Modelling and Simulation Studies for Transcranial Magnetic Stimulation|Transcranial magnetic stimulation (TMS) is a technique for noninvasively stimulating a brain area for therapeutic, rehabilitation treatments and neuroscience research. Despite our understanding of the physical principles and experimental developments pertaining to TMS, it is difficult to identify the exact brain target as the generated dosage exhibits a non-uniform distribution owing to the complicated and subject-dependent brain anatomy and the lack of biomarkers that can quantify the effects of TMS in most cortical areas. Computational dosimetry has progressed significantly and enables TMS assessment by computation of the induced electric field (the primary physical agent known to activate the brain neurons) in a digital representation of the human head. In this review, TMS dosimetry studies are summarised, clarifying the importance of the anatomical and human biophysical parameters and computational methods. This review shows that there is a high consensus on the importance of a detailed cortical folding representation and an accurate modelling of the surrounding cerebrospinal fluid. Recent studies have also enabled the prediction of individually optimised stimulation based on magnetic resonance imaging of the patient/subject and have attempted to understand the temporal effects of TMS at the cellular level by incorporating neural modelling. These efforts, together with the fast deployment of personalised TMS computations, will permit the adoption of TMS dosimetry as a standard procedure in clinical procedures.|http://arxiv.org/abs/2007.00469v1|Jose Gomez-Tames,Ilkka Laakso,Akimasa Hirata
1553|Towards learned optimal q-space sampling in diffusion MRI|Fiber tractography is an important tool of computational neuroscience that enables reconstructing the spatial connectivity and organization of white matter of the brain. Fiber tractography takes advantage of diffusion Magnetic Resonance Imaging (dMRI) which allows measuring the apparent diffusivity of cerebral water along different spatial directions. Unfortunately, collecting such data comes at the price of reduced spatial resolution and substantially elevated acquisition times, which limits the clinical applicability of dMRI. This problem has been thus far addressed using two principal strategies. Most of the efforts have been extended towards improving the quality of signal estimation for any, yet fixed sampling scheme (defined through the choice of diffusion-encoding gradients). On the other hand, optimization over the sampling scheme has also proven to be effective. Inspired by the previous results, the present work consolidates the above strategies into a unified estimation framework, in which the optimization is carried out with respect to both estimation model and sampling design {\it concurrently}. The proposed solution offers substantial improvements in the quality of signal estimation as well as the accuracy of ensuing analysis by means of fiber tractography. While proving the optimality of the learned estimation models would probably need more extensive evaluation, we nevertheless claim that the learned sampling schemes can be of immediate use, offering a way to improve the dMRI analysis without the necessity of deploying the neural network used for their estimation. We present a comprehensive comparative analysis based on the Human Connectome Project data. Code and learned sampling designs aviliable at https://github.com/tomer196/Learned_dMRI.|http://arxiv.org/abs/2009.03008v1|Tomer Weiss,Sanketh Vedula,Ortal Senouf,Oleg Michailovich,AlexBronstein
1554|Desires and Motivation: The Computational Rule, the Underlying Neural Circuitry, and the Relevant Clinical Disorders|As organism is a dissipative system. The process from multi desires to exclusive motivation is of great importance among all sensory-action loops. In this paper we argued that a proper Desire-Motivation model should be a continuous dynamic mapping from the dynamic desire vector to the sparse motivation vector. Meanwhile, it should at least have specific stability and adjustability of motivation intensity. Besides, the neuroscience evidences suggest that the Desire-Motivation model should have dynamic information acquisition and should be a recurrent neural network. A five-equation model is built based on the above arguments, namely the Recurrent Gating Desire-Motivation (RGDM) model. Additionally, a heuristic speculation based on the RGDM model about corresponding brain regions is carried out. It believes that the tonic and phasic firing of ventral tegmental area dopamine neurons should execute the respective and collective feedback functions of recurrent processing. The analysis about the RGMD model shows the expectations about individual personality from three dimensions, namely stability, intensity, and motivation decision speed. These three dimensions can be combined and create eight different personalities, which is correspondent to Jung's personality structure theorem. Furthermore, the RGDM model can be used to predict three different brand-new types of depressive disorder with different phenotypes. Moreover, it can also explain several other psychiatry disorders from new perspectives.|http://arxiv.org/abs/2011.05595v1|Yu Liu,Yinghong Zhao,Mo Chen
1555|Inferring the Type of Phase Transitions Undergone in Epileptic Seizures Using Random Graph Hidden Markov Models for Percolation in Noisy Dynamic Networks|In clinical neuroscience, epileptic seizures have been associated with the sudden emergence of coupled activity across the brain. The resulting functional networks - in which edges indicate strong enough coupling between brain regions - are consistent with the notion of percolation, which is a phenomenon in complex networks corresponding to the sudden emergence of a giant connected component. Traditionally, work has concentrated on noise-free percolation with a monotonic process of network growth, but real-world networks are more complex. We develop a class of random graph hidden Markov models (RG-HMMs) for characterizing percolation regimes in noisy, dynamically evolving networks in the presence of edge birth and edge death, as well as noise. This class is used to understand the type of phase transitions undergone in a seizure, and in particular, distinguishing between different percolation regimes in epileptic seizures. We develop a hypothesis testing framework for inferring putative percolation mechanisms. As a necessary precursor, we present an EM algorithm for estimating parameters from a sequence of noisy networks only observed at a longitudinal subsampling of time points. Our results suggest that different types of percolation can occur in human seizures. The type inferred may suggest tailored treatment strategies and provide new insights into the fundamental science of epilepsy.|http://arxiv.org/abs/2102.11948v1|Xiaojing Zhu,Heather Shappell,Mark A. Kramer,Catherine J. Chu,Eric D. Kolaczyk
1556|Estimation of Tissue Microstructure Using a Deep Network Inspired by a Sparse Reconstruction Framework|Diffusion magnetic resonance imaging (dMRI) provides a unique tool for noninvasively probing the microstructure of the neuronal tissue. The NODDI model has been a popular approach to the estimation of tissue microstructure in many neuroscience studies. It represents the diffusion signals with three types of diffusion in tissue: intra-cellular, extra-cellular, and cerebrospinal fluid compartments. However, the original NODDI method uses a computationally expensive procedure to fit the model and could require a large number of diffusion gradients for accurate microstructure estimation, which may be impractical for clinical use. Therefore, efforts have been devoted to efficient and accurate NODDI microstructure estimation with a reduced number of diffusion gradients. In this work, we propose a deep network based approach to the NODDI microstructure estimation, which is named Microstructure Estimation using a Deep Network (MEDN). Motivated by the AMICO algorithm which accelerates the computation of NODDI parameters, we formulate the microstructure estimation problem in a dictionary-based framework. The proposed network comprises two cascaded stages. The first stage resembles the solution to a dictionary-based sparse reconstruction problem and the second stage computes the final microstructure using the output of the first stage. The weights in the two stages are jointly learned from training data, which is obtained from training dMRI scans with diffusion gradients that densely sample the q-space. The proposed method was applied to brain dMRI scans, where two shells each with 30 gradient directions (60 diffusion gradients in total) were used. Estimation accuracy with respect to the gold standard was measured and the results demonstrate that MEDN outperforms the competing algorithms.|http://arxiv.org/abs/1704.01246v1|Chuyang Ye
1557|ICU Disparnumerophobia and Triskaidekaphobia: The 'Irrational Care Unit'?|Whilst evidence-based medicine is the cornerstone of modern practice, it is likely that clinicians are influenced by cultural biases. This work set out to look for evidence of number preference in invasive mechanical ventilatory therapy as a concrete example of subconscious treatment bias. A retrospective observational intensive care electronic medical record database search and analysis was carried out in adult general, specialist neurosciences and paediatric intensive care units within a tertiary referral hospital. All admitted, invasively mechanically ventilated patients between October 2014 and August 2015 were included. Set positive end-expiratory pressure (PEEP), respiratory rate (RR) and inspiratory pressure (Pinsp) settings were extracted. Statistical analysis using conventional testing and a novel Monte Carlo method were used to look for evidence of two culturally prevalent superstitions: Odd/even preference and aversion to the number 13. Patients spent significantly longer with odd choices for PEEP ($OR=0.16$, $p<2\times10^{-16}$), RR ($OR=0.31$, $p<2\times10^{-16}$) and Pinsp (OR=0.48, $p=2.9\times10^{-7}$). An aversion to the number 13 was detected for choices of RR ($p=0.00024$) and Pinsp ($p=3.9\times10^{-5}$). However a PEEP of 13 was more prevalent than expected by chance ($p=0.00028$). These findings suggest superstitious preferences in intensive care therapy do exist and practitioners should be alert to guard against other, less obvious but perhaps more clinically significant decision-making biases. The methodology described may be useful for detecting statistically significant number preferences in other domains.|http://arxiv.org/abs/1907.00846v1|Ari Ercole
1558|Multiple-view clustering for identifying subject clusters and brain sub-networks using functional connectivity matrices without vectorization|In neuroscience, the functional magnetic resonance imaging (fMRI) is a vital tool to non-invasively access brain activity. Using fMRI, the functional connectivity (FC) between brain regions can be inferred, which has contributed to a number of findings of the fundamental properties of the brain. As an important clinical application of FC, clustering of subjects based on FC recently draws much attention, which can potentially reveal important heterogeneity in subjects such as subtypes of psychiatric disorders. In particular, a multiple-view clustering method is a powerful analytical tool, which identifies clustering patterns of subjects depending on their FC in specific brain areas. However, when one applies an existing multiple-view clustering method to fMRI data, there is a need to simplify the data structure, independently dealing with elements in a FC matrix, i.e., vectorizing a correlation matrix. Such a simplification may distort the clustering results. To overcome this problem, we propose a novel multiple-view clustering method based on Wishart mixture models, which preserves the correlation matrix structure without vectorization. The uniqueness of this method is that the multiple-view clustering of subjects is based on particular networks of nodes (or regions of interest, ROIs), optimized in a data-driven manner. Hence, it can identify multiple underlying pairs of associations between a subject cluster solution and a ROI sub-network. The key assumption of the method is independence among sub-networks, which is effectively addressed by whitening correlation matrices. We applied the proposed method to synthetic and fMRI data, demonstrating the usefulness and power of the proposed method.|http://arxiv.org/abs/2010.09941v2|Tomoki Tokuda,Okito Yamashita,Junichiro Yoshimoto
1559|One Representative-Shot Learning Using a Population-Driven Template with Application to Brain Connectivity Classification and Evolution Prediction|Few-shot learning presents a challenging paradigm for training discriminative models on a few training samples representing the target classes to discriminate. However, classification methods based on deep learning are ill-suited for such learning as they need large amounts of training data --let alone one-shot learning. Recently, graph neural networks (GNNs) have been introduced to the field of network neuroscience, where the brain connectivity is encoded in a graph. However, with scarce neuroimaging datasets particularly for rare diseases and low-resource clinical facilities, such data-devouring architectures might fail in learning the target task. In this paper, we take a very different approach in training GNNs, where we aim to learn with one sample and achieve the best performance --a formidable challenge to tackle. Specifically, we present the first one-shot paradigm where a GNN is trained on a single population-driven template --namely a connectional brain template (CBT). A CBT is a compact representation of a population of brain graphs capturing the unique connectivity patterns shared across individuals. It is analogous to brain image atlases for neuroimaging datasets. Using a one-representative CBT as a training sample, we alleviate the training load of GNN models while boosting their performance across a variety of classification and regression tasks. We demonstrate that our method significantly outperformed benchmark one-shot learning methods with downstream classification and time-dependent brain graph data forecasting tasks while competing with the train-on-all conventional training strategy. Our source code can be found at https://github.com/basiralab/one-representative-shot-learning.|http://arxiv.org/abs/2110.11238v1|Umut Guvercin,Mohammed Amine Gharsallaoui,Islem Rekik
1560|The positive-negative mode link between brain connectivity, demographics, and behavior: A pre-registered replication of Smith et al. 2015|In mental health research, it has proven difficult to find measures of brain function that provide reliable indicators of mental health and well-being, including susceptibility to mental health disorders. Recently, a family of data-driven analyses have provided such reliable measures when applied to large, population-level datasets. In the current pre-registered replication study, we show that the canonical correlation analysis (CCA) methods previously developed using resting-state MRI functional connectivity and subject measures of cognition and behavior from healthy adults are also effective in measuring well-being (a "positive-negative axis") in an independent developmental dataset. Our replication was successful in two out of three of our pre-registered criteria, such that a primary CCA mode's weights displayed a significant positive relationship and explained a significant amount of variance in both functional connectivity and subject measures. The only criteria that was not successful was that compared to other modes the magnitude of variance explained by the primary CCA mode was smaller than predicted, a result which could indicate a developmental trajectory of a primary mode. This replication establishes a signature neurotypical relationship between connectivity and phenotype, opening new avenues of research in neuroscience with clear clinical applications.|http://arxiv.org/abs/2201.10598v1|Nikhil Goyal1,Dustin Moraczewski,Peter A. Bandettini,Emily S. Finn,Adam G. Thomas
1561|Deep Learning in fNIRS: A review|Significance: Optical neuroimaging has become a well-established clinical and research tool to monitor cortical activations in the human brain. It is notable that outcomes of functional Near-InfraRed Spectroscopy (fNIRS) studies depend heavily on the data processing pipeline and classification model employed. Recently, Deep Learning (DL) methodologies have demonstrated fast and accurate performances in data processing and classification tasks across many biomedical fields. Aim: We aim to review the emerging DL applications in fNIRS studies. Approach: We first introduce some of the commonly used DL techniques. Then the review summarizes current DL work in some of the most active areas of this field, including brain-computer interface, neuro-impairment diagnosis, and neuroscience discovery. Results: Of the 63 papers considered in this review, 32 report a comparative study of deep learning techniques to traditional machine learning techniques where 26 have been shown outperforming the latter in terms of classification accuracy. Additionally, 8 studies also utilize deep learning to reduce the amount of preprocessing typically done with fNIRS data or increase the amount of data via data augmentation. Conclusions: The application of DL techniques to fNIRS studies has shown to mitigate many of the hurdles present in fNIRS studies such as lengthy data preprocessing or small sample sizes while achieving comparable or improved classification accuracy.|http://arxiv.org/abs/2201.13371v2|Condell Eastmond,Aseem Subedi,Suvranu De,Xavier Intes
1562|Investigating the Predictive Reproducibility of Federated Graph Neural Networks using Medical Datasets|Graph neural networks (GNNs) have achieved extraordinary enhancements in various areas including the fields medical imaging and network neuroscience where they displayed a high accuracy in diagnosing challenging neurological disorders such as autism. In the face of medical data scarcity and high-privacy, training such data-hungry models remains challenging. Federated learning brings an efficient solution to this issue by allowing to train models on multiple datasets, collected independently by different hospitals, in fully data-preserving manner. Although both state-of-the-art GNNs and federated learning techniques focus on boosting classification accuracy, they overlook a critical unsolved problem: investigating the reproducibility of the most discriminative biomarkers (i.e., features) selected by the GNN models within a federated learning paradigm. Quantifying the reproducibility of a predictive medical model against perturbations of training and testing data distributions presents one of the biggest hurdles to overcome in developing translational clinical applications. To the best of our knowledge, this presents the first work investigating the reproducibility of federated GNN models with application to classifying medical imaging and brain connectivity datasets. We evaluated our framework using various GNN models trained on medical imaging and connectomic datasets. More importantly, we showed that federated learning boosts both the accuracy and reproducibility of GNN models in such medical learning tasks. Our source code is available at https://github.com/basiralab/reproducibleFedGNN.|http://arxiv.org/abs/2209.06032v1|Mehmet Yigit Balik,Arwa Rekik,Islem Rekik
1563|Mapping effective connectivity by virtually perturbing a surrogate brain|Effective connectivity (EC), indicative of the causal interactions between brain regions, is fundamental to understanding information processing in the brain. Traditional approaches, which infer EC from neural responses to stimulations, are not suited for mapping whole-brain EC in humans due to being invasive and having limited spatial coverage of stimulations. To address this gap, we present Neural Perturbational Inference (NPI), a data-driven framework designed to map EC across the entire brain. NPI employs an artificial neural network trained to learn large-scale neural dynamics as a computational surrogate of the brain. NPI maps EC by perturbing each region of the surrogate brain and observing the resulting responses in all other regions. NPI captures the directionality, strength, and excitatory/inhibitory properties of brain-wide EC. Our validation of NPI, using models having ground-truth EC, shows its superiority over Granger causality and dynamic causal modeling. Applying NPI to resting-state fMRI data from diverse datasets reveals consistent and structurally supported EC. Further validation using a cortico-cortical evoked potentials dataset reveals a significant correlation between NPI-inferred EC and real stimulation propagation pathways. By transitioning from correlational to causal understandings of brain functionality, NPI marks a stride in decoding the brain's functional architecture and facilitating both neuroscience research and clinical applications.|http://arxiv.org/abs/2301.00148v4|Zixiang Luo,Kaining Peng,Zhichao Liang,Shengyuan Cai,Chenyu Xu,Dan Li,Yu Hu,Changsong Zhou,Quanying Liu
1564|Explainable Brain Age Prediction using coVariance Neural Networks|In computational neuroscience, there has been an increased interest in developing machine learning algorithms that leverage brain imaging data to provide estimates of "brain age" for an individual. Importantly, the discordance between brain age and chronological age (referred to as "brain age gap") can capture accelerated aging due to adverse health conditions and therefore, can reflect increased vulnerability towards neurological disease or cognitive impairments. However, widespread adoption of brain age for clinical decision support has been hindered due to lack of transparency and methodological justifications in most existing brain age prediction algorithms. In this paper, we leverage coVariance neural networks (VNN) to propose an explanation-driven and anatomically interpretable framework for brain age prediction using cortical thickness features. Specifically, our brain age prediction framework extends beyond the coarse metric of brain age gap in Alzheimer's disease (AD) and we make two important observations: (i) VNNs can assign anatomical interpretability to elevated brain age gap in AD by identifying contributing brain regions, (ii) the interpretability offered by VNNs is contingent on their ability to exploit specific eigenvectors of the anatomical covariance matrix. Together, these observations facilitate an explainable and anatomically interpretable perspective to the task of brain age prediction.|http://arxiv.org/abs/2305.18370v3|Saurabh Sihag,Gonzalo Mateos,Corey McMillan,Alejandro Ribeiro
1565|Molecular MRI-Based Monitoring of Cancer Immunotherapy Treatment Response|Immunotherapy constitutes a paradigm shift in cancer treatment. Its FDA approval for several indications has yielded improved prognosis for cases where traditional therapy has shown limited efficiencey. However, many patients still fail to benefit from this treatment modality, and the exact mechanisms responsible for tumor response are unknown. Noninvasive treatment monitoring is crucial for longitudinal tumor characterization and the early detection of non-responders. While various medical imaging techniques can provide a morphological picture of the lesion and its surrounding tissue, a molecular-oriented imaging approach holds the key to unraveling biological effects that occur much earlier in the immunotherapy timeline. Magnetic resonance imaging (MRI) is a highly versatile imaging modality, where the image contrast can be tailored to emphasize a particular biophysical property of interest using advanced engineering of the imaging pipeline. In this review, recent advances in molecular-MRI based cancer immunotherapy monitoring are described. Next, the presentation of the underlying physics, computational, and biological features are complemented by a critical analysis of the results obtained in preclinical and clinical studies. Finally, emerging artificial intelligence (AI)-based strategies to further distill, quantify, and interpret the image-based molecular MRI information are discussed in terms of perspectives for the future.|http://arxiv.org/abs/2308.02879v1|Nikita Vladimirov,Or Perlman
1566|Implicit Gaussian process representation of vector fields over arbitrary latent manifolds|Gaussian processes (GPs) are popular nonparametric statistical models for learning unknown functions and quantifying the spatiotemporal uncertainty in data. Recent works have extended GPs to model scalar and vector quantities distributed over non-Euclidean domains, including smooth manifolds appearing in numerous fields such as computer vision, dynamical systems, and neuroscience. However, these approaches assume that the manifold underlying the data is known, limiting their practical utility. We introduce RVGP, a generalisation of GPs for learning vector signals over latent Riemannian manifolds. Our method uses positional encoding with eigenfunctions of the connection Laplacian, associated with the tangent bundle, readily derived from common graph-based approximation of data. We demonstrate that RVGP possesses global regularity over the manifold, which allows it to super-resolve and inpaint vector fields while preserving singularities. Furthermore, we use RVGP to reconstruct high-density neural dynamics derived from low-density EEG recordings in healthy individuals and Alzheimer's patients. We show that vector field singularities are important disease markers and that their reconstruction leads to a comparable classification accuracy of disease states to high-density recordings. Thus, our method overcomes a significant practical limitation in experimental and clinical applications.|http://arxiv.org/abs/2309.16746v2|Robert L. Peach,Matteo Vinao-Carl,Nir Grossman,Michael David,Emma Mallas,David Sharp,Paresh A. Malhotra,Pierre Vandergheynst,Adam Gosztolai
1567|A multi-artifact EEG denoising by frequency-based deep learning|Electroencephalographic (EEG) signals are fundamental to neuroscience research and clinical applications such as brain-computer interfaces and neurological disorder diagnosis. These signals are typically a combination of neurological activity and noise, originating from various sources, including physiological artifacts like ocular and muscular movements. Under this setting, we tackle the challenge of distinguishing neurological activity from noise-related sources. We develop a novel EEG denoising model that operates in the frequency domain, leveraging prior knowledge about noise spectral features to adaptively compute optimal convolutional filters for noise separation. The model is trained to learn an empirical relationship connecting the spectral characteristics of noise and noisy signal to a non-linear transformation which allows signal denoising. Performance evaluation on the EEGdenoiseNet dataset shows that the proposed model achieves optimal results according to both temporal and spectral metrics. The model is found to remove physiological artifacts from input EEG data, thus achieving effective EEG denoising. Indeed, the model performance either matches or outperforms that achieved by benchmark models, proving to effectively remove both muscle and ocular artifacts without the need to perform any training on the particular type of artifact.|http://arxiv.org/abs/2310.17335v1|Matteo Gabardi,Aurora Saibene,Francesca Gasparini,Daniele Rizzo,Fabio Antonio Stella
1568|Joint Alignment of Multivariate Quasi-Periodic Functional Data Using Deep Learning|The joint alignment of multivariate functional data plays an important role in various fields such as signal processing, neuroscience and medicine, including the statistical analysis of data from wearable devices. Traditional methods often ignore the phase variability and instead focus on the variability in the observed amplitude. We present a novel method for joint alignment of multivariate quasi-periodic functions using deep neural networks, decomposing, but retaining all the information in the data by preserving both phase and amplitude variability. Our proposed neural network uses a special activation of the output that builds on the unit simplex transformation, and we utilize a loss function based on the Fisher-Rao metric to train our model. Furthermore, our method is unsupervised and can provide an optimal common template function as well as subject-specific templates. We demonstrate our method on two simulated datasets and one real example, comprising data from 12-lead 10s electrocardiogram recordings.|http://arxiv.org/abs/2312.09422v1|Vi Thanh Pham,Jonas Bille Nielsen,Klaus Fuglsang Kofoed,Jrgen Tobias Khl,Andreas Kryger Jensen
1569|Diffusion MRI with Machine Learning|\hspace{2mm} Diffusion-weighted magnetic resonance imaging (dMRI) of the brain offers unique capabilities including noninvasive probing of tissue microstructure and structural connectivity. It is widely used for clinical assessment of disease and injury, and for neuroscience research. Analyzing the dMRI data to extract useful information for medical and scientific purposes can be challenging. The dMRI measurements may suffer from strong noise and artifacts, and may exhibit high inter-session and inter-scanner variability in the data, as well as inter-subject heterogeneity in brain structure. Moreover, the relationship between measurements and the phenomena of interest can be highly complex. Recent years have witnessed increasing use of machine learning methods for dMRI analysis. This manuscript aims to assess these efforts, with a focus on methods that have addressed data preprocessing and harmonization, microstructure mapping, tractography, and white matter tract analysis. We study the main findings, strengths, and weaknesses of the existing methods and suggest topics for future research. We find that machine learning may be exceptionally suited to tackle some of the difficult tasks in dMRI analysis. However, for this to happen, several shortcomings of existing methods and critical unresolved issues need to be addressed. There is a pressing need to improve evaluation practices, to increase the availability of rich training datasets and validation benchmarks, as well as model generalizability, reliability, and explainability concerns.|http://arxiv.org/abs/2402.00019v3|Davood Karimi,Simon K. Warfield
1570|FAST functional connectivity implicates P300 connectivity in working memory deficits in Alzheimer's disease|Measuring transient functional connectivity is an important challenge in Electroencephalogram (EEG) research. Here, the rich potential for insightful, discriminative information of brain activity offered by high temporal resolution is confounded by the inherent noise of the medium and the spurious nature of correlations computed over short temporal windows. We propose a novel methodology to overcome these problems called Filter Average Short-Term (FAST) functional connectivity. First, long-term, stable, functional connectivity is averaged across an entire study cohort for a given pair of Visual Short Term Memory (VSTM) tasks. The resulting average connectivity matrix, containing information on the strongest general connections for the tasks, is used as a filter to analyse the transient high temporal resolution functional connectivity of individual subjects. In simulations, we show that this method accurately discriminates differences in noisy Event-Related Potentials (ERPs) between two conditions where standard connectivity and other comparable methods fail. We then apply this to analyse activity related to visual short-term memory binding deficits in two cohorts of familial and sporadic Alzheimer's disease. Reproducible significant differences were found in the binding task with no significant difference in the shape task in the P300 ERP range. This allows new sensitive measurements of transient functional connectivity, which can be implemented to obtain results of clinical significance.|http://arxiv.org/abs/2402.18489v3|Om Roy,Yashar Moshfeghi,Agustin Ibanez,Francisco Lopera,Mario A Parra,Keith M Smith
1571|Quantifying Spatial Domain Explanations in BCI using Earth Mover's Distance|Brain-computer interface (BCI) systems facilitate unique communication between humans and computers, benefiting severely disabled individuals. Despite decades of research, BCIs are not fully integrated into clinical and commercial settings. It's crucial to assess and explain BCI performance, offering clear explanations for potential users to avoid frustration when it doesn't work as expected. This work investigates the efficacy of different deep learning and Riemannian geometry-based classification models in the context of motor imagery (MI) based BCI using electroencephalography (EEG). We then propose an optimal transport theory-based approach using earth mover's distance (EMD) to quantify the comparison of the feature relevance map with the domain knowledge of neuroscience. For this, we utilized explainable AI (XAI) techniques for generating feature relevance in the spatial domain to identify important channels for model outcomes. Three state-of-the-art models are implemented - 1) Riemannian geometry-based classifier, 2) EEGNet, and 3) EEG Conformer, and the observed trend in the model's accuracy across different architectures on the dataset correlates with the proposed feature relevance metrics. The models with diverse architectures perform significantly better when trained on channels relevant to motor imagery than data-driven channel selection. This work focuses attention on the necessity for interpretability and incorporating metrics beyond accuracy, underscores the value of combining domain knowledge and quantifying model interpretations with data-driven approaches in creating reliable and robust Brain-Computer Interfaces (BCIs).|http://arxiv.org/abs/2405.01277v1|Param Rajpura,Hubert Cecotti,Yogesh Kumar Meena
1572|A leadless power transfer and wireless telemetry solutions for an endovascular electrocorticography|Endovascular brain-computer interfaces (eBCIs) offer a minimally invasive way to connect the brain to external devices, merging neuroscience, engineering, and medical technology. Achieving wireless data and power transmission is crucial for the clinical viability of these implantable devices. Typically, solutions for endovascular electrocorticography (ECoG) include a sensing stent with multiple electrodes (e.g. in the superior sagittal sinus) in the brain, a subcutaneous chest implant for wireless energy harvesting and data telemetry, and a long (tens of centimetres) cable with a set of wires in between. This long cable presents risks and limitations, especially for younger patients or those with fragile vasculature. This work introduces a wireless and leadless telemetry and power transfer solution for endovascular ECoG. The proposed solution includes an optical telemetry module and a focused ultrasound (FUS) power transfer system. The proposed system can be miniaturised to fit in an endovascular stent. Our solution uses optical telemetry for high-speed data transmission (over 2 Mbit/s, capable of transmitting 41 ECoG channels at a 2 kHz sampling rate and 24-bit resolution) and the proposed power transferring scheme provides up to 10mW power budget into the site of the endovascular implants under the safety limit. Tests on bovine tissues confirmed the system's effectiveness, suggesting that future custom circuit designs could further enhance eBCI applications by removing wires and auxiliary implants, minimising complications.|http://arxiv.org/abs/2405.04806v1|Zhangyu Xu,Majid Khazaee,Nhan Duy Truong,Deniel Havenga,Armin Nikpour,Arman Ahnood,Omid Kavehei
1573|A Global Data-Driven Model for The Hippocampus and Nucleus Accumbens of Rat From The Local Field Potential Recordings (LFP)|In brain neural networks, Local Field Potential (LFP) signals represent the dynamic flow of information. Analyzing LFP clinical data plays a critical role in improving our understanding of brain mechanisms. One way to enhance our understanding of these mechanisms is to identify a global model to predict brain signals in different situations. This paper identifies a global data-driven based on LFP recordings of the Nucleus Accumbens and Hippocampus regions in freely moving rats. The LFP is recorded from each rat in two different situations: before and after the process of getting a reward which can be either a drug (Morphine) or natural food (like popcorn or biscuit). A comparison of five machine learning methods including Long Short Term Memory (LSTM), Echo State Network (ESN), Deep Echo State Network (DeepESN), Radial Basis Function (RBF), and Local Linear Model Tree (LLM) is conducted to develop this model. LoLiMoT was chosen with the best performance among all methods. This model can predict the future states of these regions with one pre-trained model. Identifying this model showed that Morphine and natural rewards do not change the dynamic features of neurons in these regions.|http://arxiv.org/abs/2405.06732v1|Maedeh Sadeghi,Mahdi Aliyari Shoorehdeli,Shole jamali,Abbas Haghparast
1574|LoCI-DiffCom: Longitudinal Consistency-Informed Diffusion Model for 3D Infant Brain Image Completion|The infant brain undergoes rapid development in the first few years after birth.Compared to cross-sectional studies, longitudinal studies can depict the trajectories of infants brain development with higher accuracy, statistical power and flexibility.However, the collection of infant longitudinal magnetic resonance (MR) data suffers a notorious dropout problem, resulting in incomplete datasets with missing time points. This limitation significantly impedes subsequent neuroscience and clinical modeling. Yet, existing deep generative models are facing difficulties in missing brain image completion, due to sparse data and the nonlinear, dramatic contrast/geometric variations in the developing brain. We propose LoCI-DiffCom, a novel Longitudinal Consistency-Informed Diffusion model for infant brain image Completion,which integrates the images from preceding and subsequent time points to guide a diffusion model for generating high-fidelity missing data. Our designed LoCI module can work on highly sparse sequences, relying solely on data from two temporal points. Despite wide separation and diversity between age time points, our approach can extract individualized developmental features while ensuring context-aware consistency. Our experiments on a large infant brain MR dataset demonstrate its effectiveness with consistent performance on missing infant brain MR completion even in big gap scenarios, aiding in better delineation of early developmental trajectories.|http://arxiv.org/abs/2405.10691v1|Zihao Zhu,Tianli Tao,Yitian Tao,Haowen Deng,Xinyi Cai,Gaofeng Wu,Kaidong Wang,Haifeng Tang,Lixuan Zhu,Zhuoyang Gu,Jiawei Huang,Dinggang Shen,Han Zhang
1575|Generative AI Enables EEG Super-Resolution via Spatio-Temporal Adaptive Diffusion Learning|Electroencephalogram (EEG) technology, particularly high-density EEG (HD EEG) devices, are widely used in fields such as neuroscience. HD EEG devices improve the spatial resolution of EEG by placing more electrodes on the scalp, which meet the requirements of clinical diagnostic applications such as epilepsy focus localization. However, this technique faces challenges, such as high acquisition costs and limited usage scenarios. In this paper, spatio-temporal adaptive diffusion models (STAD) are proposed to pioneer the use of diffusion models for achieving spatial SR reconstruction from low-resolution (LR, 64 channels or fewer) EEG to high-resolution (HR, 256 channels) EEG. Specifically, a spatio-temporal condition module is designed to extract the spatio-temporal features of LR EEG, which then used as conditional inputs to direct the reverse denoising process. Additionally, a multi-scale Transformer denoising module is constructed to leverage multi-scale convolution blocks and cross-attention-based diffusion Transformer blocks for conditional guidance to generate subject-adaptive SR EEG. Experimental results demonstrate that the STAD significantly enhances the spatial resolution of LR EEG and quantitatively outperforms existing methods. Furthermore, STAD demonstrate their value by applying synthetic SR EEG to classification and source localization tasks, indicating their potential to Substantially boost the spatial resolution of EEG.|http://arxiv.org/abs/2407.03089v4|Tong Zhou,Shuqiang Wang
1576|QEEGNet: Quantum Machine Learning for Enhanced Electroencephalography Encoding|Electroencephalography (EEG) is a critical tool in neuroscience and clinical practice for monitoring and analyzing brain activity. Traditional neural network models, such as EEGNet, have achieved considerable success in decoding EEG signals but often struggle with the complexity and high dimensionality of the data. Recent advances in quantum computing present new opportunities to enhance machine learning models through quantum machine learning (QML) techniques. In this paper, we introduce Quantum-EEGNet (QEEGNet), a novel hybrid neural network that integrates quantum computing with the classical EEGNet architecture to improve EEG encoding and analysis, as a forward-looking approach, acknowledging that the results might not always surpass traditional methods but it shows its potential. QEEGNet incorporates quantum layers within the neural network, allowing it to capture more intricate patterns in EEG data and potentially offering computational advantages. We evaluate QEEGNet on a benchmark EEG dataset, BCI Competition IV 2a, demonstrating that it consistently outperforms traditional EEGNet on most of the subjects and other robustness to noise. Our results highlight the significant potential of quantum-enhanced neural networks in EEG analysis, suggesting new directions for both research and practical applications in the field.|http://arxiv.org/abs/2407.19214v2|Chi-Sheng Chen,Samuel Yen-Chi Chen,Aidan Hung-Wen Tsai,Chun-Shu Wei
1577|T-FAKE: Synthesizing Thermal Images for Facial Landmarking|Facial analysis is a key component in a wide range of applications such as security, autonomous driving, entertainment, and healthcare. Despite the availability of various facial RGB datasets, the thermal modality, which plays a crucial role in life sciences, medicine, and biometrics, has been largely overlooked. To address this gap, we introduce the T-FAKE dataset, a new large-scale synthetic thermal dataset with sparse and dense landmarks. To facilitate the creation of the dataset, we propose a novel RGB2Thermal loss function, which enables the transfer of thermal style to RGB faces. By utilizing the Wasserstein distance between thermal and RGB patches and the statistical analysis of clinical temperature distributions on faces, we ensure that the generated thermal images closely resemble real samples. Using RGB2Thermal style transfer based on our RGB2Thermal loss function, we create the T-FAKE dataset, a large-scale synthetic thermal dataset of faces. Leveraging our novel T-FAKE dataset, probabilistic landmark prediction, and label adaptation networks, we demonstrate significant improvements in landmark detection methods on thermal images across different landmark conventions. Our models show excellent performance with both sparse 70-point landmarks and dense 478-point landmark annotations. Our code and models are available at https://github.com/phflot/tfake.|http://arxiv.org/abs/2408.15127v2|Philipp Flotho,Moritz Piening,Anna Kukleva,Gabriele Steidl
1578|Reliable Deep Diffusion Tensor Estimation: Rethinking the Power of Data-Driven Optimization Routine|Diffusion tensor imaging (DTI) holds significant importance in clinical diagnosis and neuroscience research. However, conventional model-based fitting methods often suffer from sensitivity to noise, leading to decreased accuracy in estimating DTI parameters. While traditional data-driven deep learning methods have shown potential in terms of accuracy and efficiency, their limited generalization to out-of-training-distribution data impedes their broader application due to the diverse scan protocols used across centers, scanners, and studies. This work aims to tackle these challenges and promote the use of DTI by introducing a data-driven optimization-based method termed DoDTI. DoDTI combines the weighted linear least squares fitting algorithm and regularization by denoising technique. The former fits DW images from diverse acquisition settings into diffusion tensor field, while the latter applies a deep learning-based denoiser to regularize the diffusion tensor field instead of the DW images, which is free from the limitation of fixed-channel assignment of the network. The optimization object is solved using the alternating direction method of multipliers and then unrolled to construct a deep neural network, leveraging a data-driven strategy to learn network parameters. Extensive validation experiments are conducted utilizing both internally simulated datasets and externally obtained in-vivo datasets. The results, encompassing both qualitative and quantitative analyses, showcase that the proposed method attains state-of-the-art performance in DTI parameter estimation. Notably, it demonstrates superior generalization, accuracy, and efficiency, rendering it highly reliable for widespread application in the field.|http://arxiv.org/abs/2409.02492v1|Jialong Li,Zhicheng Zhang,Yunwei Chen,Qiqi Lu,Ye Wu,Xiaoming Liu,QianJin Feng,Yanqiu Feng,Xinyuan Zhang
1579|NT-ViT: Neural Transcoding Vision Transformers for EEG-to-fMRI Synthesis|This paper introduces the Neural Transcoding Vision Transformer (\modelname), a generative model designed to estimate high-resolution functional Magnetic Resonance Imaging (fMRI) samples from simultaneous Electroencephalography (EEG) data. A key feature of \modelname is its Domain Matching (DM) sub-module which effectively aligns the latent EEG representations with those of fMRI volumes, enhancing the model's accuracy and reliability. Unlike previous methods that tend to struggle with fidelity and reproducibility of images, \modelname addresses these challenges by ensuring methodological integrity and higher-quality reconstructions which we showcase through extensive evaluation on two benchmark datasets; \modelname outperforms the current state-of-the-art by a significant margin in both cases, e.g. achieving a $10\times$ reduction in RMSE and a $3.14\times$ increase in SSIM on the Oddball dataset. An ablation study also provides insights into the contribution of each component to the model's overall effectiveness. This development is critical in offering a new approach to lessen the time and financial constraints typically linked with high-resolution brain imaging, thereby aiding in the swift and precise diagnosis of neurological disorders. Although it is not a replacement for actual fMRI but rather a step towards making such imaging more accessible, we believe that it represents a pivotal advancement in clinical practice and neuroscience research. Code is available at \url{https://github.com/rom42pla/ntvit}.|http://arxiv.org/abs/2409.11836v1|Romeo Lanzino,Federico Fontana,Luigi Cinque,Francesco Scarcello,Atsuto Maki
1580|Towards General Text-guided Image Synthesis for Customized Multimodal Brain MRI Generation|Multimodal brain magnetic resonance (MR) imaging is indispensable in neuroscience and neurology. However, due to the accessibility of MRI scanners and their lengthy acquisition time, multimodal MR images are not commonly available. Current MR image synthesis approaches are typically trained on independent datasets for specific tasks, leading to suboptimal performance when applied to novel datasets and tasks. Here, we present TUMSyn, a Text-guided Universal MR image Synthesis generalist model, which can flexibly generate brain MR images with demanded imaging metadata from routinely acquired scans guided by text prompts. To ensure TUMSyn's image synthesis precision, versatility, and generalizability, we first construct a brain MR database comprising 31,407 3D images with 7 MRI modalities from 13 centers. We then pre-train an MRI-specific text encoder using contrastive learning to effectively control MR image synthesis based on text prompts. Extensive experiments on diverse datasets and physician assessments indicate that TUMSyn can generate clinically meaningful MR images with specified imaging metadata in supervised and zero-shot scenarios. Therefore, TUMSyn can be utilized along with acquired MR scan(s) to facilitate large-scale MRI-based screening and diagnosis of brain diseases.|http://arxiv.org/abs/2409.16818v1|Yulin Wang,Honglin Xiong,Kaicong Sun,Shuwei Bai,Ling Dai,Zhongxiang Ding,Jiameng Liu,Qian Wang,Qian Liu,Dinggang Shen
1581|HyperBrain: Anomaly Detection for Temporal Hypergraph Brain Networks|Identifying unusual brain activity is a crucial task in neuroscience research, as it aids in the early detection of brain disorders. It is common to represent brain networks as graphs, and researchers have developed various graph-based machine learning methods for analyzing them. However, the majority of existing graph learning tools for the brain face a combination of the following three key limitations. First, they focus only on pairwise correlations between regions of the brain, limiting their ability to capture synchronized activity among larger groups of regions. Second, they model the brain network as a static network, overlooking the temporal changes in the brain. Third, most are designed only for classifying brain networks as healthy or disordered, lacking the ability to identify abnormal brain activity patterns linked to biomarkers associated with disorders. To address these issues, we present HyperBrain, an unsupervised anomaly detection framework for temporal hypergraph brain networks. HyperBrain models fMRI time series data as temporal hypergraphs capturing dynamic higher-order interactions. It then uses a novel customized temporal walk (BrainWalk) and neural encodings to detect abnormal co-activations among brain regions. We evaluate the performance of HyperBrain in both synthetic and real-world settings for Autism Spectrum Disorder and Attention Deficit Hyperactivity Disorder(ADHD). HyperBrain outperforms all other baselines on detecting abnormal co-activations in brain networks. Furthermore, results obtained from HyperBrain are consistent with clinical research on these brain disorders. Our findings suggest that learning temporal and higher-order connections in the brain provides a promising approach to uncover intricate connectivity patterns in brain networks, offering improved diagnosis.|http://arxiv.org/abs/2410.02087v1|Sadaf Sadeghian,Xiaoxiao Li,Margo Seltzer
1582|Federated Block-Term Tensor Regression for decentralised data analysis in healthcare|Block-Term Tensor Regression (BTTR) has proven to be a powerful tool for modeling complex, high-dimensional data by leveraging multilinear relationships, making it particularly well-suited for applications in healthcare and neuroscience. However, traditional implementations of BTTR rely on centralized datasets, which pose significant privacy risks and hinder collaboration across institutions. To address these challenges, we introduce Federated Block-Term Tensor Regression (FBTTR), an extension of BTTR designed for federated learning scenarios. FBTTR enables decentralized data analysis, allowing institutions to collaboratively build predictive models while preserving data privacy and complying with regulations.   FBTTR represents a major step forward in applying tensor regression to federated learning environments. Its performance is evaluated in two case studies: finger movement decoding from Electrocorticography (ECoG) signals and heart disease prediction. In the first case study, using the BCI Competition IV dataset, FBTTR outperforms non-multilinear models, demonstrating superior accuracy in decoding finger movements. For the dataset, for subject 3, the thumb obtained a performance of 0.76 $\pm$ .05 compared to 0.71 $\pm$ 0.05 for centralised BTTR. In the second case study, FBTTR is applied to predict heart disease using real-world clinical datasets, outperforming both standard federated learning approaches and centralized BTTR models. In the Fed-Heart-Disease Dataset, an AUC-ROC was obtained of 0.872 $\pm$ 0.02 and an accuracy of 0.772 $\pm$ 0.02 compared to 0.812 $\pm$ 0.003 and 0.753 $\pm$ 0.007 for the centralized model.|http://arxiv.org/abs/2412.06815v1|Axel Faes,Ashkan Pirmani,Yves Moreau,Liesbet M. Peeters
1583|Fractal and Multifractal Properties of Electrographic Recordings of Human Brain Activity: Toward Its Use as a Signal Feature for Machine Learning in Clinical Applications|The brain is a system operating on multiple time scales, and characterisation of dynamics across time scales remains a challenge. One framework to study such dynamics is that of fractal geometry. However, currently there exists no established method for the study of brain dynamics using fractal geometry, due to the many challenges in the conceptual and technical understanding of the methods. We aim to highlight some of the practical challenges of applying fractal geometry to brain dynamics and propose solutions to enable its wider use in neuroscience. Using intracranially recorded EEG and simulated data, we compared monofractal and multifractal methods with regards to their sensitivity to signal variance. We found that both correlate closely with signal variance, thus not offering new information about the signal. However, after applying an epoch-wise standardisation procedure to the signal, we found that multifractal measures could offer non-redundant information compared to signal variance, power and other established EEG signal measures. We also compared different multifractal estimation methods and found that the Chhabra-Jensen algorithm performed best. Finally, we investigated the impact of sampling frequency and epoch length on multifractal properties. Using epileptic seizures as an example event in the EEG, we show that there may be an optimal time scale for detecting temporal changes in multifractal properties around seizures. The practical issues we highlighted and our suggested solutions should help in developing a robust method for the application of fractal geometry in EEG signals. Our analyses and observations also aid the theoretical understanding of the multifractal properties of the brain and might provide grounds for new discoveries in the study of brain signals. These could be crucial for understanding of neurological function and for the developments of new treatments.|http://arxiv.org/abs/1806.03889v2|Lucas G. S. Frana,Jos G. V. Miranda,Marco Leite,Niraj K. Sharma,Matthew C. Walker,Louis Lemieux,Yujiang Wang
1584|A Generalizable Method for Automated Quality Control of Functional Neuroimaging Datasets|Over the last twenty five years, advances in the collection and analysis of fMRI data have enabled new insights into the brain basis of human health and disease. Individual behavioral variation can now be visualized at a neural level as patterns of connectivity among brain regions. Functional brain imaging is enhancing our understanding of clinical psychiatric disorders by revealing ties between regional and network abnormalities and psychiatric symptoms. Initial success in this arena has recently motivated collection of larger datasets which are needed to leverage fMRI to generate brain-based biomarkers to support development of precision medicines. Despite methodological advances and enhanced computational power, evaluating the quality of fMRI scans remains a critical step in the analytical framework. Before analysis can be performed, expert reviewers visually inspect raw scans and preprocessed derivatives to determine viability of the data. This Quality Control (QC) process is labor intensive, and the inability to automate at large scale has proven to be a limiting factor in clinical neuroscience fMRI research. We present a novel method for automating the QC of fMRI scans. We train machine learning classifiers using features derived from brain MR images to predict the "quality" of those images, based on the ground truth of an expert's opinion. We emphasize the importance of these classifiers' ability to generalize their predictions across data from different studies. To address this, we propose a novel approach entitled "FMRI preprocessing Log mining for Automated, Generalizable Quality Control" (FLAG-QC), in which features derived from mining runtime logs are used to train the classifier. We show that classifiers trained on FLAG-QC features perform much better (AUC=0.79) than previously proposed feature sets (AUC=0.56) when testing their ability to generalize across studies.|http://arxiv.org/abs/1912.10127v1|Matthew Kollada,Qingzhu Gao,Monika S Mellem,Tathagata Banerjee,William J Martin
1585|More Alike than Different: Quantifying Deviations of Brain Structure and Function in Major Depressive Disorder across Neuroimaging Modalities|Introduction: Identifying neurobiological differences between patients suffering from Major Depressive Disorder (MDD) and healthy individuals has been a mainstay of clinical neuroscience for decades. However, recent meta- and mega-analyses have raised concerns regarding the replicability and clinical relevance of brain alterations in depression. Methods: Here, we systematically investigate healthy controls and MDD patients across a comprehensive range of modalities including structural magnetic resonance imaging (MRI), diffusion tensor imaging, functional task-based and resting-state MRI under near-ideal conditions. To this end, we quantify the upper bounds of univariate effect sizes, predictive utility, and distributional dissimilarity in a fully harmonized cohort of N=1,809 participants. We compare the results to an MDD polygenic risk score (PRS) and environmental variables. Results: The upper bound of the effect sizes range from partial eta squared = .004 to .017, distributions overlap between 89% and 95%, with classification accuracies ranging between 54% and 55% across neuroimaging modalities. This pattern remains virtually unchanged when considering only acutely or chronically depressed patients. Differences are comparable to those found for PRS, but substantially smaller than for environmental variables. Discussion: We provide a large-scale, multimodal analysis of univariate biological differences between MDD patients and controls and show that even under near-ideal conditions and for maximum biological differences, deviations are extremely small and similarity dominates. We sketch an agenda for a new focus of future research in biological psychiatry facilitating quantitative, theory-driven research, an emphasis on computational psychiatry and multivariate machine learning approaches, as well as the utilization of ecologically valid phenotyping.|http://arxiv.org/abs/2112.10730v1|Nils R. Winter,Ramona Leenings,Jan Ernsting,Kelvin Sarink,Lukas Fisch,Daniel Emden,Julian Blanke,Janik Goltermann,Nils Opel,Carlotta Barkhau,Susanne Meinert,Katharina Dohm,Jonathan Repple,Marco Mauritz,Marius Gruber,Elisabeth J. Leehr,Dominik Grotegerd,Ronny Redlich,Andreas Jansen,Igor Nenadic,Markus Nthen,Andreas Forstner,Marcella Rietschel,Joachim Gro,Jochen Bauer,Walter Heindel,Till Andlauer,Simon Eickhoff,Tilo Kircher,Udo Dannlowski,Tim Hahn
1586|Decoding the human brain tissue response to radiofrequency excitation using a biophysical-model-free deep MRI on a chip framework|Magnetic resonance imaging (MRI) relies on radiofrequency (RF) excitation of proton spin. Clinical diagnosis requires a comprehensive collation of biophysical data via multiple MRI contrasts, acquired using a series of RF sequences that lead to lengthy examinations. Here, we developed a vision transformer-based framework that captures the spatiotemporal magnetic signal evolution and decodes the brain tissue response to RF excitation, constituting an MRI on a chip. Following a per-subject rapid calibration scan (28.2 s), a wide variety of image contrasts including fully quantitative molecular, water relaxation, and magnetic field maps can be generated automatically. The method was validated across healthy subjects and a cancer patient in two different imaging sites, and proved to be 94% faster than alternative protocols. The deep MRI on a chip (DeepMonC) framework may reveal the molecular composition of the human brain tissue in a wide range of pathologies, while offering clinically attractive scan times.|http://arxiv.org/abs/2408.08376v2|Dinor Nagar,Moritz Zaiss,Or Perlman
1587|Teaching Computational Neuroscience|The problems and beauty of teaching computational neuroscience are discussed by reviewing three new textbooks.|http://arxiv.org/abs/1412.5909v2|Pter rdi
1588|FaBiAN: A Fetal Brain magnetic resonance Acquisition Numerical phantom|Accurate characterization of in utero human brain maturation is critical as it involves complex and interconnected structural and functional processes that may influence health later in life. Magnetic resonance imaging is a powerful tool to investigate equivocal neurological patterns during fetal development. However, the number of acquisitions of satisfactory quality available in this cohort of sensitive subjects remains scarce, thus hindering the validation of advanced image processing techniques. Numerical phantoms can mitigate these limitations by providing a controlled environment with a known ground truth. In this work, we present FaBiAN, an open-source Fetal Brain magnetic resonance Acquisition Numerical phantom that simulates clinical T2-weighted fast spin echo sequences of the fetal brain. This unique tool is based on a general, flexible and realistic setup that includes stochastic fetal movements, thus providing images of the fetal brain throughout maturation comparable to clinical acquisitions. We demonstrate its value to evaluate the robustness and optimize the accuracy of an algorithm for super-resolution fetal brain magnetic resonance imaging from simulated motion-corrupted 2D low-resolution series as compared to a synthetic high-resolution reference volume. We also show that the images generated can complement clinical datasets to support data-intensive deep learning methods for fetal brain tissue segmentation.|http://arxiv.org/abs/2109.03624v1|Hlne Lajous,Christopher W. Roy,Tom Hilbert,Priscille de Dumast,Sbastien Tourbier,Yasser Alemn-Gmez,Jrme Yerly,Thomas Yu,Hamza Kebiri,Kelly Payette,Jean-Baptiste Ledoux,Reto Meuli,Patric Hagmann,Andras Jakab,Vincent Dunet,Mriam Koob,Tobias Kober,Matthias Stuber,Meritxell Bach Cuadra
1589|Tailored Patient Information: Some Issues and Questions|Tailored patient information (TPI) systems are computer programs which produce personalised heath-information material for patients. TPI systems are of growing interest to the natural-language generation (NLG) community; many TPI systems have also been developed in the medical community, usually with mail-merge technology. No matter what technology is used, experience shows that it is not easy to field a TPI system, even if it is shown to be effective in clinical trials. In this paper we discuss some of the difficulties in fielding TPI systems. This is based on our experiences with 2 TPI systems, one for generating asthma-information booklets and one for generating smoking-cessation letters.|http://arxiv.org/abs/cmp-lg/9707007v1|Ehud Reiter,Liesl Osman
1590|Asymptotic theorems of sequential estimation-adjusted urn models|The Generalized P\'{o}lya Urn (GPU) is a popular urn model which is widely used in many disciplines. In particular, it is extensively used in treatment allocation schemes in clinical trials. In this paper, we propose a sequential estimation-adjusted urn model (a nonhomogeneous GPU) which has a wide spectrum of applications. Because the proposed urn model depends on sequential estimations of unknown parameters, the derivation of asymptotic properties is mathematically intricate and the corresponding results are unavailable in the literature. We overcome these hurdles and establish the strong consistency and asymptotic normality for both the patient allocation and the estimators of unknown parameters, under some widely satisfied conditions. These properties are important for statistical inferences and they are also useful for the understanding of the urn limiting process. A superior feature of our proposed model is its capability to yield limiting treatment proportions according to any desired allocation target. The applicability of our model is illustrated with a number of examples.|http://arxiv.org/abs/math/0603329v1|Li-X. Zhang,Feifang Hu,Siu Hung Cheung
1591|Bayesian transformation hazard models|We propose a class of transformation hazard models for right-censored failure time data. It includes the proportional hazards model (Cox) and the additive hazards model (Lin and Ying) as special cases. Due to the requirement of a nonnegative hazard function, multidimensional parameter constraints must be imposed in the model formulation. In the Bayesian paradigm, the nonlinear parameter constraint introduces many new computational challenges. We propose a prior through a conditional-marginal specification, in which the conditional distribution is univariate, and absorbs all of the nonlinear parameter constraints. The marginal part of the prior specification is free of any constraints. This class of prior distributions allows us to easily compute the full conditionals needed for Gibbs sampling, and hence implement the Markov chain Monte Carlo algorithm in a relatively straightforward fashion. Model comparison is based on the conditional predictive ordinate and the deviance information criterion. This new class of models is illustrated with a simulation study and a real dataset from a melanoma clinical trial.|http://arxiv.org/abs/math/0611164v1|Gousheng Yin,Joseph G. Ibrahim
1592|First human trials of a dry electrophysiology sensor using a carbon nanotube array interface|Fatigue, sleepiness and disturbed sleep are important factors in health and safety in modern society and there is considerable interest in developing technologies for routine monitoring of associated physiological indicators. Electrophysiology, the measurement of the electrical activity of biological origin, is a key technique for the measurement of physiological parameters in several applications, but it has been traditionally difficult to develop sensors for measurements outside the laboratory or clinic with the required quality and robustness. In this paper we report the results from first human experiments using a new electrophysiology sensor called ENOBIO, using carbon nanotube arrays for penetration of the outer layers of the skin and improved electrical contact. These tests, which have included traditional protocols for the analysis of the electrical activity of the brain--spontaneous EEG and ERP--indicate performance on a par with state of the art research-oriented wet electrodes, suggesting that the envisioned mechanism--skin penetration--is responsible. No ill side-effects have been observed six months after the tests, and the subject did not report any pain or special sensations on application of the electrode.|http://arxiv.org/abs/physics/0701159v1|G. Ruffini,S. Dunne,L. Fuentemilla,C. Grau,E. Farres,J. Marco-Pallares,P. C. P. Watts,S. R. P. Silva
1593|Cell Cycling Models of Carcinogenesis: A Complex Systems Analysis|A new approach to the modular, complex systems analysis of nonlinear dynamics in cell cycling network transformations involved in carcinogenesis is proposed. Carcinogenesis is a complex process that involves dynamically inter-connected biomolecules in the intercellular, membrane, cytosolic, nuclear and nucleolar compartments that form numerous inter-related pathways. One such family of pathways contains the cell cyclins. Cyclins are proteins that link several critical pro-apoptotic and other cell cycling/division components, including the tumor suppressor gene TP53 and its product, the Thomsen-Friedenreich antigen (T antigen), Rb, mdm2, c-Myc, p21, p27, Bax, Bad and Bcl-2, which all play major roles in neoplastic transformation of many tissues. This novel theoretical analysis based on recently published studies of cyclin signaling, with special emphasis placed on the roles of cyclins D1 and E, suggests novel clinical trials and rational therapies of cancer through reestablishment of cell cycling inhibition in metastatic cancer cells.|http://arxiv.org/abs/q-bio/0406046v1|V. I. Prisecaru,I. C. Baianu
1594|Quantum noise influencing human behaviour could fake effectiveness of drugs in clinical trials|To test the effectiveness of a drug one can advice two randomly selected groups of patients to take or not to take it, respectively. It is well-known that the causal effect cannot be identified if not all patients comply. This holds even when the non-compliers can be identified afterwards since latent factors like patient's personality can influence both his decision and his physical response. However, one can still give bounds on the effectiveness of the drug depending on the rate of compliance. Remarkably, the proofs of these bounds given in the literature rely on models that represent all relevant latent factors (including noise) by hidden classical variables. In strong analogy to the violation of Bell's inequality, some of these bounds fail if patient's behavior is influenced by latent quantum processes (e.g. in his nervous system). Quantum effects could fake an increase of the recovery rate by about 13% although the drug would hurt as many patients as it would help if everyone took it. The other bounds are true even in the quantum case.   We do not present any realistic model showing this effect, we only point out that the physics of decision making could be relevant for the causal interpretation of every-day life statistical data.|http://arxiv.org/abs/quant-ph/0208006v1|Dominik Janzing,Thomas Beth
1595|On Biology as an Emergent Science|Biology is considered here as an "emergent science" in the sense of Anderson and of Laughlin and Pines. It is demonstrated that a straightforward mathematical definition of "biological system" is useful in showing how biology differs in structure from the lower levels in Anderson's "More is Different" hierarchy. Using cells in a chemostat as a paradigmatic exemplar of a biological system, it is found that a coherent collection of metabolic pathways through a single cell in the chemostat also satisfies the proposed definition of a biological system. This provides a theoretical and mathematical underpinning for Young's fundamental model of biological organization and integration. Evidence for the therapeutic efficacy of Young's method of analysis is provided by preliminary results of clinical trials of a specific application of Young's model to the treatment of cancer cachexia.|http://arxiv.org/abs/0705.4678v2|H. Pierre Noyes
1596|Neural networks in 3D medical scan visualization|For medical volume visualization, one of the most important tasks is to reveal clinically relevant details from the 3D scan (CT, MRI ...), e.g. the coronary arteries, without obscuring them with less significant parts. These volume datasets contain different materials which are difficult to extract and visualize with 1D transfer functions based solely on the attenuation coefficient. Multi-dimensional transfer functions allow a much more precise classification of data which makes it easier to separate different surfaces from each other. Unfortunately, setting up multi-dimensional transfer functions can become a fairly complex task, generally accomplished by trial and error. This paper explains neural networks, and then presents an efficient way to speed up visualization process by semi-automatic transfer function generation. We describe how to use neural networks to detect distinctive features shown in the 2D histogram of the volume data and how to use this information for data classification.|http://arxiv.org/abs/0806.2925v2|Denan Zuki,Andreas Elsner,Zikrija Avdagi,Gitta Domik
1597|The Gaussian approximation for multi-color generalized Friedman's urn model|The Friedman's urn model is a popular urn model which is widely used in many disciplines. In particular, it is extensively used in treatment allocation schemes in clinical trials. In this paper, we prove that both the urn composition process and the allocation proportion process can be approximated by a multi-dimensional Gaussian process almost surely for a multi-color generalized Friedman's urn model with non-homogeneous generating matrices. The Gaussian process is a solution of a stochastic differential equation. This Gaussian approximation together with the properties of the Gaussian process is important for the understanding of the behavior of the urn process and is also useful for statistical inferences. As an application, we obtain the asymptotic properties including the asymptotic normality and the law of the iterated logarithm for a multi-color generalized Friedman's urn model as well as the randomized-play-the-winner rule as a special case.|http://arxiv.org/abs/0812.3697v1|Li-Xin Zhang,Feifang Hu
1598|Inference for censored quantile regression models in longitudinal studies|We develop inference procedures for longitudinal data where some of the measurements are censored by fixed constants. We consider a semi-parametric quantile regression model that makes no distributional assumptions. Our research is motivated by the lack of proper inference procedures for data from biomedical studies where measurements are censored due to a fixed quantification limit. In such studies the focus is often on testing hypotheses about treatment equality. To this end, we propose a rank score test for large sample inference on a subset of the covariates. We demonstrate the importance of accounting for both censoring and intra-subject dependency and evaluate the performance of our proposed methodology in a simulation study. We then apply the proposed inference procedures to data from an AIDS-related clinical trial. We conclude that our framework and proposed methodology is very valuable for differentiating the influences of predictors at different locations in the conditional distribution of a response variable.|http://arxiv.org/abs/0904.0080v1|Huixia Judy Wang,Mendel Fygenson
1599|Spatio-Temporal Structuring of Brain Activity - Description of Interictal EEG in Paediatric Frontal Lobe Epilepsy|A method for the quantitative assessment of spatio-temporal structuring of brain activity is presented. This approach is employed in a longitudinal case study of a child with frontal lobe epilepsy (FLE) and tested against an age-matched control group. Several correlation measures that are sensitive to linear and/or non-linear relations in multichannel scalp EEG are combined with an hierarchical cluster algorithm. Beside a quantitative description of the overall degree of synchronization the spatial relations are investigated by means of the cluster characteristics. The chosen information measures not only demonstrate their suitability in the characterization of the ictal and interictal phases but they also follow the course of delayed recovery of the psychiatric symptomatology during successful medication. The results based on this single case study suggest testing this approach for quantitative control of therapy in an extended clinical trial.|http://arxiv.org/abs/0905.3911v1|W. Bunk,T. Aschenbrenner,G. Kluger,S. Springer
1600|Nonparametric Methodology for the Time-Dependent Partial Area under the ROC Curve|To assess the classification accuracy of a continuous diagnostic result, the receiver operating characteristic (ROC) curve is commonly used in applications. The partial area under the ROC curve (pAUC) is one of widely accepted summary measures due to its generality and ease of probability interpretation. In the field of life science, a direct extension of the pAUC into the time-to-event setting can be used to measure the usefulness of a biomarker for disease detection over time. Without using a trapezoidal rule, we propose nonparametric estimators, which are easily computed and have closed-form expressions, for the time-dependent pAUC. The asymptotic Gaussian processes of the estimators are established and the estimated variance-covariance functions are provided, which are essential in the construction of confidence intervals. The finite sample performance of the proposed inference procedures are investigated through a series of simulations. Our method is further applied to evaluate the classification ability of CD4 cell counts on patient's survival time in the AIDS Clinical Trials Group (ACTG) 175 study. In addition, the inferences can be generalized to compare the time-dependent pAUCs between patients received the prior antiretroviral therapy and those without it.|http://arxiv.org/abs/1103.1963v1|Hung Hung,Chin-Tsang Chiang
1601|Performance guarantees for individualized treatment rules|Because many illnesses show heterogeneous response to treatment, there is increasing interest in individualizing treatment to patients [Arch. Gen. Psychiatry 66 (2009) 128--133]. An individualized treatment rule is a decision rule that recommends treatment according to patient characteristics. We consider the use of clinical trial data in the construction of an individualized treatment rule leading to highest mean response. This is a difficult computational problem because the objective function is the expectation of a weighted indicator function that is nonconcave in the parameters. Furthermore, there are frequently many pretreatment variables that may or may not be useful in constructing an optimal individualized treatment rule, yet cost and interpretability considerations imply that only a few variables should be used by the individualized treatment rule. To address these challenges, we consider estimation based on $l_1$-penalized least squares. This approach is justified via a finite sample upper bound on the difference between the mean response due to the estimated individualized treatment rule and the mean response due to the optimal individualized treatment rule.|http://arxiv.org/abs/1105.3369v1|Min Qian,Susan A. Murphy
1602|Functional Uniform Priors for Nonlinear Modelling|This paper considers the topic of finding prior distributions when a major component of the statistical model depends on a nonlinear function. Using results on how to construct uniform distributions in general metric spaces, we propose a prior distribution that is uniform in the space of functional shapes of the underlying nonlinear function and then back-transform to obtain a prior distribution for the original model parameters. The primary application considered in this article is nonlinear regression, but the idea might be of interest beyond this case. For nonlinear regression the so constructed priors have the advantage that they are parametrization invariant and do not violate the likelihood principle, as opposed to uniform distributions on the parameters or the Jeffrey's prior, respectively. The utility of the proposed priors is demonstrated in the context of nonlinear regression modelling in clinical dose-finding trials, through a real data example and simulation. In addition the proposed priors are used for calculation of an optimal Bayesian design.|http://arxiv.org/abs/1110.4400v1|Bjrn Bornkamp
1603|Sequential monitoring with conditional randomization tests|Sequential monitoring in clinical trials is often employed to allow for early stopping and other interim decisions, while maintaining the type I error rate. However, sequential monitoring is typically described only in the context of a population model. We describe a computational method to implement sequential monitoring in a randomization-based context. In particular, we discuss a new technique for the computation of approximate conditional tests following restricted randomization procedures and then apply this technique to approximate the joint distribution of sequentially computed conditional randomization tests. We also describe the computation of a randomization-based analog of the information fraction. We apply these techniques to a restricted randomization procedure, Efron's [Biometrika 58 (1971) 403--417] biased coin design. These techniques require derivation of certain conditional probabilities and conditional covariances of the randomization procedure. We employ combinatoric techniques to derive these for the biased coin design.|http://arxiv.org/abs/1205.6043v1|Victoria Plamadeala,William F. Rosenberger
1604|Multi-objective optimal designs in comparative clinical trials with covariates: The reinforced doubly adaptive biased coin design|The present paper deals with the problem of allocating patients to two competing treatments in the presence of covariates or prognostic factors in order to achieve a good trade-off among ethical concerns, inferential precision and randomness in the treatment allocations. In particular we suggest a multipurpose design methodology that combines efficiency and ethical gain when the linear homoscedastic model with both treatment/covariate interactions and interactions among covariates is adopted. The ensuing compound optimal allocations of the treatments depend on the covariates and their distribution on the population of interest, as well as on the unknown parameters of the model. Therefore, we introduce the reinforced doubly adaptive biased coin design, namely a general class of covariate-adjusted response-adaptive procedures that includes both continuous and discontinuous randomization functions, aimed to target any desired allocation proportion. The properties of this proposal are described both theoretically and through simulations.|http://arxiv.org/abs/1206.0576v2|Alessandro Baldi Antognini,Maroussa Zagoraiou
1605|A Dual-Beam Irradiation Facility for a Novel Hybrid Cancer Therapy|In this paper we present the main ideas and discuss both the feasibility and the conceptual design of a novel hybrid technique and equipment for an experimental cancer therapy based on the simultaneous and/or sequential application of two beams, namely a beam of neutrons and a CW (continuous wave) or intermittent sub-terahertz wave beam produced by a gyrotron for treatment of cancerous tumors. The main simulation tools for the development of the computer aided design (CAD) of the prospective experimental facility for clinical trials and study of such new medical technology are briefly reviewed. Some tasks for a further continuation of this feasibility analysis are formulated as well.|http://arxiv.org/abs/1206.4840v1|Svilen Sabchevski,Toshitaka Idehara,Shintaro Ishiyama,Norio Miyoshi,Toshiaki Tatsukawa
1606|Evidence for early identification of Alzheimer's disease|Alzheimer's disease is a human brain disease that affects a significant fraction of the population by causing problems with short-term memory, thinking, spatial orientation and behavior, memory loss and other intellectual abilities. Up to date there is no singular test that can definitively diagnose Alzheimer's disease, although imaging technology designed to detect Alzheimer's plaques and tangles is rapidly becoming more powerful and precise. In this paper we introduce a decision-making model, based on the combination of mitochondrial hypothesis-dynamics with the role of electromagnetic influences of the metal ions into the inner mitochondrial membrane and the quantitative analysis of mitochondrial population. While there are few disappointing clinical-trial results for drug treatments in patients with Alzheimer's disease, scientific community need alternative diagnostic tools rather investing mainly in amyloid-targeting drugs.|http://arxiv.org/abs/1209.4223v2|Athanasios Alexiou,Panayiotis Vlamos
1607|Asymptotic properties of covariate-adaptive randomization|Balancing treatment allocation for influential covariates is critical in clinical trials. This has become increasingly important as more and more biomarkers are found to be associated with different diseases in translational research (genomics, proteomics and metabolomics). Stratified permuted block randomization and minimization methods [Pocock and Simon Biometrics 31 (1975) 103-115, etc.] are the two most popular approaches in practice. However, stratified permuted block randomization fails to achieve good overall balance when the number of strata is large, whereas traditional minimization methods also suffer from the potential drawback of large within-stratum imbalances. Moreover, the theoretical bases of minimization methods remain largely elusive. In this paper, we propose a new covariate-adaptive design that is able to control various types of imbalances. We show that the joint process of within-stratum imbalances is a positive recurrent Markov chain under certain conditions. Therefore, this new procedure yields more balanced allocation. The advantages of the proposed procedure are also demonstrated by extensive simulation studies. Our work provides a theoretical tool for future research in this area.|http://arxiv.org/abs/1210.4666v1|Yanqing Hu,Feifang Hu
1608|Exact Methods for Multistage Estimation of a Binomial Proportion|We first review existing sequential methods for estimating a binomial proportion. Afterward, we propose a new family of group sequential sampling schemes for estimating a binomial proportion with prescribed margin of error and confidence level. In particular, we establish the uniform controllability of coverage probability and the asymptotic optimality for such a family of sampling schemes. Our theoretical results establish the possibility that the parameters of this family of sampling schemes can be determined so that the prescribed level of confidence is guaranteed with little waste of samples. Analytic bounds for the cumulative distribution functions and expectations of sample numbers are derived. Moreover, we discuss the inherent connection of various sampling schemes. Numerical issues are addressed for improving the accuracy and efficiency of computation. Computational experiments are conducted for comparing sampling schemes. Illustrative examples are given for applications in clinical trials.|http://arxiv.org/abs/1302.3447v1|Zhengjia Chen,Xinjia Chen
1609|End-User Construction of Influence Diagrams for Bayesian Statistics|Influence diagrams are ideal knowledge representations for Bayesian statistical models. However, these diagrams are difficult for end users to interpret and to manipulate. We present a user-based architecture that enables end users to create and to manipulate the knowledge representation. We use the problem of physicians' interpretation of two-arm parallel randomized clinical trials (TAPRCT) to illustrate the architecture and its use. There are three primary data structures. Elements of statistical models are encoded as subgraphs of a restricted class of influence diagram. The interpretations of those elements are mapped into users' language in a domain-specific, user-based semantic interface, called a patient-flow diagram, in the TAPRCT problem. Pennitted transformations of the statistical model that maintain the semantic relationships of the model are encoded in a metadata-state diagram, called the cohort-state diagram, in the TAPRCT problem. The algorithm that runs the system uses modular actions called construction steps. This framework has been implemented in a system called THOMAS, that allows physicians to interpret the data reported from a TAPRCT.|http://arxiv.org/abs/1303.1459v1|Harold P. Lehmann,Ross D. Shachter
1610|Cost Effectiveness Statistic: A Proposal To Take Into Account The Patient Stratification Factors|The solution here proposed can be used to conduct economic analysis in randomized clinical trials. It is based on a statistical approach and aims at calculating a revised version of the incremental costeffective ratio (ICER) in order to take into account the key factors that can influence the choice of therapy causing confounding by indication. Let us take as an example a new therapy to treat cancer being compared to an existing therapy with effectiveness taken as time to death. A challenging problem is that the ICER is defined in terms of means over the entire treatment groups. It makes no provision for stratification by groups of patients with differing risk of death. For example, for a fair and unbiased analysis, one would desire to compare time to death in groups with similar life expectancy which would be impacted by factors such as age, gender, disease severity, etc. The method we decided to apply is borrowed by cluster analysis and aims at (i) discard any outliers in the set under analysis that may arise, (ii) identify groups (i.e. clusters) of patients with "similar" key factors.|http://arxiv.org/abs/1306.3927v2|C. D'Urso
1611|Identifying Combinatorial Biomarkers by Association Rule Mining in the CAMD Alzheimer's Database|Background: The concept of combinatorial biomarkers was conceived around 2010: it was noticed that simple biomarkers are often inadequate for recognizing and characterizing complex diseases.   Methods: Here we present an algorithmic search method for complex biomarkers which may predict or indicate Alzheimer's disease (AD) and other kinds of dementia. We applied data mining techniques that are capable to uncover implication-like logical schemes with detailed quality scoring. Our program SCARF is capable of finding multi-factor relevant association rules automatically. The new SCARF program was applied for the Tucson, Arizona based Critical Path Institute's CAMD database, containing laboratory and cognitive test data for more than 6000 patients from the placebo arm of clinical trials of large pharmaceutical companies, and consequently, the data is much more reliable than numerous other databases for dementia.   Results: The results suggest connections between liver enzyme-, B12 vitamin-, sodium- and cholesterol levels and dementia, and also some hematologic parameter-levels and dementia.|http://arxiv.org/abs/1312.1876v1|Balazs Szalkai,Vince K. Grolmusz,Vince I. Grolmusz,Coalition Against Major Diseases
1612|Identifiability of Subgroup Causal Effects in Randomized Experiments with Nonignorable Missing Covariates|Although randomized experiments are widely regarded as the gold standard for estimating causal effects, missing data of the pretreatment covariates makes it challenging to estimate the subgroup causal effects. When the missing data mechanism of the covariates is nonignorable, the parameters of interest are generally not pointly identifiable, and we can only get bounds for the parameters of interest, which may be too wide for practical use. In some real cases, we have prior knowledge that some restrictions may be plausible. We show the identifiability of the causal effects and joint distributions for four interpretable missing data mechanisms, and evaluate the performance of the statistical inference via simulation studies. One application of our methods to a real data set from a randomized clinical trial shows that one of the nonignorable missing data mechanisms fits better than the ignorable missing data mechanism, and the results conform to the study's original expert opinions. We also illustrate the potential applications of our methods to observational studies using a data set from a job-training program.|http://arxiv.org/abs/1401.1264v1|Peng Ding,Zhi Geng
1613|Bayesian data augmentation dose finding with continual reassessment method and delayed toxicity|A major practical impediment when implementing adaptive dose-finding designs is that the toxicity outcome used by the decision rules may not be observed shortly after the initiation of the treatment. To address this issue, we propose the data augmentation continual reassessment method (DA-CRM) for dose finding. By naturally treating the unobserved toxicities as missing data, we show that such missing data are nonignorable in the sense that the missingness depends on the unobserved outcomes. The Bayesian data augmentation approach is used to sample both the missing data and model parameters from their posterior full conditional distributions. We evaluate the performance of the DA-CRM through extensive simulation studies and also compare it with other existing methods. The results show that the proposed design satisfactorily resolves the issues related to late-onset toxicities and possesses desirable operating characteristics: treating patients more safely and also selecting the maximum tolerated dose with a higher probability. The new DA-CRM is illustrated with two phase I cancer clinical trials.|http://arxiv.org/abs/1401.1706v1|Suyu Liu,Guosheng Yin,Ying Yuan
1614|SurvRegCensCov: Weibull Regression for a Right-Censored Endpoint with a Censored Covariate|Biomarker data is often subject to limits of quantification and/or limits of detection. Statistically, this corresponds to left- or interval-censoring. To be able to associate a censored time-to-event endpoint to a biomarker covariate, the R package SurvRegCensCov provides software for Weibull regression for a right-censored endpoint, one interval-censored, and an arbitrary number of non-censored covariates. Furthermore, the package provides functions to estimate canonical parameters from censored samples based on several distributional assumptions, and a function to switch between different parametrizations used in R for Weibull regression. We illustrate the new software by applying it to assess Prentice's criteria for surrogacy in data simulated from a randomized clinical registration trial.|http://arxiv.org/abs/1402.0432v2|Stanislas Hubeaux,Kaspar Rufibach
1615|Selecting a Biased-Coin Design|Biased-coin designs are used in clinical trials to allocate treatments with some randomness while maintaining approximately equal allocation. More recent rules are compared with Efron's [Biometrika 58 (1971) 403-417] biased-coin rule and extended to allow balance over covariates. The main properties are loss of information, due to imbalance, and selection bias. Theoretical results, mostly large sample, are assembled and assessed by small-sample simulations. The properties of the rules fall into three clear categories. A Bayesian rule is shown to have appealing properties; at the cost of slight imbalance, bias is virtually eliminated for large samples.|http://arxiv.org/abs/1405.5051v1|Anthony C. Atkinson
1616|Sharp bounds on the variance in randomized experiments|We propose a consistent estimator of sharp bounds on the variance of the difference-in-means estimator in completely randomized experiments. Generalizing Robins [Stat. Med. 7 (1988) 773-785], our results resolve a well-known identification problem in causal inference posed by Neyman [Statist. Sci. 5 (1990) 465-472. Reprint of the original 1923 paper]. A practical implication of our results is that the upper bound estimator facilitates the asymptotically narrowest conservative Wald-type confidence intervals, with applications in randomized controlled and clinical trials.|http://arxiv.org/abs/1405.6555v1|Peter M. Aronow,Donald P. Green,Donald K. K. Lee
1617|Accounting for parameter uncertainty in two-stage designs for Phase II dose-response studies|In this paper we consider two-stage adaptive dose-response study designs, where the study design is changed at an interim analysis based on the information collected so far. In a simulation study, two approaches will be compared for these type of designs; (i) updating the study design by calculating the maximum likelihood estimate for the dose-response model parameters and then calculating the design for the second stage that is locally optimal for this estimate, and (ii) using the complete posterior distribution of the model parameter at interim to calculate a Bayesian optimal design (i.e. taking into account parameter uncertainty). In particular, for an early interim analysis respecting parameter uncertainty seems more adequate, on the other hand for a Bayesian approach dependency on the prior is expected and an adequately thought-through prior is required. A computationally efficient method is proposed for calculating the Bayesian design at interim based on approximating the full posterior sample using k-means clustering. The sigmoid Emax dose-response model and the D-optimality criterion will be used in this paper.|http://arxiv.org/abs/1408.0534v2|Emma McCallum,Bjrn Bornkamp
1618|It is hard to see a needle in a haystack: Modeling contrast masking effect in a numerical observer|Within the framework of a virtual clinical trial for breast imaging, we aim to develop numerical observers that follow the same detection performance trends as those of a typical human observer. In our prior work, we showed that by including spatiotemporal contrast sensitivity function (stCSF) of human visual system (HVS) in a multi-slice channelized Hotelling observer (msCHO), we can correctly predict trends of a typical human observer performance with the viewing parameters of browsing speed, viewing distance and contrast. In this work we further improve our numerical observer by modeling contrast masking. After stCSF, contrast masking is the second most prominent property of HVS and it refers to the fact that the presence of one signal affects the visibility threshold for another signal. Our results indicate that the improved numerical observer better predicts changes in detection performance with background complexity.|http://arxiv.org/abs/1408.1135v1|Ali R. N. Avanaki,Kathryn S. Espig,Albert Xthona,Tom R. L. Kimpe,Predrag R. Bakic,Andrew D. A. Maidment
1619|R Package multgee: A Generalized Estimating Equations Solver for Multinomial Responses|The R package multgee implements the local odds ratios generalized estimating equations (GEE) approach proposed by Touloumis et al. (2013), a GEE approach for correlated multinomial responses that circumvents theoretical and practical limitations of the GEE method. A main strength of multgee is that it provides GEE routines for both ordinal (ordLORgee) and nominal (nomLORgee) responses, while relevant softwares in R and SAS are restricted to ordinal responses under a marginal cumulative link model specification. In addition, multgee offers a marginal adjacent categories logit model for ordinal responses and a marginal baseline category logit model for nominal. Further, utility functions are available to ease the local odds ratios structure selection (intrinsic.pars) and to perform a Wald type goodness-of-fit test between two nested GEE models (waldts). We demonstrate the application of multgee through a clinical trial with clustered ordinal multinomial responses.|http://arxiv.org/abs/1410.5232v3|Anestis Touloumis
1620|A stochastic evolutionary model for capturing human dynamics|The recent interest in human dynamics has led researchers to investigate the stochastic processes that explain human behaviour in various contexts. Here we propose a generative model to capture the dynamics of survival analysis, traditionally employed in clinical trials and reliability analysis in engineering. We derive a general solution for the model in the form of a product, and then a continuous approximation to the solution via the renewal equation describing age-structured population dynamics. This enables us to model a wide range of survival distributions, according to the choice of the mortality distribution. We provide empirical evidence for the validity of the model from a longitudinal data set of popular search engine queries over 114 months, showing that the survival function of these queries is closely matched by the solution for our model with power-law mortality.|http://arxiv.org/abs/1502.07558v3|Trevor Fenner,Mark Levene,George Loizou
1621|On the physical interpretation of a meta-analysis in the presence of heterogeneity and bias: from clinical trials to Mendelian randomization|The funnel plot is a graphical visualisation of summary data estimates from a meta-analysis, and is a useful tool for detecting departures from the standard modelling assumptions. Although perhaps not widely appreciated, a simple extension of the funnel plot can help to facilitate an intuitive interpretation of the mathematics underlying a meta-analysis at a more fundamental level, by equating it to determining the centre of mass of a physical system. We used this analogy, with some success, to explain the concepts of weighing evidence and of biased evidence to a young audience at the Cambridge Science Festival, without recourse to precise definitions or statistical formulae. In this paper we aim to formalise this analogy at a more technical level using the estimating equation framework: firstly, to help elucidate some of the basic statistical models employed in a meta-analysis and secondly, to forge new connections between bias adjustment in the evidence synthesis and causal inference literatures.|http://arxiv.org/abs/1508.03768v1|Jack Bowden,Chris Jackson
1622|A Heteroscedastic Accelerated Failure Time Model for Survival Analysis|Nonparametric and semiparametric methods are commonly used in survival analysis to mitigate the bias due to model misspecification. However, such methods often cannot estimate upper-tail survival quantiles when a sizable proportion of the data are censored, in which case parametric likelihood-based estimators present a viable alternative. In this article, we extend a popular family of parametric survival models which make the Accelerated Failure Time (AFT) assumption to account for heteroscedasticity in the survival times. The conditional variances can depend on arbitrary covariates, thus adding considerable flexibility to the homoscedastic model. We present an Expectation-Conditional-Maximization (ECM) algorithm to efficiently compute the HAFT maximum likelihood estimator with right-censored data. The methodology is applied to the heavily censored data from a colon cancer clinical trial, for which a new type of highly stringent model residuals is proposed. Based on these, the HAFT model was found to eliminate most outliers from its homoscedastic counterpart.|http://arxiv.org/abs/1508.05137v2|Yifan Wang,Tian You,Martin Lysy
1623|Modeling Long-term Outcomes and Treatment Effects After Androgen Deprivation Therapy for Prostate Cancer|Analyzing outcomes in long-term cancer survivor studies can be complex. The effects of predictors on the failure process may be difficult to assess over longer periods of time, as the commonly used assumption of proportionality of hazards holding over an extended period is often questionable. In this manuscript, we compare seven different survival models that estimate the hazard rate and the effects of proportional and non-proportional covariates. In particular, we focus on an extension of the the multi-resolution hazard (MRH) estimator, combining a non-proportional hierarchical MRH approach with a data-driven pruning algorithm that allows for computational efficiency and produces robust estimates even in times of few observed failures. Using data from a large-scale randomized prostate cancer clinical trial, we examine patterns of biochemical failure and estimate the time-varying effects of androgen deprivation therapy treatment and other covariates. We compare the impact of different modeling strategies and smoothness assumptions on the estimated treatment effect. Our results show that the benefits of treatment diminish over time, possibly with implications for future treatment protocols.|http://arxiv.org/abs/1509.01275v1|Yolanda Hagar,James J. Dignam,Vanja Dukic
1624|Algorithms for Differentially Private Multi-Armed Bandits|We present differentially private algorithms for the stochastic Multi-Armed Bandit (MAB) problem. This is a problem for applications such as adaptive clinical trials, experiment design, and user-targeted advertising where private information is connected to individual rewards. Our major contribution is to show that there exist $(\epsilon, \delta)$ differentially private variants of Upper Confidence Bound algorithms which have optimal regret, $O(\epsilon^{-1} + \log T)$. This is a significant improvement over previous results, which only achieve poly-log regret $O(\epsilon^{-2} \log^{2} T)$, because of our use of a novel interval-based mechanism. We also substantially improve the bounds of previous family of algorithms which use a continual release mechanism. Experiments clearly validate our theoretical bounds.|http://arxiv.org/abs/1511.08681v1|Aristide Tossou,Christos Dimitrakakis
1625|A stochastic evolutionary model generating a mixture of exponential distributions|Recent interest in human dynamics has stimulated the investigation of the stochastic processes that explain human behaviour in various contexts, such as mobile phone networks and social media. In this paper, we extend the stochastic urn-based model proposed in \cite{FENN15} so that it can generate mixture models,in particular, a mixture of exponential distributions. The model is designed to capture the dynamics of survival analysis, traditionally employed in clinical trials, reliability analysis in engineering, and more recently in the analysis of large data sets recording human dynamics. The mixture modelling approach, which is relatively simple and well understood, is very effective in capturing heterogeneity in data. We provide empirical evidence for the validity of the model, using a data set of popular search engine queries collected over a period of 114 months. We show that the survival function of these queries is closely matched by the exponential mixture solution for our model.|http://arxiv.org/abs/1511.08712v2|Trevor Fenner,Mark Levene,George Loizou
1626|Conditional Estimation in Two-stage Adaptive Designs|We consider conditional estimation in two-stage sample size adjustable designs and the following bias. More specifically, we consider a design which permits raising the sample size when interim results look rather promising, and, which keeps the originally planned sample size when results look very promising. The estimation procedures reported comprise the unconditional maximum likelihood, the conditionally unbiased Rao-Blackwell estimator, the conditional median unbiased estimator, and the conditional maximum likelihood with and without bias correction. We compare these estimators based on analytical results and by a simulation study. We show in a real clinical trial setting how they can be applied.|http://arxiv.org/abs/1602.00564v2|Per Broberg,Frank Miller
1627|Control of Directional Errors in Fixed Sequence Multiple Testing|In this paper, we consider the problem of simultaneously testing many two-sided hypotheses when rejections of null hypotheses are accompanied by claims of the direction of the alternative. The fundamental goal is to construct methods that control the mixed directional familywise error rate, which is the probability of making any type 1 or type 3 (directional) error. In particular, attention is focused on cases where the hypotheses are ordered as $H_1 , \ldots, H_n$, so that $H_{i+1}$ is tested only if $H_1 , \ldots, H_i$ have all been previously rejected. In this situation, one can control the usual familywise error rate under arbitrary dependence by the basic procedure which tests each hypothesis at level $\alpha$, and no other multiplicity adjustment is needed. However, we show that this is far too liberal if one also accounts for directional errors. But, by imposing certain dependence assumptions on the test statistics, one can retain the basic procedure. Through a simulation study and a clinical trial example, we numerically illustrate good performance of the proposed procedures compared to the existing mdFWER controlling procedures. The proposed procedures are also implemented in the R-package FixSeqMTP.|http://arxiv.org/abs/1602.02345v2|Anjana Grandhi,Wenge Guo,Joseph P. Romano
1628|An Innovative Imputation and Classification Approach for Accurate Disease Prediction|Imputation of missing attribute values in medical datasets for extracting hidden knowledge from medical datasets is an interesting research topic of interest which is very challenging. One cannot eliminate missing values in medical records. The reason may be because some tests may not been conducted as they are cost effective, values missed when conducting clinical trials, values may not have been recorded to name some of the reasons. Data mining researchers have been proposing various approaches to find and impute missing values to increase classification accuracies so that disease may be predicted accurately. In this paper, we propose a novel imputation approach for imputation of missing values and performing classification after fixing missing values. The approach is based on clustering concept and aims at dimensionality reduction of the records. The case study discussed shows that missing values can be fixed and imputed efficiently by achieving dimensionality reduction. The importance of proposed approach for classification is visible in the case study which assigns single class label in contrary to multi-label assignment if dimensionality reduction is not performed.|http://arxiv.org/abs/1603.03281v1|Yelipe UshaRani,P. Sammulal
1629|Optimal designs for dose response curves with common parameters|A common problem in Phase II clinical trials is the comparison of dose response curves corresponding to different treatment groups. If the effect of the dose level is described by parametric regression models and the treatments differ in the administration frequency (but not in the sort of drug) a reasonable assumption is that the regression models for the different treatments share common parameters. This paper develops optimal design theory for the comparison of different regression models with common parameters. We derive upper bounds on the number of support points of admissible designs, and explicit expressions for $D$-optimal designs are derived for frequently used dose response models with a common location parameter. If the location and scale parameter in the different models coincide, minimally supported designs are determined and sufficient conditions for their optimality in the class of all designs derived. The results are illustrated in a dose-finding study comparing monthly and weekly administration.|http://arxiv.org/abs/1603.04500v1|Chrystel Feller,Kirsten Schorning,Holger Dette,Georgina Bermann,Bjrn Bornkamp
1630|On a Shape-Invariant Hazard Regression Model|In survival analysis, Cox model is widely used for most clinical trial data. Alternatives include the additive hazard model, the accelerated failure time (AFT) model and a more general transformation model. All these models assume that the effects for all covariates are on the same scale. However, it is possible that for different covariates, the effects are on different scales. In this paper, we propose a shape-invariant hazard regression model that allows us to estimate the multiplicative treatment effect with adjustment of covariates that have non-multiplicative effects. We propose moment-based inference procedures for the regression parameters. We also discuss the risk prediction and goodness of fit test for our proposed model. Numerical studies show good finite sample performance of our proposed estimator. We applied our method to Veteran's Administration (VA) lung cancer data and the HIVNET 012 data. For the latter, we found that single-dose nevirapine treatment has a significant improvement for 18-month survival with appropriate adjustment for maternal CD4 counts and virus load.|http://arxiv.org/abs/1603.06988v1|Cheng Zheng,Ying Qing Chen
1631|An Approach to Find Missing Values in Medical Datasets|Mining medical datasets is a challenging problem before data mining researchers as these datasets have several hidden challenges compared to conventional datasets.Starting from the collection of samples through field experiments and clinical trials to performing classification,there are numerous challenges at every stage in the mining process. The preprocessing phase in the mining process itself is a challenging issue when, we work on medical datasets. One of the prime challenges in mining medical datasets is handling missing values which is part of preprocessing phase. In this paper, we address the issue of handling missing values in medical dataset consisting of categorical attribute values. The main contribution of this research is to use the proposed imputation measure to estimate and fix the missing values. We discuss a case study to demonstrate the working of proposed measure.|http://arxiv.org/abs/1604.07202v1|B. Mathura Bai,N. Mangathayaru,B. Padmaja Rani
1632|On weighted parametric tests|We describe a general framework for weighted parametric multiple test procedures based on the closure principle. We utilize general weighting strategies that can reflect complex study objectives and include many procedures in the literature as special cases. The proposed weighted parametric tests bridge the gap between rejection rules using either adjusted significance levels or adjusted $p$-values. This connection is possible by allowing intersection hypotheses to be tested at level smaller than $\alpha$, which may be needed for certain study considerations. For such cases we introduce a subclass of exact $\alpha$-level parametric tests which satisfy the consonance property. When only subsets of test statistics are correlated, a new procedure is proposed to fully utilize the parametric assumptions within each subset. We illustrate the proposed weighted parametric tests using a clinical trial example.|http://arxiv.org/abs/1605.06397v1|Dong Xi,Ekkehard Glimm,Willi Maurer,Frank Bretz
1633|Bayesian index of superiority and the p-value of the conditional test for Poisson parameters|We consider the problem of comparing two Poisson parameters from the Bayesian perspective. Kawasaki and Miyaoka (2012b) proposed the Bayesian index $P(\lambda_1 < \lambda_2 | X_1,X_2)$ and expressed it using the hypergeometric series. In this paper, under some conditions, we give four other expressions of the Bayesian index in terms of the cumulative distribution functions of beta, $F$, binomial, and negative binomial distribution. Next, we investigate the relationship between the Bayesian index and the $p$-value of the conditional test with the null hypothesis $H_0: \lambda_1 \geq \lambda_2 $ versus an alternative hypothesis $H_1: \lambda_1<\lambda_2 $. Additionally, we investigate the generalized relationship between $P(\lambda_1/\lambda_2 <c | X_1, X_2)$ and the $p$-value of the conditional test with the null hypothesis $H_0: \lambda_1/\lambda_2 \geq c$ versus the alternative $H_1: \lambda_1/\lambda_2 < c$. We illustrate the utility of the Bayesian index using analyses of real data. Our finding suggests that the Bayesian index can potentially be useful in an epidemiology and in a clinical trial.|http://arxiv.org/abs/1606.01324v1|Masaaki Doi
1634|Assessing the similarity of dose response and target doses in two non-overlapping subgroups|We consider two problems that are attracting increasing attention in clinical dose finding studies. First, we assess the similarity of two non-linear regression models for two non-overlapping subgroups of patients over a restricted covariate space. To this end, we derive a confidence interval for the maximum difference between the two given models. If this confidence interval excludes the equivalence margins, similarity of dose response can be claimed. Second, we address the problem of demonstrating the similarity of two target doses for two non-overlapping subgroups, using again a confidence interval based approach. We illustrate the proposed methods with a real case study and investigate their operating characteristics (coverage probabilities, Type I error rates, power) via simulation.|http://arxiv.org/abs/1607.05424v2|Frank Bretz,Kathrin Mllenhoff,Holger Dette,Wei Liu,Matthias Trampisch
1635|L0 regularisation for the estimation of piecewise constant hazard rates in survival analysis|In a survival analysis context we suggest a new method to estimate the piecewise constant hazard rate model. The method provides an automatic procedure to find the number and location of cut points and to estimate the hazard on each cut interval. Estimation is performed through a penalized likelihood using an adaptive ridge procedure. A bootstrap procedure is proposed in order to derive valid statistical inference taking both into account the variability of the estimate and the variability in the choice of the cut points. The new method is applied both to simulated data and to the Mayo Clinic trial on primary biliary cirrhosis. The algorithm implementation is seen to work well and to be of practical relevance.|http://arxiv.org/abs/1609.04595v2|O Bouaziz,G Nuel
1636|A Bayesian Interval Dose-Finding Design Addressing Ockham's Razor: mTPI-2|There has been an increasing interest in using interval-based Bayesian designs for dose finding, one of which is the modified toxicity probability interval (mTPI) method. We show that the decision rules in mTPI correspond to an optimal rule under a formal Bayesian decision theoretic framework. However, the probability models in mTPI are overly sharpened by the Ockham's razor, which, while in general helps with parsimonious statistical inference, leads to suboptimal decisions in small-sample inference such as dose finding. We propose a new framework that blunts the Ockham's razor, and demonstrate the superior performance of the new method, called mTPI-2. An online web tool is provided for users who can generate the design, conduct clinical trials, and examine operating characteristics of the designs through big data and crowd sourcing.|http://arxiv.org/abs/1609.08737v1|Wentian Guo,Sue-Jane Wang,Shengjie Yang,Suiheng Lin,Yuan Ji
1637|Rank Verification for Exponential Families|Many statistical experiments involve comparing multiple population groups. For example, a public opinion poll may ask which of several political candidates commands the most support; a social scientific survey may report the most common of several responses to a question; or, a clinical trial may compare binary patient outcomes under several treatment conditions to determine the most effective treatment. Having observed the "winner" (largest observed response) in a noisy experiment, it is natural to ask whether that candidate, survey response, or treatment is actually the "best" (stochastically largest response). This article concerns the problem of rank verification --- post hoc significance tests of whether the orderings discovered in the data reflect the population ranks. For exponential family models, we show under mild conditions that an unadjusted two-tailed pairwise test comparing the top two observations (i.e., comparing the "winner" to the "runner-up") is a valid test of whether the winner is truly the best. We extend our analysis to provide equally simple procedures to obtain lower confidence bounds on the gap between the winning population and the others, and to verify ranks beyond the first.|http://arxiv.org/abs/1610.03944v2|Kenneth Hung,William Fithian
1638|Bonferroni-based gatekeeping procedure with retesting option|In complex clinical trials, multiple research objectives are often grouped into sets of objectives based on their inherent hierarchical relationships. Consequently, the hypotheses formulated to address these objectives are grouped into ordered families of hypotheses and thus to be tested in a pre-defined sequence. In this paper, we introduce a novel Bonferroni based multiple testing procedure for testing hierarchically ordered families of hypotheses. The proposed procedure allows the families to be sequentially tested more than once with updated local critical values. It is proved to control the global familywise error rate strongly under arbitrary dependence. Implementation of the procedure is illustrated using two examples. Finally, the procedure is extended to testing multiple families of hypotheses with a complex two-layer hierarchical structure.|http://arxiv.org/abs/1611.03439v1|Zhiying Qiu,Wenge Guo,Sanat Sarkar
1639|A Noise-Filtering Approach for Cancer Drug Sensitivity Prediction|Accurately predicting drug responses to cancer is an important problem hindering oncologists' efforts to find the most effective drugs to treat cancer, which is a core goal in precision medicine. The scientific community has focused on improving this prediction based on genomic, epigenomic, and proteomic datasets measured in human cancer cell lines. Real-world cancer cell lines contain noise, which degrades the performance of machine learning algorithms. This problem is rarely addressed in the existing approaches. In this paper, we present a noise-filtering approach that integrates techniques from numerical linear algebra and information retrieval targeted at filtering out noisy cancer cell lines. By filtering out noisy cancer cell lines, we can train machine learning algorithms on better quality cancer cell lines. We evaluate the performance of our approach and compare it with an existing approach using the Area Under the ROC Curve (AUC) on clinical trial data. The experimental results show that our proposed approach is stable and also yields the highest AUC at a statistically significant level.|http://arxiv.org/abs/1612.00525v2|Turki Turki,Zhi Wei
1640|Ranking Biomarkers Through Mutual Information|We study information theoretic methods for ranking biomarkers. In clinical trials there are two, closely related, types of biomarkers: predictive and prognostic, and disentangling them is a key challenge. Our first step is to phrase biomarker ranking in terms of optimizing an information theoretic quantity. This formalization of the problem will enable us to derive rankings of predictive/prognostic biomarkers, by estimating different, high dimensional, conditional mutual information terms. To estimate these terms, we suggest efficient low dimensional approximations, and we derive an empirical Bayes estimator, which is suitable for small or sparse datasets. Finally, we introduce a new visualisation tool that captures the prognostic and the predictive strength of a set of biomarkers. We believe this representation will prove to be a powerful tool in biomarker discovery.|http://arxiv.org/abs/1612.01316v1|Konstantinos Sechidis,Emily Turner,Paul D. Metcalfe,James Weatherall,Gavin Brown
1641|MEBoost: Variable Selection in the Presence of Measurement Error|We present a novel method for variable selection in regression models when covariates are measured with error. The iterative algorithm we propose, MEBoost, follows a path defined by estimating equations that correct for covariate measurement error. Via simulation, we evaluated our method and compare its performance to the recently-proposed Convex Conditioned Lasso (CoCoLasso) and to the "naive" Lasso which does not correct for measurement error. Increasing the degree of measurement error increased prediction error and decreased the probability of accurate covariate selection, but this loss of accuracy was least pronounced when using MEBoost. We illustrate the use of MEBoost in practice by analyzing data from the Box Lunch Study, a clinical trial in nutrition where several variables are based on self-report and hence measured with error.|http://arxiv.org/abs/1701.02349v3|Benjamin Brown,Timothy Weaver,Julian Wolfson
1642|A new class of robust two-sample Wald-type tests|Parametric hypothesis testing associated with two independent samples arises frequently in several applications in biology, medical sciences, epidemiology, reliability and many more. In this paper, we propose robust Wald-type tests for testing such two sample problems using the minimum density power divergence estimators of the underlying parameters. In particular, we consider the simple two-sample hypothesis concerning the full parametric homogeneity of the samples as well as the general two-sample (composite) hypotheses involving nuisance parameters also. The asymptotic and theoretical robustness properties of the proposed Wald-type tests have been developed for both the simple and general composite hypotheses. Some particular cases of testing against one-sided alternatives are discussed with specific attention to testing the effectiveness of a treatment in clinical trials. Performances of the proposed tests have also been illustrated numerically through appropriate real data examples.|http://arxiv.org/abs/1702.04552v1|Abhik Ghosh,Nirian Martin,Ayanendranath Basu,Leandro Pardo
1643|Context Attentive Bandits: Contextual Bandit with Restricted Context|We consider a novel formulation of the multi-armed bandit model, which we call the contextual bandit with restricted context, where only a limited number of features can be accessed by the learner at every iteration. This novel formulation is motivated by different online problems arising in clinical trials, recommender systems and attention modeling. Herein, we adapt the standard multi-armed bandit algorithm known as Thompson Sampling to take advantage of our restricted context setting, and propose two novel algorithms, called the Thompson Sampling with Restricted Context(TSRC) and the Windows Thompson Sampling with Restricted Context(WTSRC), for handling stationary and nonstationary environments, respectively. Our empirical results demonstrate advantages of the proposed approaches on several real-life datasets|http://arxiv.org/abs/1705.03821v2|Djallel Bouneffouf,Irina Rish,Guillermo A. Cecchi,Raphael Feraud
1644|Tree based weighted learning for estimating individualized treatment rules with censored data|Estimating individualized treatment rules is a central task for personalized medicine. [zhao2012estimating] and [zhang2012robust] proposed outcome weighted learning to estimate individualized treatment rules directly through maximizing the expected outcome without modeling the response directly. In this paper, we extend the outcome weighted learning to right censored survival data without requiring either an inverse probability of censoring weighting or a semiparametric modeling of the censoring and failure times as done in [zhao2015doubly]. To accomplish this, we take advantage of the tree based approach proposed in [zhu2012recursively] to nonparametrically impute the survival time in two different ways. The first approach replaces the reward of each individual by the expected survival time, while in the second approach only the censored observations are imputed by their conditional expected failure times. We establish consistency and convergence rates for both estimators. In simulation studies, our estimators demonstrate improved performance compared to existing methods. We also illustrate the proposed method on a phase III clinical trial of non-small cell lung cancer.|http://arxiv.org/abs/1707.09632v2|Yifan Cui,Ruoqing Zhu,Michael Kosorok
1645|Subgroup analysis of treatment effects for misclassified biomarkers with time-to-event data|Analysing subgroups defined by biomarkers is of increasing importance in clinical research. In some situations the biomarker is subject to misclassification error, meaning the true subgroups are identified with imperfect sensitivity and specificity. For time-to-event data, it is improper to assume the Cox proportional hazards model for the effects with respect to the true subgroups, since the survival distributions with respect to the diagnosed subgroups will not adhere to the proportional hazards assumption. This precludes the possibility of using simple adjustment procedures. Instead, we present a method based on formally modelling the data as a mixture of Cox models using an EM algorithm for estimation. An estimate of the overall population treatment effect is obtained through the interpretation of the hazard ratio as a concordance odds. Profile likelihood is used to construct individual and simultaneous confidence intervals of treatment effects. The resulting confidence intervals are shown to have close to nominal coverage for moderately large sample sizes in simulations and the method is illustrated on data from a renal-cell cancer trial.|http://arxiv.org/abs/1708.00942v2|Fang Wan,Andrew C. Titman,Thomas F. Jaki
1646|Why Adaptively Collected Data Have Negative Bias and How to Correct for It|From scientific experiments to online A/B testing, the previously observed data often affects how future experiments are performed, which in turn affects which data will be collected. Such adaptivity introduces complex correlations between the data and the collection procedure. In this paper, we prove that when the data collection procedure satisfies natural conditions, then sample means of the data have systematic \emph{negative} biases. As an example, consider an adaptive clinical trial where additional data points are more likely to be tested for treatments that show initial promise. Our surprising result implies that the average observed treatment effects would underestimate the true effects of each treatment. We quantitatively analyze the magnitude and behavior of this negative bias in a variety of settings. We also propose a novel debiasing algorithm based on selective inference techniques. In experiments, our method can effectively reduce bias and estimation error.|http://arxiv.org/abs/1708.01977v2|Xinkun Nie,Xiaoying Tian,Jonathan Taylor,James Zou
1647|An omnibus test for the global null hypothesis|Global hypothesis tests are a useful tool in the context of, e.g, clinical trials, genetic studies or meta analyses, when researchers are not interested in testing individual hypotheses, but in testing whether none of the hypotheses is false. There are several possibilities how to test the global null hypothesis when the individual null hypotheses are independent. If it is assumed that many of the individual null hypotheses are false, combinations tests have been recommended to maximise power. If, however, it is assumed that only one or a few null hypotheses are false, global tests based on individual test statistics are more powerful (e.g., Bonferroni or Simes test). However, usually there is no a-priori knowledge on the number of false individual null hypotheses. We therefore propose an omnibus test based on the combination of p-values. We show that this test yields an impressive overall performance. The proposed method is implemented in the R-package omnibus.|http://arxiv.org/abs/1709.00960v1|Andreas Futschik,Thomas Taus,Sonja Zehetmayer
1648|Likelihood Based Study Designs for Time-to-Event Endpoints|Likelihood methods for measuring statistical evidence obey the likelihood principle while maintaining bounded and well-controlled frequency properties. These methods lend themselves to sequential study designs because they measure the strength of statistical evidence in accumulating data without needing adjustments for the number of planned or unplanned examinations of data. However, sample size projections have, to date, only been developed for fixed sample size designs. In this paper, we consider sequential study designs for time-to-event outcomes assuming likelihood methods will be used to monitor the strength of statistical evidence for efficacy and futility. We develop sample size projections with the aim of controlling the probability of observing misleading evidence under the null and alternative hypotheses, and we show how efficacy and futility considerations are managed in this context. We also consider relaxing the requirement of specifying the simple alternative hypothesis in advance of the study. Finally, we end with a comparative illustration of these methods in a phase II cancer clinical trial that previously was designed within a Bayesian framework.|http://arxiv.org/abs/1711.01527v1|Jeffrey D Blume,Leena Choi
1649|Sharpening randomization-based causal inference for $2^2$ factorial designs with binary outcomes|In medical research, a scenario often entertained is randomized controlled $2^2$ factorial design with a binary outcome. By utilizing the concept of potential outcomes, Dasgupta et al. (2015) proposed a randomization-based causal inference framework, allowing flexible and simultaneous estimations and inferences of the factorial effects. However, a fundamental challenge that Dasgupta et al. (2015)'s proposed methodology faces is that the sampling variance of the randomization-based factorial effect estimator is unidentifiable, rendering the corresponding classic "Neymanian" variance estimator suffering from over-estimation. To address this issue, for randomized controlled $2^2$ factorial designs with binary outcomes, we derive the sharp lower bound of the sampling variance of the factorial effect estimator, which leads to a new variance estimator that sharpens the finite-population Neymanian causal inference. We demonstrate the advantages of the new variance estimator through a series of simulation studies, and apply our newly proposed methodology to two real-life datasets from randomized clinical trials, where we gain new insights.|http://arxiv.org/abs/1711.04432v1|Jiannan Lu
1650|Bayesian optimal designs for dose-response curves with common parameters|The issue of determining not only an adequate dose but also a dosing frequency of a drug arises frequently in Phase II clinical trials. This results in the comparison of models which have some parameters in common. Planning such studies based on Bayesian optimal designs offers robustness to our conclusions since these designs, unlike locally optimal designs, are efficient even if the parameters are misspecified. In this paper we develop approximate design theory for Bayesian $D$-optimality for nonlinear regression models with common parameters and investigate the cases of common location or common location and scale parameters separately. Analytical characterisations of saturated Bayesian $D$-optimal designs are derived for frequently used dose-response models and the advantages of our results are illustrated via a numerical investigation.|http://arxiv.org/abs/1711.05704v1|Kirsten Schorning,Maria Konstantinou
1651|Towards dense volumetric pancreas segmentation in CT using 3D fully convolutional networks|Pancreas segmentation in computed tomography imaging has been historically difficult for automated methods because of the large shape and size variations between patients. In this work, we describe a custom-build 3D fully convolutional network (FCN) that can process a 3D image including the whole pancreas and produce an automatic segmentation. We investigate two variations of the 3D FCN architecture; one with concatenation and one with summation skip connections to the decoder part of the network. We evaluate our methods on a dataset from a clinical trial with gastric cancer patients, including 147 contrast enhanced abdominal CT scans acquired in the portal venous phase. Using the summation architecture, we achieve an average Dice score of 89.7 $\pm$ 3.8 (range [79.8, 94.8]) % in testing, achieving the new state-of-the-art performance in pancreas segmentation on this dataset.|http://arxiv.org/abs/1711.06439v2|Holger Roth,Masahiro Oda,Natsuki Shimizu,Hirohisa Oda,Yuichiro Hayashi,Takayuki Kitasaka,Michitaka Fujiwara,Kazunari Misawa,Kensaku Mori
1652|Causal nearest neighbor rules for optimal treatment regimes|The estimation of optimal treatment regimes is of considerable interest to precision medicine. In this work, we propose a causal $k$-nearest neighbor method to estimate the optimal treatment regime. The method roots in the framework of causal inference, and estimates the causal treatment effects within the nearest neighborhood. Although the method is simple, it possesses nice theoretical properties. We show that the causal $k$-nearest neighbor regime is universally consistent. That is, the causal $k$-nearest neighbor regime will eventually learn the optimal treatment regime as the sample size increases. We also establish its convergence rate. However, the causal $k$-nearest neighbor regime may suffer from the curse of dimensionality, i.e. performance deteriorates as dimensionality increases. To alleviate this problem, we develop an adaptive causal $k$-nearest neighbor method to perform metric selection and variable selection simultaneously. The performance of the proposed methods is illustrated in simulation studies and in an analysis of a chronic depression clinical trial.|http://arxiv.org/abs/1711.08451v1|Xin Zhou,Michael R. Kosorok
1653|A two-stage Fisher exact test for multi-arm studies with binary outcome variables|In small sample studies with binary outcome data, use of a normal approximation for hypothesis testing can lead to substantial inflation of the type-I error-rate. Consequently, exact statistical methods are necessitated, and accordingly, much research has been conducted to facilitate this. Recently, this has included methodology for the design of two-stage multi-arm studies utilising exact binomial tests. These designs were demonstrated to carry substantial efficiency advantages over a fixed sample design, but generally suffered from strong conservatism. An alternative classical means of small sample inference with dichotomous data is Fisher's exact test. However, this method is limited to single-stage designs when there are multiple arms. Therefore, here, we propose a two-stage version of Fisher's exact test, with the potential to stop early to accept or reject null hypotheses, which is applicable to multi-arm studies. In particular, we provide precise formulae describing the requirements for achieving weak or strong control of the familywise error-rate with this design. Following this, we describe how the design parameters may be optimised to confer desirable operating characteristics. For a motivating example based on a phase II clinical trial, we demonstrate that on average our approach is less conservative than corresponding optimal designs based on exact binomial tests.|http://arxiv.org/abs/1711.10199v1|Michael Grayling,Adrian Mander,James Wason
1654|Computational Analysis for the Rational Design of Anti-Amyloid Beta (ABeta) Antibodies|Alzheimer's Disease (AD) is a neurodegenerative disorder that lacks effective treatment options. Anti-amyloid beta (ABeta) antibodies are the leading drug candidates to treat AD, but the results of clinical trials have been disappointing. Introducing rational mutations into anti-ABeta antibodies to increase their effectiveness is a way forward, but the path to take is unclear. In this study, we demonstrate the use of computational fragment-based docking and MMPBSA binding free energy calculations in the analysis of anti-ABeta antibodies for rational drug design efforts. Our fragment-based docking method successfully predicted the emergence of the common EFRH epitope, MD simulations coupled with MMPBSA binding free energy calculations were used to analyze scenarios described in prior studies, and we introduced rational mutations into PFA1 to improve its calculated binding affinity towards the pE3-ABeta3-8 form of ABeta. Two out of four proposed mutations stabilized binding. Our study demonstrates that a computational approach may lead to an improved drug candidate for AD in the future.|http://arxiv.org/abs/1801.01533v2|D'Artagnan Greene,Theodora Po,Jennifer Pan,Tanya Tabibian,Ray Luo
1655|Monte Carlo modified profile likelihood in models for clustered data|The main focus of the analysts who deal with clustered data is usually not on the clustering variables, and hence the group-specific parameters are treated as nuisance. If a fixed effects formulation is preferred and the total number of clusters is large relative to the single-group sizes, classical frequentist techniques relying on the profile likelihood are often misleading. The use of alternative tools, such as modifications to the profile likelihood or integrated likelihoods, for making accurate inference on a parameter of interest can be complicated by the presence of nonstandard modelling and/or sampling assumptions. We show here how to employ Monte Carlo simulation in order to approximate the modified profile likelihood in some of these unconventional frameworks. The proposed solution is widely applicable and is shown to retain the usual properties of the modified profile likelihood. The approach is examined in two instances particularly relevant in applications, i.e. missing-data models and survival models with unspecified censoring distribution. The effectiveness of the proposed solution is validated via simulation studies and two clinical trial applications.|http://arxiv.org/abs/1801.02597v3|Claudia Di Caterina,Giuliana Cortese,Nicola Sartori
1656|Testing for equivalence: an intersection-union permutation solution|The notion of testing for equivalence of two treatments is widely used in clinical trials, pharmaceutical experiments,bioequivalence and quality control. It is essentially approached within the intersection-union (IU) principle. According to this principle the null hypothesis is stated as the set of effects lying outside a suitably established interval and the alternative as the set of effects lying inside that interval. The solutions provided in the literature are mostly based on likelihood techniques, which in turn are rather difficult to handle, except for cases lying within the regular exponential family and the invariance principle. The main goal of present paper is to go beyond most of the limitations of likelihood based methods, i.e. to work in a nonparametric setting within the permutation frame. To obtain practical solutions, a new IU permutation test is presented and discussed. A simple simulation study for evaluating its main properties, and three application examples are also presented.|http://arxiv.org/abs/1802.01877v1|R. Arboretti,E. Carrozzo,F. Pesarin,L. Salmaso
1657|Using Deep Learning for Segmentation and Counting within Microscopy Data|Cell counting is a ubiquitous, yet tedious task that would greatly benefit from automation. From basic biological questions to clinical trials, cell counts provide key quantitative feedback that drive research. Unfortunately, cell counting is most commonly a manual task and can be time-intensive. The task is made even more difficult due to overlapping cells, existence of multiple focal planes, and poor imaging quality, among other factors. Here, we describe a convolutional neural network approach, using a recently described feature pyramid network combined with a VGG-style neural network, for segmenting and subsequent counting of cells in a given microscopy image.|http://arxiv.org/abs/1802.10548v1|Carlos X. Hernndez,Mohammad M. Sultan,Vijay S. Pande
1658|Using Survival Information in Truncation by Death Problems Without the Monotonicity Assumption|In some randomized clinical trials, patients may die before the measurements of their outcomes. Even though randomization generates comparable treatment and control groups, the remaining survivors often differ significantly in background variables that are prognostic to the outcomes. This is called the truncation by death problem. Under the potential outcomes framework, the only well-defined causal effect on the outcome is within the subgroup of patients who would always survive under both treatment and control. Because the definition of the subgroup depends on the potential values of the survival status that could not be observed jointly, without making strong parametric assumptions, we cannot identify the causal effect of interest and consequently can only obtain bounds of it. Unfortunately, however, many bounds are too wide to be useful. We propose to use detailed survival information before and after the measurements of the outcomes to sharpen the bounds of the subgroup causal effect. Because survival times contain useful information about the final outcome, carefully utilizing them could improve statistical inference without imposing strong parametric assumptions. Moreover, we propose to use a copula model to relax the commonly-invoked but often doubtful monotonicity assumption that the treatment extends the survival time for all patients.|http://arxiv.org/abs/1803.02024v1|Fan Yang,Peng Ding
1659|Characterizing Diseases and disorders in Gay Users' tweets|A lack of information exists about the health issues of lesbian, gay, bisexual, transgender, and queer (LGBTQ) people who are often excluded from national demographic assessments, health studies, and clinical trials. As a result, medical experts and researchers lack a holistic understanding of the health disparities facing these populations. Fortunately, publicly available social media data such as Twitter data can be utilized to support the decisions of public health policy makers and managers with respect to LGBTQ people. This research employs a computational approach to collect tweets from gay users on health-related topics and model these topics. To determine the nature of health-related information shared by men who have sex with men on Twitter, we collected thousands of tweets from 177 active users. We sampled these tweets using a framework that can be applied to other LGBTQ sub-populations in future research. We found 11 diseases in 7 categories based on ICD 10 that are in line with the published studies and official reports.|http://arxiv.org/abs/1803.09134v1|Frank Webb,Amir Karami,Vanessa Kitzie
1660|Measuring uncertainty during respiratory rate estimation using pressure-sensitive mats|We develop and evaluate a respiratory rate estimation algorithm that utilizes data from pressure-sensitive mat (PSM) technology for continuous patient monitoring in neonatal intensive care units (NICU). An analysis of the random effect of drift and systematic effect of creep in the PSM data is presented, showing that these are essentially dependent on the applied load and contact surface. Uncertainty measurements are pivotal when estimating physiologic parameters. The standard uncertainty in the PSM data is here represented by the percent drift. Next, we evaluate the applicability of PSM technology to estimate RR in neonatal patient simulator trials under five mixed effects including internally and externally induced motion, mattress type, grunting, laying position, and different breathing rates. We analyze the limits of agreement on the mixed effects model to derive the uncertainty in the estimated RR obtained through two estimation techniques. In comparison with the gold standard RR values, we achieved a mean bias of 0.56 breaths per minute (bpm) with an error bounded by a 95% confidence interval of [-2.26, 3.37] bpm. These results meet the clinical accuracy requirements of RR within +/-5 bpm.|http://arxiv.org/abs/1805.00082v1|S. Nizami,A. Bekele,M. Hozayen,K. Greenwood,J. Harrold,J. R. Green
1661|Crossing points in survival analysis sensitively depend on system conditions|Crossing survival curves complicate how we interpret results from a clinical trial's primary endpoint. We find the function to determine a crossing point's location depends exponentially on individual survival curves. This exponential relationship between survival curves and the crossing point transforms small survival curve errors into large crossing point errors. In most cases, crossing points are sensitive to individual survival errors and may make accurately locating a crossing point unsuccessful. We argue more complicated analyses for mitigating crossing points should be reserved only after first exploring a crossing point's variability, or hypothesis tests account for crossing point variability.|http://arxiv.org/abs/1805.02835v1|Thomas McAndrew,Bjorn Redfors,Yiran Zhang,Aaron Crowley,Shmuel Chen,Gregg Stone,Paul Jenkins
1662|Cascading Citation Expansion|Digital Science's Dimensions is envisaged as a next-generation research and discovery platform for a better and more efficient access to cross-referenced scholarly publications, grants, patents, and clinical trials. As a new addition to the growing open citation resources, it offers opportunities that may benefit a wide variety of stakeholders of scientific publications from researchers, policy makers, and the general public. In this article, we explore and demonstrate some of the practical potentials in terms of cascading citation expansions. Given a set of publications, the cascading citation expansion process can be successively applied to a set of articles so as to extend the coverage to more and more relevant articles through citation links. Although the conceptual origin can be traced back to Garfield's citation indexing, it has been largely limited, until recently, to the few who have unrestricted access to a citation database that is large enough to sustain such iterative expansions. Building on the open API of Dimensions, we integrate cascading citation expansion functions in CiteSpace and demonstrate how one may benefit from these new capabilities. In conclusion, cascading citation expansion has the potential to improve our understanding of the structure and dynamics of scientific knowledge.|http://arxiv.org/abs/1806.00089v1|Chaomei Chen
1663|A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature|We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials. Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the `PICO' elements). These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary. We acquired annotations from a diverse set of workers with varying levels of expertise and cost. We describe our data collection process and the corpus itself in detail. We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.|http://arxiv.org/abs/1806.04185v1|Benjamin Nye,Junyi Jessy Li,Roma Patel,Yinfei Yang,Iain J. Marshall,Ani Nenkova,Byron C. Wallace
1664|Protein Design by Algorithm|We review algorithms for protein design in general. Although these algorithms have a rich combinatorial, geometric, and mathematical structure, they are almost never covered in computer science classes. Furthermore, many of these algorithms admit provable guarantees of accuracy, soundness, complexity, completeness, optimality, and approximation bounds. The algorithms represent a delicate and beautiful balance between discrete and continuous computation and modeling, analogous to that which is seen in robotics, computational geometry, and other fields in computational science. Finally, computer scientists may be unaware of the almost direct impact of these algorithms for predicting and introducing molecular therapies that have gone in a short time from mathematics to algorithms to software to predictions to preclinical testing to clinical trials. Indeed, the overarching goal of these algorithms is to enable the development of new therapeutics that might be impossible or too expensive to discover using experimental methods. Thus the potential impact of these algorithms on individual, community, and global health has the potential to be quite significant.|http://arxiv.org/abs/1806.06064v1|Mark A. Hallen,Bruce R. Donald
1665|Matching Algorithms for Causal Inference with Multiple Treatments|Randomized clinical trials (RCTs) are ideal for estimating causal effects, because the distributions of background covariates are similar in expectation across treatment groups. When estimating causal effects using observational data, matching is a commonly used method to replicate the covariate balance achieved in a RCT. Matching algorithms have a rich history dating back to the mid-1900s, but have been used mostly to estimate causal effects between two treatment groups. When there are more than two treatments, estimating causal effects requires additional assumptions and techniques. We propose matching algorithms that address the drawbacks of the current methods, and we use simulations to compare current and new methods. All of the methods display improved covariate balance in the matched sets relative to the pre-matched cohorts. In addition, we provide advice to investigators on which matching algorithms are preferred for different covariate distributions.|http://arxiv.org/abs/1809.00269v2|Anthony D. Scotina,Roee Gutman
1666|Interval Estimation of Individual-Level Causal Effects Under Unobserved Confounding|We study the problem of learning conditional average treatment effects (CATE) from observational data with unobserved confounders. The CATE function maps baseline covariates to individual causal effect predictions and is key for personalized assessments. Recent work has focused on how to learn CATE under unconfoundedness, i.e., when there are no unobserved confounders. Since CATE may not be identified when unconfoundedness is violated, we develop a functional interval estimator that predicts bounds on the individual causal effects under realistic violations of unconfoundedness. Our estimator takes the form of a weighted kernel estimator with weights that vary adversarially. We prove that our estimator is sharp in that it converges exactly to the tightest bounds possible on CATE when there may be unobserved confounders. Further, we study personalized decision rules derived from our estimator and prove that they achieve optimal minimax regret asymptotically. We assess our approach in a simulation study as well as demonstrate its application in the case of hormone replacement therapy by comparing conclusions from a real observational study and clinical trial.|http://arxiv.org/abs/1810.02894v1|Nathan Kallus,Xiaojie Mao,Angela Zhou
1667|Bridging the gap between regret minimization and best arm identification, with application to A/B tests|State of the art online learning procedures focus either on selecting the best alternative ("best arm identification") or on minimizing the cost (the "regret"). We merge these two objectives by providing the theoretical analysis of cost minimizing algorithms that are also delta-PAC (with a proven guaranteed bound on the decision time), hence fulfilling at the same time regret minimization and best arm identification. This analysis sheds light on the common observation that ill-callibrated UCB-algorithms minimize regret while still identifying quickly the best arm.   We also extend these results to the non-iid case faced by many practitioners. This provides a technique to make cost versus decision time compromise when doing adaptive tests with applications ranging from website A/B testing to clinical trials.|http://arxiv.org/abs/1810.04088v2|Rmy Degenne,Thomas Nedelec,Clment Calauznes,Vianney Perchet
1668|A maximum-mean-discrepancy goodness-of-fit test for censored data|We introduce a kernel-based goodness-of-fit test for censored data, where observations may be missing in random time intervals: a common occurrence in clinical trials and industrial life-testing. The test statistic is straightforward to compute, as is the test threshold, and we establish consistency under the null. Unlike earlier approaches such as the Log-rank test, we make no assumptions as to how the data distribution might differ from the null, and our test has power against a very rich class of alternatives. In experiments, our test outperforms competing approaches for periodic and Weibull hazard functions (where risks are time dependent), and does not show the failure modes of tests that rely on user-defined features. Moreover, in cases where classical tests are provably most powerful, our test performs almost as well, while being more general.|http://arxiv.org/abs/1810.04286v1|Tamara Fernndez,Arthur Gretton
1669|Geometric characterization of data sets with unique reduced Grbner bases|Model selection based on experimental data is an important challenge in biological data science. Particularly when collecting data is expensive or time consuming, as it is often the case with clinical trial and biomolecular experiments, the problem of selecting information-rich data becomes crucial for creating relevant models. We identify geometric properties of input data that result in a unique algebraic model and we show that if the data form a staircase, or a so-called linear shift of a staircase, the ideal of the points has a unique reduced Gro \"bner basis and thus corresponds to a unique model. We use linear shifts to partition data into equivalence classes with the same basis. We demonstrate the utility of the results by applying them to a Boolean model of the well-studied lac operon in E. coli.|http://arxiv.org/abs/1811.01114v2|Elena S. Dimitrova,Qijun He,Brandilyn Stigler,Anyu Zhang
1670|contextual: Evaluating Contextual Multi-Armed Bandit Problems in R|Over the past decade, contextual bandit algorithms have been gaining in popularity due to their effectiveness and flexibility in solving sequential decision problems---from online advertising and finance to clinical trial design and personalized medicine. At the same time, there are, as of yet, surprisingly few options that enable researchers and practitioners to simulate and compare the wealth of new and existing bandit algorithms in a standardized way. To help close this gap between analytical research and empirical evaluation the current paper introduces the object-oriented R package "contextual": a user-friendly and, through its object-oriented structure, easily extensible framework that facilitates parallelized comparison of contextual and context-free bandit policies through both simulation and offline analysis.|http://arxiv.org/abs/1811.01926v4|Robin van Emden,Maurits Kaptein
1671|Dose finding for new vaccines: the role for immunostimulation/immunodynamic modelling|Current methods to optimize vaccine dose are purely empirically based, whereas in the drug development field, dosing determinations use far more advanced quantitative methodology to accelerate decision-making. Applying these established methods in the field of vaccine development may reduce the currently large clinical trial sample sizes, long time frames, high costs, and ultimately have a better potential to save lives. We propose the field of immunostimulation/immunodynamic (IS/ID) modelling, which aims to translate mathematical frameworks used for drug dosing towards optimizing vaccine dose decision-making. Analogous to PK/PD modelling, IS/ID modelling approaches apply mathematical models to describe the underlying mechanisms by which the immune response is stimulated by vaccination (IS) and the resulting measured immune response dynamics (ID). To move IS/ID modelling forward, existing datasets and further data on vaccine allometry and dose-dependent dynamics need to be generated and collate, requiring a collaborative environment with input from academia, industry, regulators, governmental and non-governmental agencies to share modelling expertise, and connect modellers to vaccine data.|http://arxiv.org/abs/1811.04024v1|Sophie J. Rhodes,Gwenan M. Knight,Denise E. Kirschner,Richard G. White,Thomas G. Evans
1672|Quantile Regression Modeling of Recurrent Event Risk|Progression of chronic disease is often manifested by repeated occurrences of disease-related events over time. Delineating the heterogeneity in the risk of such recurrent events can provide valuable scientific insight for guiding customized disease management. In this paper, we present a new modeling framework for recurrent event data, which renders a flexible and robust characterization of individual multiplicative risk of recurrent event through quantile regression that accommodates both observed covariates and unobservable frailty. The proposed modeling requires no distributional specification of the unobservable frailty, while permitting the exploration of dynamic covariate effects. We develop estimation and inference procedures for the proposed model through a novel adaptation of the principle of conditional score. The asymptotic properties of the proposed estimator, including the uniform consistency and weak convergence, are established. Extensive simulation studies demonstrate satisfactory finite-sample performance of the proposed method. We illustrate the practical utility of the new method via an application to a diabetes clinical trial that explores the risk patterns of hypoglycemia in Type 2 diabetes patients.|http://arxiv.org/abs/1811.06211v1|Huijuan Ma,Limin Peng,Chiung-Yu Huang,Haoda Fu
1673|Evaluating and Optimizing Network Sampling Designs: Decision Theory and Information Theory Perspectives|Some of the most used sampling mechanisms that implicitly leverage a social network depend on tuning parameters; for instance, Respondent-Driven Sampling (RDS) is specified by the number of seeds and maximum number of referrals. We are interested in the problem of optimizing these sampling mechanisms with respect to their tuning parameters in order to optimize the inference on a population quantity, where such quantity is a function of the network and measurements taken at the nodes. This is done by formulating the problem in terms of decision theory and information theory, in turn. We discuss how the approaches discussed in this paper relate, via theoretical results, to other formalisms aimed to compare sampling designs, namely sufficiency and the Goel-DeGroot Criterion. The optimization procedure for different network sampling mechanisms is illustrated via simulations in the fashion of the ones used for Bayesian clinical trials.|http://arxiv.org/abs/1811.07829v4|Simn Lunagmez,Marios Papamichalis,Patrick J. Wolfe,Edoardo M. Airoldi
1674|Quantification of sulfated polysaccharides in urine by the Heparin Red mix-and-read fluorescence assay|Quantification of sulfated polysaccharides in urine samples is relevant to pharmacokinetic studies in drug development projects and to the non-invasive diagnosis and therapy monitoring of mucopolysaccharidoses. The Heparin Red Kit is a particularly simple and user friendly fluorescence assay for the quantification of sulfated polysaccharides and has recently emerged as a novel tool for the monitoring of their blood levels during pharmacokinetic studies in clinical trials. The standard protocol for the blood plasma matrix is, however, not suited for urine samples due to matrix interference. The present study identifies inorganic sulfate as the interfering component in urine. The sulfate level of urine is typically 1-2 orders of magnitude higher compared with the blood plasma level. Addition of either hydrochloric acid or magnesium chloride counteracts sulfate interference but still enables sensitive detection of sulfated polysaccharides such as heparin, heparan sulfate and dermatan sulfate at low microgramm per milliliter levels. This study extends the application range of the Heparin Red Kit by a simple modification of the assay protocol to the direct quantification of various sulfated polysaccharides in human urine.|http://arxiv.org/abs/1811.09115v1|Ulrich Warttinger,Roland Krmer
1675|Applications of Blockchain in Healthcare: Current Landscape & Challenges|Several problems in healthcare stem from the complex network of intermediaries and the lack of traceability of transactions. To mention a few: healthcare data is fragmented across several silos negatively affecting research and services, about half of the clinical trials are never reported, the cost of drug discovery is ever increasing, and substandard and fake medicines are still a huge problem. Blockchain has the potential to solve these problems as it provides trust without any intermediaries, has traceability as a default feature, and promises new business models by enabling novel incentive structures. Due to its potential, blockchain has gathered significant interest in the healthcare industry. In this paper, we review major use cases of blockchain in healthcare: patient data management, pharmaceutical research, supply chain management of medical goods, prescription management, billing claims management, analytics, and telemedicine alongside the related projects. We found that most of the blockchain projects are limited as white-papers, proof of concepts, and products with a limited user base. However, we observed that the quantity, quality, and maturity of the projects are increasing. We also discuss technical, regulatory, and business challenges to the adoption of blockchain in the healthcare industry|http://arxiv.org/abs/1812.02776v1|Gajendra J. Katuwal,Sandip Pandey,Mark Hennessey,Bishal Lamichhane
1676|A Sequential Significance Test for Treatment by Covariate Interactions|Due to patient heterogeneity in response to various aspects of any treatment program, biomedical and clinical research is gradually shifting from the traditional "one-size-fits-all" approach to the new paradigm of personalized medicine. An important step in this direction is to identify the treatment by covariate interactions. We consider the setting in which there are potentially a large number of covariates of interest. Although a number of novel machine learning methodologies have been developed in recent years to aid in treatment selection in this setting, few, if any, have adopted formal hypothesis testing procedures. In this article, we present a novel testing procedure based on m-out-of-n bootstrap that can be used to sequentially identify variables that interact with treatment. We study the theoretical properties of the method and show that it is more effective in controlling the type I error rate and achieving a satisfactory power as compared to competing methods, via extensive simulations. Furthermore, the usefulness of the proposed method is illustrated using real data examples, both from a randomized trial and from an observational study.|http://arxiv.org/abs/1901.08738v2|Min Qian,Bibhas Chakraborty,Raju Maiti,Ying Kuen Cheung
1677|The Robust Kernel Association Test|Testing the association between SNP effects and a response is a common task. Such tests are often carried out through kernel machine methods based on least squares, such as the Sequence Kernel Association Test (SKAT). However, these least squares procedures assume a normally distributed response, which is often violated. Other robust procedures such as the Quantile Regression Kernel Machine (QRKM) restrict choice of loss function and only allow inference on conditional quantiles. We propose a general and robust kernel association test with flexible choice of loss function, no distributional assumptions, and has SKAT and QRKM as special cases. We evaluate our proposed robust association test (RobKAT) across various data distributions through simulation study. When errors are normally distributed, RobKAT controls type I error and shows comparable power to SKAT. In all other distributional settings investigated, our robust test has similar or greater power than SKAT. Finally, we apply our robust kernel association test on data from the CATIE clinical trial to detect associations between selected genes on chromosome 6, including the Major Histocompatibility Complex (MHC) region, and neurotrophic herpesvirus antibody levels in schizophrenia patients. RobKAT detected significant association with four SNP-sets (HST1H2BJ, MHC, POM12L2, and SLC17A1), three of which were undetected by SKAT.|http://arxiv.org/abs/1901.09419v1|Kara Martinez,Arnab Maity,Robert Yolken,Patrick Sullivan,Jung-Ying Tzeng
1678|Separable Effects for Causal Inference in the Presence of Competing Events|In time-to-event settings, the presence of competing events complicates the definition of causal effects. Here we propose the new separable effects to study the causal effect of a treatment on an event of interest. The separable direct effect is the treatment effect on the event of interest not mediated by its effect on the competing event. The separable indirect effect is the treatment effect on the event of interest only through its effect on the competing event. Similar to Robins and Richardson's extended graphical approach for mediation analysis, the separable effects can only be identified under the assumption that the treatment can be decomposed into two distinct components that exert their effects through distinct causal pathways. Unlike existing definitions of causal effects in the presence of competing events, our estimands do not require cross-world contrasts or hypothetical interventions to prevent death. As an illustration, we apply our approach to a randomized clinical trial on estrogen therapy in individuals with prostate cancer.|http://arxiv.org/abs/1901.09472v3|Mats J. Stensrud,Jessica G. Young,Vanessa Didelez,James M. Robins,Miguel A. Hernn
1679|Unsupervised Segmentation Algorithms' Implementation in ITK for Tissue Classification via Human Head MRI Scans|Tissue classification is one of the significant tasks in the field of biomedical image analysis. Magnetic Resonance Imaging (MRI) is of great importance in tissue classification especially in the areas of brain tissue classification which is able to recognize anatomical areas of interest such as surgical planning, monitoring therapy, clinical drug trials, image registration, stereotactic neurosurgery, radiotherapy etc. The task of this paper is to implement different unsupervised classification algorithms in ITK and perform tissue classification (white matter, gray matter, cerebrospinal fluid (CSF) and background of the human brain). For this purpose, 5 grayscale head MRI scans are provided. In order of classifying brain tissues, three algorithms are used. These are: Otsu thresholding, Bayesian classification and Bayesian classification with Gaussian smoothing. The obtained classification results are analyzed in the results and discussion section.|http://arxiv.org/abs/1902.11131v4|Shadman Sakib,Md. Abu Bakr Siddique
1680|Connecting Bayes factor and the Region of Practical Equivalence (ROPE) Procedure for testing interval null hypothesis|There has been strong recent interest in testing interval null hypothesis for improved scientific inference. For example, Lakens et al (2018) and Lakens and Harms (2017) use this approach to study if there is a pre-specified meaningful treatment effect in gerontology and clinical trials, which is different from the more traditional point null hypothesis that tests for any treatment effect. Two popular Bayesian approaches are available for interval null hypothesis testing. One is the standard Bayes factor and the other is the Region of Practical Equivalence (ROPE) procedure championed by Kruschke and others over many years. This paper establishes a formal connection between these two approaches with two benefits. First, it helps to better understand and improve the ROPE procedure. Second, it leads to a simple and effective algorithm for computing Bayes factor in a wide range of problems using draws from posterior distributions generated by standard Bayesian programs such as BUGS, JAGS and Stan. The tedious and error-prone task of coding custom-made software specific for Bayes factor is then avoided.|http://arxiv.org/abs/1903.03153v2|J. G. Liao,Vishal Midya,Arthur Berg
1681|Step Change Improvement in ADMET Prediction with PotentialNet Deep Featurization|The Absorption, Distribution, Metabolism, Elimination, and Toxicity (ADMET) properties of drug candidates are estimated to account for up to 50% of all clinical trial failures. Predicting ADMET properties has therefore been of great interest to the cheminformatics and medicinal chemistry communities in recent decades. Traditional cheminformatics approaches, whether the learner is a random forest or a deep neural network, leverage fixed fingerprint feature representations of molecules. In contrast, in this paper, we learn the features most relevant to each chemical task at hand by representing each molecule explicitly as a graph, where each node is an atom and each edge is a bond. By applying graph convolutions to this explicit molecular representation, we achieve, to our knowledge, unprecedented accuracy in prediction of ADMET properties. By challenging our methodology with rigorous cross-validation procedures and prospective analyses, we show that deep featurization better enables molecular predictors to not only interpolate but also extrapolate to new regions of chemical space.|http://arxiv.org/abs/1903.11789v1|Evan N. Feinberg,Robert Sheridan,Elizabeth Joshi,Vijay S. Pande,Alan C. Cheng
1682|The role of dose-density in combination cancer chemotherapy|A multicompartment mathematical model is presented with the goal of studying the role of dose-dense protocols in the context of combination cancer chemotherapy. Dose-dense protocols aim at reducing the period between courses of chemotherapy from three to two weeks or less, in order to avoid the regrowth of the tumor during the meantime and achieve maximum cell kill at the end of the treatment. Inspired by clinical trials, we carry out a randomized computational study to systematically compare a variety of protocols using two drugs of different specificity. Our results suggest that cycle specific drugs can be administered at low doses between courses of treatment to arrest the relapse of the tumor. This might be a better strategy than reducing the period between cycles.|http://arxiv.org/abs/1904.03410v1|lvaro G. Lpez,Kelly C. Iarosz,Antonio M. Batista,Jess M. Seoane,Ricardo L. Viana,Miguel A. F. Sanjun
1683|Robust Blocked Response-Adaptive Randomization Designs|In most clinical trials, patients are randomized with equal probability among treatments to obtain an unbiased estimate of the treatment effect. Response-adaptive randomization (RAR) has been proposed for ethical reasons, where the randomization ratio is tilted successively to favor the better performing treatment. However, the substantial disagreement regarding bias due to time-trends in adaptive randomization is not fully recognized. The type-I error is inflated in the traditional Bayesian RAR approaches when a time-trend is present. In our approach, patients are assigned in blocks and the randomization ratio is recomputed for blocks rather than traditional adaptive randomization where it is done per patient. We further investigate the design with a range of scenarios for both frequentist and Bayesian designs. We compare our method with equal randomization and with different numbers of blocks including the traditional RAR design where randomization ratio is altered patient by patient basis. The analysis is stratified if there are two or more patients in each block. Small blocks should be avoided due to the possibility of not acquiring any information from the $\mu_i$. On the other hand, RAR with large blocks has a good balance between efficiency and treating more subjects to the better-performing treatment, while retaining blocked RAR's unique unbiasedness.|http://arxiv.org/abs/1904.07758v2|Thevaa Chandereng,Rick Chappell
1684|Random Norming Aids Analysis of Non-linear Regression Models with Sequential Informative Dose Selection|A two-stage adaptive optimal design is an attractive option for increasing the efficiency of clinical trials. In these designs, based on interim data, the locally optimal dose is chosen for further exploration, which induces dependencies between data from the two stages. When the maximum likelihood estimator (MLE) is used under nonlinear regression models with independent normal errors in a pilot study where the first stage sample size is fixed, and the second stage sample size is large, the Fisher information fails to normalize the estimator adequately asymptotically, because of dependencies. In this situation, we present three alternative random information measures and show that they provide better normalization of the MLE asymptotically. The performance of random information measures is investigated in simulation studies, and the results suggest that the observed information performs best when the sample size is small.|http://arxiv.org/abs/1905.09722v1|Zhantao Lin,Nancy Flournoy,William F. Rosenberger
1685|A Deep Framework for Bone Age Assessment based on Finger Joint Localization|Bone age assessment is an important clinical trial to measure skeletal child maturity and diagnose of growth disorders. Conventional approaches such as the Tanner-Whitehouse (TW) and Greulich and Pyle (GP) may not perform well due to their large inter-observer and intra-observer variations. In this paper, we propose a finger joint localization strategy to filter out most non-informative parts of images. When combining with the conventional full image-based deep network, we observe a much-improved performance. % Our approach utilizes full hand and specific joints images for skeletal maturity prediction. In this study, we applied powerful deep neural network and explored a process in the forecast of skeletal bone age with the specifically combine joints images to increase the performance accuracy compared with the whole hand images.|http://arxiv.org/abs/1905.13124v2|Xiaoman Zhang,Ziyuan Zhao,Cen Chen,Songyou Peng,Min Wu,Zhongyao Cheng,Singee Teo,Le Zhang,Zeng Zeng
1686|Convergent stochastic algorithm for parameter estimation in frailty models using integrated partial likelihood|Frailty models are often the model of choice for heterogeneous survival data. A frailty model contains both random effects and fixed effects, with the random effects accommodating for the correlation in the data. Different estimation procedures have been proposed for the fixed effects and the variances of and covariances between the random effects. Especially with an unspecified baseline hazard, i.e., the Cox model, the few available methods deal only with a specific correlation structure. In this paper, an estimation procedure, based on the integrated partial likelihood, is introduced, which can generally deal with any kind of correlation structure. The new approach, namely the maximisation of the integrated partial likelihood, combined with a stochastic estimation procedure allows also for a wide choice of distributions for the random effects. First, we demonstrate the almost sure convergence of the stochastic algorithm towards a critical point of the integrated partial likelihood. Second, numerical convergence properties are evaluated by simulation. Third, the advantage of using an unspecified baseline hazard is demonstrated through application on cancer clinical trial data.|http://arxiv.org/abs/1909.07056v1|Oodally Ajmal,Luc Duchateau,Estelle Kuhn
1687|Robust Inference for Skewed data in Health Sciences|Health data are often not symmetric to be adequately modeled through the usual normal distributions; most of them exhibit skewed patterns. They can indeed be modeled better through the larger family of skew-normal distributions covering both skewed and symmetric cases. However, the existing likelihood based inference, that is routinely performed in these cases, is extremely non-robust against data contamination/outliers. Since outliers are not uncommon in complex real-life experimental datasets, a robust methodology automatically taking care of the noises in the data would be of great practical value to produce stable and more precise research insights leading to better policy formulation. In this paper, we develop a class of robust estimators and testing procedures for the family of skew-normal distributions using the minimum density power divergence approach with application to health data. In particular, a robust procedure for testing of symmetry is discussed in the presence of outliers. Two efficient computational algorithms are discussed. Besides deriving the asymptotic and robustness theory for the proposed methods, their advantages and utilities are illustrated through simulations and a couple of real-life applications for health data of athletes from Australian Institute of Sports and AIDS clinical trial data.|http://arxiv.org/abs/1909.10285v1|Amarnath Nandy,Ayanendranath Basu,Abhik Ghosh
1688|A Random Interaction Forest for Prioritizing Predictive Biomarkers|Precision medicine is becoming a focus in medical research recently, as its implementation brings values to all stakeholders in the healthcare system. Various statistical methodologies have been developed tackling problems in different aspects of this field, e.g., assessing treatment heterogeneity, identifying patient subgroups, or building treatment decision models. However, there is a lack of new tools devoted to selecting and prioritizing predictive biomarkers. We propose a novel tree-based ensemble method, random interaction forest (RIF), to generate predictive importance scores and prioritize candidate biomarkers for constructing refined treatment decision models. RIF was evaluated by comparing with the conventional random forest and univariable regression methods and showed favorable properties under various simulation scenarios. We applied the proposed RIF method to a biomarker dataset from two phase III clinical trials of bezlotoxumab on $\textit{Clostridium difficile}$ infection recurrence and obtained biologically meaningful results.|http://arxiv.org/abs/1910.01786v1|Zhen Zeng,Yuefeng Lu,Judong Shen,Wei Zheng,Peter Shaw,Mary Beth Dorr
1689|A fully likelihood-based approach to model survival data with crossing survival curves|Proportional hazards (PH), proportional odds (PO) and accelerated failure time (AFT) models have been widely used to deal with survival data in different fields of knowledge. Despite their popularity, such models are not suitable to handle survival data with crossing survival curves. Yang and Prentice (2005) proposed a semiparametric two-sample approach, denoted here as the YP model, allowing the analysis of crossing survival curves and including the PH and PO configurations as particular cases. In a general regression setting, the present work proposes a fully likelihood-based approach to fit the YP model. The main idea is to model the baseline hazard via the piecewise exponential (PE) distribution. The approach shares the flexibility of the semiparametric models and the tractability of the parametric representations. An extensive simulation study is developed to evaluate the performance of the proposed model. In addition, we demonstrate how useful is the new method through the analysis of survival times related to patients enrolled in a cancer clinical trial. The simulation results indicate that our model performs well for moderate sample sizes in the general regression setting. A superior performance is also observed with respect to the original YP model designed for the two-sample scenario.|http://arxiv.org/abs/1910.02406v1|Fabio N. Demarqui,Vinicius D. Mayrink
1690|Equivalence tests for binary efficacy-toxicity responses|Clinical trials often aim to compare a new drug with a reference treatment in terms of efficacy and/or toxicity depending on covariates such as, for example, the dose level of the drug. Equivalence of these treatments can be claimed if the difference in average outcome is below a certain threshold over the covariate range. In this paper we assume that the efficacy and toxicity of the treatments are measured as binary outcome variables and we address two problems. First, we develop a new test procedure for the assessment of equivalence of two treatments over the entire covariate range for a single binary endpoint. Our approach is based on a parametric bootstrap, which generates data under the constraint that the distance between the curves is equal to the pre-specified equivalence threshold. Second, we address equivalence for bivariate binary (correlated) outcomes by extending the previous approach for a univariate response. For this purpose we use a 2-dimensional Gumbel model for binary efficacy-toxicity responses. We investigate the operating characteristics of the proposed approaches by means of a simulation study and present a case study as an illustration.|http://arxiv.org/abs/1910.08769v1|Holger Dette,Kathrin Mllenhoff,Frank Bretz
1691|baymedr: An R Package and Web Application for the Calculation of Bayes Factors for Superiority, Equivalence, and Non-Inferiority Designs|Clinical trials often seek to determine the superiority, equivalence, or non-inferiority of an experimental condition (e.g., a new drug) compared to a control condition (e.g., a placebo or an already existing drug). The use of frequentist statistical methods to analyze data for these types of designs is ubiquitous even though they have several limitations. Bayesian inference remedies many of these shortcomings and allows for intuitive interpretations. In this article, we outline the frequentist conceptualization of superiority, equivalence, and non-inferiority designs and discuss its disadvantages. Subsequently, we explain how Bayes factors can be used to compare the relative plausibility of competing hypotheses. We present baymedr, an R package and web application, that provides user-friendly tools for the computation of Bayes factors for superiority, equivalence, and non-inferiority designs. Instructions on how to use baymedr are provided and an example illustrates how already existing results can be reanalyzed with baymedr.|http://arxiv.org/abs/1910.11616v3|Maximilian Linde,Don van Ravenzwaaij
1692|A Brief Summary of EEG Artifact Handling|The applications of Electroencephalogram (EEG) have been extended to out of laboratory and clinics recently due to the advancements in the technical capabilities. There are various advantageous of EEG, making it a preferable method for a wide range of applications; it is a noninvasive method, it is portable, it offers good time resolution and sufficient spatial resolution, besides there are low cost EEG systems available for a commercial use. Since the early uses of EEG, mainly as monitoring of diseases and pathologies, sleep staging and event related potential researches, it has been intertwined with undesired signal types which we call as artifacts. These pose great challenges in the practice of EEG based methods such as averaging for monitoring and diagnosis of diseases, and single-trial signal analysis for a relatively recent application in brain-computer interfaces. However, many techniques have been developed and under study for better detection and mitigation of these adverse events. The main artifact types and their handling are discussed in this brief summary.|http://arxiv.org/abs/2001.00693v1|Ibrahim Kaya
1693|Boosting Algorithms for Estimating Optimal Individualized Treatment Rules|We present nonparametric algorithms for estimating optimal individualized treatment rules. The proposed algorithms are based on the XGBoost algorithm, which is known as one of the most powerful algorithms in the machine learning literature. Our main idea is to model the conditional mean of clinical outcome or the decision rule via additive regression trees, and use the boosting technique to estimate each single tree iteratively. Our approaches overcome the challenge of correct model specification, which is required in current parametric methods. The major contribution of our proposed algorithms is providing efficient and accurate estimation of the highly nonlinear and complex optimal individualized treatment rules that often arise in practice. Finally, we illustrate the superior performance of our algorithms by extensive simulation studies and conclude with an application to the real data from a diabetes Phase III trial.|http://arxiv.org/abs/2002.00079v1|Duzhe Wang,Haoda Fu,Po-Ling Loh
1694|A general Bayesian bootstrap for censored data based on the beta-Stacy process|We introduce a novel procedure to perform Bayesian non-parametric inference with right-censored data, the \emph{beta-Stacy bootstrap}. This approximates the posterior law of summaries of the survival distribution (e.g. the mean survival time). More precisely, our procedure approximates the joint posterior law of functionals of the beta-Stacy process, a non-parametric process prior that generalizes the Dirichlet process and that is widely used in survival analysis. The beta-Stacy bootstrap generalizes and unifies other common Bayesian bootstraps for complete or censored data based on non-parametric priors. It is defined by an exact sampling algorithm that does not require tuning of Markov Chain Monte Carlo steps. We illustrate the beta-Stacy bootstrap by analyzing survival data from a real clinical trial.|http://arxiv.org/abs/2002.04081v2|Andrea Arf,Pietro Muliere
1695|Online Batch Decision-Making with High-Dimensional Covariates|We propose and investigate a class of new algorithms for sequential decision making that interacts with \textit{a batch of users} simultaneously instead of \textit{a user} at each decision epoch. This type of batch models is motivated by interactive marketing and clinical trial, where a group of people are treated simultaneously and the outcomes of the whole group are collected before the next stage of decision. In such a scenario, our goal is to allocate a batch of treatments to maximize treatment efficacy based on observed high-dimensional user covariates. We deliver a solution, named \textit{Teamwork LASSO Bandit algorithm}, that resolves a batch version of explore-exploit dilemma via switching between teamwork stage and selfish stage during the whole decision process. This is made possible based on statistical properties of LASSO estimate of treatment efficacy that adapts to a sequence of batch observations. In general, a rate of optimal allocation condition is proposed to delineate the exploration and exploitation trade-off on the data collection scheme, which is sufficient for LASSO to identify the optimal treatment for observed user covariates. An upper bound on expected cumulative regret of the proposed algorithm is provided.|http://arxiv.org/abs/2002.09438v2|Chi-Hua Wang,Guang Cheng
1696|Rapidly Personalizing Mobile Health Treatment Policies with Limited Data|In mobile health (mHealth), reinforcement learning algorithms that adapt to one's context without learning personalized policies might fail to distinguish between the needs of individuals. Yet the high amount of noise due to the in situ delivery of mHealth interventions can cripple the ability of an algorithm to learn when given access to only a single user's data, making personalization challenging. We present IntelligentPooling, which learns personalized policies via an adaptive, principled use of other users' data. We show that IntelligentPooling achieves an average of 26% lower regret than state-of-the-art across all generative models. Additionally, we inspect the behavior of this approach in a live clinical trial, demonstrating its ability to learn from even a small group of users.|http://arxiv.org/abs/2002.09971v1|Sabina Tomkins,Peng Liao,Predrag Klasnja,Serena Yeung,Susan Murphy
1697|Human Preference-Based Learning for High-dimensional Optimization of Exoskeleton Walking Gaits|Optimizing lower-body exoskeleton walking gaits for user comfort requires understanding users' preferences over a high-dimensional gait parameter space. However, existing preference-based learning methods have only explored low-dimensional domains due to computational limitations. To learn user preferences in high dimensions, this work presents LineCoSpar, a human-in-the-loop preference-based framework that enables optimization over many parameters by iteratively exploring one-dimensional subspaces. Additionally, this work identifies gait attributes that characterize broader preferences across users. In simulations and human trials, we empirically verify that LineCoSpar is a sample-efficient approach for high-dimensional preference optimization. Our analysis of the experimental data reveals a correspondence between human preferences and objective measures of dynamicity, while also highlighting differences in the utility functions underlying individual users' gait preferences. This result has implications for exoskeleton gait synthesis, an active field with applications to clinical use and patient rehabilitation.|http://arxiv.org/abs/2003.06495v2|Maegan Tucker,Myra Cheng,Ellen Novoseller,Richard Cheng,Yisong Yue,Joel W. Burdick,Aaron D. Ames
1698|The impact of multilateral imported cases of COVID-19 on the epidemic control in China|Nowadays, the epidemic of COVID-19 in China is under control. However, the epidemic are developing rapidly around the world. Due to the normal migration of population, China is facing high risk from imported cases. The potential specific medicine and vaccine is still in the process of clinical trials. Currently, controlling the impact of imported cases is the key to prevent new outbreak of COVID-19 in China. In this paper, we propose two impulsive systems to describe the impact of multilateral imported cases of COVID-19. Based on the published data, we simulate and discussed the epidemic trends under different control strategies. We compare four different scenarios and show the corresponding medical burden. The results help to design appropriate control strategy for imported cases in practice.|http://arxiv.org/abs/2004.02398v1|Jiwei Jia,Siyu Liu,Jing Ding,Guidong Liao,Lihua Zhang,Ran Zhang
1699|Bounds for the weight of external data in shrinkage estimation|Shrinkage estimation in a meta-analysis framework may be used to facilitate dynamical borrowing of information. This framework might be used to analyze a new study in the light of previous data, which might differ in their design (e.g., a randomized controlled trial (RCT) and a clinical registry). We show how the common study weights arise in effect and shrinkage estimation, and how these may be generalized to the case of Bayesian meta-analysis. Next we develop simple ways to compute bounds on the weights, so that the contribution of the external evidence may be assessed a priori. These considerations are illustrated and discussed using numerical examples, including applications in the treatment of Creutzfeldt-Jakob disease and in fetal monitoring to prevent the occurrence of metabolic acidosis. The target study's contribution to the resulting estimate is shown to be bounded below. Therefore, concerns of evidence being easily overwhelmed by external data are largely unwarranted.|http://arxiv.org/abs/2004.02525v3|Christian Rver,Tim Friede
1700|Quantile regression on inactivity time|The inactivity time, or lost lifespan specifically for mortality data, concerns time from occurrence of an event of interest to the current time point and has recently emerged as a new summary measure for cumulative information inherent in time-to-event data. This summary measure provides several benefits over the traditional methods, including more straightforward interpretation yet less sensitivity to heavy censoring. However, there exists no systematic modeling approach to inferring the quantile inactivity time in the literature. In this paper, we propose a regression method for the quantiles of the inactivity time distribution under right censoring. The consistency and asymptotic normality of the regression parameters are established. To avoid estimation of the probability density function of the inactivity time distribution under censoring, we propose a computationally efficient method for estimating the variance-covariance matrix of the regression coefficient estimates. Simulation results are presented to validate the finite sample properties of the proposed estimators and test statistics. The proposed method is illustrated with a real dataset from a clinical trial on breast cancer.|http://arxiv.org/abs/2004.06022v1|Lauren C. Balmert,Ruosha Li,Limin Peng,Jong-Hyeon Jeong
1701|Power Constrained Bandits|Contextual bandits often provide simple and effective personalization in decision making problems, making them popular tools to deliver personalized interventions in mobile health as well as other health applications. However, when bandits are deployed in the context of a scientific study -- e.g. a clinical trial to test if a mobile health intervention is effective -- the aim is not only to personalize for an individual, but also to determine, with sufficient statistical power, whether or not the system's intervention is effective. It is essential to assess the effectiveness of the intervention before broader deployment for better resource allocation. The two objectives are often deployed under different model assumptions, making it hard to determine how achieving the personalization and statistical power affect each other. In this work, we develop general meta-algorithms to modify existing algorithms such that sufficient power is guaranteed while still improving each user's well-being. We also demonstrate that our meta-algorithms are robust to various model mis-specifications possibly appearing in statistical studies, thus providing a valuable tool to study designers.|http://arxiv.org/abs/2004.06230v4|Jiayu Yao,Emma Brunskill,Weiwei Pan,Susan Murphy,Finale Doshi-Velez
1702|Searching inhibitors for three important proteins of COVID-19 through molecular docking studies|The lack of recommended drugs or vaccines to deal with the COVID-19 is the main concern of this pandemic. The approved drugs for similar health problems, drugs under clinical trials, and molecules from medicinal plants extracts are investigated randomly to deal with the COVID-19 infection. Molecular docking, one of the best approach to search therapeutically potent drugs/molecules in real time with possible hope to apply on COVID-19. In this communication, molecular docking studies of 18 ligands were carried out with the three therapeutic target proteins of SARS-CoV-2, i.e., RNA-dependent RNA polymerase (RdRp), angiotensin-converting enzyme 2 (ACE2) and spike glycoprotein (SGp). The obtained results revealed that the phytochemicals showed better dock score in compared to the drugs paracetmol and hydroxychloroquine. Combining the dock score and medicinal properties, we believe the terpenoids based phytochemicals limonin and scopadulcic acid B can be further explored for potential use against COVID-19.|http://arxiv.org/abs/2004.08095v3|Seshu Vardhan,Suban K Sahoo
1703|Identification of Repurposable Drugs and Adverse Drug Reactions for Various Courses of COVID-19 Based on Single-Cell RNA Sequencing Data|Coronavirus disease 2019 (COVID-19) has impacted almost every part of human life worldwide, posing a massive threat to human health. There is no specific drug for COVID-19, highlighting the urgent need for the development of effective therapeutics. To identify potentially repurposable drugs, we employed a systematic approach to mine candidates from U.S. FDA-approved drugs and preclinical small-molecule compounds by integrating the gene expression perturbation data for chemicals from the Library of Integrated Network-Based Cellular Signatures project with a publicly available single-cell RNA sequencing dataset from mild and severe COVID-19 patients. We identified 281 FDA-approved drugs that have the potential to be effective against SARS-CoV-2 infection, 16 of which are currently undergoing clinical trials to evaluate their efficacy against COVID-19. We experimentally tested the inhibitory effects of tyrphostin-AG-1478 and brefeldin-a on the replication of the single-stranded ribonucleic acid (ssRNA) virus influenza A virus. In conclusion, we have identified a list of repurposable anti-SARS-CoV-2 drugs using a systems biology approach.|http://arxiv.org/abs/2005.07856v2|Zhihan Wang,Kai Guo,Pan Gao,Qinqin Pu,Min Wu,Changlong Li,Junguk Hur
1704|Best Arm Identification in Spectral Bandits|We study best-arm identification with fixed confidence in bandit models with graph smoothness constraint. We provide and analyze an efficient gradient ascent algorithm to compute the sample complexity of this problem as a solution of a non-smooth max-min problem (providing in passing a simplified analysis for the unconstrained case). Building on this algorithm, we propose an asymptotically optimal strategy. We furthermore illustrate by numerical experiments both the strategy's efficiency and the impact of the smoothness constraint on the sample complexity. Best Arm Identification (BAI) is an important challenge in many applications ranging from parameter tuning to clinical trials. It is now very well understood in vanilla bandit models, but real-world problems typically involve some dependency between arms that requires more involved models. Assuming a graph structure on the arms is an elegant practical way to encompass this phenomenon, but this had been done so far only for regret minimization. Addressing BAI with graph constraints involves delicate optimization problems for which the present paper offers a solution.|http://arxiv.org/abs/2005.09841v1|Tom Kock,Aurlien Garivier
1705|A constrained sparse additive model for treatment effect-modifier selection|Sparse additive modeling is a class of effective methods for performing high-dimensional nonparametric regression. This paper develops a sparse additive model focused on estimation of treatment effect-modification with simultaneous treatment effect-modifier selection. We propose a version of the sparse additive model uniquely constrained to estimate the interaction effects between treatment and pretreatment covariates, while leaving the main effects of the pretreatment covariates unspecified. The proposed regression model can effectively identify treatment effect-modifiers that exhibit possibly nonlinear interactions with the treatment variable, that are relevant for making optimal treatment decisions. A set of simulation experiments and an application to a dataset from a randomized clinical trial are presented to demonstrate the method.|http://arxiv.org/abs/2006.00265v1|Hyung Park,Eva Petkova,Thaddeus Tarpey,R. Todd Ogden
1706|What Makes a Top-Performing Precision Medicine Search Engine? Tracing Main System Features in a Systematic Way|From 2017 to 2019 the Text REtrieval Conference (TREC) held a challenge task on precision medicine using documents from medical publications (PubMed) and clinical trials. Despite lots of performance measurements carried out in these evaluation campaigns, the scientific community is still pretty unsure about the impact individual system features and their weights have on the overall system performance. In order to overcome this explanatory gap, we first determined optimal feature configurations using the Sequential Model-based Algorithm Configuration (SMAC) program and applied its output to a BM25-based search engine. We then ran an ablation study to systematically assess the individual contributions of relevant system features: BM25 parameters, query type and weighting schema, query expansion, stop word filtering, and keyword boosting. For evaluation, we employed the gold standard data from the three TREC-PM installments to evaluate the effectiveness of different features using the commonly shared infNDCG metric.|http://arxiv.org/abs/2006.02785v2|Erik Faessler,Michel Oleynik,Udo Hahn
1707|Enabling Counterfactual Survival Analysis with Balanced Representations|Balanced representation learning methods have been applied successfully to counterfactual inference from observational data. However, approaches that account for survival outcomes are relatively limited. Survival data are frequently encountered across diverse medical applications, i.e., drug development, risk profiling, and clinical trials, and such data are also relevant in fields like manufacturing (e.g., for equipment monitoring). When the outcome of interest is a time-to-event, special precautions for handling censored events need to be taken, as ignoring censored outcomes may lead to biased estimates. We propose a theoretically grounded unified framework for counterfactual inference applicable to survival outcomes. Further, we formulate a nonparametric hazard ratio metric for evaluating average and individualized treatment effects. Experimental results on real-world and semi-synthetic datasets, the latter of which we introduce, demonstrate that the proposed approach significantly outperforms competitive alternatives in both survival-outcome prediction and treatment-effect estimation.|http://arxiv.org/abs/2006.07756v2|Paidamoyo Chapfuwa,Serge Assaad,Shuxi Zeng,Michael J. Pencina,Lawrence Carin,Ricardo Henao
1708|Review of COVID-19 Antibody Therapies|Under the global health emergency caused by coronavirus disease 2019 (COVID-19), efficient and specific therapies are urgently needed. Compared with traditional small-molecular drugs, antibody therapies are relatively easy to develop and as specific as vaccines in targeting severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), and thus attract much attention in the past few months. This work reviews seven existing antibodies for SARS-CoV-2 spike (S) protein with three-dimensional (3D) structures deposited in the Protein Data Bank. Five antibody structures associated with SARS-CoV are evaluated for their potential in neutralizing SARS-CoV-2. The interactions of these antibodies with the S protein receptor-binding domain (RBD) are compared with those of angiotensin-converting enzyme 2 (ACE2) and RBD complexes. Due to the orders of magnitude in the discrepancies of experimental binding affinities, we introduce topological data analysis (TDA), a variety of network models, and deep learning to analyze the binding strength and therapeutic potential of the aforementioned fourteen antibody-antigen complexes. The current COVID-19 antibody clinical trials, which are not limited to the S protein target, are also reviewed.|http://arxiv.org/abs/2006.10584v1|Jiahui Chen,Kaifu Gao,Rui Wang,Duc Duy Nguyen,Guo-Wei Wei
1709|A Causally Formulated Hazard Ratio Estimation through Backdoor Adjustment on Structural Causal Model|Identifying causal relationships for a treatment intervention is a fundamental problem in health sciences. Randomized controlled trials (RCTs) are considered the gold standard for identifying causal relationships. However, recent advancements in the theory of causal inference based on the foundations of structural causal models (SCMs) have allowed the identification of causal relationships from observational data, under certain assumptions. Survival analysis provides standard measures, such as the hazard ratio, to quantify the effects of an intervention. While hazard ratios are widely used in clinical and epidemiological studies for RCTs, a principled approach does not exist to compute hazard ratios for observational studies with SCMs. In this work, we review existing approaches to compute hazard ratios as well as their causal interpretation, if it exists. We also propose a novel approach to compute hazard ratios from observational studies using backdoor adjustment through SCMs and do-calculus. Finally, we evaluate the approach using experimental data for Ewing's sarcoma.|http://arxiv.org/abs/2006.12573v1|Riddhiman Adib,Paul Griffin,Sheikh Iqbal Ahamed,Mohammad Adibuzzaman
1710|Impact of the adjustment of stratification factors on time-to-event analyses|In a stratified clinical trial design with time to event end points, stratification factors are often accounted for the log-rank test and the Cox regression analyses. In this work, we have evaluated the impact of inclusion of stratification factors on the power of the stratified log-rank test and have compared the bias and standard error in HR estimate between multivariate and stratified Cox regression analyses through simulation. Results from our investigation suggests that both failing to consider stratification factor in presence of their prognostic effect and stratification with smaller number of events may substantially reduce the power of the log-rank test. Further, the HR estimate from the multivariate Cox analysis is more accurate and precise compared to the stratified Cox analysis. Our findings point towards the necessity of evaluating the impact of stratification factors on the time to event analyses at the time of study design which is presently not a norm.|http://arxiv.org/abs/2006.15283v1|Madan G. Kundu,Shoubhik Mondal
1711|NLNDE: The Neither-Language-Nor-Domain-Experts' Way of Spanish Medical Document De-Identification|Natural language processing has huge potential in the medical domain which recently led to a lot of research in this field. However, a prerequisite of secure processing of medical documents, e.g., patient notes and clinical trials, is the proper de-identification of privacy-sensitive information. In this paper, we describe our NLNDE system, with which we participated in the MEDDOCAN competition, the medical document anonymization task of IberLEF 2019. We address the task of detecting and classifying protected health information from Spanish data as a sequence-labeling problem and investigate different embedding methods for our neural network. Despite dealing in a non-standard language and domain setting, the NLNDE system achieves promising results in the competition.|http://arxiv.org/abs/2007.01030v1|Lukas Lange,Heike Adel,Jannik Strtgen
1712|On Second order correctness of Bootstrap in Logistic Regression|In the fields of clinical trials, biomedical surveys, marketing, banking, with dichotomous response variable, the logistic regression is considered as an alternative convenient approach to linear regression. In this paper, we develop a novel bootstrap technique based on perturbation resampling method for approximating the distribution of the maximum likelihood estimator (MLE) of the regression parameter vector. We establish second order correctness of the proposed bootstrap method after proper studentization and smoothing. It is shown that inferences drawn based on the proposed bootstrap method are more accurate compared to that based on asymptotic normality. The main challenge in establishing second order correctness remains in the fact that the response variable being binary, the resulting MLE has a lattice structure. We show the direct bootstrapping approach fails even after studentization. We adopt smoothing technique developed in Lahiri (1993) to ensure that the smoothed studentized version of the MLE has a density. Similar smoothing strategy is employed to the bootstrap version also to achieve second order correct approximation.|http://arxiv.org/abs/2007.01615v2|Debraj Das,Priyam Das
1713|Contextual Bandit with Missing Rewards|We consider a novel variant of the contextual bandit problem (i.e., the multi-armed bandit with side-information, or context, available to a decision-maker) where the reward associated with each context-based decision may not always be observed("missing rewards"). This new problem is motivated by certain online settings including clinical trial and ad recommendation applications. In order to address the missing rewards setting, we propose to combine the standard contextual bandit approach with an unsupervised learning mechanism such as clustering. Unlike standard contextual bandit methods, by leveraging clustering to estimate missing reward, we are able to learn from each incoming event, even those with missing rewards. Promising empirical results are obtained on several real-life datasets.|http://arxiv.org/abs/2007.06368v2|Djallel Bouneffouf,Sohini Upadhyay,Yasaman Khazaeni
1714|Inference on Average Treatment Effect under Minimization and Other Covariate-Adaptive Randomization Methods|Covariate-adaptive randomization schemes such as the minimization and stratified permuted blocks are often applied in clinical trials to balance treatment assignments across prognostic factors. The existing theoretical developments on inference after covariate-adaptive randomization are mostly limited to situations where a correct model between the response and covariates can be specified or the randomization method has well-understood properties. Based on stratification with covariate levels utilized in randomization and a further adjusting for covariates not used in randomization, in this article we propose several estimators for model free inference on average treatment effect defined as the difference between response means under two treatments. We establish asymptotic normality of the proposed estimators under all popular covariate-adaptive randomization schemes including the minimization whose theoretical property is unclear, and we show that the asymptotic distributions are invariant with respect to covariate-adaptive randomization methods. Consistent variance estimators are constructed for asymptotic inference. Asymptotic relative efficiencies and finite sample properties of estimators are also studied. We recommend using one of our proposed estimators for valid and model free inference after covariate-adaptive randomization.|http://arxiv.org/abs/2007.09576v1|Ting Ye,Yanyao Yi,Jun Shao
1715|Few-shot link prediction via graph neural networks for Covid-19 drug-repurposing|Predicting interactions among heterogenous graph structured data has numerous applications such as knowledge graph completion, recommendation systems and drug discovery. Often times, the links to be predicted belong to rare types such as the case in repurposing drugs for novel diseases. This motivates the task of few-shot link prediction. Typically, GCNs are ill-equipped in learning such rare link types since the relation embedding is not learned in an inductive fashion. This paper proposes an inductive RGCN for learning informative relation embeddings even in the few-shot learning regime. The proposed inductive model significantly outperforms the RGCN and state-of-the-art KGE models in few-shot learning tasks. Furthermore, we apply our method on the drug-repurposing knowledge graph (DRKG) for discovering drugs for Covid-19. We pose the drug discovery task as link prediction and learn embeddings for the biological entities that partake in the DRKG. Our initial results corroborate that several drugs used in clinical trials were identified as possible drug candidates. The method in this paper are implemented using the efficient deep graph learning (DGL)|http://arxiv.org/abs/2007.10261v1|Vassilis N. Ioannidis,Da Zheng,George Karypis
1716|Visualizing Deep Graph Generative Models for Drug Discovery|Drug discovery aims at designing novel molecules with specific desired properties for clinical trials. Over past decades, drug discovery and development have been a costly and time consuming process. Driven by big chemical data and AI, deep generative models show great potential to accelerate the drug discovery process. Existing works investigate different deep generative frameworks for molecular generation, however, less attention has been paid to the visualization tools to quickly demo and evaluate model's results. Here, we propose a visualization framework which provides interactive visualization tools to visualize molecules generated during the encoding and decoding process of deep graph generative models, and provide real time molecular optimization functionalities. Our work tries to empower black box AI driven drug discovery models with some visual interpretabilities.|http://arxiv.org/abs/2007.10333v1|Karan Yang,Chengxi Zang,Fei Wang
1717|PanRep: Graph neural networks for extracting universal node embeddings in heterogeneous graphs|Learning unsupervised node embeddings facilitates several downstream tasks such as node classification and link prediction. A node embedding is universal if it is designed to be used by and benefit various downstream tasks. This work introduces PanRep, a graph neural network (GNN) model, for unsupervised learning of universal node representations for heterogenous graphs. PanRep consists of a GNN encoder that obtains node embeddings and four decoders, each capturing different topological and node feature properties. Abiding to these properties the novel unsupervised framework learns universal embeddings applicable to different downstream tasks. PanRep can be furthered fine-tuned to account for possible limited labels. In this operational setting PanRep is considered as a pretrained model for extracting node embeddings of heterogenous graph data. PanRep outperforms all unsupervised and certain supervised methods in node classification and link prediction, especially when the labeled data for the supervised methods is small. PanRep-FT (with fine-tuning) outperforms all other supervised approaches, which corroborates the merits of pretraining models. Finally, we apply PanRep-FT for discovering novel drugs for Covid-19. We showcase the advantage of universal embeddings in drug repurposing and identify several drugs used in clinical trials as possible drug candidates.|http://arxiv.org/abs/2007.10445v2|Vassilis N. Ioannidis,Da Zheng,George Karypis
1718|One Click Lesion RECIST Measurement and Segmentation on CT Scans|In clinical trials, one of the radiologists' routine work is to measure tumor sizes on medical images using the RECIST criteria (Response Evaluation Criteria In Solid Tumors). However, manual measurement is tedious and subject to inter-observer variability. We propose a unified framework named SEENet for semi-automatic lesion \textit{SE}gmentation and RECIST \textit{E}stimation on a variety of lesions over the entire human body. The user is only required to provide simple guidance by clicking once near the lesion. SEENet consists of two main parts. The first one extracts the lesion of interest with the one-click guidance, roughly segments the lesion, and estimates its RECIST measurement. Based on the results of the first network, the second one refines the lesion segmentation and RECIST estimation. SEENet achieves state-of-the-art performance in lesion segmentation and RECIST estimation on the large-scale public DeepLesion dataset. It offers a practical tool for radiologists to generate reliable lesion measurements (i.e. segmentation mask and RECIST) with minimal human effort and greatly reduced time.|http://arxiv.org/abs/2007.11087v1|Youbao Tang,Ke Yan,Jing Xiao,Ranold M. Summers
1719|On the Programmatic Generation of Reproducible Documents|Reproducible document standards, like R Markdown, facilitate the programmatic creation of documents whose content is itself programmatically generated. While these documents are generally not complete in the sense that they will not include prose content, generated by an author to provide context, a narrative, etc., programmatic generation can provide substantial efficiencies for structuring and constructing documents. This paper explores the programmatic generation of reproducible by distinguishing components than can be created by computational means from those requiring human-generated prose, providing guidelines for the generation of these documents, and identifying a use case in clinical trial reporting. These concepts and use case are illustrated through the listdown package for the R programming environment, which is is currently available on the Comprehensive R Archive Network (CRAN).|http://arxiv.org/abs/2007.12631v1|Michael J. Kane,Simon Urbanek
1720|Application of Dynamic Linear Models to Random Allocation Clinical Trials with Covariates|A recent method using Dynamic Linear Models to improve preferred treatment allocation budget in random allocation models was proposed by Lee, Boone, et al (2020). However this model failed to include the impact covariates such as smoking, gender, etc, had on model performance. The current paper addresses random allocation to treatments using the DLM in Bayesian Adaptive Allocation Models with a single covariate. We show a reduced treatment allocation budget along with a reduced time to locate preferred treatment. Furthermore, a sensitivity analysis is performed on mean and variance parameters and a power analysis is conducted using Bayes Factor. This power analysis is used to determine the proportion of unallocated patient budgets above a specified cutoff value. Additionally a sensitivity analysis is conducted on covariate coefficients.|http://arxiv.org/abs/2009.11074v1|Albert H. Lee III
1721|Viroinformatics-based investigation of SARS-CoV-2 core proteins for potential therapeutic targets|Due to SARS-CoV-2 (Severe Acute Respiratory Syndrome Coronavirus 2) being a novel virus, there are currently no known effective antiviral drugs capable of slowing its progress. To accelerate the discovery of potential drug candidates, bioinformatics based in silico drug discovery can be applied as a very robust tool. In the present study, more than 60 antiviral drugs already available on the market, were chosen after literature survey. These can be used in clinical trials for the treatment of COVID-19. In this study, these candidate drugs were ranked based on their potential to interact with the Spike protein and RdRp (RNA-dependent RNA polymerase) of SARS-CoV-2. Additionally, the mechanism of their action as well as how the virus infection can utilize Hemoglobin to decrease the oxygen level in blood is explained. Moreover, multiple sequence alignments of the Spike protein with 75 sequences of different viruses from the Orthocoronavirinae subfamily were performed. This gives insight into the evolutionarily conserved domains that can be targeted using drug or antibody treatment. This multidimensional study opens a new layer of understanding about the most effective drug-targetable sites on the Spike protein of SARS-CoV-2.|http://arxiv.org/abs/2009.12817v1|Lokesh Agrawal,Thanasis Poullikkas,Scott Eisenhower,Carlo Monsanto,Ranjith Kumar Bakku
1722|A General Bayesian Model for Heteroskedastic Data with Fully Conjugate Full-Conditional Distributions|Models for heteroskedastic data are relevant in a wide variety of applications ranging from financial time series to environmental statistics. However, the topic of modeling the variance function conditionally has not seen near as much attention as modeling the mean. Volatility models have been used in specific applications, but these models can be difficult to fit in a Bayesian setting due to posterior distributions that are challenging to sample from efficiently. In this work, we introduce a general model for heteroskedastic data. This approach models the conditional variance in a mixed model approach as a function of any desired covariates or random effects. We rely on new distribution theory in order to construct priors that yield fully conjugate full conditional distributions. Thus, our approach can easily be fit via Gibbs sampling. Furthermore, we extend the model to a deep learning approach that can provide highly accurate estimates for time dependent data. We also provide an extension for heavy-tailed data. We illustrate our methodology via three applications. The first application utilizes a high dimensional soil dataset with inherent spatial dependence. The second application involves modeling of asset volatility. The third application focuses on clinical trial data for creatinine.|http://arxiv.org/abs/2009.13636v1|Paul A. Parker,Scott H. Holan,Skye A. Wills
1723|Relation-weighted Link Prediction for Disease Gene Identification|Identification of disease genes, which are a set of genes associated with a disease, plays an important role in understanding and curing diseases. In this paper, we present a biomedical knowledge graph designed specifically for this problem, propose a novel machine learning method that identifies disease genes on such graphs by leveraging recent advances in network biology and graph representation learning, study the effects of various relation types on prediction performance, and empirically demonstrate that our algorithms outperform its closest state-of-the-art competitor in disease gene identification by 24.1%. We also show that we achieve higher precision than Open Targets, the leading initiative for target identification, with respect to predicting drug targets in clinical trials for Parkinson's disease.|http://arxiv.org/abs/2011.05138v3|Srivamshi Pittala,William Koehler,Jonathan Deans,Daniel Salinas,Martin Bringmann,Katharina Sophia Volz,Berk Kapicioglu
1724|Stratification of Systemic Lupus Erythematosus Patients Using Gene Expression Data to Reveal Expression of Distinct Immune Pathways|Systemic lupus erythematosus (SLE) is the tenth leading cause of death in females 15-24 years old in the US. The diversity of symptoms and immune pathways expressed in SLE patients causes difficulties in treating SLE as well as in new clinical trials. This study used unsupervised learning on gene expression data from adult SLE patients to separate patients into clusters. The dimensionality of the gene expression data was reduced by three separate methods (PCA, UMAP, and a simple linear autoencoder) and the results from each of these methods were used to separate patients into six clusters with k-means clustering.   The clusters revealed three separate immune pathways in the SLE patients that caused SLE. These pathways were: (1) high interferon levels, (2) high autoantibody levels, and (3) dysregulation of the mitochondrial apoptosis pathway. The first two pathways have been extensively studied in SLE. However, mitochondrial apoptosis has not been investigated before to the best of our knowledge as a standalone cause of SLE, independent of autoantibody production, indicating that mitochondrial proteins could lead to a new set of therapeutic targets for SLE in future research.|http://arxiv.org/abs/2011.05143v1|Aditi Deokar
1725|Explainable Artificial Intelligence Recommendation System by Leveraging the Semantics of Adverse Childhood Experiences: Proof-of-Concept Prototype Development|The study of adverse childhood experiences and their consequences has emerged over the past 20 years. In this study, we aimed to leverage explainable artificial intelligence, and propose a proof-of-concept prototype for a knowledge-driven evidence-based recommendation system to improve surveillance of adverse childhood experiences. We used concepts from an ontology that we have developed to build and train a question-answering agent using the Google DialogFlow engine. In addition to the question-answering agent, the initial prototype includes knowledge graph generation and recommendation components that leverage third-party graph technology. To showcase the framework functionalities, we here present a prototype design and demonstrate the main features through four use case scenarios motivated by an initiative currently implemented at a children hospital in Memphis, Tennessee. Ongoing development of the prototype requires implementing an optimization algorithm of the recommendations, incorporating a privacy layer through a personal health library, and conducting a clinical trial to assess both usability and usefulness of the implementation. This semantic-driven explainable artificial intelligence prototype can enhance health care practitioners ability to provide explanations for the decisions they make.|http://arxiv.org/abs/2011.08090v1|Nariman Ammar,Arash Shaban-Nejad
1726|A general theory of regression adjustment for covariate-adaptive randomization: OLS, Lasso, and beyond|We consider the problem of estimating and inferring treatment effects in randomized experiments. In practice, stratified randomization, or more generally, covariate-adaptive randomization, is routinely used in the design stage to balance the treatment allocations with respect to a few variables that are most relevant to the outcomes. Then, regression is performed in the analysis stage to adjust the remaining imbalances to yield more efficient treatment effect estimators. Building upon and unifying the recent results obtained for ordinary least squares adjusted estimators under covariate-adaptive randomization, this paper presents a general theory of regression adjustment that allows for arbitrary model misspecification and the presence of a large number of baseline covariates. We exemplify the theory on two Lasso-adjusted treatment effect estimators, both of which are optimal in their respective classes. In addition, nonparametric consistent variance estimators are proposed to facilitate valid inferences, which work irrespective of the specific randomization methods used. The robustness and improved efficiency of the proposed estimators are demonstrated through a simulation study and a clinical trial example. This study sheds light on improving treatment effect estimation efficiency by implementing machine learning methods in covariate-adaptive randomized experiments.|http://arxiv.org/abs/2011.09734v1|Hanzhong Liu,Fuyi Tu,Wei Ma
1727|Real-time tracking of COVID-19 and coronavirus research updates through text mining|The novel coronavirus (SARS-CoV-2) which causes COVID-19 is an ongoing pandemic. There are ongoing studies with up to hundreds of publications uploaded to databases daily. We are exploring the use-case of artificial intelligence and natural language processing in order to efficiently sort through these publications. We demonstrate that clinical trial information, preclinical studies, and a general topic model can be used as text mining data intelligence tools for scientists all over the world to use as a resource for their own research. To evaluate our method, several metrics are used to measure the information extraction and clustering results. In addition, we demonstrate that our workflow not only have a use-case for COVID-19, but for other disease areas as well. Overall, our system aims to allow scientists to more efficiently research coronavirus. Our automatically updating modules are available on our information portal at https://ghddi-ailab.github.io/Targeting2019-nCoV/ for public viewing.|http://arxiv.org/abs/2102.07640v1|Yutong Jin,Jie Li,Xinyu Wang,Peiyao Li,Jinjiang Guo,Junfeng Wu,Dawei Leng,Lurong Pan
1728|Sharp Inference on Selected Subgroups in Observational Studies|In modern drug development, the broader availability of high-dimensional observational data provides opportunities for scientist to explore subgroup heterogeneity, especially when randomized clinical trials are unavailable due to cost and ethical constraints. However, a common practice that naively searches the subgroup with a high treatment level is often misleading due to the "subgroup selection bias." More importantly, the nature of high-dimensional observational data has further exacerbated the challenge of accurately estimating the subgroup treatment effects. To resolve these issues, we provide new inferential tools based on resampling to assess the replicability of post-hoc identified subgroups from observational studies. Through careful theoretical justification and extensive simulations, we show that our proposed approach delivers asymptotically sharp confidence intervals and debiased estimates for the selected subgroup treatment effects in the presence of high-dimensional covariates. We further demonstrate the merit of the proposed methods by analyzing the UK Biobank data.|http://arxiv.org/abs/2102.11338v1|Xinzhou Guo,Linqing Wei,Chong Wu,Jingshen Wang
1729|Batched Neural Bandits|In many sequential decision-making problems, the individuals are split into several batches and the decision-maker is only allowed to change her policy at the end of batches. These batch problems have a large number of applications, ranging from clinical trials to crowdsourcing. Motivated by this, we study the stochastic contextual bandit problem for general reward distributions under the batched setting. We propose the BatchNeuralUCB algorithm which combines neural networks with optimism to address the exploration-exploitation tradeoff while keeping the total number of batches limited. We study BatchNeuralUCB under both fixed and adaptive batch size settings and prove that it achieves the same regret as the fully sequential version while reducing the number of policy updates considerably. We confirm our theoretical results via simulations on both synthetic and real-world datasets.|http://arxiv.org/abs/2102.13028v1|Quanquan Gu,Amin Karbasi,Khashayar Khosravi,Vahab Mirrokni,Dongruo Zhou
1730|Simultaneous inference for partial areas under receiver operating curves -- with a view towards efficiency|We propose new simultaneous inference methods for diagnostic trials with elaborate factorial designs. Instead of the commonly used total area under the receiver operating characteristic (ROC) curve, our parameters of interest are partial areas under ROC curve segments that represent clinically relevant biomarker cut-off values. We construct a nonparametric multiple contrast test for these parameters and show that it asymptotically controls the family-wise type one error rate. Finite sample properties of this test are investigated in a series of computer experiments. We provide empirical and theoretical evidence supporting the conjecture that statistical inference about partial areas under ROC curves is more efficient than inference about the total areas.|http://arxiv.org/abs/2104.09401v6|Maximilian Wechsung,Frank Konietschke
1731|Incorporating baseline covariates to validate surrogate endpoints with a constant biomarker under control arm|A surrogate endpoint S in a clinical trial is an outcome that may be measured earlier or more easily than the true outcome of interest T. In this work, we extend causal inference approaches to validate such a surrogate using potential outcomes. The causal association paradigm assesses the relationship of the treatment effect on the surrogate with the treatment effect on the true endpoint. Using the principal surrogacy criteria, we utilize the joint conditional distribution of the potential outcomes T, given the potential outcomes S. In particular, our setting of interest allows us to assume the surrogate under the placebo, S(0), is zero-valued, and we incorporate baseline covariates in the setting of normally-distributed endpoints. We develop Bayesian methods to incorporate conditional independence and other modeling assumptions and explore their impact on the assessment of surrogacy. We demonstrate our approach via simulation and data that mimics an ongoing study of a muscular dystrophy gene therapy.|http://arxiv.org/abs/2104.12947v2|Emily Roberts,Michael Elliott,Jeremy M. G. Taylor
1732|A model of multiple hypothesis testing|Multiple hypothesis testing practices vary widely, without consensus on which are appropriate when. This paper provides an economic foundation for these practices designed to capture leading examples, such as regulatory approval on the basis of clinical trials. In studies of multiple treatments or sub-populations, adjustments may be appropriate depending on scale economies in the research production function, with control of classical notions of compound errors emerging in some but not all cases. In studies with multiple outcomes, indexing is appropriate and adjustments to test levels may be appropriate if the intended audience is heterogeneous. Data on actual costs in the drug approval process suggest both that some adjustment is warranted in that setting and that standard procedures may be overly conservative.|http://arxiv.org/abs/2104.13367v8|Davide Viviano,Kaspar Wuthrich,Paul Niehaus
1733|Reference based multiple imputation -- what is the right variance and how to estimate it|Reference based multiple imputation methods have become popular for handling missing data in randomised clinical trials. Rubin's variance estimator is well known to be biased compared to the reference based imputation estimator's true repeated sampling variance. Somewhat surprisingly given the increasingly popularity of these methods, there has been relatively little debate in the literature as to whether Rubin's variance estimator or alternative (smaller) variance estimators targeting the repeated sampling variance are more appropriate. We review the arguments made on both sides of this debate, and conclude that the repeated sampling variance is more appropriate. We review different approaches for estimating the frequentist variance, and suggest a recent proposal for combining bootstrapping with multiple imputation as a widely applicable general solution. At the same time, in light of the consequences of reference based assumptions for frequentist variance, we believe further scrutiny of these methods is warranted to determine whether the the strength of their assumptions are generally justifiable.|http://arxiv.org/abs/2104.14016v1|Jonathan W. Bartlett
1734|An age-structured model of hepatitis B viral infection highlights the potential of different therapeutic strategies|Hepatitis B virus is a global health threat, and its elimination by 2030 has been prioritised by the World Health Organisation. Here we present an age-structured model for the immune response to an HBV infection, which takes into account contributions from both cell-mediated and humoral immunity. The model has been validated using published patient data recorded during acute infection. It has been adapted to the scenarios of chronic infection, clearance of infection, and flare-ups via variation of the immune response parameters. The impacts of immune response exhaustion and non-infectious subviral particles on the immune response dynamics are analysed. A comparison of different treatment options in the context of this model reveals that drugs targeting aspects of the viral life cycle are more effective than exhaustion therapy, a form of therapy mitigating immune response exhaustion. Our results suggest that antiviral treatment is best started when viral load is declining rather than in a flare-up. The model suggests that a fast antibody production rate always lead to viral clearance, highlighting the promise of antibody therapies currently in clinical trials.|http://arxiv.org/abs/2108.01982v1|Farzad Fatehi,Richard J. Bingham,Eric C. Dykeman,Peter G. Stockley,Reidun Twarock
1735|Improving the Power of Economic Experiments Using Adaptive Designs|An important issue for many economic experiments is how the experimenter can ensure sufficient power for rejecting one or more hypotheses. Here, we apply methods developed mainly within the area of clinical trials for testing multiple hypotheses simultaneously in adaptive, two-stage designs. Our main goal is to illustrate how this approach can be used to improve the power of economic experiments. Having briefly introduced the relevant theory, we perform a simulation study supported by the open source R package asd in order to evaluate the power of some different designs. The simulations show that the power to reject at least one hypothesis can be improved while still ensuring strong control of the overall Type I error probability, and without increasing the total sample size and thus the costs of the study. The derived designs are further illustrated by applying them to two different real-world data sets from experimental economics.|http://arxiv.org/abs/2108.02526v1|Sebastian Jobjrnsson,Henning Schaak,Oliver Muhoff,Tim Friede
1736|Operational Learning-based Boundary Estimation in Electromagnetic Medical Imaging|Incorporating boundaries of the imaging object as a priori information to imaging algorithms can significantly improve the performance of electromagnetic medical imaging systems. To avoid overly complicating the system by using different sensors and the adverse effect of the subject's movement, a learning-based method is proposed to estimate the boundary (external contour) of the imaged object using the same electromagnetic imaging data. While imaging techniques may discard the reflection coefficients for being dominant and uninformative for imaging, these parameters are made use of for boundary detection. The learned model is verified through independent clinical human trials by using a head imaging system with a 16-element antenna array that works across the band 0.7-1.6 GHz. The evaluation demonstrated that the model achieves average dissimilarity of 0.012 in Hu-moment while detecting head boundary. The model enables fast scan and image creation while eliminating the need for additional devices for accurate boundary estimation.|http://arxiv.org/abs/2108.03233v1|A. Al-Saffar,A. Stancombe,A. Zamani,A. Abbosh
1737|Periodontitis and preeclampsia in pregnancy: A systematic review and meta-analysis|Objectives: A conflicting body of evidence suggests localized periodontal inflammation to spread systemically during pregnancy inducing adverse pregnancy outcomes. This systematic review and meta-analysis aimed to specifically evaluate the relationship between periodontitis and preeclampsia. Methods: Electronic searches were carried out in Medline, Pubmed, Cochrane Controlled Clinical Trial Register to identify and select observational case-control and cohort studies that analyzed the association between periodontal disease and preeclampsia. Prisma guidelines and Moose checklist were followed. Results: Thirty studies including six cohorts and twenty-four case-control studies were selected. Periodontitis was significantly associated with increased risk for preeclampsia, especially in a subgroup analysis including cohort studies and subgroup analysis with lower-middle-income countries. Conclusion: Periodontitis appears as a significant risk factor for preeclampsia, which might be even more pronounced in lower-middle-income countries.|http://arxiv.org/abs/2108.05186v1|Quynh-Anh Le,Rahena Akhter,Kimberly M. Coulton,Ngoc T. N Vo,Le T. Y Duong,Hoang V. Nong,Albert Yaacoub,George Condous,Joerg Eberhard,Ralph Nanan
1738|Strategies for Safe Multi-Armed Bandits with Logarithmic Regret and Risk|We investigate a natural but surprisingly unstudied approach to the multi-armed bandit problem under safety risk constraints. Each arm is associated with an unknown law on safety risks and rewards, and the learner's goal is to maximise reward whilst not playing unsafe arms, as determined by a given threshold on the mean risk.   We formulate a pseudo-regret for this setting that enforces this safety constraint in a per-round way by softly penalising any violation, regardless of the gain in reward due to the same. This has practical relevance to scenarios such as clinical trials, where one must maintain safety for each round rather than in an aggregated sense.   We describe doubly optimistic strategies for this scenario, which maintain optimistic indices for both safety risk and reward. We show that schema based on both frequentist and Bayesian indices satisfy tight gap-dependent logarithmic regret bounds, and further that these play unsafe arms only logarithmically many times in total. This theoretical analysis is complemented by simulation studies demonstrating the effectiveness of the proposed schema, and probing the domains in which their use is appropriate.|http://arxiv.org/abs/2204.00706v1|Tianrui Chen,Aditya Gangrade,Venkatesh Saligrama
1739|Semiparametric transformation Model with measurement error in Covariates: An Instrumental variable approach|Linear transformation model provides a general framework for analyzing censored survival data with covariates. The proportional hazards and proportional odds models are special cases of the linear transformation model. In biomedical studies, covariates with measurement error may occur in survival data. In this work, we propose a method to obtain estimators of the regression coefficients in the linear transformation model when the covariates are subject to measurement error. In the proposed method, we assume that instrumental variables are available. We develop counting process based estimating equations for finding the estimators of regression coefficients.   We prove the large sample properties of the estimators using the martingale representation of the regression estimators. The finite sample performance of the estimators are evaluated through an extensive Monte Carlo simulation study. Finally, we illustrate the proposed method using an AIDS clinical trial (ACTG 175) data.|http://arxiv.org/abs/2204.12724v2|Sudheesh K. K.,Deemat C. Mathew,Litty Mathew,Min Xie
1740|A formal framework for generalized reporting methods in parametric settings|Effect size measures and visualization techniques aimed at maximizing the interpretability and comparability of results from statistical models have long been of great importance and are recently again receiving increased attention in the literature. However, since the methods proposed in this context originate from a wide variety of disciplines and are more often than not practically motivated, they lack a common theoretical framework and many quantities are narrowly or heuristically defined. In this work, we put forward a common mathematical setting for effect size measures and visualization techniques aimed at the results of parametric regression and define a formal framework for the consistent derivation of both existing and new variants of such quantities. Throughout the presented theory, we utilize probability measures to derive weighted means over areas of interest. While we take a Bayesian approach to quantifying uncertainty in order to derive consistent results for every defined quantity, all proposed methods apply to the results of both frequentist and Bayesian inference. We apply selected specifications derived from the proposed framework to data from a clinical trial and a multi-analyst study to illustrate its versatility and relevance.|http://arxiv.org/abs/2211.02621v2|Hannah Kmpel,Sabine Hoffmann
1741|Modeling MRSA decolonization: Interactions between body sites and the impact of site-specific clearance|MRSA colonization is a critical public health concern. Decolonization protocols have been designed for the clearance of MRSA. Successful decolonization protocols reduce disease incidence; however, multiple protocols exist, comprising diverse therapies targeting multiple body sites, and the optimal protocol is unclear. Here, we formulate a machine learning model using data from a randomized controlled trial (RCT) of MRSA decolonization, which estimates interactions between body sites, quantifies the contribution of each therapy to successful decolonization, and enables predictions of the efficacy of therapy combinations. This work shows how a machine learning model can help design and improve complex clinical protocols.|http://arxiv.org/abs/2211.07413v1|Onur Poyraz,Mohamad R. A. Sater,Loren G. Miller,James A. Mckinnell,Susan S. Huang,Yonatan H. Grad,Pekka Marttinen
1742|Penalized Variable Selection with Broken Adaptive Ridge Regression for Semi-competing Risks Data|Semi-competing risks data arise when both non-terminal and terminal events are considered in a model. Such data with multiple events of interest are frequently encountered in medical research and clinical trials. In this framework, terminal event can censor the non-terminal event but not vice versa. It is known that variable selection is practical in identifying significant risk factors in high-dimensional data. While some recent works on penalized variable selection deal with these competing risks separately without incorporating possible correlation between them, we perform variable selection in an illness-death model using shared frailty where semiparametric hazard regression models are used to model the effect of covariates. We propose a broken adaptive ridge (BAR) penalty to encourage sparsity and conduct extensive simulation studies to compare its performance with other popular methods. We perform variable selection in an event specific manner so that the potential risk factors and covariates effects can be estimated and selected, simultaneously corresponding to each event in the study. The grouping effect, as well as the oracle property of the proposed BAR procedure are investigated using simulation studies. The proposed method is then applied to real-life data arising from a Colon Cancer study.|http://arxiv.org/abs/2211.09895v1|Fatemeh Mahmoudi,Xuewen Lu
1743|Homogeneity Tests and Interval Estimations of Risk Differences for Stratified Bilateral and Unilateral Correlated Data|In clinical trials studying paired parts of a subject with binary outcomes, it is expected to collect measurements bilaterally. However, there are cases where subjects contribute measurements for only one part. By utilizing combined data, it is possible to gain additional information compared to using bilateral or unilateral data alone. With the combined data, this article investigates homogeneity tests of risk differences with the presence of stratification effects and proposes interval estimations of a common risk difference if stratification does not introduce underlying dissimilarities. Under Dallal's model \citeyearpar{dallal1988paired}, we propose three test statistics and evaluate their performances regarding type I error controls and powers. Confidence intervals of a common risk difference with satisfactory coverage probabilities and interval length are constructed. Our simulation results show that the score test is the most robust and the profile likelihood confidence interval outperforms other methods proposed. Data from a study of acute otitis media is used to illustrate our proposed procedures.|http://arxiv.org/abs/2304.00162v2|Shuyi Liang,Kai-Tai Fang,Xin-Wei Huang,Yijing Xin,Chang-Xing Ma
1744|Experimentation Platforms Meet Reinforcement Learning: Bayesian Sequential Decision-Making for Continuous Monitoring|With the growing needs of online A/B testing to support the innovation in industry, the opportunity cost of running an experiment becomes non-negligible. Therefore, there is an increasing demand for an efficient continuous monitoring service that allows early stopping when appropriate. Classic statistical methods focus on hypothesis testing and are mostly developed for traditional high-stake problems such as clinical trials, while experiments at online service companies typically have very different features and focuses. Motivated by the real needs, in this paper, we introduce a novel framework that we developed in Amazon to maximize customer experience and control opportunity cost. We formulate the problem as a Bayesian optimal sequential decision making problem that has a unified utility function. We discuss extensively practical design choices and considerations. We further introduce how to solve the optimal decision rule via Reinforcement Learning and scale the solution. We show the effectiveness of this novel approach compared with existing methods via a large-scale meta-analysis on experiments in Amazon.|http://arxiv.org/abs/2304.00420v1|Runzhe Wan,Yu Liu,James McQueen,Doug Hains,Rui Song
1745|Inference in HIV dynamics models via hierarchical likelihood|HIV dynamical models are often based on non-linear systems of ordinary differential equations (ODE), which do not have analytical solution. Introducing random effects in such models leads to very challenging non-linear mixed-effects models. To avoid the numerical computation of multiple integrals involved in the likelihood, we propose a hierarchical likelihood (h-likelihood) approach, treated in the spirit of a penalized likelihood. We give the asymptotic distribution of the maximum h-likelihood estimators (MHLE) for fixed effects, a result that may be relevant in a more general setting. The MHLE are slightly biased but the bias can be made negligible by using a parametric bootstrap procedure. We propose an efficient algorithm for maximizing the h-likelihood. A simulation study, based on a classical HIV dynamical model, confirms the good properties of the MHLE. We apply it to the analysis of a clinical trial.|http://arxiv.org/abs/1002.0425v1|D. Commenges,D. Jolly,H. Putter,R. Thiebaut
1746|Hands-free Evolution of 3D-printable Objects via Eye Tracking|Interactive evolution has shown the potential to create amazing and complex forms in both 2-D and 3-D settings. However, the algorithm is slow and users quickly become fatigued. We propose that the use of eye tracking for interactive evolution systems will both reduce user fatigue and improve evolutionary success. We describe a systematic method for testing the hypothesis that eye tracking driven interactive evolution will be a more successful and easier-to-use design method than traditional interactive evolution methods driven by mouse clicks. We provide preliminary results that support the possibility of this proposal, and lay out future work to investigate these advantages in extensive clinical trials.|http://arxiv.org/abs/1304.4889v3|Nick Cheney,Jeff Clune,Jason Yosinski,Hod Lipson
1747|Subgroup Mixable Inference in Personalized Medicine, with an Application to Time-to-Event Outcomes|Measuring treatment efficacy in mixture of subgroups from a randomized clinical trial is a fundamental problem in personalized medicine development, in deciding whether to treat the entire patient population or to target a subgroup. We show that some commonly used efficacy measures are not suitable for a mixture population. We also show that, while it is important to adjust for imbalance in the data using least squares means (LSmeans) (not marginal means) estimation, the current practice of applying LSmeans to directly estimate the efficacy in a mixture population for any type of outcome is inappropriate. Proposing a new principle called {\em subgroup mixable estimation}, we establish the logical relationship among parameters that represent efficacy and develop a general inference procedure to confidently infer efficacy in subgroups and their mixtures. Using oncology studies with time-to-event outcomes as an example, we show that Hazard Ratio is not suitable for measuring efficacy in a mixture population, and provide alternative efficacy measures with a valid inference procedure.|http://arxiv.org/abs/1409.0713v1|Ying Ding,Hui-Min Lin,Jason C. Hsu
1748|On Prediction and Tolerance Intervals for Dynamic Treatment Regimes|We develop and evaluate tolerance interval methods for dynamic treatment regimes (DTRs) that can provide more detailed prognostic information to patients who will follow an estimated optimal regime. Although the problem of constructing confidence intervals for DTRs has been extensively studied, prediction and tolerance intervals have received little attention. We begin by reviewing in detail different interval estimation and prediction methods and then adapting them to the DTR setting. We illustrate some of the challenges associated with tolerance interval estimation stemming from the fact that we do not typically have data that were generated from the estimated optimal regime. We give an extensive empirical evaluation of the methods and discussed several practical aspects of method choice, and we present an example application using data from a clinical trial. Finally, we discuss future directions within this important emerging area of DTR research.|http://arxiv.org/abs/1704.07453v1|Daniel J. Lizotte,Arezoo Tahmasebi
1749|Efficient determination of optimised multi-arm multi-stage experimental designs with control of generalised error-rates|Primarily motivated by the drug development process, several publications have now presented methodology for the design of multi-arm multi-stage experiments with normally distributed outcome variables of known variance. Here, we extend these past considerations to allow the design of what we refer to as an abcd multi-arm multi-stage experiment. We provide a proof of how strong control of the a-generalised type-I familywise error-rate can be ensured. We then describe how to attain the power to reject at least b out of c false hypotheses, which is related to controlling the b-generalised type-II familywise error-rate. Following this, we detail how a design can be optimised for a scenario in which rejection of any d null hypotheses brings about termination of the experiment. We achieve this by proposing a highly computationally efficient approach for evaluating the performance of a candidate design. Finally, using a real clinical trial as a motivating example, we explore the effect of the design's control parameters on the statistical operating characteristics.|http://arxiv.org/abs/1712.00229v1|Michael Grayling,James Wason,Adrian Mander
1750|A brain signature highly predictive of future progression to Alzheimer's dementia|Early prognosis of Alzheimer's dementia is hard. Mild cognitive impairment (MCI) typically precedes Alzheimer's dementia, yet only a fraction of MCI individuals will progress to dementia, even when screened using biomarkers. We propose here to identify a subset of individuals who share a common brain signature highly predictive of oncoming dementia. This signature was composed of brain atrophy and functional dysconnectivity and discovered using a machine learning model in patients suffering from dementia. The model recognized the same brain signature in MCI individuals, 90% of which progressed to dementia within three years. This result is a marked improvement on the state-of-the-art in prognostic precision, while the brain signature still identified 47% of all MCI progressors. We thus discovered a sizable MCI subpopulation which represents an excellent recruitment target for clinical trials at the prodromal stage of Alzheimer's disease.|http://arxiv.org/abs/1712.08058v2|Christian Dansereau,Angela Tam,AmanPreet Badhwar,Sebastian Urchs,Pierre Orban,Pedro Rosa-Neto,Pierre Bellec
1751|What to make of non-inferiority and equivalence testing with a post-specified margin?|In order to determine whether or not an effect is absent based on a statistical test, the recommended frequentist tool is the equivalence test. Typically, it is expected that an appropriate equivalence margin has been specified before any data are observed. Unfortunately, this can be a difficult task. If the margin is too small, then the test's power will be substantially reduced. If the margin is too large, any claims of equivalence will be meaningless. Moreover, it remains unclear how defining the margin afterwards will bias one's results. In this short article, we consider a series of hypothetical scenarios in which the margin is defined post-hoc or is otherwise considered controversial. We also review a number of relevant, potentially problematic actual studies from clinical trials research, with the aim of motivating a critical discussion as to what is acceptable and desirable in the reporting and interpretation of equivalence tests.|http://arxiv.org/abs/1807.03413v5|Harlan Campbell,Paul Gustafson
1752|Design and Experimental Validation of an Active Catheter for Endovascular Navigation|Endovascular technique has many advantages but relies strongly on operator skills and experience. Robotically steerable catheters have been developed but few are clinically available. We describe here the development of an active and efficient catheter based on Shape Memory Alloys (SMA) actuators. We first establish the specifications of our device considering anatomical constraints. We then present a new method for building active SMA-based catheters. The proposed method relies on the use of a core body made of three parallel metallic beams and integrates wire-shaped SMA actuators. The complete device is encapsulated into a standard 6F catheter for safety purposes. A trial-error campaign comparing 70 different prototypes is then conducted to determine the best dimensions of the core structure and of the SMA actuators with respect to the imposed specifications. The final prototype is tested on a silicon-based arterial model and on a 23-kg pig. During these experiments we were able to cannulate the supra-aortic trunks and the renal arteries with different angulations and without any complication.|http://arxiv.org/abs/1906.01309v1|Thibault Couture,Jrome Szewczyk
1753|On the use of Pairwise Distance Learning for Brain Signal Classification with Limited Observations|The increasing access to brain signal data using electroencephalography creates new opportunities to study electrophysiological brain activity and perform ambulatory diagnoses of neuronal diseases. This work proposes a pairwise distance learning approach for Schizophrenia classification relying on the spectral properties of the signal. Given the limited number of observations (i.e. the case and/or control individuals) in clinical trials, we propose a Siamese neural network architecture to learn a discriminative feature space from pairwise combinations of observations per channel. In this way, the multivariate order of the signal is used as a form of data augmentation, further supporting the network generalization ability. Convolutional layers with parameters learned under a cosine contrastive loss are proposed to adequately explore spectral images derived from the brain signal. Results on a case-control population show that the features extracted using the proposed neural network lead to an improved Schizophrenia diagnosis (+10pp in accuracy and sensitivity) against spectral features, thus suggesting the existence of non-trivial, discriminative electrophysiological brain patterns.|http://arxiv.org/abs/1906.02076v2|David Calhas,Enrique Romero,Rui Henriques
1754|A new Bayesian two-sample t-test for effect size estimation under uncertainty based on a two-component Gaussian mixture with known allocations and the region of practical equivalence|Testing differences between a treatment and control group is common practice in biomedical research like randomized controlled trials (RCT). The standard two-sample t-test relies on null hypothesis significance testing (NHST) via p-values, which has several drawbacks. Bayesian alternatives were recently introduced using the Bayes factor, which has its own limitations. This paper introduces an alternative to current Bayesian two-sample t-tests by interpreting the underlying model as a two-component Gaussian mixture in which the effect size is the quantity of interest, which is most relevant in clinical research. Unlike p-values or the Bayes factor, the proposed method focusses on estimation under uncertainty instead of explicit hypothesis testing. Therefore, via a Gibbs sampler the posterior of the effect size is produced, which is used subsequently for either estimation under uncertainty or explicit hypothesis testing based on the region of practical equivalence (ROPE). An illustrative example, theoretical results and a simulation study show the usefulness of the proposed method, and the test is made available in the R package bayest.|http://arxiv.org/abs/1906.07524v2|Riko Kelter
1755|Optimal experimental designs for treatment contrasts in heteroscedastic models with covariates|In clinical trials, the response of a given subject often depends on the selected treatment as well as on some covariates. We study optimal approximate designs of experiments in the models with treatment and covariate effects. We allow for the variances of the responses to depend on the chosen treatments, which introduces heteroscedasticity into the models. For estimating systems of treatment contrasts and linear functions of the covariates, we extend known results on D-optimality of product designs by providing product designs that are optimal with respect to general eigenvalue-based criteria. In particular, A- and E-optimal product designs are obtained. We then formulate a method based on linear programming for constructing optimal designs with smaller supports from the optimal product designs. The sparser designs can be more easily converted to practically applicable exact designs. The provided results and the proposed sparsification method are demonstrated on some examples.|http://arxiv.org/abs/1907.04044v1|Samuel Rosa
1756|Sharp hypotheses and bispatial inference|A fundamental class of inferential problems are those characterised by there having been a substantial degree of pre-data (or prior) belief that the value of a model parameter was equal or lay close to a specified value, which may, for example, be the value that indicates the absence of an effect. Standard ways of tackling problems of this type, including the Bayesian method, are often highly inadequate in practice. To address this issue, an inferential framework called bispatial inference is put forward, which can be viewed as both a generalisation and radical reinterpretation of existing approaches to inference that are based on P values. It is shown that to obtain an appropriate post-data density function for a given parameter, it is often convenient to combine a special type of bispatial inference, which is constructed around one-sided P values, with a previously outlined form of fiducial inference. Finally, by using what are called post-data opinion curves, this bispatial-fiducial theory is naturally extended to deal with the general scenario in which any number of parameters may be unknown. The application of the theory is illustrated in various examples, which are especially relevant to the analysis of clinical trial data.|http://arxiv.org/abs/1911.09049v2|Russell J. Bowater
1757|Modeling Variables with a Detection Limit using a Truncated Normal Distribution with Censoring|When data are collected subject to a detection limit, observations below the detection limit may be considered censored. In addition, the domain of such observations may be restricted; for example, values may be required to be non-negative. We propose a regression method for censored observations that also accounts for domain restriction. The method finds maximum likelihood estimates assuming an underlying truncated normal distribution. We show that our method, tcensReg, outperforms other methods commonly used for data with detection limits such as Tobit regression and single imputation of the detection limit or half detection limit with respect to bias and mean squared error under a range of simulation settings. We apply our method to analyze vision quality data collected from ophthalmology clinical trials comparing different types of intraocular lenses implanted during cataract surgery.|http://arxiv.org/abs/1911.11221v1|Justin R. Williams,Hyung-Woo Kim,Catherine M. Crespi
1758|Predicting in vivo escape dynamics of HIV-1 from a broadly neutralizing antibody|Broadly neutralizing antibodies are promising candidates for treatment and prevention of HIV-1 infections. Such antibodies can temporarily suppress viral load in infected individuals; however, the virus often rebounds by escape mutants that have evolved resistance. In this paper, we map an in vivo fitness landscape of HIV-1 interacting with broadly neutralizing antibodies, using data from a recent clinical trial. We identify two fitness factors, antibody dosage and viral load, that determine viral reproduction rates reproducibly across different hosts. The model successfully predicts the escape dynamics of HIV-1 in the course of an antibody treatment, including a characteristic frequency turnover between sensitive and resistant strains. This turnover is governed by a dosage-dependent fitness ranking, resulting from an evolutionary tradeoff between antibody resistance and its collateral cost in drug-free growth. Our analysis suggests resistance-cost tradeoff curves as a measure of antibody performance in the presence of resistance evolution.|http://arxiv.org/abs/2008.02547v1|Matthijs Meijers,Kanika Vanshylla,Henning Gruell,Florian Klein,Michael Laessig
1759|Retrofitting Vector Representations of Adverse Event Reporting Data to Structured Knowledge to Improve Pharmacovigilance Signal Detection|Adverse drug events (ADE) are prevalent and costly. Clinical trials are constrained in their ability to identify potential ADEs, motivating the development of spontaneous reporting systems for post-market surveillance. Statistical methods provide a convenient way to detect signals from these reports but have limitations in leveraging relationships between drugs and ADEs given their discrete count-based nature. A previously proposed method, aer2vec, generates distributed vector representations of ADE report entities that capture patterns of similarity but cannot utilize lexical knowledge. We address this limitation by retrofitting aer2vec drug embeddings to knowledge from RxNorm and developing a novel retrofitting variant using vector rescaling to preserve magnitude. When evaluated in the context of a pharmacovigilance signal detection task, aer2vec with retrofitting consistently outperforms disproportionality metrics when trained on minimally preprocessed data. Retrofitting with rescaling results in further improvements in the larger and more challenging of two pharmacovigilance reference sets used for evaluation.|http://arxiv.org/abs/2008.03340v1|Xiruo Ding,Trevor Cohen
1760|Improving Company Valuations with Automated Knowledge Discovery, Extraction and Fusion|Performing company valuations within the domain of biotechnology, pharmacy and medical technology is a challenging task, especially when considering the unique set of risks biotech start-ups face when entering new markets. Companies specialized in global valuation services, therefore, combine valuation models and past experience with heterogeneous metrics and indicators that provide insights into a company's performance. This paper illustrates how automated knowledge discovery, extraction and data fusion can be used to (i) obtain additional indicators that provide insights into the success of a company's product development efforts, and (ii) support labor-intensive data curation processes. We apply deep web knowledge acquisition methods to identify and harvest data on clinical trials that is hidden behind proprietary search interfaces and integrate the extracted data into the industry partner's company valuation ontology. In addition, focused Web crawls and shallow semantic parsing yield information on the company's key personnel and respective contact data, notifying domain experts of relevant changes that get then incorporated into the industry partner's company data.|http://arxiv.org/abs/2010.09249v1|Albert Weichselbraun,Philipp Kuntschik,Sandro Hrler
1761|Bayesian Multivariate Probability of Success Using Historical Data with Strict Control of Family-wise Error Rate|Given the cost and duration of phase III and phase IV clinical trials, the development of statistical methods for go/no-go decisions is vital. In this paper, we introduce a Bayesian methodology to compute the probability of success based on the current data of a treatment regimen for the multivariate linear model. Our approach utilizes a Bayesian seemingly unrelated regression model, which allows for multiple endpoints to be modeled jointly even if the covariates between the endpoints are different. Correlations between endpoints are explicitly modeled. This Bayesian joint modeling approach unifies single and multiple testing procedures under a single framework. We develop an approach to multiple testing that asymptotically guarantees strict family-wise error rate control, and is more powerful than frequentist approaches to multiplicity. The method effectively yields those of Ibrahim et al. and Chuang-Stein as special cases, and, to our knowledge, is the only method that allows for robust sample size determination for multiple endpoints and/or hypotheses and the only method that provides strict family-wise type I error control in the presence of multiplicity.|http://arxiv.org/abs/2010.13774v1|Ethan M. Alt,Matthew A. Psioda,Joseph G. Ibrahim
1762|Multi-Decoder Networks with Multi-Denoising Inputs for Tumor Segmentation|Automatic segmentation of brain glioma from multimodal MRI scans plays a key role in clinical trials and practice. Unfortunately, manual segmentation is very challenging, time-consuming, costly, and often inaccurate despite human expertise due to the high variance and high uncertainty in the human annotations. In the present work, we develop an end-to-end deep-learning-based segmentation method using a multi-decoder architecture by jointly learning three separate sub-problems using a partly shared encoder. We also propose to apply smoothing methods to the input images to generate denoised versions as additional inputs to the network. The validation performance indicate an improvement when using the proposed method. The proposed method was ranked 2nd in the task of Quantification of Uncertainty in Segmentation in the Brain Tumors in Multimodal Magnetic Resonance Imaging Challenge 2020.|http://arxiv.org/abs/2012.03684v1|Minh H. Vu,Tufve Nyholm,Tommy Lfstedt
1763|Effect of right censoring bias on survival analysis|Kaplan-Meier survival analysis represents the most objective measure of treatment efficacy in oncology, though subjected to potential bias, which is worrisome in an era of precision medicine. Independent of the bias inherent to the design and execution of clinical trials, bias may be the result of patient censoring, or incomplete observation. Unlike disease/progression free survival, overall survival is based on a well defined time point and thus avoids interval censoring, but right-censoring, due to incomplete follow-up, may still be a source of bias. We study three mechanisms of right-censoring and find that one of them, surrogate of patient lost to follow-up, is able to impact Kaplan-Meier survival, improving significantly the estimation of survival in comparison with complete follow-up datasets, as measured by the hazard ratio. We also present two bias indexes able to signal datasets with right-censoring associated overestimation of survival. These bias indexes can detect bias in public available datasets|http://arxiv.org/abs/2012.08649v1|Enrique Barrajn,Laura Barrajn
1764|Etat de l'art sur l'application des bandits multi-bras|The Multi-armed bandit offer the advantage to learn and exploit the already learnt knowledge at the same time. This capability allows this approach to be applied in different domains, going from clinical trials where the goal is investigating the effects of different experimental treatments while minimizing patient losses, to adaptive routing where the goal is to minimize the delays in a network. This article provides a review of the recent results on applying bandit to real-life scenario and summarize the state of the art for each of these fields. Different techniques has been proposed to solve this problem setting, like epsilon-greedy, Upper confident bound (UCB) and Thompson Sampling (TS). We are showing here how this algorithms were adapted to solve the different problems of exploration exploitation.|http://arxiv.org/abs/2101.00001v1|Djallel Bouneffouf
1765|Applications of artificial intelligence in drug development using real-world data|The US Food and Drug Administration (FDA) has been actively promoting the use of real-world data (RWD) in drug development. RWD can generate important real-world evidence reflecting the real-world clinical environment where the treatments are used. Meanwhile, artificial intelligence (AI), especially machine- and deep-learning (ML/DL) methods, have been increasingly used across many stages of the drug development process. Advancements in AI have also provided new strategies to analyze large, multidimensional RWD. Thus, we conducted a rapid review of articles from the past 20 years, to provide an overview of the drug development studies that use both AI and RWD. We found that the most popular applications were adverse event detection, trial recruitment, and drug repurposing. Here, we also discuss current research gaps and future opportunities.|http://arxiv.org/abs/2101.08904v2|Zhaoyi Chen,Xiong Liu,William Hogan,Elizabeth Shenkman,Jiang Bian
1766|Health and Demographic Surveillance Systems and the 2030 Agenda: Sustainable Development Goals|The health and demographic surveillance system (HDSS) is an old method for intensively monitoring a population to assess the effects of healthcare or other population-level interventions - often clinical trials. The strengths of HDSS include very detailed descriptions of whole populations with frequent updates. This often provides long time series of accurate population and health indicators for the HDSS study population. The primary weakness of HDSS is that the data describe only the HDSS study population and cannot be generalized beyond that.   The 2030 agenda is the ecosystem of activities - many including population-level monitoring - that relate to the United Nations (UN) Sustainable Development Goals (SDG). With respect to the 2030 agenda, HDSS can contribute by: continuing to conduct cause-and-effect studies; contributing to data triangulation or amalgamation initiatives; characterizing the bias in and calibrating 'big data'; and contributing more to the rapid training of data-oriented professionals, especially in the population and health fields.|http://arxiv.org/abs/2103.03910v1|Samuel J. Clark
1767|Encrypted Linear Contextual Bandit|Contextual bandit is a general framework for online learning in sequential decision-making problems that has found application in a wide range of domains, including recommendation systems, online advertising, and clinical trials.   A critical aspect of bandit methods is that they require to observe the contexts --i.e., individual or group-level data-- and rewards in order to solve the sequential problem. The large deployment in industrial applications has increased interest in methods that preserve the users' privacy. In this paper, we introduce a privacy-preserving bandit framework based on homomorphic encryption{\color{violet} which allows computations using encrypted data}. The algorithm \textit{only} observes encrypted information (contexts and rewards) and has no ability to decrypt it. Leveraging the properties of homomorphic encryption, we show that despite the complexity of the setting, it is possible to solve linear contextual bandits over encrypted data with a $\widetilde{O}(d\sqrt{T})$ regret bound in any linear contextual bandit problem, while keeping data encrypted.|http://arxiv.org/abs/2103.09927v2|Evrard Garcelon,Vianney Perchet,Matteo Pirotta
1768|Distance Assisted Recursive Testing|In many applications, a large number of features are collected with the goal to identify a few important ones. Sometimes, these features lie in a metric space with a known distance matrix, which partially reflects their co-importance pattern. Proper use of the distance matrix will boost the power of identifying important features. Hence, we develop a new multiple testing framework named the Distance Assisted Recursive Testing (DART). DART has two stages. In stage 1, we transform the distance matrix into an aggregation tree, where each node represents a set of features. In stage 2, based on the aggregation tree, we set up dynamic node hypotheses and perform multiple testing on the tree. All rejections are mapped back to the features. Under mild assumptions, the false discovery proportion of DART converges to the desired level in high probability converging to one. We illustrate by theory and simulations that DART has superior performance under various models compared to the existing methods. We applied DART to a clinical trial in the allogeneic stem cell transplantation study to identify the gut microbiota whose abundance will be impacted by the after-transplant care.|http://arxiv.org/abs/2103.11085v2|Xuechan Li,Anthony Sung,Jichun Xie
1769|Pair-switching rerandomization|Rerandomization discards assignments with covariates unbalanced in the treatment and control groups to improve estimation and inference efficiency. However, the acceptance-rejection sampling method used in rerandomization is computationally inefficient. As a result, it is time-consuming for rerandomization to draw numerous independent assignments, which are necessary for performing Fisher randomization tests and constructing randomization-based confidence intervals. To address this problem, we propose a pair-switching rerandomization method to draw balanced assignments efficiently. We obtain the unbiasedness and variance reduction of the difference-in-means estimator and show that the Fisher randomization tests are valid under pair-switching rerandomization. Moreover, we propose an exact approach to invert Fisher randomization tests to confidence intervals, which is faster than the existing methods. In addition, our method is applicable to both non-sequentially and sequentially randomized experiments. We conduct comprehensive simulation studies to compare the finite-sample performance of the proposed method with that of classical rerandomization. Simulation results indicate that pair-switching rerandomization leads to comparable power of Fisher randomization tests and is 3--23 times faster than classical rerandomization. Finally, we apply the pair-switching rerandomization method to analyze two clinical trial datasets, both of which demonstrate the advantages of our method.|http://arxiv.org/abs/2103.13051v2|Ke Zhu,Hanzhong Liu
1770|Exploring, browsing and interacting with multi-scale structures of knowledge|The ICT revolution has given birth to a world of digital traces. A wide number of knowledgedriven domains like science are daily fueled by unlimited flows of textual contents. In order to navigate across these growing constellations of words, interdisciplinary innovations are emerging at the crossroad between social and computational sciences. In particular, complex systems approaches make it now possible to reconstruct multi-level and multi-scale structures of knowledge by means of phylomemies: inheritance networks of elements of knowledge. In this article, we will introduce an endogenous way to visualize the outcomes of the phylomemy reconstruction process by combining both synchronic and diachronic approaches. Our aim is to translate high-dimensional phylomemetic networks into graphical projections and interactive visualizations. To that end, we will use seabed and kinship views to translate the multilevel and multi-scale properties of complex branches of knowledge. We will then define a generic macro-to-micro methodology of exploration implemented within an open source software called Memiescape and validate our approach by browsing through the reconstructed histories of thousands of scientific publications and clinical trials.|http://arxiv.org/abs/2103.15448v1|Quentin Lobb,Alexandre Delano,David Chavalarias
1771|Fair Exploration via Axiomatic Bargaining|Exploration is often necessary in online learning to maximize long-term reward, but it comes at the cost of short-term 'regret'. We study how this cost of exploration is shared across multiple groups. For example, in a clinical trial setting, patients who are assigned a sub-optimal treatment effectively incur the cost of exploration. When patients are associated with natural groups on the basis of, say, race or age, it is natural to ask whether the cost of exploration borne by any single group is 'fair'. So motivated, we introduce the 'grouped' bandit model. We leverage the theory of axiomatic bargaining, and the Nash bargaining solution in particular, to formalize what might constitute a fair division of the cost of exploration across groups. On the one hand, we show that any regret-optimal policy strikingly results in the least fair outcome: such policies will perversely leverage the most 'disadvantaged' groups when they can. More constructively, we derive policies that are optimally fair and simultaneously enjoy a small 'price of fairness'. We illustrate the relative merits of our algorithmic framework with a case study on contextual bandits for warfarin dosing where we are concerned with the cost of exploration across multiple races and age groups.|http://arxiv.org/abs/2106.02553v2|Jackie Baek,Vivek F. Farias
1772|Robust Stochastic Linear Contextual Bandits Under Adversarial Attacks|Stochastic linear contextual bandit algorithms have substantial applications in practice, such as recommender systems, online advertising, clinical trials, etc. Recent works show that optimal bandit algorithms are vulnerable to adversarial attacks and can fail completely in the presence of attacks. Existing robust bandit algorithms only work for the non-contextual setting under the attack of rewards and cannot improve the robustness in the general and popular contextual bandit environment. In addition, none of the existing methods can defend against attacked context. In this work, we provide the first robust bandit algorithm for stochastic linear contextual bandit setting under a fully adaptive and omniscient attack with sub-linear regret. Our algorithm not only works under the attack of rewards, but also under attacked context. Moreover, it does not need any information about the attack budget or the particular form of the attack. We provide theoretical guarantees for our proposed algorithm and show by experiments that our proposed algorithm improves the robustness against various kinds of popular attacks.|http://arxiv.org/abs/2106.02978v3|Qin Ding,Cho-Jui Hsieh,James Sharpnack
1773|Multi-armed Bandit Requiring Monotone Arm Sequences|In many online learning or multi-armed bandit problems, the taken actions or pulled arms are ordinal and required to be monotone over time. Examples include dynamic pricing, in which the firms use markup pricing policies to please early adopters and deter strategic waiting, and clinical trials, in which the dose allocation usually follows the dose escalation principle to prevent dose limiting toxicities. We consider the continuum-armed bandit problem when the arm sequence is required to be monotone. We show that when the unknown objective function is Lipschitz continuous, the regret is $O(T)$. When in addition the objective function is unimodal or quasiconcave, the regret is $\tilde O(T^{3/4})$ under the proposed algorithm, which is also shown to be the optimal rate. This deviates from the optimal rate $\tilde O(T^{2/3})$ in the continuous-armed bandit literature and demonstrates the cost to the learning efficiency brought by the monotonicity requirement.|http://arxiv.org/abs/2106.03790v4|Ningyuan Chen
1774|Infinite-color randomly reinforced urns with dominant colors|We define and prove limit results for a class of dominant P\'olya sequences, which are randomly reinforced urn processes with color-specific random weights and unbounded number of possible colors. Under fairly mild assumptions on the expected reinforcement, we show that the predictive and the empirical distributions converge almost surely (a.s.) in total variation to the same random probability measure $\tilde{P}$; moreover, $\tilde{P}(\mathcal{D})=1$ a.s., where $\mathcal{D}$ denotes the set of dominant colors for which the expected reinforcement is maximum. In the general case, the predictive probabilities and the empirical frequencies of any $\delta$-neighborhood of $\mathcal{D}$ converge a.s. to one. That is, although non-dominant colors continue to be regularly observed, their distance to $\mathcal{D}$ converges in probability to zero. We refine the above results with rates of convergence. We further hint potential applications of dominant P\'olya sequences in randomized clinical trials and species sampling, and use our central limit results for Bayesian inference.|http://arxiv.org/abs/2106.04307v2|Hristo Sariev,Sandra Fortini,Sonia Petrone
1775|Copula-Frailty Models for Recurrent Event Data Based on Monte Carlo EM Algorithm|Multi-type recurrent events are often encountered in medical applications when two or more different event types could repeatedly occur over an observation period. For example, patients may experience recurrences of multi-type nonmelanoma skin cancers in a clinical trial for skin cancer prevention. The aims in those applications are to characterize features of the marginal processes, evaluate covariate effects, and quantify both the within-subject recurrence dependence and the dependence among different event types. We use copula-frailty models to analyze correlated recurrent events of different types. Parameter estimation and inference are carried out by using a Monte Carlo expectation-maximization (MCEM) algorithm, which can handle a relatively large (i.e., three or more) number of event types. Performances of the proposed methods are evaluated via extensive simulation studies. The developed methods are used to model the recurrences of skin cancer with different types.|http://arxiv.org/abs/2106.05204v1|Khaled F. Bedair,Yili Hong,Hussein R. Al-Khalidi
1776|A Nonmyopic Approach to Cost-Constrained Bayesian Optimization|Bayesian optimization (BO) is a popular method for optimizing expensive-to-evaluate black-box functions. BO budgets are typically given in iterations, which implicitly assumes each evaluation has the same cost. In fact, in many BO applications, evaluation costs vary significantly in different regions of the search space. In hyperparameter optimization, the time spent on neural network training increases with layer size; in clinical trials, the monetary cost of drug compounds vary; and in optimal control, control actions have differing complexities. Cost-constrained BO measures convergence with alternative cost metrics such as time, money, or energy, for which the sample efficiency of standard BO methods is ill-suited. For cost-constrained BO, cost efficiency is far more important than sample efficiency. In this paper, we formulate cost-constrained BO as a constrained Markov decision process (CMDP), and develop an efficient rollout approximation to the optimal CMDP policy that takes both the cost and future iterations into account. We validate our method on a collection of hyperparameter optimization problems as well as a sensor set selection application.|http://arxiv.org/abs/2106.06079v1|Eric Hans Lee,David Eriksson,Valerio Perrone,Matthias Seeger
1777|Advance in Reversible Covalent Kinase Inhibitors|Reversible covalent kinase inhibitors (RCKIs) are a class of novel kinase inhibitors attracting increasing attention because they simultaneously show the selectivity of covalent kinase inhibitors, yet avoid permanent protein-modification-induced adverse effects. Over the last decade, RCKIs have been reported to target different kinases, including atypical kinases. Currently, three RCKIs are undergoing clinical trials to treat specific diseases, for example, Pemphigus, an autoimmune disorder. In this perspective, first, RCKIs are systematically summarized, including characteristics of electrophilic groups, chemical scaffolds, nucleophilic residues, and binding modes. Second, we provide insights into privileged electrophiles, the distribution of nucleophiles and hence effective design strategies for RCKIs. Finally, we provide a brief perspective on future design strategies for RCKIs, including those that target proteins other than kinases.|http://arxiv.org/abs/2106.11698v2|Zheng Zhao,Philip E. Bourne
1778|Heterogeneous network-based drug repurposing for COVID-19|The Corona Virus Disease 2019 (COVID-19) belongs to human coronaviruses (HCoVs), which spreads rapidly around the world. Compared with new drug development, drug repurposing may be the best shortcut for treating COVID-19. Therefore, we constructed a comprehensive heterogeneous network based on the HCoVs-related target proteins and use the previously proposed deepDTnet, to discover potential drug candidates for COVID-19. We obtain high performance in predicting the possible drugs effective for COVID-19 related proteins. In summary, this work utilizes a powerful heterogeneous network-based deep learning method, which may be beneficial to quickly identify candidate repurposable drugs toward future clinical trials for COVID-19. The code and data are available at https://github.com/stjin-XMU/HnDR-COVID.|http://arxiv.org/abs/2107.09217v1|Shuting Jin,Xiangxiang Zeng,Wei Huang,Feng Xia,Changzhi Jiang,Xiangrong Liu,Shaoliang Peng
1779|On matching-adjusted indirect comparison and calibration estimation|Indirect comparisons have been increasingly used to compare data from different sources such as clinical trials and observational data in, e.g., a disease registry. To adjust for population differences between data sources, matching-adjusted indirect comparison (MAIC) has been used in several applications including health technology assessment and drug regulatory submissions. In fact, MAIC can be considered as a special case of a range of methods known as calibration estimation in survey sampling. However, to our best knowledge, this connection has not been examined in detail. This paper makes three contributions: 1. We examined this connection by comparing MAIC and a few commonly used calibration estimation methods, including the entropy balancing approach, which is equivalent to MAIC. 2. We considered the standard error (SE) estimation of the MAIC estimators and propose a model-independent SE estimator and examine its performance by simulation. 3. We conducted a simulation to compare these commonly used approaches to evaluate their performance in indirect comparison scenarios.|http://arxiv.org/abs/2107.11687v1|Jixian Wang
1780|Ignorable and non-ignorable missing data in hidden Markov models|We consider missing data in the context of hidden Markov models with a focus on situations where data is missing not at random (MNAR) and missingness depends on the identity of the hidden states. In simulations, we show that including a submodel for state-dependent missingness reduces bias when data is MNAR and state-dependent, whilst not reducing accuracy when data is missing at random (MAR). When missingness depends on time but not the hidden states, a model which only allows for state-dependent missingness is biased, whilst a model that allows for both state- and time-dependent missingness is not. Overall, these results show that modelling missingness as state-dependent, and including other relevant covariates, is a useful strategy in applications of hidden Markov models to time-series with missing data. We conclude with an application of the state- and time-dependent MNAR hidden Markov model to a real dataset, involving severity of schizophrenic symptoms in a clinical trial.|http://arxiv.org/abs/2109.02770v1|Maarten Speekenbrink,Ingmar Visser
1781|Thematic analysis of multiple sclerosis research by enhanced strategic diagram|This bibliometric review summarised the research trends and analysed research areas in multiple sclerosis (MS) over the last decade. The documents containing the term "multiple sclerosis" in the article title were retrieved from the Scopus database. We found a total of 18003 articles published in journals in the English language between 2012 and 2021. The emerging keywords identified utilising the enhanced strategic diagram were "covid-19", "teriflunomide", "clinical trial", "microglia", "b cells", "myelin", "brain", "white matter", "functional connectivity", "pain", "employment", "health-related quality of life", "meta-analysis" and "comorbidity". In conclusion, this study demonstrates the tremendous growth of MS literature worldwide, which is expected to grow more than double during the next decade especially in the identified emerging topics.|http://arxiv.org/abs/2109.05688v1|Nazlahshaniza Shafina,Che Aishah Nazariah Ismaila,Mohd Zulkifli Mustafa,Nurhafizah Ghani,Asma Hayati Ahmad,Zahiruddin Othman,Adi Wijaya,Rahimah Zakaria
1782|Parametric Modeling Approach to COVID-19 Pandemic Data|The problem of skewness is common among clinical trials and survival data which has being the research focus derivation and proposition of different flexible distributions. Thus, a new distribution called Extended Rayleigh Lomax distribution is constructed from Rayleigh Lomax distribution to capture the excessiveness of some survival data. We derive the new distribution by using beta logit function proposed by Jones (2004). Some statistical properties of the distribution such as probability density function, cumulative density function, reliability rate, hazard rate, reverse hazard rate, moment generating functions, likelihood functions, skewness, kurtosis and coefficient of variation are obtained. We also performed the expected estimation of model parameters by maximum likelihood; goodness of fit and model selection criteria including Anderson Darling (AD), CramerVon Misses (CVM), Kolmogorov Smirnov (KS), Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC) and Consistent Akaike Information Criterion (CAIC) are employed to select the better distribution from those models considered in the work. The results from the statistics criteria show that the proposed distribution performs better with better representation of the States in Nigeria COVID-19 death cases data than other competing models.|http://arxiv.org/abs/2109.06254v1|N. I. Badmus,O. Faweya,S. A. Ige
1783|Canonical fundamental skew-t linear mixed models|In clinical trials, studies often present longitudinal data or clustered data. These studies are commonly analyzed using linear mixed models (LMMs), usually considering Gaussian assumptions for random effect and error terms. Recently, several proposals extended the restrictive assumptions from traditional LMM by more flexible ones that can accommodate skewness and heavy-tails and consequently are more robust to outliers. This work proposes a canonical fundamental skew-t linear mixed model (ST-LMM), that allows for asymmetric and heavy-tailed random effects and errors and includes several important cases as special cases, which are presented and considered for model selection. For this robust and flexible model, we present an efficient EM-type algorithm for parameter estimation via maximum likelihood, implemented in a closed form by exploring the hierarchical representation of the ST-LMM. In addition, the estimation of standard errors and random effects is discussed. The methodology is illustrated through an application to schizophrenia data and some simulation studies.|http://arxiv.org/abs/2109.12152v1|Fernanda L. Schumacher,Larissa A. Matos,Celso R. B. Cabral
1784|SurvTRACE: Transformers for Survival Analysis with Competing Events|In medicine, survival analysis studies the time duration to events of interest such as mortality. One major challenge is how to deal with multiple competing events (e.g., multiple disease diagnoses). In this work, we propose a transformer-based model that does not make the assumption for the underlying survival distribution and is capable of handling competing events, namely SurvTRACE. We account for the implicit \emph{confounders} in the observational setting in multi-events scenarios, which causes selection bias as the predicted survival probability is influenced by irrelevant factors. To sufficiently utilize the survival data to train transformers from scratch, multiple auxiliary tasks are designed for multi-task learning. The model hence learns a strong shared representation from all these tasks and in turn serves for better survival analysis. We further demonstrate how to inspect the covariate relevance and importance through interpretable attention mechanisms of SurvTRACE, which suffices to great potential in enhancing clinical trial design and new treatment development. Experiments on METABRIC, SUPPORT, and SEER data with 470k patients validate the all-around superiority of our method.|http://arxiv.org/abs/2110.00855v2|Zifeng Wang,Jimeng Sun
1785|Interval Estimation of Relative Risks for Combined Unilateral and Bilateral Correlated Data|Measurements are generally collected as unilateral or bilateral data in clinical trials or observational studies. For example, in ophthalmology studies, the primary outcome is often obtained from one eye or both eyes of an individual. In medical studies, the relative risk is usually the parameter of interest and is commonly used. In this article, we develop three confidence intervals for the relative risk for combined unilateral and bilateral correlated data under the equal dependence assumption. The proposed confidence intervals are based on maximum likelihood estimates of parameters derived using the Fisher scoring method. Simulation studies are conducted to evaluate the performance of proposed confidence intervals with respect to the empirical coverage probability, the mean interval width, and the ratio of mesial non-coverage probability to the distal non-coverage probability. We also compare the proposed methods with the confidence interval based on the method of variance estimates recovery and the confidence interval obtained from the modified Poisson regression model with correlated binary data. We recommend the score confidence interval for general applications because it best controls converge probabilities at the 95% level with reasonable mean interval width. We illustrate the methods with a real-world example.|http://arxiv.org/abs/2110.15468v1|Kejia Wang,Chang-Xing Ma
1786|Identifying and mitigating bias in algorithms used to manage patients in a pandemic|Numerous COVID-19 clinical decision support systems have been developed. However many of these systems do not have the merit for validity due to methodological shortcomings including algorithmic bias. Methods Logistic regression models were created to predict COVID-19 mortality, ventilator status and inpatient status using a real-world dataset consisting of four hospitals in New York City and analyzed for biases against race, gender and age. Simple thresholding adjustments were applied in the training process to establish more equitable models. Results Compared to the naively trained models, the calibrated models showed a 57% decrease in the number of biased trials, while predictive performance, measured by area under the receiver/operating curve (AUC), remained unchanged. After calibration, the average sensitivity of the predictive models increased from 0.527 to 0.955. Conclusion We demonstrate that naively training and deploying machine learning models on real world data for predictive analytics of COVID-19 has a high risk of bias. Simple implemented adjustments or calibrations during model training can lead to substantial and sustained gains in fairness on subsequent deployment.|http://arxiv.org/abs/2111.00340v1|Yifan Li,Garrett Yoon,Mustafa Nasir-Moin,David Rosenberg,Sean Neifert,Douglas Kondziolka,Eric Karl Oermann
1787|Pseudo-domains in imaging data improve prediction of future disease status in multi-center studies|In multi-center randomized clinical trials imaging data can be diverse due to acquisition technology or scanning protocols. Models predicting future outcome of patients are impaired by this data heterogeneity. Here, we propose a prediction method that can cope with a high number of different scanning sites and a low number of samples per site. We cluster sites into pseudo-domains based on visual appearance of scans, and train pseudo-domain specific models. Results show that they improve the prediction accuracy for steatosis after 48 weeks from imaging data acquired at an initial visit and 12-weeks follow-up in liver disease|http://arxiv.org/abs/2111.07634v1|Matthias Perkonigg,Peter Mesenbrink,Alexander Goehler,Miljen Martic,Ahmed Ba-Ssalamah,Georg Langs
1788|Model-assisted complier average treatment effect estimates in randomized experiments with non-compliance and a binary outcome|In randomized experiments, the actual treatments received by some experimental units may differ from their treatment assignments. This non-compliance issue often occurs in clinical trials, social experiments, and the applications of randomized experiments in many other fields. Under certain assumptions, the average treatment effect for the compliers is identifiable and equal to the ratio of the intention-to-treat effects of the potential outcomes to that of the potential treatment received. To improve the estimation efficiency, we propose three model-assisted estimators for the complier average treatment effect in randomized experiments with a binary outcome. We study their asymptotic properties, compare their efficiencies with that of the Wald estimator, and propose the Neyman-type conservative variance estimators to facilitate valid inferences. Moreover, we extend our methods and theory to estimate the multiplicative complier average treatment effect. Our analysis is randomization-based, allowing the working models to be misspecified. Finally, we conduct simulation studies to illustrate the advantages of the model-assisted methods and apply these analysis methods in a randomized experiment to evaluate the effect of academic services or incentives on academic performance.|http://arxiv.org/abs/2111.10109v2|Jiyang Ren
1789|Towards Integrative Multi-Modal Personal Health Navigation Systems: Framework and Application|It is well understood that an individual's health trajectory is influenced by choices made in each moment, such as from lifestyle or medical decisions. With the advent of modern sensing technologies, individuals have more data and information about themselves than any other time in history. How can we use this data to make the best decisions to keep the health state optimal? We propose a generalized Personal Health Navigation (PHN) framework. PHN takes individuals towards their personal health goals through a system which perpetually digests data streams, estimates current health status, computes the best route through intermediate states utilizing personal models, and guides the best inputs that carry a user towards their goal.   In addition to describing the general framework, we test the PHN system in two experiments within the field of cardiology. First, we prospectively test a knowledge-infused cardiovascular PHN system with a pilot clinical trial of 41 users. Second, we build a data-driven personalized model on cardiovascular exercise response variability on a smartwatch data-set of 33,269 real-world users. We conclude with critical challenges in health computing for PHN systems that require deep future investigation.|http://arxiv.org/abs/2111.10403v2|Nitish Nag,Hyungik Oh,Mengfan Tang,Mingshu Shi,Ramesh Jain
1790|A Unified Decision Framework for Phase I Dose-Finding Designs|The purpose of a phase I dose-finding clinical trial is to investigate the toxicity profiles of various doses for a new drug and identify the maximum tolerated dose. Over the past three decades, various dose-finding designs have been proposed and discussed, including conventional model-based designs, new model-based designs using toxicity probability intervals, and rule-based designs. We present a simple decision framework that can generate several popular designs as special cases. We show that these designs share common elements under the framework, such as the same likelihood function, the use of loss functions, and the nature of the optimal decisions as Bayes rules. They differ mostly in the choice of the prior distributions. We present theoretical results on the decision framework and its link to specific and popular designs like mTPI, BOIN, and CRM. These results provide useful insights into the designs and their underlying assumptions, and convey information to help practitioners select an appropriate design.|http://arxiv.org/abs/2111.12244v1|Yunshan Duan,Shijie Yuan,Yuan Ji,Peter Mueller
1791|Generalizing Off-Policy Learning under Sample Selection Bias|Learning personalized decision policies that generalize to the target population is of great relevance. Since training data is often not representative of the target population, standard policy learning methods may yield policies that do not generalize target population. To address this challenge, we propose a novel framework for learning policies that generalize to the target population. For this, we characterize the difference between the training data and the target population as a sample selection bias using a selection variable. Over an uncertainty set around this selection variable, we optimize the minimax value of a policy to achieve the best worst-case policy value on the target population. In order to solve the minimax problem, we derive an efficient algorithm based on a convex-concave procedure and prove convergence for parametrized spaces of policies such as logistic policies. We prove that, if the uncertainty set is well-specified, our policies generalize to the target population as they can not do worse than on the training data. Using simulated data and a clinical trial, we demonstrate that, compared to standard policy learning methods, our framework improves the generalizability of policies substantially.|http://arxiv.org/abs/2112.01387v1|Tobias Hatt,Daniel Tschernutter,Stefan Feuerriegel
1792|A Unifying Bayesian Approach for Sample Size Determination Using Design and Analysis Priors|Power and sample size analysis comprises a critical component of clinical trial study design. There is an extensive collection of methods addressing this problem from diverse perspectives. The Bayesian paradigm, in particular, has attracted noticeable attention and includes different perspectives for sample size determination. Building upon a cost-effectiveness analysis undertaken by O'Hagan and Stevens (2001) with different priors in the design and analysis stage, we develop a general Bayesian framework for simulation-based sample size determination that can be easily implemented on modest computing architectures. We further qualify the need for different priors for the design and analysis stage. We work primarily in the context of conjugate Bayesian linear regression models, where we consider the situation with known and unknown variances. Throughout, we draw parallels with frequentist solutions, which arise as special cases, and alternate Bayesian approaches with an emphasis on how the numerical results from existing methods arise as special cases in our framework.|http://arxiv.org/abs/2112.03509v1|Jane Pan,Sudipto Banerjee
1793|Multiply robust estimators in longitudinal studies with missing data under control-based imputation|Longitudinal studies are often subject to missing data. The ICH E9(R1) addendum addresses the importance of defining a treatment effect estimand with the consideration of intercurrent events. Jump-to-reference (J2R) is one classically envisioned control-based scenario for the treatment effect evaluation using the hypothetical strategy, where the participants in the treatment group after intercurrent events are assumed to have the same disease progress as those with identical covariates in the control group. We establish new estimators to assess the average treatment effect based on a proposed potential outcomes framework under J2R. Various identification formulas are constructed under the assumptions addressed by J2R, motivating estimators that rely on different parts of the observed data distribution. Moreover, we obtain a novel estimator inspired by the efficient influence function, with multiple robustness in the sense that it achieves $n^{1/2}$-consistency if any pairs of multiple nuisance functions are correctly specified, or if the nuisance functions converge at a rate not slower than $n^{-1/4}$ when using flexible modeling approaches. The finite-sample performance of the proposed estimators is validated in simulation studies and an antidepressant clinical trial.|http://arxiv.org/abs/2112.06000v2|Siyi Liu,Shu Yang,Yilong Zhang,Guanghan,Liu
1794|Stratified modestly-weighted log-rank tests in settings with an anticipated delayed separation of survival curves|Delayed separation of survival curves is a common occurrence in confirmatory studies in immuno-oncology. Many novel statistical methods that aim to efficiently capture potential long-term survival improvements have been proposed in recent years. However, the vast majority do not consider stratification, which is a major limitation considering that most (if not all) large confirmatory studies currently employ a stratified primary analysis. In this article, we combine recently proposed weighted log-rank tests that have been designed to work well under a delayed separation of survival curves, with stratification by a baseline variable. The aim is to increase the efficiency of the test when the stratifying variable is highly prognostic for survival. As there are many potential ways to combine the two techniques, we compare several possibilities in an extensive simulation study. We also apply the techniques retrospectively to two recent randomized clinical trials.|http://arxiv.org/abs/2201.10445v1|Dominic Magirr,Jos L. Jimnez
1795|Covariate-Adjusted Log-Rank Test: Guaranteed Efficiency Gain and Universal Applicability|Nonparametric covariate adjustment is considered for log-rank type tests of treatment effect with right-censored time-to-event data from clinical trials applying covariate-adaptive randomization. Our proposed covariate-adjusted log-rank test has a simple explicit formula and a guaranteed efficiency gain over the unadjusted test. We also show that our proposed test achieves universal applicability in the sense that the same formula of test can be universally applied to simple randomization and all commonly used covariate-adaptive randomization schemes such as the stratified permuted block and Pocock and Simon's minimization, which is not a property enjoyed by the unadjusted log-rank test. Our method is supported by novel asymptotic theory and empirical results for type I error and power of tests.|http://arxiv.org/abs/2201.11948v2|Ting Ye,Jun Shao,Yanyao Yi
1796|Optimizing Warfarin Dosing using Deep Reinforcement Learning|Warfarin is a widely used anticoagulant, and has a narrow therapeutic range. Dosing of warfarin should be individualized, since slight overdosing or underdosing can have catastrophic or even fatal consequences. Despite much research on warfarin dosing, current dosing protocols do not live up to expectations, especially for patients sensitive to warfarin. We propose a deep reinforcement learning-based dosing model for warfarin. To overcome the issue of relatively small sample sizes in dosing trials, we use a Pharmacokinetic/ Pharmacodynamic (PK/PD) model of warfarin to simulate dose-responses of virtual patients. Applying the proposed algorithm on virtual test patients shows that this model outperforms a set of clinically accepted dosing protocols by a wide margin. We tested the robustness of our dosing protocol on a second PK/PD model and showed that its performance is comparable to the set of baseline protocols.|http://arxiv.org/abs/2202.03486v3|Sadjad Anzabi Zadeh,W. Nick Street,Barrett W. Thomas
1797|Automated Architecture Search for Brain-inspired Hyperdimensional Computing|This paper represents the first effort to explore an automated architecture search for hyperdimensional computing (HDC), a type of brain-inspired neural network. Currently, HDC design is largely carried out in an application-specific ad-hoc manner, which significantly limits its application. Furthermore, the approach leads to inferior accuracy and efficiency, which suggests that HDC cannot perform competitively against deep neural networks. Herein, we present a thorough study to formulate an HDC architecture search space. On top of the search space, we apply reinforcement-learning to automatically explore the HDC architectures. The searched HDC architectures show competitive performance on case studies involving a drug discovery dataset and a language recognition task. On the Clintox dataset, which tries to learn features from developed drugs that passed/failed clinical trials for toxicity reasons, the searched HDC architecture obtains the state-of-the-art ROC-AUC scores, which are 0.80% higher than the manually designed HDC and 9.75% higher than conventional neural networks. Similar results are achieved on the language recognition task, with 1.27% higher performance than conventional methods.|http://arxiv.org/abs/2202.05827v1|Junhuan Yang,Yi Sheng,Sizhe Zhang,Ruixuan Wang,Kenneth Foreman,Mikell Paige,Xun Jiao,Weiwen Jiang,Lei Yang
1798|A bias-adjusted estimator in quantile regression for clustered data|The manuscript discusses how to incorporate random effects for quantile regression models for clustered data with focus on settings with many but small clusters. The paper has three contributions: (i) documenting that existing methods may lead to severely biased estimators for fixed effects parameters; (ii) proposing a new two-step estimation methodology where predictions of the random effects are first computed {by a pseudo likelihood approach (the LQMM method)} and then used as offsets in standard quantile regression; (iii) proposing a novel bootstrap sampling procedure in order to reduce bias of the two-step estimator and compute confidence intervals. The proposed estimation and associated inference is assessed numerically through rigorous simulation studies and applied to an AIDS Clinical Trial Group (ACTG) study.|http://arxiv.org/abs/2202.11501v1|Maria Laura Battagliola,Helle Srensen,Anders Tolver,Ana-Maria Staicu
1799|Ensemble Method for Estimating Individualized Treatment Effects|In many medical and business applications, researchers are interested in estimating individualized treatment effects using data from a randomized experiment. For example in medical applications, doctors learn the treatment effects from clinical trials and in technology companies, researchers learn them from A/B testing experiments. Although dozens of machine learning models have been proposed for this task, it is challenging to determine which model will be best for the problem at hand because ground-truth treatment effects are unobservable. In contrast to several recent papers proposing methods to select one of these competing models, we propose an algorithm for aggregating the estimates from a diverse library of models. We compare ensembling to model selection on 43 benchmark datasets, and find that ensembling wins almost every time. Theoretically, we prove that our ensemble model is (asymptotically) at least as accurate as the best model under consideration, even if the number of candidate models is allowed to grow with the sample size.|http://arxiv.org/abs/2202.12445v2|Kevin Wu Han,Han Wu
1800|A one-size-fits-all artificial pancreas for people with type 1 diabetes based on physiological insight and feedback control|We propose a model-free artificial pancreas (AP) for people with type 1 diabetes. The algorithmic parameters are tuned to a virtual population of 1,000,000 individuals, and the AP repeatedly estimates the basal and bolus insulin requirements necessary for maintaining normal blood glucose levels. Therefore, the AP can be used without healthcare personnel or engineers customizing the algorithm to each user. The estimates are based on bodyweight, measurements from a continuous glucose monitor (CGM), and estimates of the meal carbohydrate contents. In a virtual clinical trial with all 1,000,000 individuals (i.e., a Monte Carlo closed-loop simulation), the AP achieves a mean time in range of more than 87% and almost 89% of the participants satisfy several glycemic targets.|http://arxiv.org/abs/2202.13338v1|Tobias K. S. Ritschel,Asbjrn Thode Reenberg,Emilie B. Lindkvist,Christian Laugesen,Jannet Svensson,Ajenthen G. Ranjan,Kirsten Nrgaard,Bernd Dammann,John Bagterp Jrgensen
1801|Learning Conditional Variational Autoencoders with Missing Covariates|Conditional variational autoencoders (CVAEs) are versatile deep generative models that extend the standard VAE framework by conditioning the generative model with auxiliary covariates. The original CVAE model assumes that the data samples are independent, whereas more recent conditional VAE models, such as the Gaussian process (GP) prior VAEs, can account for complex correlation structures across all data samples. While several methods have been proposed to learn standard VAEs from partially observed datasets, these methods fall short for conditional VAEs. In this work, we propose a method to learn conditional VAEs from datasets in which auxiliary covariates can contain missing values as well. The proposed method augments the conditional VAEs with a prior distribution for the missing covariates and estimates their posterior using amortised variational inference. At training time, our method marginalises the uncertainty associated with the missing covariates while simultaneously maximising the evidence lower bound. We develop computationally efficient methods to learn CVAEs and GP prior VAEs that are compatible with mini-batching. Our experiments on simulated datasets as well as on a clinical trial study show that the proposed method outperforms previous methods in learning conditional VAEs from non-temporal, temporal, and longitudinal datasets.|http://arxiv.org/abs/2203.01218v1|Siddharth Ramchandran,Gleb Tikhonov,Otto Lnnroth,Pekka Tiikkainen,Harri Lhdesmki
1802|A Single Index Model for Longitudinal Outcomes to Optimize Individual Treatment Decision Rules|A pressing challenge in medical research is to identify optimal treatments for individual patients. This is particularly challenging in mental health settings where mean responses are often similar across multiple treatments. For example, the mean longitudinal trajectories for patients treated with an active drug and placebo may be very similar but different treatments may exhibit distinctly different individual trajectory shapes. Most precision medicine approaches using longitudinal data often ignore information from the longitudinal data structure. This paper investigates a powerful precision medicine approach by examining the impact of baseline covariates on longitudinal outcome trajectories to guide treatment decisions instead of traditional scalar outcome measures derived from longitudinal data, such as a change score. We introduce a method of estimating "biosignatures" defined as linear combinations of baseline characteristics (i.e., a single index) that optimally separate longitudinal trajectories among different treatment groups. The criterion used is to maximize the Kullback-Leibler Divergence between different treatment outcome distributions. The approach is illustrated via simulation studies and a depression clinical trial. The approach is also contrasted with more traditional methods and compares performance in the presence of missing data.|http://arxiv.org/abs/2203.03523v1|Lanqiu Yao,Thaddeus Tarpey
1803|bayesassurance: An R package for calculating sample size and Bayesian assurance|We present a bayesassurance R package that computes the Bayesian assurance under various settings characterized by different assumptions and objectives. The package offers a constructive set of simulation-based functions suitable for addressing a wide range of clinical trial study design problems. We provide a detailed description of the underlying framework embedded within each of the power and assurance functions and demonstrate their usage through a series of worked-out examples. Through these examples, we hope to corroborate the advantages that come with using a two-stage generalized structure. We also illustrate scenarios where the Bayesian assurance and frequentist power overlap, allowing the user to address both Bayesian and classical inference problems provided that the parameters are properly defined. All assurance-related functions included in this R package rely on a two-stage Bayesian method that assigns two distinct priors to evaluate the unconditional probability of observing a positive outcome, which in turn addresses subtle limitations that take place when using the standard single-prior approach.|http://arxiv.org/abs/2203.15154v1|Jane Pan,Sudipto Banerjee
1804|Toward Data-Driven Digital Therapeutics Analytics: Literature Review and Research Directions|With the advent of Digital Therapeutics (DTx), the development of software as a medical device (SaMD) for mobile and wearable devices has gained significant attention in recent years. Existing DTx evaluations, such as randomized clinical trials, mostly focus on verifying the effectiveness of DTx products. To acquire a deeper understanding of DTx engagement and behavioral adherence, beyond efficacy, a large amount of contextual and interaction data from mobile and wearable devices during field deployment would be required for analysis. In this work, the overall flow of the data-driven DTx analytics is reviewed to help researchers and practitioners to explore DTx datasets, to investigate contextual patterns associated with DTx usage, and to establish the (causal) relationship of DTx engagement and behavioral adherence. This review of the key components of data-driven analytics provides novel research directions in the analysis of mobile sensor and interaction datasets, which helps to iteratively improve the receptivity of existing DTx.|http://arxiv.org/abs/2205.01851v3|Uichin Lee,Gyuwon Jung,Eun-Yeol Ma,Jin San Kim,Heepyung Kim,Jumabek Alikhanov,Youngtae Noh,Heeyoung Kim
1805|A Deep Bayesian Bandits Approach for Anticancer Therapy: Exploration via Functional Prior|Learning personalized cancer treatment with machine learning holds great promise to improve cancer patients' chance of survival. Despite recent advances in machine learning and precision oncology, this approach remains challenging as collecting data in preclinical/clinical studies for modeling multiple treatment efficacies is often an expensive, time-consuming process. Moreover, the randomization in treatment allocation proves to be suboptimal since some participants/samples are not receiving the most appropriate treatments during the trial. To address this challenge, we formulate drug screening study as a "contextual bandit" problem, in which an algorithm selects anticancer therapeutics based on contextual information about cancer cell lines while adapting its treatment strategy to maximize treatment response in an "online" fashion. We propose using a novel deep Bayesian bandits framework that uses functional prior to approximate posterior for drug response prediction based on multi-modal information consisting of genomic features and drug structure. We empirically evaluate our method on three large-scale in vitro pharmacogenomic datasets and show that our approach outperforms several benchmarks in identifying optimal treatment for a given cell line.|http://arxiv.org/abs/2205.02944v2|Mingyu Lu,Yifang Chen,Su-In Lee
1806|A Comparative Tutorial of Bayesian Sequential Design and Reinforcement Learning|Reinforcement Learning (RL) is a computational approach to reward-driven learning in sequential decision problems. It implements the discovery of optimal actions by learning from an agent interacting with an environment rather than from supervised data. We contrast and compare RL with traditional sequential design, focusing on simulation-based Bayesian sequential design (BSD). Recently, there has been an increasing interest in RL techniques for healthcare applications. We introduce two related applications as motivating examples. In both applications, the sequential nature of the decisions is restricted to sequential stopping. Rather than a comprehensive survey, the focus of the discussion is on solutions using standard tools for these two relatively simple sequential stopping problems. Both problems are inspired by adaptive clinical trial design. We use examples to explain the terminology and mathematical background that underlie each framework and map one to the other. The implementations and results illustrate the many similarities between RL and BSD. The results motivate the discussion of the potential strengths and limitations of each approach.|http://arxiv.org/abs/2205.04023v2|Mauricio Tec,Yunshan Duan,Peter Mller
1807|A Survey of Risk-Aware Multi-Armed Bandits|In several applications such as clinical trials and financial portfolio optimization, the expected value (or the average reward) does not satisfactorily capture the merits of a drug or a portfolio. In such applications, risk plays a crucial role, and a risk-aware performance measure is preferable, so as to capture losses in the case of adverse events. This survey aims to consolidate and summarise the existing research on risk measures, specifically in the context of multi-armed bandits. We review various risk measures of interest, and comment on their properties. Next, we review existing concentration inequalities for various risk measures. Then, we proceed to defining risk-aware bandit problems, We consider algorithms for the regret minimization setting, where the exploration-exploitation trade-off manifests, as well as the best-arm identification setting, which is a pure exploration problem -- both in the context of risk-sensitive measures. We conclude by commenting on persisting challenges and fertile areas for future research.|http://arxiv.org/abs/2205.05843v1|Vincent Y. F. Tan,Prashanth L. A.,Krishna Jagannathan
1808|Closed-Form Solution of the Unit Normal Loss Integral in Two-Dimensions|In Value of Information (VoI) analysis, the unit normal loss integral (UNLI) frequently emerges as a solution for the computation of various VoI metrics. However, one limitation of the UNLI has been that its closed-form solution is available for only one dimension, and thus can be used for comparisons involving only two strategies (where it is applied to the scalar incremental net benefit). We derived a closed-form solution for the two-dimensional UNLI, enabling closed-form VoI calculations for three strategies. We verified the accuracy of this method via simulation studies. A case study based on a three-arm clinical trial was used as an example. VoI methods based on the closed-form solutions for the UNLI can now be extended to three-decision comparisons, taking a fraction of a second to compute and not being subject to Monte Carlo error. An R implementation of this method is provided as part of the predtools package (https://github.com/resplab/predtools/).|http://arxiv.org/abs/2205.06364v3|Tae Yoon Lee,Paul Gustafson,Mohsen Sadatsafavi
1809|Risk Filtering and Risk-Averse Control of Markovian Systems Subject to Model Uncertainty|We consider a Markov decision process subject to model uncertainty in a Bayesian framework, where we assume that the state process is observed but its law is unknown to the observer. In addition, while the state process and the controls are observed at time $t$, the actual cost that may depend on the unknown parameter is not known at time $t$. The controller optimizes total cost by using a family of special risk measures, that we call risk filters and that are appropriately defined to take into account the model uncertainty of the controlled system. These key features lead to non-standard and non-trivial risk-averse control problems, for which we derive the Bellman principle of optimality. We illustrate the general theory on two practical examples: optimal investment and clinical trials.|http://arxiv.org/abs/2206.09235v1|Tomasz R. Bielecki,Igor Cialenco
1810|Bayesian Optimization under Stochastic Delayed Feedback|Bayesian optimization (BO) is a widely-used sequential method for zeroth-order optimization of complex and expensive-to-compute black-box functions. The existing BO methods assume that the function evaluation (feedback) is available to the learner immediately or after a fixed delay. Such assumptions may not be practical in many real-life problems like online recommendations, clinical trials, and hyperparameter tuning where feedback is available after a random delay. To benefit from the experimental parallelization in these problems, the learner needs to start new function evaluations without waiting for delayed feedback. In this paper, we consider the BO under stochastic delayed feedback problem. We propose algorithms with sub-linear regret guarantees that efficiently address the dilemma of selecting new function queries while waiting for randomly delayed feedback. Building on our results, we also make novel contributions to batch BO and contextual Gaussian process bandits. Experiments on synthetic and real-life datasets verify the performance of our algorithms.|http://arxiv.org/abs/2206.09341v1|Arun Verma,Zhongxiang Dai,Bryan Kian Hsiang Low
1811|Constrained D-optimal Design for Paid Research Study|We consider constrained sampling problems in paid research studies or clinical trials. When qualified volunteers are more than the budget allowed, we recommend a D-optimal sampling strategy based on the optimal design theory and develop a constrained lift-one algorithm to find the optimal allocation. Unlike the literature which mainly deals with linear models, our solution solves the constrained sampling problem under fairly general statistical models, including generalized linear models and multinomial logistic models, and with more general constraints. We justify theoretically the optimality of our sampling strategy and show by simulation studies and real-world examples the advantages over simple random sampling and proportionally stratified sampling strategies.|http://arxiv.org/abs/2207.05281v4|Yifei Huang,Liping Tong,Jie Yang
1812|Toward a Human-Centered AI-assisted Colonoscopy System|AI-assisted colonoscopy has received lots of attention in the last decade. Several randomised clinical trials in the previous two years showed exciting results of the improving detection rate of polyps. However, current commercial AI-assisted colonoscopy systems focus on providing visual assistance for detecting polyps during colonoscopy. There is a lack of understanding of the needs of gastroenterologists and the usability issues of these systems. This paper aims to introduce the recent development and deployment of commercial AI-assisted colonoscopy systems to the HCI community, identify gaps between the expectation of the clinicians and the capabilities of the commercial systems, and highlight some unique challenges in Australia.|http://arxiv.org/abs/2208.02523v1|Hsiang-Ting Chen,Yuan Zhang,Gustavo Carneiro,Seon Ho Shin,Rajvinder Singh
1813|Comparing Results of Thermographic Images Based Diagnosis for Breast Diseases|This paper examines the potential contribution of infrared (IR) imaging in breast diseases detection. It compares obtained results using some algorithms for detection of malignant breast conditions such as Support Vector Machine (SVM) regarding the consistency of different approaches when applied to public data. Moreover, in order to avail the actual IR imaging's capability as a complement on clinical trials and to promote researches using high-resolution IR imaging we deemed the use of a public database revised by confidently trained breast physicians as essential. Only the static acquisition protocol is regarded in our work. We used lO2 IR single breast images from the Pro Engenharia (PROENG) public database (54 normal and 48 with some finding). These images were collected from Universidade Federal de Pernambuco (UFPE) University's Hospital. We employed the same features proposed by the authors of the work that presented the best results and achieved an accuracy of 61.7 % and Youden index of 0.24 using the Sequential Minimal Optimization (SMO) classifier.|http://arxiv.org/abs/2208.14410v1|E. O. Rodrigues,A. Conci,T. B. Borchartt,A. C. Paiva,A. C. Silva,T. MacHenry
1814|An Empathetic AI Coach for Self-Attachment Therapy|In this work, we present a new dataset and a computational strategy for a digital coach that aims to guide users in practicing the protocols of self-attachment therapy. Our framework augments a rule-based conversational agent with a deep-learning classifier for identifying the underlying emotion in a user's text response, as well as a deep-learning assisted retrieval method for producing novel, fluent and empathetic utterances. We also craft a set of human-like personas that users can choose to interact with. Our goal is to achieve a high level of engagement during virtual therapy sessions. We evaluate the effectiveness of our framework in a non-clinical trial with N=16 participants, all of whom have had at least four interactions with the agent over the course of five days. We find that our platform is consistently rated higher for empathy, user engagement and usefulness than the simple rule-based framework. Finally, we provide guidelines to further improve the design and performance of the application, in accordance with the feedback received.|http://arxiv.org/abs/2209.08316v2|Lisa Alazraki,Ali Ghachem,Neophytos Polydorou,Foaad Khosmood,Abbas Edalat
1815|Consistent Covariance estimation for stratum imbalances under minimization method for covariate-adaptive randomization|Pocock and Simon's minimization method is a popular approach for covariate-adaptive randomization in clinical trials. Valid statistical inference with data collected under the minimization method requires the knowledge of the limiting covariance matrix of within-stratum imbalances, whose existence is only recently established. In this work, we propose a bootstrap-based estimator for this limit and establish its consistency, in particular, by Le Cam's third lemma. As an application, we consider in simulation studies adjustments to existing robust tests for treatment effects with survival data by the proposed estimator. It shows that the adjusted tests achieve a size close to the nominal level, and unlike other designs, the robust tests without adjustment may have an asymptotic size inflation issue under the minimization method.|http://arxiv.org/abs/2209.13117v3|Zixuan Zhao,Yanglei Song,Wenyu Jiang,Dongsheng Tu
1816|Selection of the Optimal Personalized Treatment from Multiple Treatments with Right-censored Multivariate Outcome Measures|We propose a novel personalized concept for the optimal treatment selection for a situation where the response is a multivariate vector, that could contain right-censored variables such as survival time. The proposed method can be applied with any number of treatments and outcome variables, under a broad set of models. Following a working semiparametric Single Index Model that relates covariates and responses, we first define a patient-specific composite score, constructed from individual covariates. We then estimate conditional means of each response, given the patient score, correspond to each treatment, using a nonparametric smooth estimator. Next, a rank aggregation technique is applied to estimate an ordering of treatments based on ranked lists of treatment performance measures given by conditional means. We handle the right-censored data by incorporating the inverse probability of censoring weighting to the corresponding estimators. An empirical study illustrates the performance of the proposed method in finite sample problems. To show the applicability of the proposed procedure for real data, we also present a data analysis using HIV clinical trial data, that contained a right-censored survival event as one of the endpoints.|http://arxiv.org/abs/2209.15068v1|Chathura Siriwardhana,K. B. Kulasekera,Somnath Datta
1817|When to encourage using Gaussian regression for feature selection tasks with time-to-event outcome|IMPORTANCE: Feature selection with respect to time-to-event outcomes is one of the fundamental problems in clinical trials and biomarker discovery studies. But it's unclear which statistical methods should be used when sample size is small or some of the key covariates are not measured. DESIGN: In this simulation study, the true models are multivariate Cox proportional hazards models with 10 covariates. It's assumed that only 5 out the 10 true features are observed/measured for all model fitting, along with 5 random noise features. Each sample size scenario is explored using 10,000 simulation datasets. Eight regression models are applied to each dataset to estimate feature effects, including both regularized Gaussian regression (elastic net penalty) and regularized Cox regression (glmnet Cox). RESULTS: If the covariates are highly correlated Gaussian, the Gaussian regression of log-transformed survival time with only two covariates outperforms all tested Cox regression models when total number of events <500.|http://arxiv.org/abs/2210.04409v2|Rong Lu
1818|Deep conditional transformation models for survival analysis|An every increasing number of clinical trials features a time-to-event outcome and records non-tabular patient data, such as magnetic resonance imaging or text data in the form of electronic health records. Recently, several neural-network based solutions have been proposed, some of which are binary classifiers. Parametric, distribution-free approaches which make full use of survival time and censoring status have not received much attention. We present deep conditional transformation models (DCTMs) for survival outcomes as a unifying approach to parametric and semiparametric survival analysis. DCTMs allow the specification of non-linear and non-proportional hazards for both tabular and non-tabular data and extend to all types of censoring and truncation. On real and semi-synthetic data, we show that DCTMs compete with state-of-the-art DL approaches to survival analysis.|http://arxiv.org/abs/2210.11366v2|Gabriele Campanella,Lucas Kook,Ida Hggstrm,Torsten Hothorn,Thomas J. Fuchs
1819|Hybrid Censored Quantile Regression Forest to Assess the Heterogeneous Effects|In many applications, heterogeneous treatment effects on a censored response variable are of primary interest, and it is natural to evaluate the effects at different quantiles (e.g., median). The large number of potential effect modifiers, the unknown structure of the treatment effects, and the presence of right censoring pose significant challenges. In this paper, we develop a hybrid forest approach called Hybrid Censored Quantile Regression Forest (HCQRF) to assess the heterogeneous effects varying with high-dimensional variables. The hybrid estimation approach takes advantage of the random forests and the censored quantile regression. We propose a doubly-weighted estimation procedure that consists of a redistribution-of-mass weight to handle censoring and an adaptive nearest neighbor weight derived from the forest to handle high-dimensional effect functions. We propose a variable importance decomposition to measure the impact of a variable on the treatment effect function. Extensive simulation studies demonstrate the efficacy and stability of HCQRF. The result of the simulation study also convinces us of the effectiveness of the variable importance decomposition. We apply HCQRF to a clinical trial of colorectal cancer. We achieve insightful estimations of the treatment effect and meaningful variable importance results. The result of the variable importance also confirms the necessity of the decomposition.|http://arxiv.org/abs/2212.05672v1|Huichen Zhu,Yifei Sun,Ying Wei
1820|Tree-based exploratory identification of predictive biomarkers in observational data|The idea of "stratified medicine" is an important driver of methodological research on the identification of predictive biomarkers. Most methods proposed so far for this purpose have been developed for the use on randomized data only. However, especially for rare cancers, data from clinical registries or observational studies might be the only available data source. For such data, methods for an unbiased estimation of the average treatment effect are well established. Research on confounder adjustment when investigating the heterogeneity of treatment effects and the variables responsible for this is usually restricted to regression modelling. In this paper, we demonstrate how the predMOB, a tree-based method that specifically searches for predictive factors, can be combined with common strategies for confounder adjustment (covariate adjustment, matching, Inverse Probability of Treatment Weighting (IPTW)). In an extensive simulation study, we show that covariate adjustment allows the correct identification of predictive factors in the presence of confounding whereas IPTW fails in situations in which the true predictive factor is not completely independent of the confounding mechanism. A combination of both, covariate adjustment and IPTW performs as well as covariate adjustment alone, but might be more robust in complex settings. An application to the German Breast Cancer Study Group (GBSG) Trial 2 illustrates these conclusions.|http://arxiv.org/abs/2212.08460v1|Julia Krzykalla,Axel Benner,Annette Kopp-Schneider
1821|Frailty Model with Change Point for Survival Analysis|We propose a novel frailty model with change points applying random effects to a Cox proportional hazard model to adjust the heterogeneity between clusters. Because the frailty model includes random effects, the parameters are estimated using the expectation-maximization (EM) algorithm. Additionally, our model needs to estimate change points; we thus propose a new algorithm extending the conventional estimation algorithm to the frailty model with change points to solve the problem. We show a practical example to demonstrate how to estimate the change point and random effect. Our proposed model can be easily analyzed using the existing R package. We conducted simulation studies with three scenarios to confirm the performance of our proposed model. We re-analyzed data of two clinical trials to show the difference in analysis results with and without random effect. In conclusion, we confirmed that the frailty model with change points has a higher accuracy than the model without the random effect. Our proposed model is useful when heterogeneity needs to be taken into account. Additionally, the absence of heterogeneity did not affect the estimation of the regression coefficient parameters.|http://arxiv.org/abs/2301.04387v1|Masahiro Kojima,Shunichiro Orihara
1822|Mechanisms in neurodegenerative disorders and role of non-pharmacological interventions in improving neurodegeneration and its clinical correlates: A review|Mild cognitive impairment (MCI) leading to dementia results in a constellation of psychiatric disorders including depression, mood disorders, schizophrenia and others. With increasing age, mild cognitive impairment leads to increased disability-adjusted life-years and healthcare burden. A huge number of drug trials for the treatment of MCI associated with Alzheimer's disease have undergone failure leading to the development of drugs that could avert the progression of the disease. However, some novel non-drug-based therapies like ultrasound ablation of amyloid plaques have influenced researchers to explore the non-pharmacological modalities for the treatment of mild cognitive impairment.   To compensate for neurodegenerative loss resulting in coexisting psychiatric disorders, neurofeedback therapy has also been proven to improve behavioural outcomes by inducing neuroplasticity. The aim of the current review is to highlight the pathophysiological aspects of mild cognitive impairment leading to dementia that could be addressed with no pharmacological interventions and to understand the mechanisms behind the effects of these interventions.|http://arxiv.org/abs/2301.06076v2|Sheng Mai
1823|Delayed Feedback in Kernel Bandits|Black box optimisation of an unknown function from expensive and noisy evaluations is a ubiquitous problem in machine learning, academic research and industrial production. An abstraction of the problem can be formulated as a kernel based bandit problem (also known as Bayesian optimisation), where a learner aims at optimising a kernelized function through sequential noisy observations. The existing work predominantly assumes feedback is immediately available; an assumption which fails in many real world situations, including recommendation systems, clinical trials and hyperparameter tuning. We consider a kernel bandit problem under stochastically delayed feedback, and propose an algorithm with $\tilde{\mathcal{O}}(\sqrt{\Gamma_k(T)T}+\mathbb{E}[\tau])$ regret, where $T$ is the number of time steps, $\Gamma_k(T)$ is the maximum information gain of the kernel with $T$ observations, and $\tau$ is the delay random variable. This represents a significant improvement over the state of the art regret bound of $\tilde{\mathcal{O}}(\Gamma_k(T)\sqrt{T}+\mathbb{E}[\tau]\Gamma_k(T))$ reported in Verma et al. (2022). In particular, for very non-smooth kernels, the information gain grows almost linearly in time, trivializing the existing results. We also validate our theoretical results with simulations.|http://arxiv.org/abs/2302.00392v1|Sattar Vakili,Danyal Ahmed,Alberto Bernacchia,Ciara Pike-Burke
1824|Factorial survival analysis for treatment effects under dependent censoring|Factorial analyses offer a powerful nonparametric means to detect main or interaction effects among multiple treatments. For survival outcomes, e.g. from clinical trials, such techniques can be adopted for comparing reasonable quantifications of treatment effects. The key difficulty to solve in survival analysis concerns the proper handling of censoring. So far, all existing factorial analyses for survival data were developed under the independent censoring assumption, which is too strong for many applications. As a solution, the central aim of this article is to develop new methods in factorial survival analyses under quite general dependent censoring regimes. This will be accomplished by combining existing results for factorial survival analyses with techniques developed for survival copula models. As a result, we will present an appealing F-test that exhibits sound performance in our simulation study. The new methods are illustrated in real data analysis. We implement the proposed method in an R function surv.factorial(.) in the R package compound.Cox.|http://arxiv.org/abs/2302.01617v1|Takeshi Emura,Marc Ditzhaus,Dennis Dobler,Kenta Murotani
1825|Multi-task Representation Learning for Pure Exploration in Linear Bandits|Despite the recent success of representation learning in sequential decision making, the study of the pure exploration scenario (i.e., identify the best option and minimize the sample complexity) is still limited. In this paper, we study multi-task representation learning for best arm identification in linear bandits (RepBAI-LB) and best policy identification in contextual linear bandits (RepBPI-CLB), two popular pure exploration settings with wide applications, e.g., clinical trials and web content optimization. In these two problems, all tasks share a common low-dimensional linear representation, and our goal is to leverage this feature to accelerate the best arm (policy) identification process for all tasks. For these problems, we design computationally and sample efficient algorithms DouExpDes and C-DouExpDes, which perform double experimental designs to plan optimal sample allocations for learning the global representation. We show that by learning the common representation among tasks, our sample complexity is significantly better than that of the native approach which solves tasks independently. To the best of our knowledge, this is the first work to demonstrate the benefits of representation learning for multi-task pure exploration.|http://arxiv.org/abs/2302.04441v2|Yihan Du,Longbo Huang,Wen Sun
1826|Number of Repetitions in Re-randomization Tests|In covariate-adaptive or response-adaptive randomization, the treatment assignment and outcome can be correlated. Under this situation, re-randomization tests are a straightforward and attractive method to provide valid statistical inference. In this paper, we investigate the number of repetitions in the re-randomization tests. This is motivated by the group sequential design in clinical trials, where the nominal significance bound can be very small at an interim analysis. Accordingly, re-randomization tests lead to a very large number of required repetitions, which may be computationally intractable. To reduce the number of repetitions, we propose an adaptive procedure and compare it with multiple approaches under pre-defined criteria. Monte Carlo simulations are conducted to show the performance of different approaches in a limited sample size. We also suggest strategies to reduce total computation time and provide practical guidance in preparing, executing and reporting before and after data are unblinded at an interim analysis, so one can complete the computation within a reasonable time frame.|http://arxiv.org/abs/2302.05977v2|Yilong Zhang,Yujie Zhao,Yiwen Luo
1827|Order in Innovation|Is calendar time the true clock of innovation? By combining complexity science with innovation economics and using vaccine datasets containing over three million citations and eight regulatory authorisations, we discover that calendar time and network order describe innovation progress at varying accuracy. First, we present a method to establish a mathematical link between technological evolution and complex networks. The result is a path of events that narrates innovation bottlenecks. Next, we quantify the position and proximity of documents to these innovation paths and find that research, by and large, proceed from basic research, applied research, development, to commercialisation. By extension, we are able to causally quantify the participation of innovation funders. When it comes to vaccine innovation, diffusion-oriented entities are preoccupied with basic, later-stage research; biopharmaceuticals tend to participate in applied development activities and clinical trials at the later-stage; while mission-oriented entities tend to initiate early-stage research. Future innovation programs and funding allocations would benefit from better understanding innovation orders.|http://arxiv.org/abs/2302.13076v1|Martin Ho,Henry CW Price,Tim S Evans,Eoin O'Sullivan
1828|An Efficient Data Integration Scheme for Synthesizing Information from Multiple Secondary Datasets for the Parameter Inference of the Main Analysis|Many observational studies and clinical trials collect various secondary outcomes that may be highly correlated with the primary endpoint. These secondary outcomes are often analyzed in secondary analyses separately from the main data analysis. However, these secondary outcomes can be used to improve the estimation precision in the main analysis. We propose a method called Multiple Information Borrowing (MinBo) that borrows information from secondary data (containing secondary outcomes and covariates) to improve the efficiency of the main analysis. The proposed method is robust against model misspecification of the secondary data. Both theoretical and case studies demonstrate that MinBo outperforms existing methods in terms of efficiency gain. We apply MinBo to data from the Atherosclerosis Risk in Communities study to assess risk factors for hypertension.|http://arxiv.org/abs/2303.03512v1|Chixiang Chen,Ming Wang,Shuo Chen
1829|LEAP: The latent exchangeability prior for borrowing information from historical data|It is becoming increasingly popular to elicit informative priors on the basis of historical data. Popular existing priors, including the power prior, commensurate prior, and robust meta-analytic prior provide blanket discounting. Thus, if only a subset of participants in the historical data are exchangeable with the current data, these priors may not be appropriate. In order to combat this issue, propensity score (PS) approaches have been proposed. However, PS approaches are only concerned with the covariate distribution, whereas exchangeability is typically assessed with parameters pertaining to the outcome. In this paper, we introduce the latent exchangeability prior (LEAP), where observations in the historical data are classified into exchangeable and non-exchangeable groups. The LEAP discounts the historical data by identifying the most relevant subjects from the historical data. We compare our proposed approach against alternative approaches in simulations and present a case study using our proposed prior to augment a control arm in a phase 3 clinical trial in plaque psoriasis with an unbalanced randomization scheme.|http://arxiv.org/abs/2303.05223v1|Ethan M. Alt,Xiuya Chang,Xun Jiang,Qing Liu,May Mo,H. Amy Xia,Joseph G. Ibrahim
1830|SmartState: An Automated Research Protocol Adherence System|Developing and enforcing study protocols is crucial in medical research, especially as interactions with participants become more intricate. Traditional rules-based systems struggle to provide the automation and flexibility required for real-time, personalized data collection. We introduce SmartState, a state-based system designed to act as a personal agent for each participant, continuously managing and tracking their unique interactions. Unlike traditional reporting systems, SmartState enables real-time, automated data collection with minimal oversight. By integrating large language models to distill conversations into structured data, SmartState reduces errors and safeguards data integrity through built-in protocol and participant auditing. We demonstrate its utility in research trials involving time-dependent participant interactions, addressing the increasing need for reliable automation in complex clinical studies.|http://arxiv.org/abs/2305.04411v6|Samuel E. Armstrong,Mitchell A. Klusty,Aaron D. Mullen,Jeffery C. Talbert,V. K. Cody Bumgardner
1831|Dirichlet process mixture models for the Analysis of Repeated Attempt Designs|In longitudinal studies, it is not uncommon to make multiple attempts to collect a measurement after baseline. Recording whether these attempts are successful provides useful information for the purposes of assessing missing data assumptions. This is because measurements from subjects who provide the data after numerous failed attempts may differ from those who provide the measurement after fewer attempts. Previous models for these designs were parametric and/or did not allow sensitivity analysis. For the former, there are always concerns about model misspecification and for the latter, sensitivity analysis is essential when conducting inference in the presence of missing data. Here, we propose a new approach which minimizes issues with model misspecification by using Bayesian nonparametrics for the observed data distribution. We also introduce a novel approach for identification and sensitivity analysis. We re-analyze the repeated attempts data from a clinical trial involving patients with severe mental illness and conduct simulations to better understand the properties of our approach.|http://arxiv.org/abs/2305.05099v1|Michael J. Daniels,Minji Lee,Wei Feng
1832|An Ensemble Learning Approach for Exercise Detection in Type 1 Diabetes Patients|Type 1 diabetes is a serious disease in which individuals are unable to regulate their blood glucose levels, leading to various medical complications. Artificial pancreas (AP) systems have been developed as a solution for type 1 diabetic patients to mimic the behavior of the pancreas and regulate blood glucose levels. However, current AP systems lack detection capabilities for exercise-induced glucose intake, which can last up to 4 to 8 hours. This incapability can lead to hypoglycemia, which if left untreated, could have serious consequences, including death. Existing exercise detection methods are either limited to single sensor data or use inaccurate models for exercise detection, making them less effective in practice. In this work, we propose an ensemble learning framework that combines a data-driven physiological model and a Siamese network to leverage multiple physiological signal streams for exercise detection with high accuracy. To evaluate the effectiveness of our proposed approach, we utilized a public dataset with 12 diabetic patients collected from an 8-week clinical trial. Our approach achieves a true positive rate for exercise detection of 86.4% and a true negative rate of 99.1%, outperforming state-of-the-art solutions.|http://arxiv.org/abs/2305.10353v1|Ke Ma,Hongkai Chen,Shan Lin
1833|Covariance Adaptive Best Arm Identification|We consider the problem of best arm identification in the multi-armed bandit model, under fixed confidence. Given a confidence input $\delta$, the goal is to identify the arm with the highest mean reward with a probability of at least 1 -- $\delta$, while minimizing the number of arm pulls. While the literature provides solutions to this problem under the assumption of independent arms distributions, we propose a more flexible scenario where arms can be dependent and rewards can be sampled simultaneously. This framework allows the learner to estimate the covariance among the arms distributions, enabling a more efficient identification of the best arm. The relaxed setting we propose is relevant in various applications, such as clinical trials, where similarities between patients or drugs suggest underlying correlations in the outcomes. We introduce new algorithms that adapt to the unknown covariance of the arms and demonstrate through theoretical guarantees that substantial improvement can be achieved over the standard setting. Additionally, we provide new lower bounds for the relaxed setting and present numerical simulations that support their theoretical findings.|http://arxiv.org/abs/2306.02630v2|El Mehdi Saad,Gilles Blanchard,Nicolas Verzelen
1834|Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations|Randomized experiments often need to be stopped prematurely due to the treatment having an unintended harmful effect. Existing methods that determine when to stop an experiment early are typically applied to the data in aggregate and do not account for treatment effect heterogeneity. In this paper, we study the early stopping of experiments for harm on heterogeneous populations. We first establish that current methods often fail to stop experiments when the treatment harms a minority group of participants. We then use causal machine learning to develop CLASH, the first broadly-applicable method for heterogeneous early stopping. We demonstrate CLASH's performance on simulated and real data and show that it yields effective early stopping for both clinical trials and A/B tests.|http://arxiv.org/abs/2306.11839v3|Hammaad Adam,Fan Yin,Huibin,Hu,Neil Tenenholtz,Lorin Crawford,Lester Mackey,Allison Koenecke
1835|Medical ministrations through web scraping|Web scraping is a technique that allows us to extract data from websites automatically. in the field of medicine, web scraping can be used to collect information about medical procedures, treatments, and healthcare providers. this information can be used to improve patient care, monitor the quality of healthcare services, and identify areas for improvement. one area where web scraping can be particularly useful is in medical ministrations. medical ministrations are the actions taken to provide medical care to patients, and web scraping can help healthcare providers identify the most effective ministrations for their patients. for example, healthcare providers can use web scraping to collect data about the symptoms and medical histories of their patients, and then use this information to determine the most appropriate ministrations. they can also use web scraping to gather information about the latest medical research and clinical trials, which can help them stay up-to-date with the latest treatments and procedures.|http://arxiv.org/abs/2306.12310v1|Niketha Sabesan,Nivethitha,J. N Shreyah,Pranauv A J,Shyam R
1836|A Direct Approach to Simultaneous Tests of Superiority and Noninferiority with Multiple Endpoints|Simultaneous tests of superiority and non-inferiority hypotheses on multiple endpoints are often performed in clinical trials to demonstrate that a new treatment is superior over a control on at least one endpoint and non-inferior on the remaining endpoints. Existing methods tackle this problem by testing the superiority and non-inferiority hypotheses separately and control the Type I error rate each at $\alpha$ level. In this paper we propose a unified approach to testing the superiority and non-inferiority hypotheses simultaneously. The proposed approach is based on the UI-IU test and the least favorable configurations of the combined superiority and non-inferiority hypotheses, which leads to the solution of an adjusted significance level $\alpha'$ for marginal tests that controls the overall Type I error rate at pre-defined $\alpha$. Simulations show that the proposed approach maintains a higher power than existing methods in the settings under investigation. Since the adjusted significance level $\alpha'$ is obtained by controlling the Type I error rate at $\alpha$, one can easily construct the exact $(1 - \alpha)\%$ simultaneous confidence intervals for treatment effects on all endpoints. The proposed approach is illustrated with two real examples.|http://arxiv.org/abs/2307.00189v2|Wenfeng Chen,Naiqing Zhao,Guoyou Qin,Jie Chen
1837|Challenge Results Are Not Reproducible|While clinical trials are the state-of-the-art methods to assess the effect of new medication in a comparative manner, benchmarking in the field of medical image analysis is performed by so-called challenges. Recently, comprehensive analysis of multiple biomedical image analysis challenges revealed large discrepancies between the impact of challenges and quality control of the design and reporting standard. This work aims to follow up on these results and attempts to address the specific question of the reproducibility of the participants methods. In an effort to determine whether alternative interpretations of the method description may change the challenge ranking, we reproduced the algorithms submitted to the 2019 Robust Medical Image Segmentation Challenge (ROBUST-MIS). The leaderboard differed substantially between the original challenge and reimplementation, indicating that challenge rankings may not be sufficiently reproducible.|http://arxiv.org/abs/2307.07226v1|Annika Reinke,Georg Grab,Lena Maier-Hein
1838|Identification of Parkinson's Disease Subtypes with Divisive Hierarchical Bayesian Clustering for Longitudinal and Time-to-Event Data|In heterogeneous disorders like Parkinson's disease (PD), differentiating the affected population into subgroups plays a key role in future research. Discovering subgroups can lead to improved treatments through more powerful enrichment of clinical trials, elucidating pathogenic mechanisms, and identifying biomarkers of progression and prognosis. Cluster analysis is a commonly used method to identify subgroups; however, cluster analysis methods are typically restricted to static data or temporal data of a single variable. Progression of a complex disease process may be more appropriately represented by several longitudinal and/or time-to-event variables. Clustering with longitudinal and time-to-event data presents challenges, such as correlations between clustering variables, temporal dependencies, missing data, and censoring. To address these challenges, we present Divisive Hierarchical Bayesian Clustering methods featuring models for multivariate longitudinal trajectories and semi-parametric models for survival data to identify subgroups in idiopathic PD with differing progression patterns from the Parkinson's Progression Markers Initiative (PPMI) database.|http://arxiv.org/abs/2308.02577v1|Elliot Burghardt,Daniel Sewell,Joseph Cavanaugh
1839|Towards Automatic Scoring of Spinal X-ray for Ankylosing Spondylitis|Manually grading structural changes with the modified Stoke Ankylosing Spondylitis Spinal Score (mSASSS) on spinal X-ray imaging is costly and time-consuming due to bone shape complexity and image quality variations. In this study, we address this challenge by prototyping a 2-step auto-grading pipeline, called VertXGradeNet, to automatically predict mSASSS scores for the cervical and lumbar vertebral units (VUs) in X-ray spinal imaging. The VertXGradeNet utilizes VUs generated by our previously developed VU extraction pipeline (VertXNet) as input and predicts mSASSS based on those VUs. VertXGradeNet was evaluated on an in-house dataset of lateral cervical and lumbar X-ray images for axial spondylarthritis patients. Our results show that VertXGradeNet can predict the mSASSS score for each VU when the data is limited in quantity and imbalanced. Overall, it can achieve a balanced accuracy of 0.56 and 0.51 for 4 different mSASSS scores (i.e., a score of 0, 1, 2, 3) on two test datasets. The accuracy of the presented method shows the potential to streamline the spinal radiograph readings and therefore reduce the cost of future clinical trials.|http://arxiv.org/abs/2308.05123v1|Yuanhan Mo,Yao Chen,Aimee Readie,Gregory Ligozio,Thibaud Coroller
1840|DrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs|A ChatGPT-like system for drug compounds could be a game-changer in pharmaceutical research, accelerating drug discovery, enhancing our understanding of structure-activity relationships, guiding lead optimization, aiding drug repurposing, reducing the failure rate, and streamlining clinical trials. In this work, we make an initial attempt towards enabling ChatGPT-like capabilities on drug molecule graphs, by developing a prototype system DrugChat. DrugChat works in a similar way as ChatGPT. Users upload a compound molecule graph and ask various questions about this compound. DrugChat will answer these questions in a multi-turn, interactive manner. The DrugChat system consists of a graph neural network (GNN), a large language model (LLM), and an adaptor. The GNN takes a compound molecule graph as input and learns a representation for this graph. The adaptor transforms the graph representation produced by the GNN into another representation that is acceptable to the LLM. The LLM takes the compound representation transformed by the adaptor and users' questions about this compound as inputs and generates answers. All these components are trained end-to-end. To train DrugChat, we collected instruction tuning datasets which contain 10,834 drug compounds and 143,517 question-answer pairs. The code and data is available at \url{https://github.com/UCSD-AI4H/drugchat}|http://arxiv.org/abs/2309.03907v1|Youwei Liang,Ruiyi Zhang,Li Zhang,Pengtao Xie
1841|HealthFC: Verifying Health Claims with Evidence-Based Medical Fact-Checking|In the digital age, seeking health advice on the Internet has become a common practice. At the same time, determining the trustworthiness of online medical content is increasingly challenging. Fact-checking has emerged as an approach to assess the veracity of factual claims using evidence from credible knowledge sources. To help advance automated Natural Language Processing (NLP) solutions for this task, in this paper we introduce a novel dataset HealthFC. It consists of 750 health-related claims in German and English, labeled for veracity by medical experts and backed with evidence from systematic reviews and clinical trials. We provide an analysis of the dataset, highlighting its characteristics and challenges. The dataset can be used for NLP tasks related to automated fact-checking, such as evidence retrieval, claim verification, or explanation generation. For testing purposes, we provide baseline systems based on different approaches, examine their performance, and discuss the findings. We show that the dataset is a challenging test bed with a high potential for future use.|http://arxiv.org/abs/2309.08503v2|Juraj Vladika,Phillip Schneider,Florian Matthes
1842|Adaptive Neyman Allocation|In experimental design, Neyman allocation refers to the practice of allocating subjects into treated and control groups, potentially in unequal numbers proportional to their respective standard deviations, with the objective of minimizing the variance of the treatment effect estimator. This widely recognized approach increases statistical power in scenarios where the treated and control groups have different standard deviations, as is often the case in social experiments, clinical trials, marketing research, and online A/B testing. However, Neyman allocation cannot be implemented unless the standard deviations are known in advance. Fortunately, the multi-stage nature of the aforementioned applications allows the use of earlier stage observations to estimate the standard deviations, which further guide allocation decisions in later stages. In this paper, we introduce a competitive analysis framework to study this multi-stage experimental design problem. We propose a simple adaptive Neyman allocation algorithm, which almost matches the information-theoretic limit of conducting experiments. Using online A/B testing data from a social media site, we demonstrate the effectiveness of our adaptive Neyman allocation algorithm, highlighting its practicality especially when applied with only a limited number of stages.|http://arxiv.org/abs/2309.08808v2|Jinglong Zhao
1843|Survival causal rule ensemble method considering the main effect for estimating heterogeneous treatment effects|With an increasing focus on precision medicine in medical research, numerous studies have been conducted in recent years to clarify the relationship between treatment effects and patient characteristics. The treatment effects for patients with different characteristics are always heterogeneous, and various heterogeneous treatment effect machine learning estimation methods have been proposed owing to their flexibility and high prediction accuracy. However, most machine learning methods rely on black-box models, preventing direct interpretation of the relationship between patient characteristics and treatment effects. Moreover, most of these studies have focused on continuous or binary outcomes, although survival outcomes are also important in medical research. To address these challenges, we propose a heterogeneous treatment effect estimation method for survival data based on RuleFit, an interpretable machine learning method. Numerical simulation results confirmed that the prediction performance of the proposed method was comparable to that of existing methods. We also applied a dataset from an HIV study, the AIDS Clinical Trials Group Protocol 175 dataset, to illustrate the interpretability of the proposed method using real data. Consequently, the proposed method established an interpretable model with sufficient prediction accuracy.|http://arxiv.org/abs/2309.11914v1|Ke Wan,Kensuke Tanioka,Toshio Shimokawa
1844|Measuring the Robustness of Predictive Probability for Early Stopping in Experimental Design|Physical experiments in the national security domain are often expensive and time-consuming. Test engineers must certify the compatibility of aircraft and their weapon systems before they can be deployed in the field, but the testing required is time consuming, expensive, and resource limited. Adopting Bayesian adaptive designs are a promising way to borrow from the successes seen in the clinical trials domain. The use of predictive probability (PP) to stop testing early and make faster decisions is particularly appealing given the aforementioned constraints. Given the high-consequence nature of the tests performed in the national security space, a strong understanding of new methods is required before being deployed. Although PP has been thoroughly studied for binary data, there is less work with continuous data, which often in reliability studies interested in certifying the specification limits of components. A simulation study evaluating the robustness of this approach indicate early stopping based on PP is reasonably robust to minor assumption violations, especially when only a few interim analyses are conducted. A post-hoc analysis exploring whether release requirements of a weapon system from an aircraft are within specification with desired reliability resulted in stopping the experiment early and saving 33% of the experimental runs.|http://arxiv.org/abs/2309.17241v1|Daniel Ries,Victoria R. C. Sieck,Philip Jones,Julie Shaffer
1845|Attention-based Multi-task Learning for Base Editor Outcome Prediction|Human genetic diseases often arise from point mutations, emphasizing the critical need for precise genome editing techniques. Among these, base editing stands out as it allows targeted alterations at the single nucleotide level. However, its clinical application is hindered by low editing efficiency and unintended mutations, necessitating extensive trial-and-error experimentation in the laboratory. To speed up this process, we present an attention-based two-stage machine learning model that learns to predict the likelihood of all possible editing outcomes for a given genomic target sequence. We further propose a multi-task learning schema to jointly learn multiple base editors (i.e. variants) at once. Our model's predictions consistently demonstrated a strong correlation with the actual experimental results on multiple datasets and base editor variants. These results provide further validation for the models' capacity to enhance and accelerate the process of refining base editing designs.|http://arxiv.org/abs/2310.02919v2|Amina Mollaysa,Ahmed Allam,Michael Krauthammer
1846|Sequential linear regression for conditional mean imputation of longitudinal continuous outcomes under reference-based assumptions|In clinical trials of longitudinal continuous outcomes, reference based imputation (RBI) has commonly been applied to handle missing outcome data in settings where the estimand incorporates the effects of intercurrent events, e.g. treatment discontinuation. RBI was originally developed in the multiple imputation framework, however recently conditional mean imputation (CMI) combined with the jackknife estimator of the standard error was proposed as a way to obtain deterministic treatment effect estimates and correct frequentist inference. For both multiple and CMI, a mixed model for repeated measures (MMRM) is often used for the imputation model, but this can be computationally intensive to fit to multiple data sets (e.g. the jackknife samples) and lead to convergence issues with complex MMRM models with many parameters. Therefore, a step-wise approach based on sequential linear regression (SLR) of the outcomes at each visit was developed for the imputation model in the multiple imputation framework, but similar developments in the CMI framework are lacking. In this article, we fill this gap in the literature by proposing a SLR approach to implement RBI in the CMI framework, and justify its validity using theoretical results and simulations. We also illustrate our proposal on a real data application.|http://arxiv.org/abs/2310.05151v2|Sean Yiu
|Siddhant Chaudhary,Abhishek Sinha
1848|Data-integration with pseudoweights and survey-calibration: application to developing US-representative lung cancer risk models for use in screening|Accurate cancer risk estimation is crucial to clinical decision-making, such as identifying high-risk people for screening. However, most existing cancer risk models incorporate data from epidemiologic studies, which usually cannot represent the target population. While population-based health surveys are ideal for making inference to the target population, they typically do not collect time-to-cancer incidence data. Instead, time-to-cancer specific mortality is often readily available on surveys via linkage to vital statistics. We develop calibrated pseudoweighting methods that integrate individual-level data from a cohort and a survey, and summary statistics of cancer incidence from national cancer registries. By leveraging individual-level cancer mortality data in the survey, the proposed methods impute time-to-cancer incidence for survey sample individuals and use survey calibration with auxiliary variables of influence functions generated from Cox regression to improve robustness and efficiency of the inverse-propensity pseudoweighting method in estimating pure risks. We develop a lung cancer incidence pure risk model from the Prostate, Lung, Colorectal, and Ovarian (PLCO) Cancer Screening Trial using our proposed methods by integrating data from the National Health Interview Survey (NHIS) and cancer registries.|http://arxiv.org/abs/2310.16650v1|Lingxiao Wang,Yan Li,Barry Graubard,Hormuzd Katki
|Thomas Guilmeau,Nicola Branchini,Emilie Chouzenoux,Vctor Elvira
1850|Morphology of Vaccine RD&D translation|Translation as a concept coordinates participation in innovation but remains a qualitative construct. We provide multivariate accounting of linkages between market entries of vaccines, clinical trials, patents, publications, funders, and grants to quantify biomedical translation. We found that the most prevalent types of biomedical translation are those between basic and applied research (52 percent) followed by those between research and product development (36 percent). Although many biomedical stakeholders assume knowledge flows one way from upstream research to downstream application, knowledge feedbacks that mediate translation are prevalent. We also cluster biomedical funders based on the types of translations they fund. Large-scale funding agencies such as NIH are similarly involved in early-stage translation, whereas pharmaceuticals and mission-oriented agencies such as DARPA involve diverse translation types, and each leaves different translation footprints.|http://arxiv.org/abs/2310.18193v1|Martin Ho,Henry CW Price,Tim S Evans,Eoin O'Sullivan
1851|A Multilingual Virtual Guide for Self-Attachment Technique|In this work, we propose a computational framework that leverages existing out-of-language data to create a conversational agent for the delivery of Self-Attachment Technique (SAT) in Mandarin. Our framework does not require large-scale human translations, yet it achieves a comparable performance whilst also maintaining safety and reliability. We propose two different methods of augmenting available response data through empathetic rewriting. We evaluate our chatbot against a previous, English-only SAT chatbot through non-clinical human trials (N=42), each lasting five days, and quantitatively show that we are able to attain a comparable level of performance to the English SAT chatbot. We provide qualitative analysis on the limitations of our study and suggestions with the aim of guiding future improvements.|http://arxiv.org/abs/2310.18366v1|Alicia Jiayun Law,Ruoyu Hu,Lisa Alazraki,Anandha Gopalan,Neophytos Polydorou,Abbas Edalat
1852|A Synopsis of Stent Graft Technology Development|Coronary artery disease (CAD) is a leading cause of death worldwide. Treatments have evolved, with stenting becoming the primary approach over bypass surgery. This article reviews the evolution of coronary stent technology, starting from the first angioplasty in 1977. Pioneers like Forssmann, Dotter, and Gruentzig established the foundation. The late 1980s saw the introduction of bare metal stents (BMS) to address angioplasty limitations. However, BMS had issues, leading to the development of first-generation drug-eluting stents (DES) in the early 2000s, which reduced restenosis but had safety concerns. Subsequent innovations introduced second-generation DES with better results and the latest bioresorbable vascular scaffolds (BVS) that dissolve over time. Clinical trials have been crucial in validating each stent's effectiveness. Despite progress, challenges remain in stent selection, approval processes, and minimizing risks. The future may see personalized stenting based on patient needs, highlighting the significant advancements in stent technology and its impact on patient care.|http://arxiv.org/abs/2310.19235v1|Umme Hafsa Momy
1853|Attention-based Multi-task Learning for Base Editor Outcome Prediction|Human genetic diseases often arise from point mutations, emphasizing the critical need for precise genome editing techniques. Among these, base editing stands out as it allows targeted alterations at the single nucleotide level. However, its clinical application is hindered by low editing efficiency and unintended mutations, necessitating extensive trial-and-error experimentation in the laboratory. To speed up this process, we present an attention-based two-stage machine learning model that learns to predict the likelihood of all possible editing outcomes for a given genomic target sequence. We further propose a multi-task learning schema to jointly learn multiple base editors (i.e. variants) at once. Our model's predictions consistently demonstrated a strong correlation with the actual experimental results on multiple datasets and base editor variants. These results provide further validation for the models' capacity to enhance and accelerate the process of refining base editing designs.|http://arxiv.org/abs/2311.07636v2|Amina Mollaysa,Ahmed Allam,Michael Krauthammer
1854|Incremental Cost-Effectiveness Statistical Inference: Calculations and Communications|We illustrate use of nonparametric statistical methods to compare alternative treatments for a particular disease or condition on both their relative effectiveness and their relative cost. These Incremental Cost Effectiveness (ICE) methods are based upon Bootstrapping, i.e. Resampling with Replacement from observational or clinical-trial data on individual patients. We first show how a reasonable numerical value for the "Shadow Price of Health" can be chosen using functions within the ICEinfer R-package when effectiveness is not measured in "QALY"s. We also argue that simple histograms are ideal for communicating key findings to regulators, while our more detailed graphics may well be more informative and compelling for other health-care stakeholders.|http://arxiv.org/abs/2311.08604v2|Robert L. Obenchain
1855|SurvTimeSurvival: Survival Analysis On The Patient With Multiple Visits/Records|The accurate prediction of survival times for patients with severe diseases remains a critical challenge despite recent advances in artificial intelligence. This study introduces "SurvTimeSurvival: Survival Analysis On Patients With Multiple Visits/Records", utilizing the Transformer model to not only handle the complexities of time-varying covariates but also covariates data. We also tackle the data sparsity issue common to survival analysis datasets by integrating synthetic data generation into the learning process of our model. We show that our method outperforms state-of-the-art deep learning approaches on both covariates and time-varying covariates datasets. Our approach aims not only to enhance the understanding of individual patient survival trajectories across various medical conditions, thereby improving prediction accuracy, but also to play a pivotal role in designing clinical trials and creating new treatments.|http://arxiv.org/abs/2311.09854v1|Hung Le,Ong Eng-Jon,Bober Miroslaw
1856|Thompson sampling for zero-inflated count outcomes with an application to the Drink Less mobile health study|Mobile health (mHealth) interventions often aim to improve distal outcomes, such as clinical conditions, by optimizing proximal outcomes through just-in-time adaptive interventions. Contextual bandits provide a suitable framework for customizing such interventions according to individual time-varying contexts. However, unique challenges, such as modeling count outcomes within bandit frameworks, have hindered the widespread application of contextual bandits to mHealth studies. The current work addresses this challenge by leveraging count data models into online decision-making approaches. Specifically, we combine four common offline count data models (Poisson, negative binomial, zero-inflated Poisson, and zero-inflated negative binomial regressions) with Thompson sampling, a popular contextual bandit algorithm. The proposed algorithms are motivated by and evaluated on a real dataset from the Drink Less trial, where they are shown to improve user engagement with the mHealth platform. The proposed methods are further evaluated on simulated data, achieving improvement in maximizing cumulative proximal outcomes over existing algorithms. Theoretical results on regret bounds are also derived. The countts R package provides an implementation of our approach.|http://arxiv.org/abs/2311.14359v2|Xueqing Liu,Nina Deliu,Tanujit Chakraborty,Lauren Bell,Bibhas Chakraborty
1857|Interaction tests with covariate-adaptive randomization|Treatment-covariate interaction tests are commonly applied by researchers to examine whether the treatment effect varies across patient subgroups defined by baseline characteristics. The objective of this study is to explore treatment-covariate interaction tests involving covariate-adaptive randomization. Without assuming a parametric data generating model, we investigate usual interaction tests and observe that they tend to be conservative: specifically, their limiting rejection probabilities under the null hypothesis do not exceed the nominal level and are typically strictly lower than it. To address this problem, we propose modifications to the usual tests to obtain corresponding valid tests. Moreover, we introduce a novel class of stratified-adjusted interaction tests that are simple, more powerful than the usual and modified tests, and broadly applicable to most covariate-adaptive randomization methods. The results are general to encompass two types of interaction tests: one involving stratification covariates and the other involving additional covariates that are not used for randomization. Our study clarifies the application of interaction tests in clinical trials and offers valuable tools for revealing treatment heterogeneity, crucial for advancing personalized medicine.|http://arxiv.org/abs/2311.17445v2|Likun Zhang,Wei Ma
1858|Enhancing efficiency and robustness in high-dimensional linear regression with additional unlabeled data|In semi-supervised learning, the prevailing understanding suggests that observing additional unlabeled samples improves estimation accuracy for linear parameters only in the case of model misspecification. This paper challenges this notion, demonstrating its inaccuracy in high dimensions. Initially focusing on a dense scenario, we introduce robust semi-supervised estimators for the regression coefficient without relying on sparse structures in the population slope. Even when the true underlying model is linear, we show that leveraging information from large-scale unlabeled data improves both estimation accuracy and inference robustness. Moreover, we propose semi-supervised methods with further enhanced efficiency in scenarios with a sparse linear slope. Diverging from the standard semi-supervised literature, we also allow for covariate shift. The performance of the proposed methods is illustrated through extensive numerical studies, including simulations and a real-data application to the AIDS Clinical Trials Group Protocol 175 (ACTG175).|http://arxiv.org/abs/2311.17685v1|Kai Chen,Yuqian Zhang
1859|Eliminating confounder-induced bias in the statistics of intervention|Experimental and observational studies often lead to spurious association between the outcome and independent variables describing the intervention, because of confounding to third-party factors. Even in randomized clinical trials, confounding might be unavoidable due to small sample sizes. Practically, this poses a problem, because it is either expensive to re-design and conduct a new study or even impossible to alleviate the contribution of some confounders due to e.g. ethical concerns. Here, we propose a method to consistently derive hypothetical studies that retain as many of the dependencies in the original study as mathematically possible, while removing any association of observed confounders to the independent variables. Using historic studies, we illustrate how the confounding-free scenario re-estimates the effect size of the intervention. The new effect size estimate represents a concise prediction in the hypothetical scenario which paves a way from the original data towards the design of future studies.|http://arxiv.org/abs/2312.00225v1|Orestis Loukas,Ho Ryun Chung
1860|A Class of Computational Methods to Reduce Selection Bias when Designing Phase 3 Clinical Trials|When designing confirmatory Phase 3 studies, one usually evaluates one or more efficacious and safe treatment option(s) based on data from previous studies. However, several retrospective research articles reported the phenomenon of ``diminished treatment effect in Phase 3'' based on many case studies. Even under basic assumptions, it was shown that the commonly used estimator could substantially overestimate the efficacy of selected group(s). As alternatives, we propose a class of computational methods to reduce estimation bias and mean squared error (MSE) with a broader scope of multiple treatment groups and flexibility to accommodate summary results by group as input. Based on simulation studies and a real data example, we provide practical implementation guidance for this class of methods under different scenarios. For more complicated problems, our framework can serve as a starting point with additional layers built in. Proposed methods can also be widely applied to other selection problems.|http://arxiv.org/abs/2312.07697v1|Tianyu Zhan
1861|Maximum Likelihood Estimation under the Emax Model: Existence, Geometry and Efficiency|This study focuses on the estimation of the Emax dose-response model, a widely utilized framework in clinical trials, agriculture, and environmental experiments. Existing challenges in obtaining maximum likelihood estimates (MLE) for model parameters are often ascribed to computational issues but, in reality, stem from the absence of a MLE. Our contribution provides a new understanding and control of all the experimental situations that practitioners might face, guiding them in the estimation process. We derive the exact MLE for a three-point experimental design and we identify the two scenarios where the MLE fails. To address these challenges, we propose utilizing Firth's modified score, providing its analytical expression as a function of the experimental design. Through a simulation study, we demonstrate that, in one of the problematic cases, the Firth modification yields a finite estimate. For the remaining case, we introduce a design-augmentation strategy akin to a hypothesis test.|http://arxiv.org/abs/2401.00354v2|Giacomo Aletti,Nancy Flournoy,Caterina May,Chiara Tommasi
1862|A New Cure Rate Model with Discrete and Multiple Exposures|Cure rate models are mostly used to study data arising from cancer clinical trials. Its use in the context of infectious diseases has not been explored well. In 2008, Tournoud and Ecochard first proposed a mechanistic formulation of cure rate model in the context of infectious diseases with multiple exposures to infection. However, they assumed a simple Poisson distribution to capture the unobserved pathogens at each exposure time. In this paper, we propose a new cure rate model to study infectious diseases with discrete multiple exposures to infection. Our formulation captures both over-dispersion and under-dispersion with respect to the count on pathogens at each time of exposure. We also propose a new estimation method based on the expectation maximization algorithm to calculate the maximum likelihood estimates of the model parameters. We carry out a detailed Monte Carlo simulation study to demonstrate the performance of the proposed model and estimation algorithm. The flexibility of our proposed model also allows us to carry out a model discrimination. For this purpose, we use both likelihood ratio test and information-based criteria. Finally, we illustrate our proposed model using a recently collected data on COVID-19.|http://arxiv.org/abs/2401.04240v1|Suvra Pal
1863|Testing for similarity of multivariate mixed outcomes using generalised joint regression models with application to efficacy-toxicity responses|A common problem in clinical trials is to test whether the effect of an explanatory variable on a response of interest is similar between two groups, e.g. patient or treatment groups. In this regard, similarity is defined as equivalence up to a pre-specified threshold that denotes an acceptable deviation between the two groups. This issue is typically tackled by assessing if the explanatory variable's effect on the response is similar. This assessment is based on, for example, confidence intervals of differences or a suitable distance between two parametric regression models. Typically, these approaches build on the assumption of a univariate continuous or binary outcome variable. However, multivariate outcomes, especially beyond the case of bivariate binary response, remain underexplored. This paper introduces an approach based on a generalised joint regression framework exploiting the Gaussian copula. Compared to existing methods, our approach accommodates various outcome variable scales, such as continuous, binary, categorical, and ordinal, including mixed outcomes in multi-dimensional spaces. We demonstrate the validity of this approach through a simulation study and an efficacy-toxicity case study, hence highlighting its practical relevance.|http://arxiv.org/abs/2401.05817v1|Niklas Hagemann,Giampiero Marra,Frank Bretz,Kathrin Mllenhoff
1864|Posterior shrinkage towards linear subspaces|It is common to hold prior beliefs that are not characterized by points in the parameter space but instead are relational in nature and can be described by a linear subspace. While some previous work has been done to account for such prior beliefs, the focus has primarily been on point estimators within a regression framework. We argue, however, that prior beliefs about parameters ought to be encoded into the prior distribution rather than in the formation of a point estimator. In this way, the prior beliefs help shape \textit{all} inference. Through exponential tilting, we propose a fully generalizable method of taking existing prior information from, e.g., a pilot study, and combining it with additional prior beliefs represented by parameters lying on a linear subspace. We provide computationally efficient algorithms for posterior inference that, once inference is made using a non-tilted prior, does not depend on the sample size. We illustrate our proposed approach on an antihypertensive clinical trial dataset where we shrink towards a power law dose-response relationship, and on monthly influenza and pneumonia data where we shrink moving average lag parameters towards smoothness. Software to implement the proposed approach is provided in the R package \verb+SUBSET+ available on GitHub.|http://arxiv.org/abs/2401.07820v1|Daniel K. Sewell
1865|On the estimation and interpretation of effect size metrics|Effect size estimates are thought to capture the collective, two-way response to an intervention or exposure in a three-way problem among the intervention/exposure, various confounders and the outcome. For meaningful causal inference from the estimated effect size, the joint distribution of observed confounders must be identical across all intervention/exposure groups. However, real-world observational studies and even randomized clinical trials often lack such structural symmetry. To address this issue, various methods have been proposed and widely utilized. Recently, elementary combinatorics and information theory have motivated a consistent way to completely eliminate observed confounding in any given study. In this work, we leverage these new techniques to evaluate conventional methods based on their ability to (a) consistently differentiate between collective and individual responses to intervention/exposure and (b) establish the desired structural parity for sensible effect size estimation. Our findings reveal that a straightforward application of logistic regression homogenizes the three-way stratified analysis, but fails to restore structural symmetry leaving in particular the two-way effect size estimate unadjusted. Conversely, the Mantel-Haenszel estimator struggles to separate three-way effects from the two-way effect of intervention/exposure, leading to inconsistencies in interpreting pooled estimates as two-way risk metrics.|http://arxiv.org/abs/2402.04327v1|Orestis Loukas,Ho Ryun Chung
1866|A Perspective on Individualized Treatment Effects Estimation from Time-series Health Data|The burden of diseases is rising worldwide, with unequal treatment efficacy for patient populations that are underrepresented in clinical trials. Healthcare, however, is driven by the average population effect of medical treatments and, therefore, operates in a "one-size-fits-all" approach, not necessarily what best fits each patient. These facts suggest a pressing need for methodologies to study individualized treatment effects (ITE) to drive personalized treatment. Despite the increased interest in machine-learning-driven ITE estimation models, the vast majority focus on tabular data with limited review and understanding of methodologies proposed for time-series electronic health records (EHRs). To this end, this work provides an overview of ITE works for time-series data and insights into future research. The work summarizes the latest work in the literature and reviews it in light of theoretical assumptions, types of treatment settings, and computational frameworks. Furthermore, this work discusses challenges and future research directions for ITEs in a time-series setting. We hope this work opens new directions and serves as a resource for understanding one of the exciting yet under-studied research areas.|http://arxiv.org/abs/2402.04668v1|Ghadeer O. Ghosheh,Moritz Ggl,Tingting Zhu
1867|Federated Learning for Estimating Heterogeneous Treatment Effects|Machine learning methods for estimating heterogeneous treatment effects (HTE) facilitate large-scale personalized decision-making across various domains such as healthcare, policy making, education, and more. Current machine learning approaches for HTE require access to substantial amounts of data per treatment, and the high costs associated with interventions makes centrally collecting so much data for each intervention a formidable challenge. To overcome this obstacle, in this work, we propose a novel framework for collaborative learning of HTE estimators across institutions via Federated Learning. We show that even under a diversity of interventions and subject populations across clients, one can jointly learn a common feature representation, while concurrently and privately learning the specific predictive functions for outcomes under distinct interventions across institutions. Our framework and the associated algorithm are based on this insight, and leverage tabular transformers to map multiple input data to feature representations which are then used for outcome prediction via multi-task learning. We also propose a novel way of federated training of personalised transformers that can work with heterogeneous input feature spaces. Experimental results on real-world clinical trial data demonstrate the effectiveness of our method.|http://arxiv.org/abs/2402.17705v2|Disha Makhija,Joydeep Ghosh,Yejin Kim
1868|VQSynery: Robust Drug Synergy Prediction With Vector Quantization Mechanism|The pursuit of optimizing cancer therapies is significantly advanced by the accurate prediction of drug synergy. Traditional methods, such as clinical trials, are reliable yet encumbered by extensive time and financial demands. The emergence of high-throughput screening and computational innovations has heralded a shift towards more efficient methodologies for exploring drug interactions. In this study, we present VQSynergy, a novel framework that employs the Vector Quantization (VQ) mechanism, integrated with gated residuals and a tailored attention mechanism, to enhance the precision and generalizability of drug synergy predictions. Our findings demonstrate that VQSynergy surpasses existing models in terms of robustness, particularly under Gaussian noise conditions, highlighting its superior performance and utility in the complex and often noisy domain of drug synergy research. This study underscores the potential of VQSynergy in revolutionizing the field through its advanced predictive capabilities, thereby contributing to the optimization of cancer treatment strategies.|http://arxiv.org/abs/2403.03089v1|Jiawei Wu,Mingyuan Yan,Dianbo Liu
1869|Using Smartphones to Study Vaccination Decisions in the Wild|One of the most important tools available to limit the spread and impact of infectious diseases is vaccination. It is therefore important to understand what factors determine people's vaccination decisions. To this end, previous behavioural research made use of, (i) controlled but often abstract or hypothetical studies (e.g., vignettes) or, (ii) realistic but typically less flexible studies that make it difficult to understand individual decision processes (e.g., clinical trials). Combining the best of these approaches, we propose integrating real-world Bluetooth contacts via smartphones in several rounds of a game scenario, as a novel methodology to study vaccination decisions and disease spread. In our 12-week proof-of-concept study conducted with $N$ = 494 students, we found that participants strongly responded to some of the information provided to them during or after each decision round, particularly those related to their individual health outcomes. In contrast, information related to others' decisions and outcomes (e.g., the number of vaccinated or infected individuals) appeared to be less important. We discuss the potential of this novel method and point to fruitful areas for future research.|http://arxiv.org/abs/2403.03143v1|Nicol Alessandro Girardini,Arkadiusz Stopczynski,Olga Baranov,Cornelia Betsch,Dirk Brockmann,Sune Lehmann,Robert Bhm
1870|KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing Patient Data with Knowledge Graphs|Treatment effect estimation (TEE) is the task of determining the impact of various treatments on patient outcomes. Current TEE methods fall short due to reliance on limited labeled data and challenges posed by sparse and high-dimensional observational patient data. To address the challenges, we introduce a novel pre-training and fine-tuning framework, KG-TREAT, which synergizes large-scale observational patient data with biomedical knowledge graphs (KGs) to enhance TEE. Unlike previous approaches, KG-TREAT constructs dual-focus KGs and integrates a deep bi-level attention synergy method for in-depth information fusion, enabling distinct encoding of treatment-covariate and outcome-covariate relationships. KG-TREAT also incorporates two pre-training tasks to ensure a thorough grounding and contextualization of patient data and KGs. Evaluation on four downstream TEE tasks shows KG-TREAT's superiority over existing methods, with an average improvement of 7% in Area under the ROC Curve (AUC) and 9% in Influence Function-based Precision of Estimating Heterogeneous Effects (IF-PEHE). The effectiveness of our estimated treatment effects is further affirmed by alignment with established randomized clinical trial findings.|http://arxiv.org/abs/2403.03791v1|Ruoqi Liu,Lingfei Wu,Ping Zhang
1871|An End-to-End Deep Learning Generative Framework for Refinable Shape Matching and Generation|Generative modelling for shapes is a prerequisite for In-Silico Clinical Trials (ISCTs), which aim to cost-effectively validate medical device interventions using synthetic anatomical shapes, often represented as 3D surface meshes. However, constructing AI models to generate shapes closely resembling the real mesh samples is challenging due to variable vertex counts, connectivities, and the lack of dense vertex-wise correspondences across the training data. Employing graph representations for meshes, we develop a novel unsupervised geometric deep-learning model to establish refinable shape correspondences in a latent space, construct a population-derived atlas and generate realistic synthetic shapes. We additionally extend our proposed base model to a joint shape generative-clustering multi-atlas framework to incorporate further variability and preserve more details in the generated shapes. Experimental results using liver and left-ventricular models demonstrate the approach's applicability to computational medicine, highlighting its suitability for ISCTs through a comparative analysis.|http://arxiv.org/abs/2403.06317v1|Soodeh Kalaie,Andy Bulpitt,Alejandro F. Frangi,Ali Gooya
1872|Copula based dependent censoring in cure models|In this paper we consider a time-to-event variable $T$ that is subject to random right censoring, and we assume that the censoring time $C$ is stochastically dependent on $T$ and that there is a positive probability of not observing the event. There are various situations in practice where this happens, and appropriate models and methods need to be considered to avoid biased estimators of the survival function or incorrect conclusions in clinical trials. We consider a fully parametric model for the bivariate distribution of $(T,C)$, that takes these features into account. The model depends on a parametric copula (with unknown association parameter) and on parametric marginal distributions for $T$ and $C$. Sufficient conditions are developed under which the model is identified, and an estimation procedure is proposed. In particular, our model allows to identify and estimate the association between $T$ and $C$, even though only the smallest of these variables is observable. The finite sample performance of the estimated parameters is illustrated by means of a thorough simulation study and the analysis of breast cancer data.|http://arxiv.org/abs/2403.07963v1|Morine Delhelle,Ingrid Van Keilegom
1873|Log-rank test with coarsened exact matching|It is of special importance in the clinical trial to compare survival times between the treatment group and the control group. Propensity score methods with a logistic regression model are often used to reduce the effects of confounders. However, the modeling of complex structures between the covariates, the treatment assignment and the survival time is difficult. In this paper, we consider coarsened exact matching (CEM), which does not need any parametric models, and we propose the weighted log-rank statistic based on CEM. We derive asymptotic properties of the weighted log-rank statistic, such as the weak convergence to a Gaussian process in Skorokhod space, in particular the asymptotic normality, under the null hypothesis and the consistency of the log-rank test. Simulation experiments are also conducted to compare the performance of the log-rank statistic with a propensity score method and CEM. Simulation studies show that the log-rank statistic based on CEM is more robust than the log-rank statistic based on the propensity score.|http://arxiv.org/abs/2403.16121v3|Tomoya Baba,Nakahiro Yoshida
1874|BayesPPDSurv: An R Package for Bayesian Sample Size Determination Using the Power and Normalized Power Prior for Time-To-Event Data|The BayesPPDSurv (Bayesian Power Prior Design for Survival Data) R package supports Bayesian power and type I error calculations and model fitting using the power and normalized power priors incorporating historical data with for the analysis of time-to-event outcomes. The package implements the stratified proportional hazards regression model with piecewise constant hazard within each stratum. The package allows the historical data to inform the treatment effect parameter, parameter effects for other covariates in the regression model, as well as the baseline hazard parameters. The use of multiple historical datasets is supported. A novel algorithm is developed for computationally efficient use of the normalized power prior. In addition, the package supports the use of arbitrary sampling priors for computing Bayesian power and type I error rates, and has built-in features that semi-automatically generate sampling priors from the historical data. We demonstrate the use of BayesPPDSurv in a comprehensive case study for a melanoma clinical trial design.|http://arxiv.org/abs/2404.05118v1|Yueqi Shen,Matthew A. Psioda,Joseph G. Ibrahim
1875|Learning Chemotherapy Drug Action via Universal Physics-Informed Neural Networks|Quantitative systems pharmacology (QSP) is widely used to assess drug effects and toxicity before the drug goes to clinical trial. However, significant manual distillation of the literature is needed in order to construct a QSP model. Parameters may need to be fit, and simplifying assumptions of the model need to be made. In this work, we apply Universal Physics-Informed Neural Networks (UPINNs) to learn unknown components of various differential equations that model chemotherapy pharmacodynamics. We learn three commonly employed chemotherapeutic drug actions (log-kill, Norton-Simon, and E_max) from synthetic data. Then, we use the UPINN method to fit the parameters for several synthetic datasets simultaneously. Finally, we learn the net proliferation rate in a model of doxorubicin (a chemotherapeutic) pharmacodynamics. As these are only toy examples, we highlight the usefulness of UPINNs in learning unknown terms in pharmacodynamic and pharmacokinetic models.|http://arxiv.org/abs/2404.08019v1|Lena Podina,Ali Ghodsi,Mohammad Kohandel
1876|Unlocking Insights: Enhanced Analysis of Covariance in General Factorial Designs through Multiple Contrast Tests under Variance Heteroscedasticity|A common goal in clinical trials is to conduct tests on estimated treatment effects adjusted for covariates such as age or sex. Analysis of Covariance (ANCOVA) is often used in these scenarios to test the global null hypothesis of no treatment effect using an $F$-test. However, in several samples, the $F$-test does not provide any information about individual null hypotheses and has strict assumptions such as variance homoscedasticity. We extend the method proposed by Konietschke et al. (2021) to a multiple contrast test procedure (MCTP), which allows us to test arbitrary linear hypotheses and provides information about the global as well as the individual null hypotheses. Further, we can calculate compatible simultaneous confidence intervals for the individual effects. We derive a small sample size approximation of the distribution of the test statistic via a multivariate t-distribution. As an alternative, we introduce a Wild-bootstrap method. Extensive simulations show that our methods are applicable even when sample sizes are small. Their application is further illustrated within a real data example.|http://arxiv.org/abs/2404.13939v2|Matthias Becher,Ludwig A. Hothorn,Frank Konietschke
1877|Predicting the Temporal Dynamics of Prosthetic Vision|Retinal implants are a promising treatment option for degenerative retinal disease. While numerous models have been developed to simulate the appearance of elicited visual percepts ("phosphenes"), these models often either focus solely on spatial characteristics or inadequately capture the complex temporal dynamics observed in clinical trials, which vary heavily across implant technologies, subjects, and stimulus conditions. Here we introduce two computational models designed to accurately predict phosphene fading and persistence under varying stimulus conditions, cross-validated on behavioral data reported by nine users of the Argus II Retinal Prosthesis System. Both models segment the time course of phosphene perception into discrete intervals, decomposing phosphene fading and persistence into either sinusoidal or exponential components. Our spectral model demonstrates state-of-the-art predictions of phosphene intensity over time (r = 0.7 across all participants). Overall, this study lays the groundwork for enhancing prosthetic vision by improving our understanding of phosphene temporal dynamics.|http://arxiv.org/abs/2404.14591v3|Yuchen Hou,Laya Pullela,Jiaxin Su,Sriya Aluru,Shivani Sista,Xiankun Lu,Michael Beyeler
1878|Exploring Machine Learning Algorithms for Infection Detection Using GC-IMS Data: A Preliminary Study|The developing field of enhanced diagnostic techniques in the diagnosis of infectious diseases, constitutes a crucial domain in modern healthcare. By utilizing Gas Chromatography-Ion Mobility Spectrometry (GC-IMS) data and incorporating machine learning algorithms into one platform, our research aims to tackle the ongoing issue of precise infection identification. Inspired by these difficulties, our goals consist of creating a strong data analytics process, enhancing machine learning (ML) models, and performing thorough validation for clinical applications. Our research contributes to the emerging field of advanced diagnostic technologies by integrating Gas Chromatography-Ion Mobility Spectrometry (GC-IMS) data and machine learning algorithms within a unified Laboratory Information Management System (LIMS) platform. Preliminary trials demonstrate encouraging levels of accuracy when employing various ML algorithms to differentiate between infected and non-infected samples. Continuing endeavors are currently concentrated on enhancing the effectiveness of the model, investigating techniques to clarify its functioning, and incorporating many types of data to further support the early detection of diseases.|http://arxiv.org/abs/2404.15757v1|Christos Sardianos,Chrysostomos Symvoulidis,Matthias Schlgl,Iraklis Varlamis,Georgios Th. Papadopoulos
1879|Investigating the causal effects of multiple treatments using longitudinal data: a simulation study|Many clinical questions involve estimating the effects of multiple treatments using observational data. When using longitudinal data, the interest is often in the effect of treatment strategies that involve sustaining treatment over time. This requires causal inference methods appropriate for handling multiple treatments and time-dependent confounding. Robins Generalised methods (g-methods) are a family of methods which can deal with time-dependent confounding and some of these have been extended to situations with multiple treatments, although there are currently no studies comparing different methods in this setting. We show how five g-methods (inverse-probability-of-treatment weighted estimation of marginal structural models, g-formula, g-estimation, censoring and weighting, and a sequential trials approach) can be extended to situations with multiple treatments, compare their performances in a simulation study, and demonstrate their application with an example using data from the UK CF Registry.|http://arxiv.org/abs/2405.01110v1|Emily Granger,Gwyneth Davies,Ruth H. Keogh
1880|VascularPilot3D: Toward a 3D fully autonomous navigation for endovascular robotics|This research reports VascularPilot3D, the first 3D fully autonomous endovascular robot navigation system. As an exploration toward autonomous guidewire navigation, VascularPilot3D is developed as a complete navigation system based on intra-operative imaging systems (fluoroscopic X-ray in this study) and typical endovascular robots. VascularPilot3D adopts previously researched fast 3D-2D vessel registration algorithms and guidewire segmentation methods as its perception modules. We additionally propose three modules: a topology-constrained 2D-3D instrument end-point lifting method, a tree-based fast path planning algorithm, and a prior-free endovascular navigation strategy. VascularPilot3D is compatible with most mainstream endovascular robots. Ex-vivo experiments validate that VascularPilot3D achieves 100% success rate among 25 trials. It reduces the human surgeon's overall control loops by 18.38%. VascularPilot3D is promising for general clinical autonomous endovascular navigations.|http://arxiv.org/abs/2405.09375v2|Jingwei Song,Keke Yang,Han Chen,Jiayi Liu,Yinan Gu,Qianxin Hui,Yanqi Huang,Meng Li,Zheng Zhang,Tuoyu Cao,Maani Ghaffari
1881|Graph Feedback Bandits with Similar Arms|In this paper, we study the stochastic multi-armed bandit problem with graph feedback. Motivated by the clinical trials and recommendation problem, we assume that two arms are connected if and only if they are similar (i.e., their means are close enough). We establish a regret lower bound for this novel feedback structure and introduce two UCB-based algorithms: D-UCB with problem-independent regret upper bounds and C-UCB with problem-dependent upper bounds. Leveraging the similarity structure, we also consider the scenario where the number of arms increases over time. Practical applications related to this scenario include Q\&A platforms (Reddit, Stack Overflow, Quora) and product reviews in Amazon and Flipkart. Answers (product reviews) continually appear on the website, and the goal is to display the best answers (product reviews) at the top. When the means of arms are independently generated from some distribution, we provide regret upper bounds for both algorithms and discuss the sub-linearity of bounds in relation to the distribution of means. Finally, we conduct experiments to validate the theoretical results.|http://arxiv.org/abs/2405.11171v1|Han Qi,Guo Fei,Li Zhu
1882|Efficient algorithms for the sensitivities of the Pearson correlation coefficient and its statistical significance to online data|Reliably measuring the collinearity of bivariate data is crucial in statistics, particularly for time-series analysis or ongoing studies in which incoming observations can significantly impact current collinearity estimates. Leveraging identities from Welford's online algorithm for sample variance, we develop a rigorous theoretical framework for analyzing the maximal change to the Pearson correlation coefficient and its p-value that can be induced by additional data. Further, we show that the resulting optimization problems yield elegant closed-form solutions that can be accurately computed by linear- and constant-time algorithms. Our work not only creates new theoretical avenues for robust correlation measures, but also has broad practical implications for disciplines that span econometrics, operations research, clinical trials, climatology, differential privacy, and bioinformatics. Software implementations of our algorithms in Cython-wrapped C are made available at https://github.com/marc-harary/sensitivity for reproducibility, practical deployment, and future theoretical development.|http://arxiv.org/abs/2405.14686v3|Marc Harary
1883|Improving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework|Commonly, AI or machine learning (ML) models are evaluated on benchmark datasets. This practice supports innovative methodological research, but benchmark performance can be poorly correlated with performance in real-world applications -- a construct validity issue. To improve the validity and practical usefulness of evaluations, we propose using an estimands framework adapted from international clinical trials guidelines. This framework provides a systematic structure for inference and reporting in evaluations, emphasizing the importance of a well-defined estimation target. We illustrate our proposal on examples of commonly used evaluation methodologies - involving cross-validation, clustering evaluation, and LLM benchmarking - that can lead to incorrect rankings of competing models (rank reversals) with high probability, even when performance differences are large. We demonstrate how the estimands framework can help uncover underlying issues, their causes, and potential solutions. Ultimately, we believe this framework can improve the validity of evaluations through better-aligned inference, and help decision-makers and model users interpret reported results more effectively.|http://arxiv.org/abs/2406.10366v1|Olivier Binette,Jerome P. Reiter
1884|Causal Inference with Outcomes Truncated by Death and Missing Not at Random|In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analyses. In this paper, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a Human Immunodeficiency Virus (HIV) study.|http://arxiv.org/abs/2406.10554v2|Wei Li,Yuan Liu,Shanshan Luo,Zhi Geng
1885|Oralytics Reinforcement Learning Algorithm|Dental disease is still one of the most common chronic diseases in the United States. While dental disease is preventable through healthy oral self-care behaviors (OSCB), this basic behavior is not consistently practiced. We have developed Oralytics, an online, reinforcement learning (RL) algorithm that optimizes the delivery of personalized intervention prompts to improve OSCB. In this paper, we offer a full overview of algorithm design decisions made using prior data, domain expertise, and experiments in a simulation test bed. The finalized RL algorithm was deployed in the Oralytics clinical trial, conducted from fall 2023 to summer 2024.|http://arxiv.org/abs/2406.13127v2|Anna L. Trella,Kelly W. Zhang,Stephanie M. Carpenter,David Elashoff,Zara M. Greer,Inbal Nahum-Shani,Dennis Ruenger,Vivek Shetty,Susan A. Murphy
1886|Rule-based outlier detection of AI-generated anatomy segmentations|There is a dire need for medical imaging datasets with accompanying annotations to perform downstream patient analysis. However, it is difficult to manually generate these annotations, due to the time-consuming nature, and the variability in clinical conventions. Artificial intelligence has been adopted in the field as a potential method to annotate these large datasets, however, a lack of expert annotations or ground truth can inhibit the adoption of these annotations. We recently made a dataset publicly available including annotations and extracted features of up to 104 organs for the National Lung Screening Trial using the TotalSegmentator method. However, the released dataset does not include expert-derived annotations or an assessment of the accuracy of the segmentations, limiting its usefulness. We propose the development of heuristics to assess the quality of the segmentations, providing methods to measure the consistency of the annotations and a comparison of results to the literature. We make our code and related materials publicly available at https://github.com/ImagingDataCommons/CloudSegmentatorResults and interactive tools at https://huggingface.co/spaces/ImagingDataCommons/CloudSegmentatorResults.|http://arxiv.org/abs/2406.14486v1|Deepa Krishnaswamy,Vamsi Krishna Thiriveedhi,Cosmin Ciausu,David Clunie,Steve Pieper,Ron Kikinis,Andrey Fedorov
1887|ACR: A Benchmark for Automatic Cohort Retrieval|Identifying patient cohorts is fundamental to numerous healthcare tasks, including clinical trial recruitment and retrospective studies. Current cohort retrieval methods in healthcare organizations rely on automated queries of structured data combined with manual curation, which are time-consuming, labor-intensive, and often yield low-quality results. Recent advancements in large language models (LLMs) and information retrieval (IR) offer promising avenues to revolutionize these systems. Major challenges include managing extensive eligibility criteria and handling the longitudinal nature of unstructured Electronic Medical Records (EMRs) while ensuring that the solution remains cost-effective for real-world application. This paper introduces a new task, Automatic Cohort Retrieval (ACR), and evaluates the performance of LLMs and commercial, domain-specific neuro-symbolic approaches. We provide a benchmark task, a query dataset, an EMR dataset, and an evaluation framework. Our findings underscore the necessity for efficient, high-quality ACR systems capable of longitudinal reasoning across extensive patient databases.|http://arxiv.org/abs/2406.14780v2|Dung Ngoc Thai,Victor Ardulov,Jose Ulises Mena,Simran Tiwari,Gleb Erofeev,Ramy Eskander,Karim Tarabishy,Ravi B Parikh,Wael Salloum
1888|Statistical inference on partially shape-constrained function-on-scalar linear regression models|We consider functional linear regression models where functional outcomes are associated with scalar predictors by coefficient functions with shape constraints, such as monotonicity and convexity, that apply to sub-domains of interest. To validate the partial shape constraints, we propose testing a composite hypothesis of linear functional constraints on regression coefficients. Our approach employs kernel- and spline-based methods within a unified inferential framework, evaluating the statistical significance of the hypothesis by measuring an $L^2$-distance between constrained and unconstrained model fits. In the theoretical study of large-sample analysis under mild conditions, we show that both methods achieve the standard rate of convergence observed in the nonparametric estimation literature. Through numerical experiments of finite-sample analysis, we demonstrate that the type I error rate keeps the significance level as specified across various scenarios and that the power increases with sample size, confirming the consistency of the test procedure under both estimation methods. Our theoretical and numerical results provide researchers the flexibility to choose a method based on computational preference. The practicality of partial shape-constrained inference is illustrated by two data applications: one involving clinical trials of NeuroBloc in type A-resistant cervical dystonia and the other with the National Institute of Mental Health Schizophrenia Study.|http://arxiv.org/abs/2407.00859v1|Kyunghee Han,Yeonjoo Park,Soo-Young Kim
1889|DrugCLIP: Contrastive Drug-Disease Interaction For Drug Repurposing|Bringing a novel drug from the original idea to market typically requires more than ten years and billions of dollars. To alleviate the heavy burden, a natural idea is to reuse the approved drug to treat new diseases. The process is also known as drug repurposing or drug repositioning. Machine learning methods exhibited huge potential in automating drug repurposing. However, it still encounter some challenges, such as lack of labels and multimodal feature representation. To address these issues, we design DrugCLIP, a cutting-edge contrastive learning method, to learn drug and disease's interaction without negative labels. Additionally, we have curated a drug repurposing dataset based on real-world clinical trial records. Thorough empirical studies are conducted to validate the effectiveness of the proposed DrugCLIP method.|http://arxiv.org/abs/2407.02265v1|Yingzhou Lu,Yaojun Hu,Chenhao Li
1890|A Randomized Exchange Algorithm for Optimal Design of Multi-Response Experiments|Despite the increasing prevalence of vector observations, computation of optimal experimental design for multi-response models has received limited attention. To address this problem within the framework of approximate designs, we introduce mREX, an algorithm that generalizes the randomized exchange algorithm REX (J Am Stat Assoc 115:529, 2020), originally specialized for single-response models. The mREX algorithm incorporates several improvements: a novel method for computing efficient sparse initial designs, an extension to all differentiable Kiefer's optimality criteria, and an efficient method for performing optimal exchanges of weights. For the most commonly used D-optimality criterion, we propose a technique for optimal weight exchanges based on the characteristic matrix polynomial. The mREX algorithm is applicable to linear, nonlinear, and generalized linear models, and scales well to large problems. It typically converges to optimal designs faster than available alternative methods, although it does not require advanced mathematical programming solvers. We demonstrate the application of mREX to bivariate dose-response Emax models for clinical trials, both without and with the inclusion of covariates.|http://arxiv.org/abs/2407.16283v1|Pl Somogyi,Samuel Rosa,Radoslav Harman
1891|Preference-Based Abstract Argumentation for Case-Based Reasoning (with Appendix)|In the pursuit of enhancing the efficacy and flexibility of interpretable, data-driven classification models, this work introduces a novel incorporation of user-defined preferences with Abstract Argumentation and Case-Based Reasoning (CBR). Specifically, we introduce Preference-Based Abstract Argumentation for Case-Based Reasoning (which we call AA-CBR-P), allowing users to define multiple approaches to compare cases with an ordering that specifies their preference over these comparison approaches. We prove that the model inherently follows these preferences when making predictions and show that previous abstract argumentation for case-based reasoning approaches are insufficient at expressing preferences over constituents of an argument. We then demonstrate how this can be applied to a real-world medical dataset sourced from a clinical trial evaluating differing assessment methods of patients with a primary brain tumour. We show empirically that our approach outperforms other interpretable machine learning models on this dataset.|http://arxiv.org/abs/2408.00108v2|Adam Gould,Guilherme Paulino-Passos,Seema Dadhania,Matthew Williams,Francesca Toni
1892|An Upper Confidence Bound Approach to Estimating the Maximum Mean|Estimating the maximum mean finds a variety of applications in practice. In this paper, we study estimation of the maximum mean using an upper confidence bound (UCB) approach where the sampling budget is adaptively allocated to one of the systems. We study in depth the existing grand average (GA) estimator, and propose a new largest-size average (LSA) estimator. Specifically, we establish statistical guarantees, including strong consistency, asymptotic mean squared errors, and central limit theorems (CLTs) for both estimators, which are new to the literature. We show that LSA is preferable over GA, as the bias of the former decays at a rate much faster than that of the latter when sample size increases. By using the CLTs, we further construct asymptotically valid confidence intervals for the maximum mean, and propose a single hypothesis test for a multiple comparison problem with application to clinical trials. Statistical efficiency of the resulting point and interval estimates and the proposed single hypothesis test is demonstrated via numerical examples.|http://arxiv.org/abs/2408.04179v1|Zhang Kun,Liu Guangwu,Shi Wen
1893|Causal Graph Aided Causal Discovery in an Observational Aneurysmal Subarachnoid Hemorrhage Study|Causal inference methods for observational data are increasingly recognized as a valuable complement to randomized clinical trials (RCTs). They can, under strong assumptions, emulate RCTs or help refine their focus. Our approach to causal inference uses causal directed acyclic graphs (DAGs). We are motivated by a concern that many observational studies in medicine begin without a clear definition of their objectives, without awareness of the scientific potential, and without tools to identify the necessary in itinere adjustments. We present and illustrate methods that provide "midway insights" during study's course, identify meaningful causal questions within the study's reach and point to the necessary data base enhancements for these questions to be meaningfully tackled. The method hinges on concepts of identification and positivity. Concepts are illustrated through an analysis of data generated by patients with aneurysmal Subarachnoid Hemorrhage (aSAH) halfway through a study, focusing in particular on the consequences of external ventricular drain (EVD) in strata of the aSAH population. In addition, we propose a method for multicenter studies, to monitor the impact of changes in practice at an individual center's level, by leveraging principles of instrumental variable (IV) inference.|http://arxiv.org/abs/2408.06464v1|Carlo Berzuini,Davide Luciani,Hiren C. Patel
1894|Applications of aligned nanofiber for tissue engineering|In tissue engineering, we seek to address comprehensive tissue repair and regeneration needs. Aligned nanofibers have emerged as powerful and versatile tools, attributable to their structural and biochemical congruence with the natural extracellular matrix (ECM). This review delineates the contemporary applications of aligned nanofibers in tissue engineering, spotlighting their implementation across musculoskeletal, neural, and cardiovascular tissue domains. The influence of fiber alignment on critical cellular behaviors - cell adhesion, migration, orientation, and differentiation - is reviewed. We also discuss how nanofibers are improved by adding growth factors, peptides, and drugs to help tissues regenerate better. Comprehensive analyses of in vivo trials and clinical studies corroborate the efficacy and safety of these fibers in tissue engineering applications. The review culminates with exploring extant challenges, concurrently charting prospective avenues in aligned nanofiber-centric tissue engineering.|http://arxiv.org/abs/2408.07909v1|Gayatri Patel,Louis-S. Bouchard
1895|Learning Robust Treatment Rules for Censored Data|There is a fast-growing literature on estimating optimal treatment rules directly by maximizing the expected outcome. In biomedical studies and operations applications, censored survival outcome is frequently observed, in which case the restricted mean survival time and survival probability are of great interest. In this paper, we propose two robust criteria for learning optimal treatment rules with censored survival outcomes; the former one targets at an optimal treatment rule maximizing the restricted mean survival time, where the restriction is specified by a given quantile such as median; the latter one targets at an optimal treatment rule maximizing buffered survival probabilities, where the predetermined threshold is adjusted to account the restricted mean survival time. We provide theoretical justifications for the proposed optimal treatment rules and develop a sampling-based difference-of-convex algorithm for learning them. In simulation studies, our estimators show improved performance compared to existing methods. We also demonstrate the proposed method using AIDS clinical trial data.|http://arxiv.org/abs/2408.09155v1|Yifan Cui,Junyi Liu,Tao Shen,Zhengling Qi,Xi Chen
1896|SPORTSCausal: Spill-Over Time Series Causal Inference|Randomized controlled trials (RCTs) have long been the gold standard for causal inference across various fields, including business analysis, economic studies, sociology, clinical research, and network learning. The primary advantage of RCTs over observational studies lies in their ability to significantly reduce noise from individual variance. However, RCTs depend on strong assumptions, such as group independence, time independence, and group randomness, which are not always feasible in real-world applications. Traditional inferential methods, including analysis of covariance (ANCOVA), often fail when these assumptions do not hold. In this paper, we propose a novel approach named \textbf{Sp}ill\textbf{o}ve\textbf{r} \textbf{T}ime \textbf{S}eries \textbf{Causal} (\verb+SPORTSCausal+), which enables the estimation of treatment effects without relying on these stringent assumptions. We demonstrate the practical applicability of \verb+SPORTSCausal+ through a real-world budget-control experiment. In this experiment, data was collected from both a 5\% live experiment and a 50\% live experiment using the same treatment. Due to the spillover effect, the vanilla estimation of the treatment effect was not robust across different treatment sizes, whereas \verb+SPORTSCausal+ provided a robust estimation.|http://arxiv.org/abs/2408.11951v1|Carol Liu
1897|Quantum-machine-assisted Drug Discovery: Survey and Perspective|Drug discovery and development is a highly complex and costly endeavor, typically requiring over a decade and substantial financial investment to bring a new drug to market. Traditional computer-aided drug design (CADD) has made significant progress in accelerating this process, but the development of quantum computing offers potential due to its unique capabilities. This paper discusses the integration of quantum computing into drug discovery and development, focusing on how quantum technologies might accelerate and enhance various stages of the drug development cycle. Specifically, we explore the application of quantum computing in addressing challenges related to drug discovery, such as molecular simulation and the prediction of drug-target interactions, as well as the optimization of clinical trial outcomes. By leveraging the inherent capabilities of quantum computing, we might be able to reduce the time and cost associated with bringing new drugs to market, ultimately benefiting public health.|http://arxiv.org/abs/2408.13479v3|Yidong Zhou,Jintai Chen,Jinglei Cheng,Gopal Karemore,Marinka Zitnik,Frederic T. Chong,Junyu Liu,Tianfan Fu,Zhiding Liang
1898|MiWaves Reinforcement Learning Algorithm|The escalating prevalence of cannabis use poses a significant public health challenge globally. In the U.S., cannabis use is more prevalent among emerging adults (EAs) (ages 18-25) than any other age group, with legalization in the multiple states contributing to a public perception that cannabis is less risky than in prior decades. To address this growing concern, we developed MiWaves, a reinforcement learning (RL) algorithm designed to optimize the delivery of personalized intervention prompts to reduce cannabis use among EAs. MiWaves leverages domain expertise and prior data to tailor the likelihood of delivery of intervention messages. This paper presents a comprehensive overview of the algorithm's design, including key decisions and experimental outcomes. The finalized MiWaves RL algorithm was deployed in a clinical trial from March to May 2024.|http://arxiv.org/abs/2408.15076v1|Susobhan Ghosh,Yongyi Guo,Pei-Yao Hung,Lara Coughlin,Erin Bonar,Inbal Nahum-Shani,Maureen Walton,Susan Murphy
1899|Chasing Shadows: How Implausible Assumptions Skew Our Understanding of Causal Estimands|The ICH E9 (R1) addendum on estimands, coupled with recent advancements in causal inference, has prompted a shift towards using model-free treatment effect estimands that are more closely aligned with the underlying scientific question. This represents a departure from traditional, model-dependent approaches where the statistical model often overshadows the inquiry itself. While this shift is a positive development, it has unintentionally led to the prioritization of an estimand's ability to perfectly answer the key scientific question over its practical learnability from data under plausible assumptions. We illustrate this by scrutinizing assumptions in the recent clinical trials literature on principal stratum estimands, demonstrating that some popular assumptions are not only implausible but often inevitably violated. We advocate for a more balanced approach to estimand formulation, one that carefully considers both the scientific relevance and the practical feasibility of estimation under realistic conditions.|http://arxiv.org/abs/2409.11162v2|Stijn Vansteelandt,Kelly Van Lancker
1900|Early diagnosis of Alzheimer's disease from MRI images with deep learning model|It is acknowledged that the most common cause of dementia worldwide is Alzheimer's disease (AD). This condition progresses in severity from mild to severe and interferes with people's everyday routines. Early diagnosis plays a critical role in patient care and clinical trials. Convolutional neural networks (CNN) are used to create a framework for identifying specific disease features from MRI scans Classification of dementia involves approaches such as medical history review, neuropsychological tests, and magnetic resonance imaging (MRI). However, the image dataset obtained from Kaggle faces a significant issue of class imbalance, which requires equal distribution of samples from each class to address. In this article, to address this imbalance, the Synthetic Minority Oversampling Technique (SMOTE) is utilized. Furthermore, a pre-trained convolutional neural network has been applied to the DEMNET dementia network to extract key features from AD images. The proposed model achieved an impressive accuracy of 98.67%.|http://arxiv.org/abs/2409.18814v1|Sajjad Aghasi Javid,Mahmood Mohassel Feghhi
1901|Estimating Interpretable Heterogeneous Treatment Effect with Causal Subgroup Discovery in Survival Outcomes|Estimating heterogeneous treatment effect (HTE) for survival outcomes has gained increasing attention, as it captures the variation in treatment efficacy across patients or subgroups in delaying disease progression. However, most existing methods focus on post-hoc subgroup identification rather than simultaneously estimating HTE and selecting relevant subgroups. In this paper, we propose an interpretable HTE estimation framework that integrates three meta-learners that simultaneously estimate CATE for survival outcomes and identify predictive subgroups. We evaluated the performance of our method through comprehensive simulation studies across various randomized clinical trial (RCT) settings. Additionally, we demonstrated its application in a large RCT for age-related macular degeneration (AMD), a polygenic progressive eye disease, to estimate the HTE of an antioxidant and mineral supplement on time-to-AMD progression and to identify genetics-based subgroups with enhanced treatment effects. Our method offers a direct interpretation of the estimated HTE and provides evidence to support precision healthcare.|http://arxiv.org/abs/2409.19241v2|Na Bo,Ying Ding
1902|Robust Emax Model Fitting: Addressing Nonignorable Missing Binary Outcome in Dose-Response Analysis|The Binary Emax model is widely employed in dose-response analysis during drug development, where missing data often pose significant challenges. Addressing nonignorable missing binary responses, where the likelihood of missing data is related to unobserved outcomes, is particularly important, yet existing methods often lead to biased estimates. This issue is compounded when using the regulatory-recommended imputing as treatment failure approach, known as non-responder imputation. Moreover, the problem of separation, where a predictor perfectly distinguishes between outcome classes, can further complicate likelihood maximization. In this paper, we introduce a penalized likelihood-based method that integrates a modified Expectation-Maximization algorithm in the spirit of Ibrahim and Lipsitz to effectively manage both nonignorable missing data and separation issues. Our approach applies a noninformative Jeffreys prior to the likelihood, reducing bias in parameter estimation. Simulation studies demonstrate that our method outperforms existing methods, such as NRI, and the superiority is further supported by its application to data from a Phase II clinical trial. Additionally, we have developed an R package, ememax, to facilitate the implementation of the proposed method.|http://arxiv.org/abs/2410.00259v1|Jiangshan Zhang,Vivek Pradhan,Yuxi Zhao
1903|Nonparametric tests of treatment effect homogeneity for policy-makers|Recent work has focused on nonparametric estimation of conditional treatment effects, but inference has remained relatively unexplored. We propose a class of nonparametric tests for both quantitative and qualitative treatment effect heterogeneity. The tests can incorporate a variety of structured assumptions on the conditional average treatment effect, allow for both continuous and discrete covariates, and do not require sample splitting. Furthermore, we show how the tests are tailored to detect alternatives where the population impact of adopting a personalized decision rule differs from using a rule that discards covariates. The proposal is thus relevant for guiding treatment policies. The utility of the proposal is borne out in simulation studies and a re-analysis of an AIDS clinical trial.|http://arxiv.org/abs/2410.00985v2|Oliver Dukes,Mats J. Stensrud,Riccardo Brioschi,Aaron Hudson
1904|Multilayer network approaches to omics data integration in Digital Twins for cancer research|This review examines current and potential applications of DTs in healthcare, focusing on the integration of multi-omics data using multilayer network approaches in cancer research. We discuss methodologies, tools and platforms commonly used for this integration, while highlighting case studies, challenges, and research gaps. Finally, we advocate the need for incorporating diverse data types to produce more effective DTs for personalization of cancer treatments and in silico clinical trials.|http://arxiv.org/abs/2410.07252v1|Hugo Chenel,Malvina Marku,Tim James,Andrei Zinovyev,Vera Pancaldi
1905|A scientific review on advances in statistical methods for crossover design|A comprehensive review of the literature on crossover design is needed to highlight its evolution, applications, and methodological advancements across various fields. Given its widespread use in clinical trials and other research domains, understanding this design's challenges, assumptions, and innovations is essential for optimizing its implementation and ensuring accurate, unbiased results. This article extensively reviews the history and statistical inference methods for crossover designs. A primary focus is given to the AB-BA design as it is the most widely used design in literature. Extension from two periods to higher-order designs is discussed, and a general inference procedure for continuous response is studied. Analysis of multivariate and categorical responses is also reviewed in this context. A bunch of open problems in this area are shortlisted.|http://arxiv.org/abs/2410.08441v1|Salil Koner
1906|Hierarchical Upper Confidence Bounds for Constrained Online Learning|The multi-armed bandit (MAB) problem is a foundational framework in sequential decision-making under uncertainty, extensively studied for its applications in areas such as clinical trials, online advertising, and resource allocation. Traditional MAB formulations, however, do not adequately capture scenarios where decisions are structured hierarchically, involve multi-level constraints, or feature context-dependent action spaces. In this paper, we introduce the hierarchical constrained bandits (HCB) framework, which extends the contextual bandit problem to incorporate hierarchical decision structures and multi-level constraints. We propose the hierarchical constrained upper confidence bound (HC-UCB) algorithm, designed to address the complexities of the HCB problem by leveraging confidence bounds within a hierarchical setting. Our theoretical analysis establishes sublinear regret bounds for HC-UCB and provides high-probability guarantees for constraint satisfaction at all hierarchical levels. Furthermore, we derive a minimax lower bound on the regret for the HCB problem, demonstrating the near-optimality of our algorithm. The results are significant for real-world applications where decision-making processes are inherently hierarchical and constrained, offering a robust and efficient solution that balances exploration and exploitation across multiple levels of decision-making.|http://arxiv.org/abs/2410.17216v2|Ali Baheri
1907|CDsampling: An R Package for Constrained D-Optimal Sampling in Paid Research Studies|In the context of paid research studies and clinical trials, budget considerations often require patient sampling from available populations which comes with inherent constraints. We introduce the R package CDsampling, which is the first to our knowledge to integrate optimal design theories within the framework of constrained sampling. This package offers the possibility to find both D-optimal approximate and exact allocations for samplings with or without constraints. Additionally, it provides functions to find constrained uniform sampling as a robust sampling strategy when the model information is limited. To demonstrate its efficacy, we provide simulated examples and a real-data example with datasets embedded in the package and compare them with classical sampling methods. Furthermore, it revisits the theoretical results of the Fisher information matrix for generalized linear models (including regular linear regression model) and multinomial logistic models, offering functions for its computation.|http://arxiv.org/abs/2410.20606v2|Yifei Huang,Liping Tong,Jie Yang
1908|RapidDock: Unlocking Proteome-scale Molecular Docking|Accelerating molecular docking -- the process of predicting how molecules bind to protein targets -- could boost small-molecule drug discovery and revolutionize medicine. Unfortunately, current molecular docking tools are too slow to screen potential drugs against all relevant proteins, which often results in missed drug candidates or unexpected side effects occurring in clinical trials. To address this gap, we introduce RapidDock, an efficient transformer-based model for blind molecular docking. RapidDock achieves at least a $100 \times$ speed advantage over existing methods without compromising accuracy. On the Posebusters and DockGen benchmarks, our method achieves $52.1\%$ and $44.0\%$ success rates ($\text{RMSD}<2$\r{A}), respectively. The average inference time is $0.04$ seconds on a single GPU, highlighting RapidDock's potential for large-scale docking studies. We examine the key features of RapidDock that enable leveraging the transformer architecture for molecular docking, including the use of relative distance embeddings of $3$D structures in attention matrices, pre-training on protein folding, and a custom loss function invariant to molecular symmetries.|http://arxiv.org/abs/2411.00004v1|Rafa Powalski,Bazyli Klockiewicz,Bartosz Topolski,Dariusz Plewczynski
1909|On Novel Approach for Computing Distance based Indices of Anti-tuberculosis Drugs|This work aims to assess the molecular architectures of anti-tuberculosis drugs using both degree-based topological indices and novel distance based indices. We can represent the chemical arrangement as a graph, with atoms serving as the vertices and connections as the edges. Here, the multi bonds were considered as multi edges and included all the hydrogen atoms. Also, we consider three dimensional molecular graph. As a result, the actual bond lengths have been used for computation of new distance based indices. Compared to numerous studies, this is a significant improvement. Furthermore, the investigation of these indices includes a study on the quantitative structure-property relationship (QSPR). The research demonstrates a notable correlation between these indicators and the physical attributes of anti-tuberculosis drugs. Here, Since we reduced the some of existing critical assumptions in the literature, chemists and pharmaceutical professionals might potentially eliminate the need for clinical trials by employing this theoretical model. These models would enable them to predict the characteristics of anti-tuberculosis medications.|http://arxiv.org/abs/2411.02416v1|D. C. Gunawardhana,G. H. J. Lanel,K. K. K. R. Perera,A. G. M. J. Gunaratna
1910|Energy-based generative models for monoclonal antibodies|Since the approval of the first antibody drug in 1986, a total of 162 antibodies have been approved for a wide range of therapeutic areas, including cancer, autoimmune, infectious, or cardiovascular diseases. Despite advances in biotechnology that accelerated the development of antibody drugs, the drug discovery process for this modality remains lengthy and costly, requiring multiple rounds of optimizations before a drug candidate can progress to preclinical and clinical trials. This multi-optimization problem involves increasing the affinity of the antibody to the target antigen while refining additional biophysical properties that are essential to drug development such as solubility, thermostability or aggregation propensity. Additionally, antibodies that resemble natural human antibodies are particularly desirable, as they are likely to offer improved profiles in terms of safety, efficacy, and reduced immunogenicity, further supporting their therapeutic potential. In this article, we explore the use of energy-based generative models to optimize a candidate monoclonal antibody. We identify tradeoffs when optimizing for multiple properties, concentrating on solubility, humanness and affinity and use the generative model we develop to generate candidate antibodies that lie on an optimal Pareto front that satisfies these constraints.|http://arxiv.org/abs/2411.13390v1|Paul Pereira,Herv Minoux,Aleksandra M. Walczak,Thierry Mora
1911|Regional consistency evaluation and sample size calculation under two MRCTs|Multi-regional clinical trial (MRCT) has been common practice for drug development and global registration. The FDA guidance "Demonstrating Substantial Evidence of Effectiveness for Human Drug and Biological Products Guidance for Industry" (FDA, 2019) requires that substantial evidence of effectiveness of a drug/biologic product to be demonstrated for market approval. In the situations where two pivotal MRCTs are needed to establish effectiveness of a specific indication for a drug or biological product, a systematic approach of consistency evaluation for regional effect is crucial. In this paper, we first present some existing regional consistency evaluations in a unified way that facilitates regional sample size calculation under the simple fixed effect model. Second, we extend the two commonly used consistency assessment criteria of MHLW (2007) in the context of two MRCTs and provide their evaluation and regional sample size calculation. Numerical studies demonstrate the proposed regional sample size attains the desired probability of showing regional consistency. A hypothetical example is provided for illustration of application. We provide an R package for implementation.|http://arxiv.org/abs/2411.15567v1|Kunhai Qing,Xinru Ren,Jin Xu
1912|STORM: Strategic Orchestration of Modalities for Rare Event Classification|In domains such as biomedical, expert insights are crucial for selecting the most informative modalities for artificial intelligence (AI) methodologies. However, using all available modalities poses challenges, particularly in determining the impact of each modality on performance and optimizing their combinations for accurate classification. Traditional approaches resort to manual trial and error methods, lacking systematic frameworks for discerning the most relevant modalities. Moreover, although multi-modal learning enables the integration of information from diverse sources, utilizing all available modalities is often impractical and unnecessary. To address this, we introduce an entropy-based algorithm STORM to solve the modality selection problem for rare event. This algorithm systematically evaluates the information content of individual modalities and their combinations, identifying the most discriminative features essential for rare class classification tasks. Through seizure onset zone detection case study, we demonstrate the efficacy of our algorithm in enhancing classification performance. By selecting useful subset of modalities, our approach paves the way for more efficient AI-driven biomedical analyses, thereby advancing disease diagnosis in clinical settings.|http://arxiv.org/abs/2412.02805v1|Payal Kamboj,Ayan Banerjee,Sandeep K. S. Gupta
1913|Nonparametric estimation of the Patient Weighted While-Alive Estimand|In clinical trials with recurrent events, such as repeated hospitalizations terminating with death, it is important to consider the patient events overall history for a thorough assessment of treatment effects. The occurrence of fewer events due to early deaths can lead to misinterpretation, emphasizing the importance of a while-alive strategy as suggested in Schmidli et al. (2023). We focus in this paper on the patient weighted while-alive estimand represented as the expected number of events divided by the time alive within a target window and develop efficient estimation for this estimand. We derive its efficient influence function and develop a one-step estimator, initially applied to the irreversible illness-death model. For the broader context of recurrent events, due to the increased complexity, the one-step estimator is practically intractable. We therefore suggest an alternative estimator that is also expected to have high efficiency focusing on the randomized treatment setting. We compare the efficiency of these two estimators in the illness-death setting. Additionally, we apply our proposed estimator to a real-world case study involving metastatic colorectal cancer patients, demonstrating the practical applicability and benefits of the while-alive approach.|http://arxiv.org/abs/2412.03246v1|Alessandra Ragni,Torben Martinussen,Thomas Scheike
1914|ASPIRE: Assistive System for Performance Evaluation in IR|Information Retrieval (IR) evaluation involves far more complexity than merely presenting performance measures in a table. Researchers often need to compare multiple models across various dimensions, such as the Precision-Recall trade-off and response time, to understand the reasons behind the varying performance of specific queries for different models. We introduce ASPIRE (Assistive System for Performance Evaluation in IR), a visual analytics tool designed to address these complexities by providing an extensive and user-friendly interface for in-depth analysis of IR experiments. ASPIRE supports four key aspects of IR experiment evaluation and analysis: single/multi-experiment comparisons, query-level analysis, query characteristics-performance interplay, and collection-based retrieval analysis. We showcase the functionality of ASPIRE using the TREC Clinical Trials collection. ASPIRE is an open-source toolkit available online: https://github.com/GiorgosPeikos/ASPIRE|http://arxiv.org/abs/2412.15759v1|Georgios Peikos,Wojciech Kusa,Symeon Symeonidis
1915|Denoising Data with Measurement Error Using a Reproducing Kernel-based Diffusion Model|The ongoing technological revolution in measurement systems enables the acquisition of high-resolution samples in fields such as engineering, biology, and medicine. However, these observations are often subject to errors from measurement devices. Motivated by this challenge, we propose a denoising framework that employs diffusion models to generate denoised data whose distribution closely approximates the unobservable, error-free data, thereby permitting standard data analysis based on the denoised data. The key element of our framework is a novel Reproducing Kernel Hilbert Space-based method that trains the diffusion model with only error-contaminated data, admits a closed-form solution, and achieves a fast convergence rate in terms of estimation error. Furthermore, we verify the effectiveness of our method by deriving an upper bound on the Kullback--Leibler divergence between the distributions of the generated denoised data and the error-free data. A series of conducted simulations also verify the promising empirical performance of the proposed method compared to other state-of-the-art methods. To further illustrate the potential of this denoising framework in a real-world application, we apply it in a digital health context, showing how measurement error in continuous glucose monitors can influence conclusions drawn from a clinical trial on diabetes Mellitus disease.|http://arxiv.org/abs/2501.00212v1|Mingyang Yi,Marcos Matabuena,Ruoyu Wang
1916|EOG Communication Interface for Quadriplegics: Prototype & Signal Processing|Electrooculography (EOG) is an electrophysiological signal that determines the human eye orientation and is therefore widely used in Human Tracking Interfaces (HCI). The purpose of this project is to develop a communication method for quadriplegic patients using EOG signals aimed at text and voice generation. The system consists of 3D eye movement tracking embedded using a custom-built prototype to measure the eyeball's left-right and up-down movements. The ESP32 board, which has a set of parameters to convert the data into content displayed on LCDs and MP3 players, is used to capture and process the signal. helps people by facilitating more natural and efficient symptom expression. The blink system will be able to incorporate face masks and more eye tests as it continues to develop. Even if it might work, more research and clinical trials are needed to evaluate the system's usefulness and ensure that it performs as planned in real-world scenarios. With this project, assistive technology will make significant progress and improve the lives of many who suffer from severe motor impairments.|http://arxiv.org/abs/2501.02465v1|Aniket Raj,Amit Kumar
1917|Truthful mechanisms for linear bandit games with private contexts|The contextual bandit problem, where agents arrive sequentially with personal contexts and the system adapts its arm allocation decisions accordingly, has recently garnered increasing attention for enabling more personalized outcomes. However, in many healthcare and recommendation applications, agents have private profiles and may misreport their contexts to gain from the system. For example, in adaptive clinical trials, where hospitals sequentially recruit volunteers to test multiple new treatments and adjust plans based on volunteers' reported profiles such as symptoms and interim data, participants may misreport severe side effects like allergy and nausea to avoid perceived suboptimal treatments. We are the first to study this issue of private context misreporting in a stochastic contextual bandit game between the system and non-repeated agents. We show that traditional low-regret algorithms, such as UCB family algorithms and Thompson sampling, fail to ensure truthful reporting and can result in linear regret in the worst case, while traditional truthful algorithms like explore-then-commit (ETC) and $\epsilon$-greedy algorithm incur sublinear but high regret. We propose a mechanism that uses a linear program to ensure truthfulness while minimizing deviation from Thompson sampling, yielding an $O(\ln T)$ frequentist regret. Our numerical experiments further demonstrate strong performance in multiple contexts and across other distribution families.|http://arxiv.org/abs/2501.03865v1|Yiting Hu,Lingjie Duan
1918|A general, flexible and harmonious framework to construct interpretable functions in regression analysis|An interpretable model or method has several appealing features, such as reliability to adversarial examples, transparency of decision-making, and communication facilitator. However, interpretability is a subjective concept, and even its definition can be diverse. The same model may be deemed as interpretable by a study team, but regarded as a black-box algorithm by another squad. Simplicity, accuracy and generalizability are some additional important aspects of evaluating interpretability. In this work, we present a general, flexible and harmonious framework to construct interpretable functions in regression analysis with a focus on continuous outcomes. We formulate a functional skeleton in light of users' expectations of interpretability. A new measure based on Mallows's $C_p$-statistic is proposed for model selection to balance approximation, generalizability, and interpretability. We apply this approach to derive a sample size formula in adaptive clinical trial designs to demonstrate the general workflow, and to explain operating characteristics in a Bayesian Go/No-Go paradigm to show the potential advantages of using meaningful intermediate variables. Generalization to categorical outcomes is illustrated in an example of hypothesis testing based on Fisher's exact test. A real data analysis of NHANES (National Health and Nutrition Examination Survey) is conducted to investigate relationships between some important laboratory measurements. We also discuss some extensions of this method.|http://arxiv.org/abs/2501.15526v1|Tianyu Zhan,Jian Kang
1919|Ideal trials, target trials and actual randomized trials|Causal inference is the goal of randomized controlled trials and many observational studies. The first step in a formal approach to causal inference is to define the estimand of interest, and in both types of study this can be intuitively defined as the effect in an ideal trial: a hypothetical perfect randomized experiment (with representative sample, perfect adherence, etc.). The target trial framework is an increasingly popular approach to causal inference in observational studies, but clarity is lacking in how a target trial should be specified and, crucially, how it relates to the ideal trial. In this paper, we consider these questions and use an example from respiratory epidemiology to highlight challenges with an approach that is commonly seen in applications: to specify a target trial in a way that is closely aligned to the observational study (e.g. uses the same eligibility criteria, outcome measure, etc.). The main issue is that such a target trial generally deviates from the ideal trial. Thus, even if the target trial can be emulated perfectly apart from randomization, biases beyond baseline confounding are likely to remain, relative to the estimand of interest. Without consideration of the ideal trial, these biases may go unnoticed, mirroring the often-overlooked biases of actual trials. Therefore, we suggest that, in both actual trials and observational studies, specifying the ideal trial and how the target or actual trial differs from it is necessary to systematically assess all potential sources of biases, and therefore appropriately design analyses and interpret findings.|http://arxiv.org/abs/2405.10026v3|Margarita Moreno-Betancur,Rushani Wijesuriya,John B. Carlin
1920|Malignant field signature analysis in biopsy samples at diagnosis identifies lethal disease in patients with localized Gleason 6 and 7 prostate cancer|Overtreatment of early-stage low-risk prostate cancer (PC) patients represents a significant problem in disease management and has socio-economic implications. Development of genetic and molecular markers of clinically significant disease in patients diagnosed with low grade localized PC would have a major impact in disease management. A gene expression signature (GES) is reported for lethal PC in biopsy specimens obtained at the time of diagnosis from patients with Gleason 6 and Gleason 7 tumors in a Swedish watchful waiting cohort with up to 30 years follow-up. A 98-genes GES identified 89 and 100 percent of all death events 4 years after diagnosis in G7 and G6 patients, respectively; at 6 years follow-up, 83 and 100 percent of all deaths events were captured. Remarkably, the 98-genes GES appears to perform successfully in patients stratification with as little as 2% of cancer cells in a specimen, strongly indicating that it captures a malignant field effect in prostates harboring cancer cells of different degrees of aggressiveness. In G6 and G7 tumors from PC patients of age 65 or younger, GES identified 86 percent of all death events during the entire follow-up period. In G6 and G7 tumors from PC patients of age 70 or younger, GES identified 90 percent of all death events 6 years after diagnosis. Classification performance of the reported in this study 98-genes GES of lethal PC appeared suitable to meet design and feasibility requirements of a prospective 4 to 6 years clinical trial, which is essential for regulatory approval of diagnostic and prognostic tests in clinical setting. Prospectively validated GES of lethal PC in biopsy specimens of G6 and G7 tumors will help physicians to identify, at the time of diagnosis, patients who should be considered for exclusion from active surveillance programs and who would most likely benefit from immediate curative interventions.|http://arxiv.org/abs/1602.06504v1|Gennadi Glinsky
1921|Patient-Specific 3D Volumetric Reconstruction of Bioresorbable Stents: A Method to Generate 3D Geometries for Computational Analysis of Coronaries Treated with Bioresorbable Stents|As experts continue to debate the optimal surgery practice for coronary disease - percutaneous coronary intervention (PCI) or coronary aortic bypass graft (CABG) - computational tools may provide a quantitative assessment of each option. Computational fluid dynamics (CFD) has been used to assess the interplay between hemodynamics and stent struts; it is of particular interest in Bioresorbable Vascular Stents (BVS), since their thicker struts may result in impacted flow patterns and possible pathological consequences. Many proofs of concept are presented in the literature; however, a practical method for extracting patient-specific stented coronary artery geometries from images over a large number of patients remains an open problem.   This work provides a possible pipeline for the reconstruction of the BVS. Using Optical Coherence Tomographies (OCT) and Invasive Coronary Angiographies (ICA), we can reconstruct the 3D geometry of deployed BVS in vivo. We illustrate the stent reconstruction process: (i) automatic strut detection, (ii) identification of stent components, (iii) 3D registration of stent curvature, and (iv) final stent volume reconstruction. The methodology is designed for use on clinical OCT images, as opposed to approaches that relied on a small number of virtually deployed stents.   The proposed reconstruction process is validated with a virtual phantom stent, providing quantitative assessment of the methodology, and with selected clinical cases, confirming feasibility. Using multimodality image analysis, we obtain reliable reconstructions within a reasonable timeframe. This work is the first step toward a fully automated reconstruction and simulation procedure aiming at an extensive quantitative analysis of the impact of BVS struts on hemodynamics via CFD in clinical trials, going beyond the proof-of-concept stage.|http://arxiv.org/abs/1810.03270v1|Boyi Yang,Marina Piccinelli,Gaetano Esposito,Tianli Han,Yasir Bouchi,Bill Gogas,Don Giddens,Habib Samady,Alessandro Veneziani
1922|Advancing PICO Element Detection in Biomedical Text via Deep Neural Networks|In evidence-based medicine (EBM), defining a clinical question in terms of the specific patient problem aids the physicians to efficiently identify appropriate resources and search for the best available evidence for medical treatment. In order to formulate a well-defined, focused clinical question, a framework called PICO is widely used, which identifies the sentences in a given medical text that belong to the four components typically reported in clinical trials: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome (O). In this work, we propose a novel deep learning model for recognizing PICO elements in biomedical abstracts. Based on the previous state-of-the-art bidirectional long-short term memory (biLSTM) plus conditional random field (CRF) architecture, we add another layer of biLSTM upon the sentence representation vectors so that the contextual information from surrounding sentences can be gathered to help infer the interpretation of the current one. In addition, we propose two methods to further generalize and improve the model: adversarial training and unsupervised pre-training over large corpora. We tested our proposed approach over two benchmark datasets. One is the PubMed-PICO dataset, where our best results outperform the previous best by 5.5%, 7.9%, and 5.8% for P, I, and O elements in terms of F1 score, respectively. And for the other dataset named NICTA-PIBOSO, the improvements for P/I/O elements are 2.4%, 13.6%, and 1.0% in F1 score, respectively. Overall, our proposed deep learning model can obtain unprecedented PICO element detection accuracy while avoiding the need for any manual feature selection.|http://arxiv.org/abs/1810.12780v4|Di Jin,Peter Szolovits
1923|Distanced LSTM: Time-Distanced Gates in Long Short-Term Memory Models for Lung Cancer Detection|The field of lung nodule detection and cancer prediction has been rapidly developing with the support of large public data archives. Previous studies have largely focused on cross-sectional (single) CT data. Herein, we consider longitudinal data. The Long Short-Term Memory (LSTM) model addresses learning with regularly spaced time points (i.e., equal temporal intervals). However, clinical imaging follows patient needs with often heterogeneous, irregular acquisitions. To model both regular and irregular longitudinal samples, we generalize the LSTM model with the Distanced LSTM (DLSTM) for temporally varied acquisitions. The DLSTM includes a Temporal Emphasis Model (TEM) that enables learning across regularly and irregularly sampled intervals. Briefly, (1) the time intervals between longitudinal scans are modeled explicitly, (2) temporally adjustable forget and input gates are introduced for irregular temporal sampling; and (3) the latest longitudinal scan has an additional emphasis term. We evaluate the DLSTM framework in three datasets including simulated data, 1794 National Lung Screening Trial (NLST) scans, and 1420 clinically acquired data with heterogeneous and irregular temporal accession. The experiments on the first two datasets demonstrate that our method achieves competitive performance on both simulated and regularly sampled datasets (e.g. improve LSTM from 0.6785 to 0.7085 on F1 score in NLST). In external validation of clinically and irregularly acquired data, the benchmarks achieved 0.8350 (CNN feature) and 0.8380 (LSTM) on the area under the ROC curve (AUC) score, while the proposed DLSTM achieves 0.8905.|http://arxiv.org/abs/1909.05321v1|Riqiang Gao,Yuankai Huo,Shunxing Bao,Yucheng Tang,Sanja L. Antic,Emily S. Epstein,Aneri B. Balar,Steve Deppen,Alexis B. Paulson,Kim L. Sandler,Pierre P. Massion,Bennett A. Landman
1924|Technical Background for "A Precision Medicine Approach to Develop and Internally Validate Optimal Exercise and Weight Loss Treatments for Overweight and Obese Adults with Knee Osteoarthritis"|We provide additional statistical background for the methodology developed in the clinical analysis of knee osteoarthritis in "A Precision Medicine Approach to Develop and Internally Validate Optimal Exercise and Weight Loss Treatments for Overweight and Obese Adults with Knee Osteoarthritis" (Jiang et al. 2020). Jiang et al. 2020 proposed a pipeline to learn optimal treatment rules with precision medicine models and compared them with zero-order models with a Z-test. The model performance was based on value functions, a scalar that predicts the future reward of each decision rule. The jackknife (i.e., leave-one-out cross validation) method was applied to estimate the value function and its variance of several outcomes in IDEA. IDEA is a randomized clinical trial studying three interventions (exercise (E), dietary weight loss (D), and D+E) on overweight and obese participants with knee osteoarthritis. In this report, we expand the discussion and justification with additional statistical background. We elaborate more on the background of precision medicine, the derivation of the jackknife estimator of value function and its estimated variance, the consistency property of jackknife estimator, as well as additional simulation results that reflect more of the performance of jackknife estimators. We recommend reading Jiang et al. 2020 for clinical application and interpretation of the optimal ITR of knee osteoarthritis as well as the overall understanding of the pipeline and recommend using this article to understand the underlying statistical derivation and methodology.|http://arxiv.org/abs/2001.09930v3|Xiaotong Jiang,Amanda E. Nelson,Rebecca J. Cleveland,Daniel P. Beavers,Todd A. Schwartz,Liubov Arbeeva,Carolina Alvarez,Leigh F. Callahan,Stephen Messier,Richard Loeser,Michael R. Kosorok
1925|Distance to healthy cardiovascular dynamics from fetal heart rate scale-dependent features in pregnant sheep model of human labor predicts cardiovascular decompensation|The overarching goal of the present work is to contribute to the understanding of the relations between fetal heart rate (FHR) temporal dynamics and the well-being of the fetus, notably in terms of predicting cardiovascular decompensation (CVD). It makes uses of an established animal model of human labor, where fourteen near-term ovine fetuses subjected to umbilical cord occlusions (UCO) were instrumented to permit regular intermittent measurements of metabolites, pH, and continuous recording of electrocardiogram (ECG) and systemic arterial blood pressure (to identify CVD) during UCO. ECG-derived FHR was digitized at the sampling rate of 1000 Hz and resampled to 4Hz, as used in clinical routine. We focused on four FHR variability features which are tunable to temporal scales of FHR dynamics, robustly computable from FHR sampled at $4$Hz and within short-time sliding windows, hence permitting a time-dependent, or local, analysis of FHR which helps dealing with signal noise. Results show the sensitivity of the proposed features for early detection of CVD, correlation to metabolites and pH, useful for early acidosis detection and the importance of coarse time scales (2.5 to 8 seconds) which are not disturbed by the low FHR sampling rate. Further, we introduce the performance of an individualized self-referencing metric of the distance to healthy state, based on a combination of the four features. We demonstrate that this novel metric, applied to clinically available FHR temporal dynamics alone, accurately predicts the time occurrence of CVD which heralds a clinically significant degradation of the fetal health reserve to tolerate the trial of labor.|http://arxiv.org/abs/2102.07768v2|Stphane G. Roux,Nicolas B. Garnier,Patrice Abry,Nathan Gold,Martin G. Frasch
1926|Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology|The potential of large language models in medicine for education and decision making purposes has been demonstrated as they achieve decent scores on medical exams such as the United States Medical Licensing Exam (USMLE) and the MedQA exam. In this work, we evaluate the performance of ChatGPT-4 in the specialized field of radiation oncology using the 38th American College of Radiology (ACR) radiation oncology in-training (TXIT) exam and the 2022 Red Journal Gray Zone cases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of 63.65% and 74.57%, respectively, highlighting the advantage of the latest ChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in radiation oncology are identified to some extent. Specifically, ChatGPT-4 demonstrates better knowledge of statistics, CNS & eye, pediatrics, biology, and physics than knowledge of bone & soft tissue and gynecology, as per the ACR knowledge domain. Regarding clinical care paths, ChatGPT-4 performs better in diagnosis, prognosis, and toxicity than brachytherapy and dosimetry. It lacks proficiency in in-depth details of clinical trials. For the Gray Zone cases, ChatGPT-4 is able to suggest a personalized treatment approach to each case with high correctness and comprehensiveness. Importantly, it provides novel treatment aspects for many cases, which are not suggested by any human experts. Both evaluations demonstrate the potential of ChatGPT-4 in medical education for the general public and cancer patients, as well as the potential to aid clinical decision-making, while acknowledging its limitations in certain domains. Because of the risk of hallucination, facts provided by ChatGPT always need to be verified.|http://arxiv.org/abs/2304.11957v4|Yixing Huang,Ahmed Gomaa,Sabine Semrau,Marlen Haderlein,Sebastian Lettmaier,Thomas Weissmann,Johanna Grigo,Hassen Ben Tkhayat,Benjamin Frey,Udo S. Gaipl,Luitpold V. Distel,Andreas Maier,Rainer Fietkau,Christoph Bert,Florian Putz
1927|A multi-center prospective evaluation of THEIA to detect diabetic retinopathy (DR) and diabetic macular edema (DME) in the New Zealand screening program|Purpose: to assess the efficacy of THEIA, an artificial intelligence for screening diabetic retinopathy in a multi-center prospective study. To validate the potential application of THEIA as clinical decision making assistant in a national screening program. Methods: 902 patients were recruited from either an urban large eye hospital, or a semi-rural optometrist led screening provider, as they were attending their appointment as part of New Zealand Diabetic Screening programme. These clinics used a variety of retinal cameras and a range of operators. The de-identified images were then graded independently by three senior retinal specialists, and final results were aggregated using New Zealand grading scheme, which is then converted to referable\non-referable and Healthy\mild\more than mild\vision threatening categories. Results: compared to ground truth, THEIA achieved 100% sensitivity and [95.35%-97.44%] specificity, and negative predictive value of 100%. THEIA also did not miss any patients with more than mild or vision threatening disease. The level of agreement between the clinicians and the aggregated results was (k value: 0.9881, 0.9557, and 0.9175), and the level of agreement between THEIA and the aggregated labels was (k value: 0.9515). Conclusion: Our multi-centre prospective trial showed that THEIA does not miss referable disease when screening for diabetic retinopathy and maculopathy. It also has a very high level of granularity in reporting the disease level. Since THEIA is being tested on a variety of cameras, operating in a range of clinics (rural\urban, ophthalmologist-led\optometrist-led), we believe that it will be a suitable addition to a public diabetic screening program.|http://arxiv.org/abs/2106.12979v1|Ehsan Vaghefi,Song Yang,Li Xie,David Han,David Squirrell
1928|Decision curve analysis for personalized treatment choice between multiple options|Decision curve analysis can be used to determine whether a personalized model for treatment benefit would lead to better clinical decisions. Decision curve analysis methods have been described to estimate treatment benefit using data from a single RCT. Our main objective is to extend the decision curve analysis methodology to the scenario where several treatment options exist and evidence about their effects comes from a set of trials, synthesized using network meta-analysis (NMA). We describe the steps needed to estimate the net benefit of a prediction model using evidence from studies synthesized in an NMA. We show how to compare personalized versus one-size-fit-all treatment decision-making strategies, like "treat none" or "treat all patients with a specific treatment" strategies. The net benefit per strategy can then be plotted for a plausible range of threshold probabilities to reveal the most clinically useful strategy. We applied our methodology to an NMA prediction model for relapsing-remitting multiple sclerosis, which can be used to choose between Natalizumab, Dimethyl Fumarate, Glatiramer Acetate, and placebo. We illustrated the extended decision curve analysis methodology using several threshold values combinations for each available treatment. For the examined threshold values, the "treat patients according to the prediction model" strategy performs either better than or close to the one-size-fit-all treatment strategies. However, even small differences may be important in clinical decision-making. As the advantage of the personalized model was not consistent across all thresholds, an improved model may be needed before advocating its applicability for decision-making. This novel extension of decision curve analysis can be applied to NMA based prediction models to evaluate their use to aid treatment decision-making.|http://arxiv.org/abs/2202.02102v2|Konstantina Chalkou,Andrew J. Vickers,Fabio Pellegrini,Andrea Manca,Georgia Salanti
1929|Longitudinal abnormalities in white matter extracellular free water volume fraction and neuropsychological functioning in patients with traumatic brain injury|Traumatic brain injury is a global public health problem associated with chronic neurological complications and long-term disability. Biomarkers that map onto the underlying brain pathology driving these complications are urgently needed to identify individuals at risk for poor recovery and to inform design of clinical trials of neuroprotective therapies. Neuroinflammation and neurodegeneration are two endophenotypes associated with increases in brain extracellular water content after trauma. The objective of this study was to describe the relationship between a neuroimaging biomarker of extracellular free water content and the clinical features of patients with traumatic brain injury. We analyzed a cohort of 64 adult patients requiring hospitalization for non-penetrating traumatic brain injury of all severities as well as 32 healthy controls. Patients underwent brain MRI and clinical neuropsychological assessment in the subacute (2-weeks) and chronic (6-months) post-injury period, and controls underwent a single MRI. For each subject, we derived a summary score representing deviations in whole brain white matter (1) extracellular free water volume fraction (VF) and (2) free water-corrected fractional anisotropy (fw-FA). The summary specific anomaly score (SAS) for VF was significantly higher in TBI patients in the subacute and chronic post-injury period relative to controls. SAS for VF significantly correlated with neuropsychological functioning in the subacute, but not chronic post-injury period. These findings indicate abnormalities in whole brain white matter extracellular water fraction in patients with TBI and are an important step toward identifying and validating noninvasive biomarkers that map onto the pathology driving disability after TBI.|http://arxiv.org/abs/2206.01080v1|James J Gugger,Alexa E Walter,Drew Parker,Nishant Sinha,Justin Morrison,Jeffrey Ware,Andrea LC Schneider,Dmitriy Petrov,Danielle K Sandsmark,Ragini Verma,Ramon Diaz-Arrastia
1930|An efficient semi-supervised quality control system trained using physics-based MRI-artefact generators and adversarial training|Large medical imaging data sets are becoming increasingly available, but ensuring sample quality without significant artefacts is challenging. Existing methods for identifying imperfections in medical imaging rely on data-intensive approaches, compounded by a scarcity of artefact-rich scans for training machine learning models in clinical research. To tackle this problem, we propose a framework with four main components: 1) artefact generators inspired by magnetic resonance physics to corrupt brain MRI scans and augment a training dataset, 2) abstract and engineered features to represent images compactly, 3) a feature selection process depending on the artefact class to improve classification, and 4) SVM classifiers to identify artefacts. Our contributions are threefold: first, physics-based artefact generators produce synthetic brain MRI scans with controlled artefacts for data augmentation. This will avoid the labour-intensive collection and labelling process of scans with rare artefacts. Second, we propose a pool of abstract and engineered image features to identify 9 different artefacts for structural MRI. Finally, we use an artefact-based feature selection block that, for each class of artefacts, finds the set of features providing the best classification performance. We performed validation experiments on a large data set of scans with artificially-generated artefacts, and in a multiple sclerosis clinical trial where real artefacts were identified by experts, showing that the proposed pipeline outperforms traditional methods. In particular, our data augmentation increases performance by up to 12.5 percentage points on accuracy, precision, and recall. The computational efficiency of our pipeline enables potential real-time deployment, promising high-throughput clinical applications through automated image-processing pipelines driven by quality control systems.|http://arxiv.org/abs/2206.03359v2|Daniele Ravi,Frederik Barkhof,Daniel C. Alexander,Lemuel Puglisi,Geoffrey JM Parker,Arman Eshaghi
1931|Implementation and pratical aspects of quantitative decision-making in clinical drug development|Quantitative decision-making (QDM) principles address the issues related to the mapping of results to decisions, the synthesis of information and the quantification of uncertainty. Since the clinical drug development involves a succession of decisions to be made, QDM methods can be applied at various levels. At the study level, it can be used to properly design a study, and improve the decisions that are made either during the trial or at its end. Establishing decision criteria ahead of the study is essential here to address the need for speedy decisions, potentially in real time. At the project level, QDM can be used to inform decisions to continue, adapt or stop a drug development programme based on results from previous studies. At the portfolio level, QDM can be used to choose, prioritise and optimise the development portfolio, e.g. using the probability to reach market access or target sales within a predefined timeline. The increasing interest in QDM and its statistical nature led in 2017 to the development a cross-industry and academia Special Interest Group on QDM within the Society and the European Federation of Statisticians in the Pharmaceutical Industry (PSI and EFSPI). The activities of the group included discussing QDM examples, some published in the literature and some real anonymised ones covering several settings. While the methodologies, and to some extent the terminology, employed also varied depending on the context, discussions within the group distilled common principles to be considered when implementing QDM, particularly around the construction of QDM frameworks, assessment of operating characteristics and communication with the clinical team. The present manuscript presents those points to consider, hoping they can be helpful to statisticians interested in implementing QDM.|http://arxiv.org/abs/2207.07977v1|Juan J. Abellan,Nicolas Bonnet,Alex Carlton,Paul Frewer,Heiko Gtte,John-Philip Lawo,Jesper Madsen,Oliver Sailer,Guido Thmmes,Galle Saint-Hilary
1932|Joint regional uptake quantification of Thorium-227 and Radium-223 using a multiple-energy-window projection-domain quantitative SPECT method|Thorium-227-based alpha-particle radiopharmaceutical therapies ({\alpha}-RPTs) are being investigated in several clinical and pre-clinical studies. After administration, Thorium-227 decays to Radium-223, another alpha-particle-emitting isotope, which redistributes within the patient. Reliable dose quantification of both Thorium-227 and Radium-223 is clinically important, and SPECT may perform this quantification as these isotopes also emit X- and gamma-ray photons. However, reliable quantification is challenged by the orders-of-magnitude lower activity compared to conventional SPECT, resulting in a very low number of detected counts, the presence of multiple photopeaks, substantial overlap in the emission spectra of these isotopes, and the image-degrading effects in SPECT. To address these issues, we propose a multiple-energy-window projection-domain quantification (MEW-PDQ) method that jointly estimates the regional activity uptake of both Thorium-227 and Radium-223 directly using the SPECT projection from multiple energy windows. We evaluated the method with realistic simulation studies using anthropomorphic digital phantoms, in the context of imaging patients with bone metastases of prostate cancer and treated with Thorium-227-based {\alpha}-RPTs. The proposed method yielded reliable (accurate and precise) regional uptake estimates of both isotopes and outperformed state-of-the-art methods across different lesion sizes and contrasts, in a virtual imaging trial, as well as with moderate levels of intra-regional heterogeneous uptake and with moderate inaccuracies in the definitions of the support of various regions. Additionally, we demonstrated the effectiveness of using multiple energy windows and the variance of the estimated uptake using the proposed method approached the Cram\'er-Rao-lower-bound-defined theoretical limit.|http://arxiv.org/abs/2305.17117v4|Zekun Li,Nadia Benabdallah,Richard Laforest,Richard L. Wahl,Daniel L. J. Thorek,Abhinav K. Jha
1933|On the acceptance, commissioning, and quality assurance of electron FLASH units|Background & Purpose: FLASH or ultra-high dose rate (UHDR) radiation therapy (RT) has gained attention in recent years for its ability to spare normal tissues relative to conventional dose rate (CDR) RT in various preclinical trials. However, clinical implementation of this promising treatment option has been limited because of the lack of availability of accelerators capable of delivering UHDR RT. We established a framework for the acceptance, commissioning, and periodic quality assurance (QA) of electron FLASH units and present an example of commissioning.   Methods: A protocol for acceptance, commissioning, and QA of UHDR linear accelerators was established by combining and adapting standards and professional recommendations for standard linear accelerators based on the experience with UHDR at four clinical centers that use different UHDR devices. Non-standard dosimetric beam parameters considered included pulse width, pulse repetition frequency, dose per pulse, and instantaneous dose rate, together with recommendations on how to acquire these measurements.   Results: The 6 and 9 MeV beams of an UHDR electron device were commissioned by using this developed protocol. Measurements were acquired with a combination of ion chambers, beam current transformers (BCTs), and dose rate independent passive dosimeters. The unit was calibrated according to the concept of redundant dosimetry using a reference setup.   Conclusions: This study provides detailed recommendations for the acceptance testing, commissioning, and routine QA of low-energy electron UHDR linear accelerators. The proposed framework is not limited to any specific unit, making it applicable to all existing eFLASH units in the market. Through practical insights and theoretical discourse, this document establishes a benchmark for the commissioning of UHDR devices for clinical use.|http://arxiv.org/abs/2405.15146v1|Allison Palmiero,Kevin Liu,Julie Colnot,Nitish Chopra,Denae Neill,Luke Connell,Brett Velasquez,Albert C. Koong,Steven H. Lin,Peter Balter,Ramesh Tailor,Charlotte Robert,Jean-Franois Germond,Patrik Gonalves Jorge,Reiner Geyer,Sam Beddar,Raphael Moeckli,Emil Schler
1934|AbdomenAtlas: A Large-Scale, Detailed-Annotated, & Multi-Center Dataset for Efficient Transfer Learning and Open Algorithmic Benchmarking|We introduce the largest abdominal CT dataset (termed AbdomenAtlas) of 20,460 three-dimensional CT volumes sourced from 112 hospitals across diverse populations, geographies, and facilities. AbdomenAtlas provides 673K high-quality masks of anatomical structures in the abdominal region annotated by a team of 10 radiologists with the help of AI algorithms. We start by having expert radiologists manually annotate 22 anatomical structures in 5,246 CT volumes. Following this, a semi-automatic annotation procedure is performed for the remaining CT volumes, where radiologists revise the annotations predicted by AI, and in turn, AI improves its predictions by learning from revised annotations. Such a large-scale, detailed-annotated, and multi-center dataset is needed for two reasons. Firstly, AbdomenAtlas provides important resources for AI development at scale, branded as large pre-trained models, which can alleviate the annotation workload of expert radiologists to transfer to broader clinical applications. Secondly, AbdomenAtlas establishes a large-scale benchmark for evaluating AI algorithms -- the more data we use to test the algorithms, the better we can guarantee reliable performance in complex clinical scenarios. An ISBI & MICCAI challenge named BodyMaps: Towards 3D Atlas of Human Body was launched using a subset of our AbdomenAtlas, aiming to stimulate AI innovation and to benchmark segmentation accuracy, inference efficiency, and domain generalizability. We hope our AbdomenAtlas can set the stage for larger-scale clinical trials and offer exceptional opportunities to practitioners in the medical imaging community. Codes, models, and datasets are available at https://www.zongweiz.com/dataset|http://arxiv.org/abs/2407.16697v1|Wenxuan Li,Chongyu Qu,Xiaoxi Chen,Pedro R. A. S. Bassi,Yijia Shi,Yuxiang Lai,Qian Yu,Huimin Xue,Yixiong Chen,Xiaorui Lin,Yutong Tang,Yining Cao,Haoqi Han,Zheyuan Zhang,Jiawei Liu,Tiezheng Zhang,Yujiu Ma,Jincheng Wang,Guang Zhang,Alan Yuille,Zongwei Zhou
1935|Demystifying Large Language Models for Medicine: A Primer|Large language models (LLMs) represent a transformative class of AI tools capable of revolutionizing various aspects of healthcare by generating human-like responses across diverse contexts and adapting to novel tasks following human instructions. Their potential application spans a broad range of medical tasks, such as clinical documentation, matching patients to clinical trials, and answering medical questions. In this primer paper, we propose an actionable guideline to help healthcare professionals more efficiently utilize LLMs in their work, along with a set of best practices. This approach consists of several main phases, including formulating the task, choosing LLMs, prompt engineering, fine-tuning, and deployment. We start with the discussion of critical considerations in identifying healthcare tasks that align with the core capabilities of LLMs and selecting models based on the selected task and data, performance requirements, and model interface. We then review the strategies, such as prompt engineering and fine-tuning, to adapt standard LLMs to specialized medical tasks. Deployment considerations, including regulatory compliance, ethical guidelines, and continuous monitoring for fairness and bias, are also discussed. By providing a structured step-by-step methodology, this tutorial aims to equip healthcare professionals with the tools necessary to effectively integrate LLMs into clinical practice, ensuring that these powerful technologies are applied in a safe, reliable, and impactful manner.|http://arxiv.org/abs/2410.18856v3|Qiao Jin,Nicholas Wan,Robert Leaman,Shubo Tian,Zhizheng Wang,Yifan Yang,Zifeng Wang,Guangzhi Xiong,Po-Ting Lai,Qingqing Zhu,Benjamin Hou,Maame Sarfo-Gyamfi,Gongbo Zhang,Aidan Gilson,Balu Bhasuran,Zhe He,Aidong Zhang,Jimeng Sun,Chunhua Weng,Ronald M. Summers,Qingyu Chen,Yifan Peng,Zhiyong Lu
1936|A Paradigm Shift in Neuroscience Driven by Big Data: State of art, Challenges, and Proof of Concept|A recent editorial in Nature noted that cognitive neuroscience is at a crossroads where it is a thorny issue to reliably reveal brain-behavior associations. This commentary sketches a big data science way out for cognitive neuroscience, namely population neuroscience. In terms of design, analysis, and interpretations, population neuroscience research takes the design control to an unprecedented level, greatly expands the dimensions of the data analysis space, and paves a paradigm shift for exploring mechanisms on brain-behavior associations.|http://arxiv.org/abs/2212.04195v2|Zi-Xuan Zhou,Xi-Nian Zuo
1937|Detecting Danger: Applying a Novel Immunological Concept to Intrusion Detection Systems|In recent years computer systems have become increasingly complex and consequently the challenge of protecting these systems has become increasingly difficult. Various techniques have been implemented to counteract the misuse of computer systems in the form of firewalls, anti-virus software and intrusion detection systems. The complexity of networks and dynamic nature of computer systems leaves current methods with significant room for improvement. Computer scientists have recently drawn inspiration from mechanisms found in biological systems and, in the context of computer security, have focused on the human immune system (HIS). The human immune system provides a high level of protection from constant attacks. By examining the precise mechanisms of the human immune system, it is hoped the paradigm will improve the performance of real intrusion detection systems. This paper presents an introduction to recent developments in the field of immunology. It discusses the incorporation of a novel immunological paradigm, Danger Theory, and how this concept is inspiring artificial immune systems (AIS). Applications within the context of computer security are outlined drawing direct reference to the underlying principles of Danger Theory and finally, the current state of intrusion detection systems is discussed and improvements suggested.|http://arxiv.org/abs/1002.0696v1|Julie Greensmith,Uwe Aickelin,Jamie Twycross
1938|A simple derivation of the Gompertz law for human mortality|The Gompertz law of dependence of human mortality rate on age is derived from a simple model of death as a result of the exponentially rare escape of abnormal cells from immunological response.|http://arxiv.org/abs/q-bio/0411019v3|B. I. Shklovskii
1939|An Immune System Inspired Approach to Automated Program Verification|An immune system inspired Artificial Immune System (AIS) algorithm is presented, and is used for the purposes of automated program verification. Relevant immunological concepts are discussed and the field of AIS is briefly reviewed. It is proposed to use this AIS algorithm for a specific automated program verification task: that of predicting shape of program invariants. It is shown that the algorithm correctly predicts program invariant shape for a variety of benchmarked programs.|http://arxiv.org/abs/0905.2649v1|Soumya Banerjee
1940|A model of host response to a multi-stage pathogen|We model the immune surveillance of a pathogen which passes through $n$ immunologically distinct stages. The biological parameters of this system induce a partial order on the stages, and this, in turn, determines which stages will be subject to immune regulation. This corresponds to the system's unique asymptotically stable fixed point.|http://arxiv.org/abs/1005.0332v1|Edgar Delgado-Eckert,Michael Shapiro
1941|Missing and spurious interaction in additive, multiplicative and odds ratio models|Additive, multiplicative, and odd ratio neutral models for interactions are for long advocated and controversial in epidemiology. We show here that these commonly advocated models are biased, leading to spurious interactions, and missing true interactions.|http://arxiv.org/abs/1712.04412v1|Jorge Fernandez-de-Cossio,Jorge Fernandez-de-Cossio-Diaz,Toshifumi Takao,Yasser Perera
1942|ECTIL: Label-efficient Computational Tumour Infiltrating Lymphocyte (TIL) assessment in breast cancer: Multicentre validation in 2,340 patients with breast cancer|The level of tumour-infiltrating lymphocytes (TILs) is a prognostic factor for patients with (triple-negative) breast cancer (BC). Computational TIL assessment (CTA) has the potential to assist pathologists in this labour-intensive task, but current CTA models rely heavily on many detailed annotations. We propose and validate a fundamentally simpler deep learning based CTA that can be trained in only ten minutes on hundredfold fewer pathologist annotations. We collected whole slide images (WSIs) with TILs scores and clinical data of 2,340 patients with BC from six cohorts including three randomised clinical trials. Morphological features were extracted from whole slide images (WSIs) using a pathology foundation model. Our label-efficient Computational stromal TIL assessment model (ECTIL) directly regresses the TILs score from these features. ECTIL trained on only a few hundred samples (ECTIL-TCGA) showed concordance with the pathologist over five heterogeneous external cohorts (r=0.54-0.74, AUROC=0.80-0.94). Training on all slides of five cohorts (ECTIL-combined) improved results on a held-out test set (r=0.69, AUROC=0.85). Multivariable Cox regression analyses indicated that every 10% increase of ECTIL scores was associated with improved overall survival independent of clinicopathological variables (HR 0.86, p<0.01), similar to the pathologist score (HR 0.87, p<0.001). We demonstrate that ECTIL is highly concordant with an expert pathologist and obtains a similar hazard ratio. ECTIL has a fundamentally simpler design than existing methods and can be trained on orders of magnitude fewer annotations. Such a CTA may be used to pre-screen patients for, e.g., immunotherapy clinical trial inclusion, or as a tool to assist clinicians in the diagnostic work-up of patients with BC. Our model is available under an open source licence (https://github.com/nki-ai/ectil).|http://arxiv.org/abs/2501.14379v1|Yoni Schirris,Rosie Voorthuis,Mark Opdam,Marte Liefaard,Gabe S Sonke,Gwen Dackus,Vincent de Jong,Yuwei Wang,Annelot Van Rossum,Tessa G Steenbruggen,Lars C Steggink,Liesbeth G. E. de Vries,Marc van de Vijver,Roberto Salgado,Efstratios Gavves,Paul J van Diest,Sabine C Linn,Jonas Teuwen,Renee Menezes,Marleen Kok,Hugo Horlings
1943|Pathologist-Level Grading of Prostate Biopsies with Artificial Intelligence|Background: An increasing volume of prostate biopsies and a world-wide shortage of uro-pathologists puts a strain on pathology departments. Additionally, the high intra- and inter-observer variability in grading can result in over- and undertreatment of prostate cancer. Artificial intelligence (AI) methods may alleviate these problems by assisting pathologists to reduce workload and harmonize grading.   Methods: We digitized 6,682 needle biopsies from 976 participants in the population based STHLM3 diagnostic study to train deep neural networks for assessing prostate biopsies. The networks were evaluated by predicting the presence, extent, and Gleason grade of malignant tissue for an independent test set comprising 1,631 biopsies from 245 men. We additionally evaluated grading performance on 87 biopsies individually graded by 23 experienced urological pathologists from the International Society of Urological Pathology. We assessed discriminatory performance by receiver operating characteristics (ROC) and tumor extent predictions by correlating predicted millimeter cancer length against measurements by the reporting pathologist. We quantified the concordance between grades assigned by the AI and the expert urological pathologists using Cohen's kappa.   Results: The performance of the AI to detect and grade cancer in prostate needle biopsy samples was comparable to that of international experts in prostate pathology. The AI achieved an area under the ROC curve of 0.997 for distinguishing between benign and malignant biopsy cores, and 0.999 for distinguishing between men with or without prostate cancer. The correlation between millimeter cancer predicted by the AI and assigned by the reporting pathologist was 0.96. For assigning Gleason grades, the AI achieved an average pairwise kappa of 0.62. This was within the range of the corresponding values for the expert pathologists (0.60 to 0.73).|http://arxiv.org/abs/1907.01368v1|Peter Strm,Kimmo Kartasalo,Henrik Olsson,Leslie Solorzano,Brett Delahunt,Daniel M. Berney,David G. Bostwick,Andrew J. Evans,David J. Grignon,Peter A. Humphrey,Kenneth A. Iczkowski,James G. Kench,Glen Kristiansen,Theodorus H. van der Kwast,Katia R. M. Leite,Jesse K. McKenney,Jon Oxley,Chin-Chen Pan,Hemamali Samaratunga,John R. Srigley,Hiroyuki Takahashi,Toyonori Tsuzuki,Murali Varma,Ming Zhou,Johan Lindberg,Cecilia Bergstrm,Pekka Ruusuvuori,Carolina Whlby,Henrik Grnberg,Mattias Rantalainen,Lars Egevad,Martin Eklund
1944|Visual Modulation of Human Responses to Support Surface Translation|Vision is known to improve human postural responses to external perturbations. This study investigates the role of vision for the responses to continuous pseudorandom support surface translations in the body sagittal plane in three visual conditions: with the eyes closed (EC), in stroboscopic illumination (EO/SI; only visual position information) and with eyes open in continuous illumination (EO/CI; position and velocity information) with the room as static visual scene (or the interior of a moving cabin, in some of the trials). In the frequency spectrum of the translation stimulus we distinguished on the basis of the response patterns between a low-frequency, mid-frequency, and high-frequency range (LFR: 0.0165-0.14 Hz; MFR: 0.15-0.57 Hz; HFR: 0.58-2.46 Hz). With EC, subjects' mean sway response gain was very low in the LFR. On average it increased with EO/SI (although not to a significant degree p = 0.078) and more so with EO/CI (p < 10<sup>-6</sup>). In contrast, the average gain in the MFR decreased from EC to EO/SI (although not to a significant degree, p = 0.548) and further to EO/CI (p = 0.0002). In the HFR, all three visual conditions produced, similarly, high gain levels. A single inverted pendulum (SIP) model controlling center of mass (COM) balancing about the ankle joints formally described the EC response as being strongly shaped by a resonance phenomenon arising primarily from the control's proprioceptive feedback loop. The effect of adding visual information in these simulations lies in a reduction of the resonance, similar as in the experiments. Extending the model to a double inverted pendulum (DIP) suggested in addition a biomechanical damping effective from trunk sway in the hip joints on the resonance.|http://arxiv.org/abs/2103.03722v1|Mustafa Emre Akay,Vittorio Lippi,Thomas Mergner
1945|Reverse engineering recurrent neural networks with Jacobian switching linear dynamical systems|Recurrent neural networks (RNNs) are powerful models for processing time-series data, but it remains challenging to understand how they function. Improving this understanding is of substantial interest to both the machine learning and neuroscience communities. The framework of reverse engineering a trained RNN by linearizing around its fixed points has provided insight, but the approach has significant challenges. These include difficulty choosing which fixed point to expand around when studying RNN dynamics and error accumulation when reconstructing the nonlinear dynamics with the linearized dynamics. We present a new model that overcomes these limitations by co-training an RNN with a novel switching linear dynamical system (SLDS) formulation. A first-order Taylor series expansion of the co-trained RNN and an auxiliary function trained to pick out the RNN's fixed points govern the SLDS dynamics. The results are a trained SLDS variant that closely approximates the RNN, an auxiliary function that can produce a fixed point for each point in state-space, and a trained nonlinear RNN whose dynamics have been regularized such that its first-order terms perform the computation, if possible. This model removes the post-training fixed point optimization and allows us to unambiguously study the learned dynamics of the SLDS at any point in state-space. It also generalizes SLDS models to continuous manifolds of switching points while sharing parameters across switches. We validate the utility of the model on two synthetic tasks relevant to previous work reverse engineering RNNs. We then show that our model can be used as a drop-in in more complex architectures, such as LFADS, and apply this LFADS hybrid to analyze single-trial spiking activity from the motor system of a non-human primate.|http://arxiv.org/abs/2111.01256v1|Jimmy T. H. Smith,Scott W. Linderman,David Sussillo
1946|Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data|Characterizing the relationship between neural population activity and behavioral data is a central goal of neuroscience. While latent variable models (LVMs) are successful in describing high-dimensional time-series data, they are typically only designed for a single type of data, making it difficult to identify structure shared across different experimental data modalities. Here, we address this shortcoming by proposing an unsupervised LVM which extracts temporally evolving shared and independent latents for distinct, simultaneously recorded experimental modalities. We do this by combining Gaussian Process Factor Analysis (GPFA), an interpretable LVM for neural spiking data with temporally smooth latent space, with Gaussian Process Variational Autoencoders (GP-VAEs), which similarly use a GP prior to characterize correlations in a latent space, but admit rich expressivity due to a deep neural network mapping to observations. We achieve interpretability in our model by partitioning latent variability into components that are either shared between or independent to each modality. We parameterize the latents of our model in the Fourier domain, and show improved latent identification using this approach over standard GP-VAE methods. We validate our model on simulated multi-modal data consisting of Poisson spike counts and MNIST images that scale and rotate smoothly over time. We show that the multi-modal GP-VAE (MM-GPVAE) is able to not only identify the shared and independent latent structure across modalities accurately, but provides good reconstructions of both images and neural rates on held-out trials. Finally, we demonstrate our framework on two real world multi-modal experimental settings: Drosophila whole-brain calcium imaging alongside tracked limb positions, and Manduca sexta spike train measurements from ten wing muscles as the animal tracks a visual stimulus.|http://arxiv.org/abs/2310.03111v1|Rabia Gondur,Usama Bin Sikandar,Evan Schaffer,Mikio Christian Aoi,Stephen L Keeley
1947|Reduction of Prolonged Excessive Pressure in Seated Persons With Paraplegia Using Wireless Lingual Tactile Feedback: A Randomized Controlled Trial|Pressure ulcers (PU) are known to be a high-cost disease with a risk of severe morbidity. This work evaluates a new clinical strategy based on an innovative medical device (Tongue Display Unit-TDU) that implements perceptive supplementation in order to reduce prolonged excessive pressure, recognized as one of the main causes of PU. A randomized, controlled, parallel-group trial was carried out with 12 subjects with spinal cord injuries (SCI). Subjects were assigned to the control (without TDU, n=6) or intervention (with TDU, n=5) group. Each subject took part in two sessions, during which the subject, seated on a pressure map sensor, watched a movie for one hour. The TDU was activated during the second session of the intervention group. Intention-to-treat analysis showed that the improvement in adequate weight shifting between the two sessions was higher in the intervention group (0.84 [0.24; 0.89]) than in the control group (0.01 [-0.01; 0.09]; p=0.004) and that the ratio of prolonged excessive pressure between the two sessions was lower in the intervention group (0.74 [0.37; 1.92]) than in the control group (1.72 [1.32; 2.56]; p=0.06). The pressure map sensor was evaluated as being convenient for use in daily life, however this was not the case for the TDU. This work shows that persons with SCI could benefit from a system based on perceptive supplementation that alerts and guides the user on how to adapt their posture in order to reduce prolonged excessive pressure, one of the main causes of PU.|http://arxiv.org/abs/1811.09106v1|A. Moreau-Gaudry,O. Chenu,M. Dang,J. L. Bosson,M. Hommel,J. Demongeot,F. Cannard,B. Diot,A. Prince,C. Hughes,Nicolas Vuillerme,Yohan Payan
1948|Estimating Individualized Treatment Regimes from Crossover Designs|The field of precision medicine aims to tailor treatment based on patient-specific factors in a reproducible way. To this end, estimating an optimal individualized treatment regime (ITR) that recommends treatment decisions based on patient characteristics to maximize the mean of a pre-specified outcome is of particular interest. Several methods have been proposed for estimating an optimal ITR from clinical trial data in the parallel group setting where each subject is randomized to a single intervention. However, little work has been done in the area of estimating the optimal ITR from crossover study designs. Such designs naturally lend themselves to precision medicine, because they allow for observing the response to multiple treatments for each patient. In this paper, we introduce a method for estimating the optimal ITR using data from a 2x2 crossover study with or without carryover effects. The proposed method is similar to policy search methods such as outcome weighted learning; however, we take advantage of the crossover design by using the difference in responses under each treatment as the observed reward. We establish Fisher and global consistency, present numerical experiments, and analyze data from a feeding trial to demonstrate the improved performance of the proposed method compared to standard methods for a parallel study design.|http://arxiv.org/abs/1902.05499v1|Crystal T. Nguyen,Daniel J. Luckett,Anna R. Kahkoska,Grace E. Shearrer,Donna Spruijt-Metz,Jaimie N. Davis,Michael R. Kosorok
1949|Concentration of Benefit index: A threshold-free summary metric for quantifying the capacity of covariates to yield efficient treatment rules|When data on treatment assignment, outcomes, and covariates from a randomized trial are available, a question of interest is to what extent covariates can be used to optimize treatment decisions. Statistical hypothesis testing of covariate-by-treatment interaction is ill-suited for this purpose. The application of decision theory results in treatment rules that compare the expected benefit of treatment given the patient's covariates against a treatment threshold. However, determining treatment threshold is often context-specific, and any given threshold might seem arbitrary when the overall capacity towards predicting treatment benefit is of concern. We propose the Concentration of Benefit index (Cb), a threshold-free metric that quantifies the combined performance of covariates towards finding individuals who will benefit the most from treatment. The construct of the proposed index is comparing expected treatment outcomes with and without knowledge of covariates when one of a two randomly selected patients are to be treated. We show that the resulting index can also be expressed in terms of the integrated efficiency of individualized treatment decision over the entire range of treatment thresholds. We propose parametric and semi-parametric estimators, the latter being suitable for out-of-sample validation and correction for optimism. We used data from a clinical trial to demonstrate the calculations in a step-by-step fashion, and have provided the R code for implementation (https://github.com/msadatsafavi/txBenefit). The proposed index has intuitive and theoretically sound interpretation and can be estimated with relative ease for a wide class of regression models. Beyond the conceptual developments, various aspects of estimation and inference for such a metric need to be pursued in future research.|http://arxiv.org/abs/2001.00299v2|Mohsen Sadatsafavi,Mohammad Ali Mansournia,Paul Gustafson
1950|Eliciting judgements about dependent quantities of interest: The SHELF extension and copula methods illustrated using an asthma case study|Pharmaceutical companies regularly need to make decisions about drug development programs based on the limited knowledge from early stage clinical trials. In this situation, eliciting the judgements of experts is an attractive approach for synthesising evidence on the unknown quantities of interest. When calculating the probability of success for a drug development program, multiple quantities of interest - such as the effect of a drug on different endpoints - should not be treated as unrelated.   We discuss two approaches for establishing a multivariate distribution for several related quantities within the SHeffield ELicitation Framework (SHELF). The first approach elicits experts' judgements about a quantity of interest conditional on knowledge about another one. For the second approach, we first elicit marginal distributions for each quantity of interest. Then, for each pair of quantities, we elicit the concordance probability that both lie on the same side of their respective elicited medians. This allows us to specify a copula to obtain the joint distribution of the quantities of interest.   We show how these approaches were used in an elicitation workshop that was performed to assess the probability of success of the registrational program of an asthma drug. The judgements of the experts, which were obtained prior to completion of the pivotal studies, were well aligned with the final trial results.|http://arxiv.org/abs/2102.02852v2|Bjrn Holzhauer,Lisa V. Hampson,John Paul Gosling,Bjrn Bornkamp,Joseph Kahn,Markus R. Lange,Wen-Lin Luo,Caterina Brindicci,David Lawrence,Steffen Ballerstedt,Anthony O'Hagan
1951|Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics|We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics questions. Because popular exams like AP Physics, LSAT, and GRE have large test-taker populations and ample test preparation resources in circulation, they may not allow for accurately assessing the true potential of LLMs. This paper proposes evaluating LLMs on a highly-specialized topic, radiation oncology physics, which may be more pertinent to scientific and medical communities in addition to being a valuable benchmark of LLMs. We developed an exam consisting of 100 radiation oncology physics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT (GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs as well as medical physicists, on average. The performance of ChatGPT (GPT-4) was further improved when prompted to explain first, then answer. ChatGPT (GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices across a number of trials, whether correct or incorrect, a characteristic that was not observed in the human test groups. In evaluating ChatGPTs (GPT-4) deductive reasoning ability using a novel approach (substituting the correct answer with "None of the above choices is the correct answer."), ChatGPT (GPT-4) demonstrated surprising accuracy, suggesting the potential presence of an emergent ability. Finally, although ChatGPT (GPT-4) performed well overall, its intrinsic properties did not allow for further improvement when scoring based on a majority vote across trials. In contrast, a team of medical physicists were able to greatly outperform ChatGPT (GPT-4) using a majority vote. This study suggests a great potential for LLMs to work alongside radiation oncology experts as highly knowledgeable assistants.|http://arxiv.org/abs/2304.01938v1|Jason Holmes,Zhengliang Liu,Lian Zhang,Yuzhen Ding,Terence T. Sio,Lisa A. McGee,Jonathan B. Ashman,Xiang Li,Tianming Liu,Jiajian Shen,Wei Liu
1952|A mutual information-based in vivo monitoring of adaptive response to targeted therapies in melanoma|The mechanisms of adaptive resistance to genetic-based targeted therapies of solid malignancies have been the subject of intense research. These studies hold great promise for finding co-targetable hub/pathways which in turn would control the downstream non-genetic mechanisms of adaptive resistance. Many such mechanisms have been described in the paradigmatic BRAF-mutated melanoma model of adaptive response to BRAF inhibition. Currently, a major challenge for these mechanistic studies is to confirm in vivo, at the single-cell proteomic level, the existence of dependencies between the co-targeted hub/pathways and their downstream effectors. Moreover, the drug-induced in vivo modulation of these dependencies needs to be demonstrated. Here, we implement such single-cell-based in vivo expression dependency quantification using immunohistochemistry (IHC)-based analyses of sequential biopsies in two xenograft models. These mimic phase 2 and 3 trials in our own therapeutic strategy to prevent the adaptive response to BRAF inhibition. In this mechanistic model, the dependencies between the targeted Li2CO3-inducible hub HuR and the resistance effectors are more likely time-shifted and transient since the minority of HuRLow cells, which act as a reservoir of adaptive plasticity, switch to a HuRHigh state as they paradoxically proliferate under BRAF inhibition. Nevertheless, we show that a copula/kernel density estimator (KDE)-based quantification of mutual information (MI) efficiently captures, at the individual level, the dependencies between HuR and two relevant resistance markers pERK and EGFR, and outperforms classic expression correlation coefficients. Ultimately, the validation of MI as a predictive IHC-based metric of response to our therapeutic strategy will be carried in clinical trials.|http://arxiv.org/abs/2101.05746v2|Aurore Bugi-Marteyn,Fanny Noulet,Nicolas Liaudet,Rastine Merat
1953|Defining and Estimating Effects in Cluster Randomized Trials: A Methods Comparison|Across research disciplines, cluster randomized trials (CRTs) are commonly implemented to evaluate interventions delivered to groups of participants, such as communities and clinics. Despite advances in the design and analysis of CRTs, several challenges remain. First, there are many possible ways to specify the causal effect of interest (e.g., at the individual-level or at the cluster-level). Second, the theoretical and practical performance of common methods for CRT analysis remain poorly understood. Here, we present a general framework to formally define an array of causal effects in terms of summary measures of counterfactual outcomes. Next, we provide a comprehensive overview of CRT estimators, including the t-test, generalized estimating equations (GEE), augmented-GEE, and targeted maximum likelihood estimation (TMLE). Using finite sample simulations, we illustrate the practical performance of these estimators for different causal effects and when, as commonly occurs, there are limited numbers of clusters of different sizes. Finally, our application to data from the Preterm Birth Initiative (PTBi) study demonstrates the real-world impact of varying cluster sizes and targeting effects at the cluster-level or at the individual-level. Specifically, the relative effect of the PTBI intervention was 0.81 at the cluster-level, corresponding to a 19% reduction in outcome incidence, and was 0.66 at the individual-level, corresponding to a 34% reduction in outcome risk. Given its flexibility to estimate a variety of user-specified effects and ability to adaptively adjust for covariates for precision gains while maintaining Type-I error control, we conclude TMLE is a promising tool for CRT analysis.|http://arxiv.org/abs/2110.09633v4|Alejandra Benitez,Maya L. Petersen,Mark J. van der Laan,Nicole Santos,Elizabeth Butrick,Dilys Walker,Rakesh Ghosh,Phelgona Otieno,Peter Waiswa,Laura B. Balzer
1954|Cost-effectiveness analysis for therapy sequence in advanced cancer: A microsimulation approach with application to metastatic prostate cancer|Purpose. Patients with advanced cancer may undergo multiple lines of treatment, switching therapies as their disease progresses. Motivated by a study of metastatic prostate cancer, we develop a microsimulation framework to study therapy sequence. Methods. We propose a discrete-time state transition model to study two lines of anti-cancer therapy. Based on digitized published progression-free survival (PFS) and overall survival (OS) curves, we infer event types (progression or death), and estimate transition probabilities using cumulative incidence functions with competing risks. Our model incorporates within-patient dependence over time, such that response to first-line therapy informs subsequent event probabilities. Parameters governing the degree of within-patient dependence can be used to calibrate the model-based results to those of a target trial. We demonstrate these methods in a study of two therapy sequences for metastatic prostate cancer, where Docetaxel (DCT) and Abiraterone Acetate (AA) are both appropriate for use in either first or second line treatment. We assess costs, Quality-Adjusted Life Years (QALYs) and Incremental Cost Effectiveness Ratio (ICER) for two treatment strategies: DCT then AA vs AA then DCT. Results. Using digitized survival curves from relevant clinical trials, we identified 8.6-13.9% of PFS times that should be categorized as deaths, allowing for estimation of cumulative incidence functions. Models assuming within-patient independence overestimated OS time, corrected with our calibration approach. Correction resulted in meaningful changes in the difference in QALYs between treatment strategies (0.07 vs 0.15) and the ICER (-\$76,836/QALY vs -\$21,030/QALY). Conclusions. Microsimulation models can be successfully used to study cost-effectiveness of therapy sequences, taking care to account correctly for within-patient dependence.|http://arxiv.org/abs/2210.05086v1|Elizabeth A. Handorf,J. Robert Beck,Andres Correa,Chethan Ramamurthy,Daniel M. Geynisman
1955|Incorporation of Eye-Tracking and Gaze Feedback to Characterize and Improve Radiologist Search Patterns of Chest X-rays: A Randomized Controlled Clinical Trial|Diagnostic errors in radiology often occur due to incomplete visual assessments by radiologists, despite their knowledge of predicting disease classes. This insufficiency is possibly linked to the absence of required training in search patterns. Additionally, radiologists lack consistent feedback on their visual search patterns, relying on ad-hoc strategies and peer input to minimize errors and enhance efficiency, leading to suboptimal patterns and potential false negatives. This study aimed to use eye-tracking technology to analyze radiologist search patterns, quantify performance using established metrics, and assess the impact of an automated feedback-driven educational framework on detection accuracy. Ten residents participated in a controlled trial focused on detecting suspicious pulmonary nodules. They were divided into an intervention group (received automated feedback) and a control group. Results showed that the intervention group exhibited a 38.89% absolute improvement in detecting suspicious-for-cancer nodules, surpassing the control group's improvement (5.56%, p-value=0.006). Improvement was more rapid over the four training sessions (p-value=0.0001). However, other metrics such as speed, search pattern heterogeneity, distractions, and coverage did not show significant changes. In conclusion, implementing an automated feedback-driven educational framework improved radiologist accuracy in detecting suspicious nodules. The study underscores the potential of such systems in enhancing diagnostic performance and reducing errors. Further research and broader implementation are needed to consolidate these promising results and develop effective training strategies for radiologists, ultimately benefiting patient outcomes.|http://arxiv.org/abs/2308.06280v1|Carolina Ramirez-Tamayo,Syed Hasib Akhter Faruqui,Stanford Martinez,Angel Brisco,Nicholas Czarnek,Adel Alaeddini,Jeffrey R. Mock,Edward J. Golob,Kal L. Clark
1956|Item-Level Heterogeneous Treatment Effects of Selective Serotonin Reuptake Inhibitors (SSRIs) on Depression: Implications for Inference, Generalizability, and Identification|In analysis of randomized controlled trials (RCTs) with patient-reported outcome measures (PROMs), Item Response Theory (IRT) models that allow for heterogeneity in the treatment effect at the item level merit consideration. These models for ``item-level heterogeneous treatment effects'' (IL-HTE) can provide more accurate statistical inference, allow researchers to better generalize their results, and resolve critical identification problems in the estimation of interaction effects. In this study, we extend the IL-HTE model to polytomous data and apply the model to determine how the effect of selective serotonin reuptake inhibitors (SSRIs) on depression varies across the items on a depression rating scale. We first conduct a Monte Carlo simulation study to assess the performance of the polytomous IL-HTE model under a range of conditions. We then apply the IL-HTE model to item-level data from 28 RCTs measuring the effect of SSRIs on depression using the 17-item Hamilton Depression Rating Scale (HDRS-17) and estimate potential heterogeneity by subscale (HDRS-6). Our results show that the IL-HTE model provides more accurate statistical inference, allows for generalizability of results to out-of-sample items, and resolves identification problems in the estimation of interaction effects. Our empirical application shows that while the average effect of SSRIs on depression is beneficial (i.e., negative) and statistically significant, there is substantial IL-HTE, with estimates of the standard deviation of item-level effects nearly as large as the average effect. We show that this substantial IL-HTE is driven primarily by systematically larger effects on the HDRS-6 subscale items. The IL-HTE model has the potential to provide new insights for the inference, generalizability, and identification of treatment effects in clinical trials using patient reported outcome measures.|http://arxiv.org/abs/2402.04487v2|Joshua B. Gilbert,Fredrik Hieronymus,Elias Eriksson,Benjamin W. Domingue
1957|Self-organized clustering, prediction, and superposition of long-term cognitive decline from short-term individual cognitive test scores in Alzheimer's disease|Progressive cognitive decline spanning across decades is characteristic of Alzheimer's disease (AD). Various predictive models have been designed to realize its early onset and study the long-term trajectories of cognitive test scores across populations of interest. Research efforts have been geared towards superimposing patients' cognitive test scores with the long-term trajectory denoting gradual cognitive decline, while considering the heterogeneity of AD. Multiple trajectories representing cognitive assessment for the long-term have been developed based on various parameters, highlighting the importance of classifying several groups based on disease progression patterns. In this study, a novel method capable of self-organized prediction, classification, and the overlay of long-term cognitive trajectories based on short-term individual data was developed, based on statistical and differential equation modeling. We validated the predictive accuracy of the proposed method for the long-term trajectory of cognitive test score results on two cohorts: the Alzheimer's Disease Neuroimaging Initiative (ADNI) study and the Japanese ADNI study. We also presented two practical illustrations of the simultaneous evaluation of risk factor associated with both the onset and the longitudinal progression of AD, and an innovative randomized controlled trial design for AD that standardizes the heterogeneity of patients enrolled in a clinical trial. These resources would improve the power of statistical hypothesis testing and help evaluate the therapeutic effect. The application of predicting the trajectory of longitudinal disease progression goes beyond AD, and is especially relevant for progressive and neurodegenerative disorders.|http://arxiv.org/abs/2402.12205v1|Hiroyuki Sato,Keisuke Suzuki,Atsushi Hashizume,Ryoichi Hanazawa,Masanao Sasaki,Akihiro Hirakawa,the Japanese Alzheimer's Disease Neuroimaging Initiative,the Alzheimer's Disease Neuroimaging Initiative
1958|Data Format Standardization and DICOM Integration for Hyperpolarized 13C MRI|Hyperpolarized (HP) 13C MRI has shown promise as a valuable modality for in vivo measurements of metabolism and is currently in human trials at 15 research sites worldwide. With this growth it is important to adopt standardized data storage practices as it will allow sites to meaningfully compare data.   In this paper we (1) describe data that we believe should be stored and (2) demonstrate pipelines and methods that utilize the Digital Imaging and Communications in Medicine (DICOM) standard. This includes proposing a set of minimum set of information that is specific to HP 13C MRI studies. We then show where the majority of these can be fit into existing DICOM Attributes, primarily via the "Contrast/Bolus" module.   We also demonstrate pipelines for utilizing DICOM for HP 13C MRI. DICOM is the most common standard for clinical medical image storage and provides the flexibility to accommodate the unique aspects of HP 13C MRI, including the HP agent information but also spectroscopic and metabolite dimensions. The pipelines shown include creating DICOM objects for studies on human and animal imaging systems with various pulse sequences. We also show a python-based method to efficiently modify DICOM objects to incorporate the unique HP 13C MRI information that is not captured by existing pipelines. Moreover, we propose best practices for HP 13C MRI data storage that will support future multi-site trials, research studies and technical developments of this imaging technique.|http://arxiv.org/abs/2405.03147v1|Ernesto Diaz,Renuka Sriram,Jeremy W. Gordon,Avantika Sinha,Xiaoxi Liu,Sule Sahin,Jason Crane,Marram P Olson,Hsin-Yu Chen,Jenna Bernard,Daniel B. Vigneron,Zhen Jane Wang,Duan Xu,Peder E. Z. Larson
1959|OCTCube-M: A 3D multimodal optical coherence tomography foundation model for retinal and systemic diseases with cross-cohort and cross-device validation|We present OCTCube-M, a 3D OCT-based multi-modal foundation model for jointly analyzing OCT and en face images. OCTCube-M first developed OCTCube, a 3D foundation model pre-trained on 26,685 3D OCT volumes encompassing 1.62 million 2D OCT images. It then exploits a novel multi-modal contrastive learning framework COEP to integrate other retinal imaging modalities, such as fundus autofluorescence and infrared retinal imaging, into OCTCube, efficiently extending it into multi-modal foundation models. OCTCube achieves best performance on predicting 8 retinal diseases, demonstrating strong generalizability on cross-cohort, cross-device and cross-modality prediction. OCTCube can also predict cross-organ nodule malignancy (CT) and low cardiac ejection fraction as well as systemic diseases, such as diabetes and hypertension, revealing its wide applicability beyond retinal diseases. We further develop OCTCube-IR using COEP with 26,685 OCT and IR image pairs. OCTCube-IR can accurately retrieve between OCT and IR images, allowing joint analysis between 3D and 2D retinal imaging modalities. Finally, we trained a tri-modal foundation model OCTCube-EF from 4 million 2D OCT images and 400K en face retinal images. OCTCube-EF attains the best performance on predicting the growth rate of geographic atrophy (GA) across datasets collected from 6 multi-center global trials conducted in 23 countries. This improvement is statistically equivalent to running a clinical trial with more than double the size of the original study. Our analysis based on another retrospective case study reveals OCTCube-EF's ability to avoid false positive Phase-III results according to its accurate treatment effect estimation on the Phase-II results. In sum, OCTCube-M is a 3D multi-modal foundation model framework that integrates OCT and other retinal imaging modalities revealing substantial diagnostic and prognostic benefits.|http://arxiv.org/abs/2408.11227v2|Zixuan Liu,Hanwen Xu,Addie Woicik,Linda G. Shapiro,Marian Blazes,Yue Wu,Verena Steffen,Catherine Cukras,Cecilia S. Lee,Miao Zhang,Aaron Y. Lee,Sheng Wang
1960|Immuno-inspired robotic applications: a review|Artificial immune systems primarily mimic the adaptive nature of biological immune functions. Their ability to adapt to varying pathogens makes such systems a suitable choice for various robotic applications. Generally, AIS-based robotic applications map local instantaneous sensory information into either an antigen or a co-stimulatory signal, according to the choice of representation schema. Algorithms then use relevant immune functions to output either evolved antibodies or maturity of dendritic cells, in terms of actuation signals. It is observed that researchers, in an attempt to solve the problem in hand, do not try to replicate the biological immunity but select necessary immune functions instead, resulting in an ad-hoc manner these applications are reported. Authors, therefore, present a comprehensive review of immuno-inspired robotic applications in an attempt to categorize them according to underlying immune definitions. Implementation details are tabulated in terms of corresponding mathematical expressions and their representation schema that include binary, real or hybrid data. Limitations of reported applications are also identified in light of modern immunological interpretations. As a result of this study, authors suggest a renewed focus on innate immunity and also emphasize that immunological representations should benefit from robot embodiment and must be extended to include modern trends.|http://arxiv.org/abs/1202.4261v1|Ali Raza,Benito R. Fernandez
1961|Lymphocyte repertoire selection and intracellular self/not-self discrimination: historical overview|Immunological self/not-self discrimination is conventionally seen as an extracellular event, involving interactions been receptors on T cells pre-educated to discriminate, and peptides bound to major histocompatibility complex proteins (pMHCs). Mechanisms by which not-self peptides might first be sorted intracellularly to distinguish them from the vast excess of self-peptides have long been called for. Recent demonstrations of endogenous peptide-specific clustering of pMHCs on membrane rafts are indicative of intracellular enrichment before surface display. The clustering could follow the specific aggregation of a foreign protein that exceeded its solubility limit in the crowded intracellular environment. Predominantly entropy-driven, this homoaggregation would co-localize identical peptides, so facilitating their collective presentation. Concentrations of self-proteins are fine-tuned over evolutionary time to avoid this. Disparate observations, such as pyrexia, and female susceptibility to autoimmune disease, can be explained in terms of the need to cosegregate cognate pMHC complexes internally prior to extracellular display.|http://arxiv.org/abs/1408.3321v1|Donald R. Forsdyke
1962|A Review of Mathematical Models for Muscular Dystrophy: A Systems Biology Approach|Muscular dystrophy (MD) describes generalized progressive muscular weakness due to the wasting of muscle fibers. The progression of the disease is affected by known immunological and mechanical factors, and possibly other unknown mechanisms. These dynamics have begun to be elucidated in the last two decades. This article reviews mathematical models of MD that characterize molecular and cellular components implicated in MD progression. A biological background for these processes is also presented. Molecular effectors that contribute to MD include mitochondrial bioenergetics and genetic factors; both drive cellular metabolism, communication and signaling. These molecular events leave cells vulnerable to mechanical stress which can activate an immunological cascade that weakens cells and surrounding tissues. This review article lays the foundation for a systems biology approach to study MD progression.|http://arxiv.org/abs/1610.03521v2|Amanda N. Cameron,Matthew T. Houston,Juan B. Gutierrez
1963|A dynamical modeling to study the adaptive immune system and the influence of antibodies in the immune memory|Immunological systems have been an abundant inspiration to contemporary computer scientists. Problem solving strategies, stemming from known immune system phenomena, have been successfully applied to challenging problems of modern computing (MONROY, SAAB, GODINEZ, 2004). Simulation systems and mathematical modeling are also beginning use to answer more complex immunological questions as immune memory process and duration of vaccines, where the regulation mechanisms are not still known sufficiently (LundegaarD, Lund, Kesmir, Brunak, Nielsen, 2007).In this article we studied in machina a approach to simulate the process of antigenic mutation and its implications for the process of memory. Our results have suggested that the durability of the immune memory is affected by the process of antigenic mutation and by populations of soluble antibodies in the blood. The results also strongly suggest that the decrease of the production of antibodies favors the global maintenance of immune memory.|http://arxiv.org/abs/1705.08327v1|Alexandre de Castro,Carlos Frederico Fronza,Domingos Alves
1964|Exploring the Potential of the Innate Immune System for Computers Network Security|The human body has a very effective Immune system used to protect the body from dangerous foreign pathogens. This paper aims at studying the immunology and understanding how it works, it also shaded light on the usage of the immunology principles in the computer network security. It also suggested a new network security model which detects attacks that invades the LANs. This study based on human immune system (IS). This model help protecting the datalink layer by suggesting solution to detect the foreign frames in computer network traffic. In this model, the frame format is changed in a way that prevents the sender from sending his MAC address, and he send a unique identifier (ID) instead. Moreover, a special network switch will replace the sender ID with the corresponding MAC address and forward the packets to their right destination.|http://arxiv.org/abs/1707.07226v1|Almotasem Bellah Alajlouni
1965|Computational strategies for dissecting the high-dimensional complexity of adaptive immune repertoires|The adaptive immune system recognizes antigens via an immense array of antigen-binding antibodies and T-cell receptors, the immune repertoire. The interrogation of immune repertoires is of high relevance for understanding the adaptive immune response in disease and infection (e.g., autoimmunity, cancer, HIV). Adaptive immune receptor repertoire sequencing (AIRR-seq) has driven the quantitative and molecular-level profiling of immune repertoires thereby revealing the high-dimensional complexity of the immune receptor sequence landscape. Several methods for the computational and statistical analysis of large-scale AIRR-seq data have been developed to resolve immune repertoire complexity in order to understand the dynamics of adaptive immunity. Here, we review the current research on (i) diversity, (ii) clustering and network, (iii) phylogenetic and (iv) machine learning methods applied to dissect, quantify and compare the architecture, evolution, and specificity of immune repertoires. We summarize outstanding questions in computational immunology and propose future directions for systems immunology towards coupling AIRR-seq with the computational discovery of immunotherapeutics, vaccines, and immunodiagnostics.|http://arxiv.org/abs/1711.11070v1|Enkelejda Miho,Alexander Yermanos,Cdric R. Weber,Christoph T. Berger,Sai T. Reddy,Victor Greiff
1966|Clustering and Retrieval Method of Immunological Memory Cell in Clonal Selection Algorithm|The clonal selection principle explains the basic features of an adaptive immune response to a antigenic stimulus. It established the idea that only those cells that recognize the antigens are selected to proliferate and differentiate. This paper explains a computational implementation of the clonal selection principle that explicitly takes into account the affinity maturation of the immune response. Antibodies generated by the clonal selection algorithm are clustered in some categories according to the affinity maturation, so that immunological memory cells which respond to the specified pathogen are created. Experimental results to classify the medical database of Coronary Heart Disease databases are reported. For the dataset, our proposed method shows the 99.6\% classification capability of training data.|http://arxiv.org/abs/1804.02628v1|Takumi Ichimura,Shin Kamada
1967|Infection severity across scales in multi-strain immuno-epidemiological Dengue model structured by host antibody level|Infection by distinct Dengue virus serotypes and host immunity are intricately linked. In particular, certain levels of cross-reactive antibodies in the host may actually enhance infection severity leading to Dengue hemorrhagic fever (DHF). The coupled immunological and epidemiological dynamics of Dengue calls for a multi-scale modeling approach. In this work, we formulate a within-host model which mechanistically recapitulates characteristics of antibody dependent enhancement (ADE) in Dengue infection. The within-host scale is then linked to epidemiological spread by a vector-host partial differential equation model structured by host antibody level. The coupling allows for dynamic population-wide antibody levels to be tracked through primary and secondary infections by distinct Dengue strains, along with waning of cross-protective immunity after primary infection. Analysis of both the within-host and between-host systems are conducted. Stability results in the epidemic model are formulated via basic and invasion reproduction numbers as a function of immunological variables. Additionally, we develop numerical methods in order to simulate the multi-scale model and assess the influence of parameters on disease spread and DHF prevalence in the population.|http://arxiv.org/abs/1912.08305v1|Hayriye Gulbudak,Cameron J. Browne
1968|Computational and Systems Biology Advances to Enable Bioagent-Agnostic Signatures|Enumerated threat agent lists have long driven biodefense priorities. The global SARS-CoV-2 pandemic demonstrated the limitations of searching for known threat agents as compared to a more agnostic approach. Recent technological advances are enabling agent-agnostic biodefense, especially through the integration of multi-modal observations of host-pathogen interactions directed by a human immunological model. Although well-developed technical assays exist for many aspects of human-pathogen interaction, the analytic methods and pipelines to combine and holistically interpret the results of such assays are immature and require further investments to exploit new technologies. In this manuscript, we discuss potential immunologically based bioagent-agnostic approaches and the computational tool gaps the community should prioritize filling.|http://arxiv.org/abs/2310.13898v3|Andy Lin,Cameron Torres,Errett C. Hobbs,Jaydeep Bardhan,Stephen B. Aley,Charles T. Spencer,Karen L. Taylor,Tony Chiang
1969|Synaptic mechanisms of interference in working memory|Information from preceding trials of cognitive tasks can bias performance in the current trial, a phenomenon referred to as interference. Subjects performing visual working memory tasks exhibit interference in their trial-to-trial response correlations: the recalled target location in the current trial is biased in the direction of the target presented on the previous trial. We present modeling work that (a) develops a probabilistic inference model of this history-dependent bias, and (b) links our probabilistic model to computations of a recurrent network wherein short-term facilitation accounts for the dynamics of the observed bias. Network connectivity is reshaped dynamically during each trial, providing a mechanism for generating predictions from prior trial observations. Applying timescale separation methods, we can obtain a low-dimensional description of the trial-to-trial bias based on the history of target locations. The model has response statistics whose mean is centered at the true target location across many trials, typical of such visual working memory tasks. Furthermore, we demonstrate task protocols for which the plastic model performs better than a model with static connectivity: repetitively presented targets are better retained in working memory than targets drawn from uncorrelated sequences.|http://arxiv.org/abs/1706.05395v2|Zachary P Kilpatrick
1970|Generalizability analyses with a partially nested trial design: the Necrotizing Enterocolitis Surgery Trial|We discuss generalizability analyses under a partially nested trial design, where part of the trial is nested within a cohort of trial-eligible individuals, while the rest of the trial is not nested. This design arises, for example, when only some centers participating in a trial are able to collect data on non-randomized individuals, or when data on non-randomized individuals cannot be collected for the full duration of the trial. Our work is motivated by the Necrotizing Enterocolitis Surgery Trial (NEST) that compared initial laparotomy versus peritoneal drain for infants with necrotizing enterocolitis or spontaneous intestinal perforation. During the first phase of the study, data were collected from randomized individuals as well as consenting non-randomized individuals; during the second phase of the study, however, data were only collected from randomized individuals, resulting in a partially nested trial design. We propose methods for generalizability analyses with partially nested trial designs. We describe identification conditions and propose estimators for causal estimands in the target population of all trial-eligible individuals, both randomized and non-randomized, in the part of the data where the trial is nested, while using trial information spanning both parts. We evaluate the estimators in a simulation study.|http://arxiv.org/abs/2306.00855v1|Sarah E. Robertson,Matthew A. Rysavy,Martin L. Blakely,Jon A. Steingrimsson,Issa J. Dahabreh
1971|Generalizing and transporting causal inferences from randomized trials in the presence of trial engagement effects|Trial engagement effects are effects of trial participation on the outcome that are not mediated by treatment assignment. Most work on extending (generalizing or transporting) causal inferences from a randomized trial to a target population has, explicitly or implicitly, assumed that trial engagement effects are absent, allowing evidence about the effects of the treatments examined in trials to be applied to non-experimental settings. Here, we define novel causal estimands and present identification results for generalizability and transportability analyses in the presence of trial engagement effects. Our approach allows for trial engagement effects under assumptions of no causal interaction between trial participation and treatment assignment on the absolute or relative scales. We show that under these assumptions, even in the presence of trial engagement effects, the trial data can be combined with covariate data from the target population to identify average treatment effects in the context of usual care as implemented in the target population (i.e., outside the experimental setting). The identifying observed data functionals under these no-interaction assumptions are the same as those obtained under the stronger identifiability conditions that have been invoked in prior work. Therefore, our results suggest a new interpretation for previously proposed generalizability and transportability estimators; this interpretation may be useful in analyses under causal structures where background knowledge suggests that trial engagement effects are present but interactions between trial participation and treatment are negligible.|http://arxiv.org/abs/2407.14703v1|Lawson Ung,Tyler J. VanderWeele,Issa J. Dahabreh
1972|Reproducible radiomics through automated machine learning validated on twelve clinical applications|Radiomics uses quantitative medical imaging features to predict clinical outcomes. Currently, in a new clinical application, finding the optimal radiomics method out of the wide range of available options has to be done manually through a heuristic trial-and-error process. In this study we propose a framework for automatically optimizing the construction of radiomics workflows per application. To this end, we formulate radiomics as a modular workflow and include a large collection of common algorithms for each component. To optimize the workflow per application, we employ automated machine learning using a random search and ensembling. We evaluate our method in twelve different clinical applications, resulting in the following area under the curves: 1) liposarcoma (0.83); 2) desmoid-type fibromatosis (0.82); 3) primary liver tumors (0.80); 4) gastrointestinal stromal tumors (0.77); 5) colorectal liver metastases (0.61); 6) melanoma metastases (0.45); 7) hepatocellular carcinoma (0.75); 8) mesenteric fibrosis (0.80); 9) prostate cancer (0.72); 10) glioma (0.71); 11) Alzheimer's disease (0.87); and 12) head and neck cancer (0.84). We show that our framework has a competitive performance compared human experts, outperforms a radiomics baseline, and performs similar or superior to Bayesian optimization and more advanced ensemble approaches. Concluding, our method fully automatically optimizes the construction of radiomics workflows, thereby streamlining the search for radiomics biomarkers in new applications. To facilitate reproducibility and future research, we publicly release six datasets, the software implementation of our framework, and the code to reproduce this study.|http://arxiv.org/abs/2108.08618v2|Martijn P. A. Starmans,Sebastian R. van der Voort,Thomas Phil,Milea J. M. Timbergen,Melissa Vos,Guillaume A. Padmos,Wouter Kessels,David Hanff,Dirk J. Grunhagen,Cornelis Verhoef,Stefan Sleijfer,Martin J. van den Bent,Marion Smits,Roy S. Dwarkasing,Christopher J. Els,Federico Fiduzi,Geert J. L. H. van Leenders,Anela Blazevic,Johannes Hofland,Tessa Brabander,Renza A. H. van Gils,Gaston J. H. Franssen,Richard A. Feelders,Wouter W. de Herder,Florian E. Buisman,Francois E. J. A. Willemssen,Bas Groot Koerkamp,Lindsay Angus,Astrid A. M. van der Veldt,Ana Rajicic,Arlette E. Odink,Mitchell Deen,Jose M. Castillo T.,Jifke Veenland,Ivo Schoots,Michel Renckens,Michail Doukas,Rob A. de Man,Jan N. M. IJzermans,Razvan L. Miclea,Peter B. Vermeulen,Esther E. Bron,Maarten G. Thomeer,Jacob J. Visser,Wiro J. Niessen,Stefan Klein
1973|The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE) Challenge: Results after 1 Year Follow-up|We present the findings of "The Alzheimer's Disease Prediction Of Longitudinal Evolution" (TADPOLE) Challenge, which compared the performance of 92 algorithms from 33 international teams at predicting the future trajectory of 219 individuals at risk of Alzheimer's disease. Challenge participants were required to make a prediction, for each month of a 5-year future time period, of three key outcomes: clinical diagnosis, Alzheimer's Disease Assessment Scale Cognitive Subdomain (ADAS-Cog13), and total volume of the ventricles. The methods used by challenge participants included multivariate linear regression, machine learning methods such as support vector machines and deep neural networks, as well as disease progression models. No single submission was best at predicting all three outcomes. For clinical diagnosis and ventricle volume prediction, the best algorithms strongly outperform simple baselines in predictive ability. However, for ADAS-Cog13 no single submitted prediction method was significantly better than random guesswork. Two ensemble methods based on taking the mean and median over all predictions, obtained top scores on almost all tasks. Better than average performance at diagnosis prediction was generally associated with the additional inclusion of features from cerebrospinal fluid (CSF) samples and diffusion tensor imaging (DTI). On the other hand, better performance at ventricle volume prediction was associated with inclusion of summary statistics, such as the slope or maxima/minima of biomarkers. TADPOLE's unique results suggest that current prediction algorithms provide sufficient accuracy to exploit biomarkers related to clinical diagnosis and ventricle volume, for cohort refinement in clinical trials for Alzheimer's disease. However, results call into question the usage of cognitive test scores for patient selection and as a primary endpoint in clinical trials.|http://arxiv.org/abs/2002.03419v2|Razvan V. Marinescu,Neil P. Oxtoby,Alexandra L. Young,Esther E. Bron,Arthur W. Toga,Michael W. Weiner,Frederik Barkhof,Nick C. Fox,Arman Eshaghi,Tina Toni,Marcin Salaterski,Veronika Lunina,Manon Ansart,Stanley Durrleman,Pascal Lu,Samuel Iddi,Dan Li,Wesley K. Thompson,Michael C. Donohue,Aviv Nahon,Yarden Levy,Dan Halbersberg,Mariya Cohen,Huiling Liao,Tengfei Li,Kaixian Yu,Hongtu Zhu,Jose G. Tamez-Pena,Aya Ismail,Timothy Wood,Hector Corrada Bravo,Minh Nguyen,Nanbo Sun,Jiashi Feng,B. T. Thomas Yeo,Gang Chen,Ke Qi,Shiyang Chen,Deqiang Qiu,Ionut Buciuman,Alex Kelner,Raluca Pop,Denisa Rimocea,Mostafa M. Ghazi,Mads Nielsen,Sebastien Ourselin,Lauge Sorensen,Vikram Venkatraghavan,Keli Liu,Christina Rabe,Paul Manser,Steven M. Hill,James Howlett,Zhiyue Huang,Steven Kiddle,Sach Mukherjee,Anais Rouanet,Bernd Taschler,Brian D. M. Tom,Simon R. White,Noel Faux,Suman Sedai,Javier de Velasco Oriol,Edgar E. V. Clemente,Karol Estrada,Leon Aksman,Andre Altmann,Cynthia M. Stonnington,Yalin Wang,Jianfeng Wu,Vivek Devadas,Clementine Fourrier,Lars Lau Raket,Aristeidis Sotiras,Guray Erus,Jimit Doshi,Christos Davatzikos,Jacob Vogel,Andrew Doyle,Angela Tam,Alex Diaz-Papkovich,Emmanuel Jammeh,Igor Koval,Paul Moore,Terry J. Lyons,John Gallacher,Jussi Tohka,Robert Ciszek,Bruno Jedynak,Kruti Pandya,Murat Bilgel,William Engels,Joseph Cole,Polina Golland,Stefan Klein,Daniel C. Alexander
1974|Predictive Modelling of Toxicity Resulting from Radiotherapy Treatments of Head and Neck Cancer|In radiotherapy for head and neck cancer, the radiation dose delivered to the pharyngeal mucosa (mucosal lining of the throat) is thought to be a major contributing factor to dysphagia (swallowing dysfunction), the most commonly reported severe toxicity. There is a variation in the severity of dysphagia experienced by patients. Understanding the role of the dose distribution in dysphagia would allow improvements in the radiotherapy technique to be explored. The 3D dose distributions delivered to the pharyngeal mucosa of 249 patients treated as part of clinical trials were reconstructed. Pydicom was used to extract DICOM (digital imaging and communications in medicine) data (the standard file formats for medical imaging and radiotherapy data). NumPy and SciPy were used to manipulate the data to generate 3D maps of the dose distribution delivered to the pharyngeal mucosa and calculate metrics describing the dose distribution. Multivariate predictive modelling of severe dysphagia, including descriptions of the dose distribution and relevant clinical factors, was performed using Pandas and SciKit-Learn. Matplotlib and Mayavi were used for 2D and 3D data visualisation. A support vector classification model, with feature selection using randomised logistic regression, to predict radiation-induced severe dysphagia, was trained. When this model was independently validated, the area under the receiver operating characteristic curve was 0.54. The model has poor predictive power and work is ongoing to improve the model through alternative feature engineering and statistical modelling approaches.|http://arxiv.org/abs/1412.6399v1|Jamie A. Dean,Liam C. Welsh,Kevin J. Harrington,Christopher M. Nutting,Sarah L. Gulliford
1975|Clinically applicable Monte Carlo-based biological dose optimization for the treatment of head and neck cancers with spot-scanning proton therapy|Purpose: To demonstrate the feasibility of fast Monte Carlo (MC) based inverse biological planning for the treatment of head and neck tumors in spot-scanning proton therapy. Methods: Recently, a fast and accurate Graphics Processor Unit (GPU)-based MC simulation of proton transport was developed and used as the dose calculation engine in a GPU-accelerated IMPT optimizer. Besides dose, the dose-averaged linear energy transfer (LETd) can be simultaneously scored, which makes biological dose (BD) optimization possible. To convert from LETd to BD, a linear relation was assumed. Using this novel optimizer, inverse biological planning was applied to 4 patients: 2 small and 1 large thyroid tumor targets, and 1 glioma case. To create these plans, constraints were placed to maintain the physical dose (PD) within 1.25 times the prescription while maximizing target BD. For comparison, conventional IMRT and IMPT plans were created for each case in Eclipse (Varian, Inc). The same critical structure PD constraints were used for the IMRT, IMPT and bio-optimized plans. The BD for the IMPT plans were obtained through MC re-calculations. Results: Compared to standard IMPT, the bio-optimal plans for patients with small tumor targets displayed a BD escalation that was around twice the PD increase. Dose sparing to critical structures was improved compared to both IMRT and IMPT. No significant BD increase could be achieved for the large thyroid case, and when the presence of critical structures mitigated the contribution of additional fields. The calculation of the bio-optimized plans can be completed in a clinically viable time (<30 minutes) on a 24-GPU system. Conclusion: By exploiting GPU acceleration, MC-based, biologically optimized plans were created for small-target tumor patients. This optimizer will be used in an upcoming feasibility trial on LETd painting for radio-resistant tumors.|http://arxiv.org/abs/1603.03115v3|H. Wan Chan Tseung,J. Ma,C. R. Kreofsky,D. Ma,C. Beltran
1976|A dynamic stress model explains the delayed drug effect in artemisinin treatment of Plasmodium falciparum|Artemisinin resistance constitutes a major threat to the continued success of control programs for malaria. With alternative antimalarial drugs not yet available, improving our understanding of how artemisinin-based drugs act and how resistance manifests is essential to enable optimisation of dosing regimens in order to prolong the lifespan of current first-line treatment options. Here, through introduction of a novel model of the dynamics of the parasites' response to drug, we explore how artemisinin-based therapies may be adjusted to maintain efficacy and how artemisinin resistance may manifest and be overcome. We introduce a dynamic mathematical model, extending on the traditional pharmacokinetic-pharmacodynamic framework, to capture the time-dependent development of a stress response in parasites. We fit the model to in vitro data and establish that the parasites' stress response explains the recently identified complex interplay between drug concentration, exposure time and parasite viability. Our model demonstrates that the previously reported hypersensitivity of early ring stage parasites of the 3D7 strain to dihydroartemisinin (DHA) is primarily due to the rapid development of stress, rather than any change in the maximum achievable killing rate. Of direct clinical relevance, we demonstrate that the complex temporal features of artemisinin action observed in vitro have a significant impact on predictions of in vivo parasite clearance using PK-PD models. Given the important role that such models play in the design and evaluation of clinical trials for alternative drug dosing regimens, our model contributes an enhanced predictive platform for the continued efforts to minimise the burden of malaria.|http://arxiv.org/abs/1612.00396v1|Pengxing Cao,Nectarios Klonis,Sophie Zaloumis,Con Dogovski,Stanley C. Xie,Sompob Saralamba,Lisa J. White,Freya J. I. Fowkes,Leann Tilley,Julie A. Simpson,James M. McCaw
1977|Modelling Segmented Cardiotocography Time-Series Signals Using One-Dimensional Convolutional Neural Networks for the Early Detection of Abnormal Birth Outcomes|Gynaecologists and obstetricians visually interpret cardiotocography (CTG) traces using the International Federation of Gynaecology and Obstetrics (FIGO) guidelines to assess the wellbeing of the foetus during antenatal care. This approach has raised concerns among professionals with regards to inter- and intra-variability where clinical diagnosis only has a 30\% positive predictive value when classifying pathological outcomes. Machine learning models, trained with FIGO and other user derived features extracted from CTG traces, have been shown to increase positive predictive capacity and minimise variability. This is only possible however when class distributions are equal which is rarely the case in clinical trials where case-control observations are heavily skewed in favour of normal outcomes. Classes can be balanced using either synthetic data derived from resampled case training data or by decreasing the number of control instances. However, this either introduces bias or removes valuable information. Concerns have also been raised regarding machine learning studies and their reliance on manually handcrafted features. While this has led to some interesting results, deriving an optimal set of features is considered to be an art as well as a science and is often an empirical and time consuming process. In this paper, we address both of these issues and propose a novel CTG analysis methodology that a) splits CTG time-series signals into n-size windows with equal class distributions, and b) automatically extracts features from time-series windows using a one dimensional convolutional neural network (1DCNN) and multilayer perceptron (MLP) ensemble. Collectively, the proposed approach normally distributes classes and removes the need to handcrafted features from CTG traces.|http://arxiv.org/abs/1908.02338v2|Paul Fergus,Carl Chalmers,Casimiro Curbelo Montanez,Denis Reilly,Paulo Lisboa,Beth Pineles
1978|Network Medicine Framework for Identifying Drug Repurposing Opportunities for COVID-19|The current pandemic has highlighted the need for methodologies that can quickly and reliably prioritize clinically approved compounds for their potential effectiveness for SARS-CoV-2 infections. In the past decade, network medicine has developed and validated multiple predictive algorithms for drug repurposing, exploiting the sub-cellular network-based relationship between a drug's targets and disease genes. Here, we deployed algorithms relying on artificial intelligence, network diffusion, and network proximity, tasking each of them to rank 6,340 drugs for their expected efficacy against SARS-CoV-2. To test the predictions, we used as ground truth 918 drugs that had been experimentally screened in VeroE6 cells, and the list of drugs under clinical trial, that capture the medical community's assessment of drugs with potential COVID-19 efficacy. We find that while most algorithms offer predictive power for these ground truth data, no single method offers consistently reliable outcomes across all datasets and metrics. This prompted us to develop a multimodal approach that fuses the predictions of all algorithms, showing that a consensus among the different predictive methods consistently exceeds the performance of the best individual pipelines. We find that 76 of the 77 drugs that successfully reduced viral infection do not bind the proteins targeted by SARS-CoV-2, indicating that these drugs rely on network-based actions that cannot be identified using docking-based strategies. These advances offer a methodological pathway to identify repurposable drugs for future pathogens and neglected diseases underserved by the costs and extended timeline of de novo drug development.|http://arxiv.org/abs/2004.07229v2|Deisy Morselli Gysi,talo Do Valle,Marinka Zitnik,Asher Ameli,Xiao Gan,Onur Varol,Susan Dina Ghiassian,JJ Patten,Robert Davey,Joseph Loscalzo,Albert-Lszl Barabsi
1979|Confidence intervals of prediction accuracy measures for multivariable prediction models based on the bootstrap-based optimism correction methods|In assessing prediction accuracy of multivariable prediction models, optimism corrections are essential for preventing biased results. However, in most published papers of clinical prediction models, the point estimates of the prediction accuracy measures are corrected by adequate bootstrap-based correction methods, but their confidence intervals are not corrected, e.g., the DeLong's confidence interval is usually used for assessing the C-statistic. These naive methods do not adjust for the optimism bias and do not account for statistical variability in the estimation of parameters in the prediction models. Therefore, their coverage probabilities of the true value of the prediction accuracy measure can be seriously below the nominal level (e.g., 95%). In this article, we provide two generic bootstrap methods, namely (1) location-shifted bootstrap confidence intervals and (2) two-stage bootstrap confidence intervals, that can be generally applied to the bootstrap-based optimism correction methods, i.e., the Harrell's bias correction, 0.632, and 0.632+ methods. In addition, they can be widely applied to various methods for prediction model development involving modern shrinkage methods such as the ridge and lasso regressions. Through numerical evaluations by simulations, the proposed confidence intervals showed favourable coverage performances. Besides, the current standard practices based on the optimism-uncorrected methods showed serious undercoverage properties. To avoid erroneous results, the optimism-uncorrected confidence intervals should not be used in practice, and the adjusted methods are recommended instead. We also developed the R package predboot for implementing these methods (https://github.com/nomahi/predboot). The effectiveness of the proposed methods are illustrated via applications to the GUSTO-I clinical trial.|http://arxiv.org/abs/2005.01457v5|Hisashi Noma,Tomohiro Shinozaki,Katsuhiro Iba,Satoshi Teramukai,Toshi A. Furukawa
1980|Computing the Hazard Ratios Associated with Explanatory Variables Using Machine Learning Models of Survival Data|Purpose: The application of Cox Proportional Hazards (CoxPH) models to survival data and the derivation of Hazard Ratio (HR) is well established. While nonlinear, tree-based Machine Learning (ML) models have been developed and applied to the survival analysis, no methodology exists for computing HRs associated with explanatory variables from such models. We describe a novel way to compute HRs from tree-based ML models using the Shapley additive explanation (SHAP) values, which is a locally accurate and consistent methodology to quantify explanatory variables' contribution to predictions.   Methods: We used three sets of publicly available survival data consisting of patients with colon, breast or pan cancer and compared the performance of CoxPH to the state-of-art ML model, XGBoost. To compute the HR for explanatory variables from the XGBoost model, the SHAP values were exponentiated and the ratio of the means over the two subgroups calculated. The confidence interval was computed via bootstrapping the training data and generating the ML model 1000 times. Across the three data sets, we systematically compared HRs for all explanatory variables. Open-source libraries in Python and R were used in the analyses.   Results: For the colon and breast cancer data sets, the performance of CoxPH and XGBoost were comparable and we showed good consistency in the computed HRs. In the pan-cancer dataset, we showed agreement in most variables but also an opposite finding in two of the explanatory variables between the CoxPH and XGBoost result. Subsequent Kaplan-Meier plots supported the finding of the XGBoost model.   Conclusion: Enabling the derivation of HR from ML models can help to improve the identification of risk factors from complex survival datasets and enhance the prediction of clinical trial outcomes.|http://arxiv.org/abs/2102.00637v1|Sameer Sundrani,James Lu
1981|Artificial Intelligence to Assist in Exclusion of Coronary Atherosclerosis during CCTA Evaluation of Chest-Pain in the Emergency Department: Preparing an Application for Real-World Use|Coronary Computed Tomography Angiography (CCTA) evaluation of chest-pain patients in an Emergency Department (ED) is considered appropriate. While a negative CCTA interpretation supports direct patient discharge from an ED, labor-intensive analyses are required, with accuracy in jeopardy from distractions. We describe the development of an Artificial Intelligence (AI) algorithm and workflow for assisting interpreting physicians in CCTA screening for the absence of coronary atherosclerosis. The two-phase approach consisted of (1) Phase 1 - focused on the development and preliminary testing of an algorithm for vessel-centerline extraction classification in a balanced study population (n = 500 with 50% disease prevalence) derived by retrospective random case selection; and (2) Phase 2 - concerned with simulated-clinical Trialing of the developed algorithm on a per-case basis in a more real-world study population (n = 100 with 28% disease prevalence) from an ED chest-pain series. This allowed pre-deployment evaluation of the AI-based CCTA screening application which provides a vessel-by-vessel graphic display of algorithm inference results integrated into a clinically capable viewer. Algorithm performance evaluation used Area Under the Receiver-Operating-Characteristic Curve (AUC-ROC); confusion matrices reflected ground-truth vs AI determinations. The vessel-based algorithm demonstrated strong performance with AUC-ROC = 0.96. In both Phase 1 and Phase 2, independent of disease prevalence differences, negative predictive values at the case level were very high at 95%. The rate of completion of the algorithm workflow process (96% with inference results in 55-80 seconds) in Phase 2 depended on adequate image quality. There is potential for this AI application to assist in CCTA interpretation to help extricate atherosclerosis from chest-pain presentations.|http://arxiv.org/abs/2008.04802v1|Richard D. White,Barbaros S. Erdal,Mutlu Demirer,Vikash Gupta,Matthew T. Bigelow,Engin Dikici,Sema Candemir,Mauricio S. Galizia,Jessica L. Carpenter,Thomas P. O Donnell,Abdul H. Halabi,Luciano M. Prevedello
1982|Principal Stratum Strategy: Potential Role in Drug Development|A randomized trial allows estimation of the causal effect of an intervention compared to a control in the overall population and in subpopulations defined by baseline characteristics. Often, however, clinical questions also arise regarding the treatment effect in subpopulations of patients, which would experience clinical or disease related events post-randomization. Events that occur after treatment initiation and potentially affect the interpretation or the existence of the measurements are called {\it intercurrent events} in the ICH E9(R1) guideline. If the intercurrent event is a consequence of treatment, randomization alone is no longer sufficient to meaningfully estimate the treatment effect. Analyses comparing the subgroups of patients without the intercurrent events for intervention and control will not estimate a causal effect. This is well known, but post-hoc analyses of this kind are commonly performed in drug development. An alternative approach is the principal stratum strategy, which classifies subjects according to their potential occurrence of an intercurrent event on both study arms. We illustrate with examples that questions formulated through principal strata occur naturally in drug development and argue that approaching these questions with the ICH E9(R1) estimand framework has the potential to lead to more transparent assumptions as well as more adequate analyses and conclusions. In addition, we provide an overview of assumptions required for estimation of effects in principal strata. Most of these assumptions are unverifiable and should hence be based on solid scientific understanding. Sensitivity analyses are needed to assess robustness of conclusions.|http://arxiv.org/abs/2008.05406v2|Bjrn Bornkamp,Kaspar Rufibach,Jianchang Lin,Yi Liu,Devan V. Mehrotra,Satrajit Roychoudhury,Heinz Schmidli,Yue Shentu,Marcel Wolbers
1983|Drug Repurposing for COVID-19 via Knowledge Graph Completion|Objective: To discover candidate drugs to repurpose for COVID-19 using literature-derived knowledge and knowledge graph completion methods. Methods: We propose a novel, integrative, and neural network-based literature-based discovery (LBD) approach to identify drug candidates from both PubMed and COVID-19-focused research literature. Our approach relies on semantic triples extracted using SemRep (via SemMedDB). We identified an informative subset of semantic triples using filtering rules and an accuracy classifier developed on a BERT variant, and used this subset to construct a knowledge graph. Five SOTA, neural knowledge graph completion algorithms were used to predict drug repurposing candidates. The models were trained and assessed using a time slicing approach and the predicted drugs were compared with a list of drugs reported in the literature and evaluated in clinical trials. These models were complemented by a discovery pattern-based approach. Results: Accuracy classifier based on PubMedBERT achieved the best performance (F1= 0.854) in classifying semantic predications. Among five knowledge graph completion models, TransE outperformed others (MR = 0.923, Hits@1=0.417). Some known drugs linked to COVID-19 in the literature were identified, as well as some candidate drugs that have not yet been studied. Discovery patterns enabled generation of plausible hypotheses regarding the relationships between the candidate drugs and COVID-19. Among them, five highly ranked and novel drugs (paclitaxel, SB 203580, alpha 2-antiplasmin, pyrrolidine dithiocarbamate, and butylated hydroxytoluene) with their mechanistic explanations were further discussed. Conclusion: We show that an LBD approach can be feasible for discovering drug candidates for COVID-19, and for generating mechanistic explanations. Our approach can be generalized to other diseases as well as to other clinical questions.|http://arxiv.org/abs/2010.09600v2|Rui Zhang,Dimitar Hristovski,Dalton Schutte,Andrej Kastrin,Marcelo Fiszman,Halil Kilicoglu
1984|Machine Learning for Real-World Evidence Analysis of COVID-19 Pharmacotherapy|Introduction: Real-world data generated from clinical practice can be used to analyze the real-world evidence (RWE) of COVID-19 pharmacotherapy and validate the results of randomized clinical trials (RCTs). Machine learning (ML) methods are being used in RWE and are promising tools for precision-medicine. In this study, ML methods are applied to study the efficacy of therapies on COVID-19 hospital admissions in the Valencian Region in Spain. Methods: 5244 and 1312 COVID-19 hospital admissions - dated between January 2020 and January 2021 from 10 health departments, were used respectively for training and validation of separate treatment-effect models (TE-ML) for remdesivir, corticosteroids, tocilizumab, lopinavir-ritonavir, azithromycin and chloroquine/hydroxychloroquine. 2390 admissions from 2 additional health departments were reserved as an independent test to analyze retrospectively the survival benefits of therapies in the population selected by the TE-ML models using cox-proportional hazard models. TE-ML models were adjusted using treatment propensity scores to control for pre-treatment confounding variables associated to outcome and further evaluated for futility. ML architecture was based on boosted decision-trees. Results: In the populations identified by the TE-ML models, only Remdesivir and Tocilizumab were significantly associated with an increase in survival time, with hazard ratios of 0.41 (P = 0.04) and 0.21 (P = 0.001), respectively. No survival benefits from chloroquine derivatives, lopinavir-ritonavir and azithromycin were demonstrated. Tools to explain the predictions of TE-ML models are explored at patient-level as potential tools for personalized decision making and precision medicine. Conclusion: ML methods are suitable tools toward RWE analysis of COVID-19 pharmacotherapies. Results obtained reproduce published results on RWE and validate the results from RCTs.|http://arxiv.org/abs/2107.10239v1|Aurelia Bustos,Patricio Mas_Serrano,Mari L. Boquera,Jose M. Salinas
1985|Fast and Scalable Image Search For Histology|The expanding adoption of digital pathology has enabled the curation of large repositories of histology whole slide images (WSIs), which contain a wealth of information. Similar pathology image search offers the opportunity to comb through large historical repositories of gigapixel WSIs to identify cases with similar morphological features and can be particularly useful for diagnosing rare diseases, identifying similar cases for predicting prognosis, treatment outcomes, and potential clinical trial success. A critical challenge in developing a WSI search and retrieval system is scalability, which is uniquely challenging given the need to search a growing number of slides that each can consist of billions of pixels and are several gigabytes in size. Such systems are typically slow and retrieval speed often scales with the size of the repository they search through, making their clinical adoption tedious and are not feasible for repositories that are constantly growing. Here we present Fast Image Search for Histopathology (FISH), a histology image search pipeline that is infinitely scalable and achieves constant search speed that is independent of the image database size while being interpretable and without requiring detailed annotations. FISH uses self-supervised deep learning to encode meaningful representations from WSIs and a Van Emde Boas tree for fast search, followed by an uncertainty-based ranking algorithm to retrieve similar WSIs. We evaluated FISH on multiple tasks and datasets with over 22,000 patient cases spanning 56 disease subtypes. We additionally demonstrate that FISH can be used to assist with the diagnosis of rare cancer types where sufficient cases may not be available to train traditional supervised deep models. FISH is available as an easy-to-use, open-source software package (https://github.com/mahmoodlab/FISH).|http://arxiv.org/abs/2107.13587v1|Chengkuan Chen,Ming Y. Lu,Drew F. K. Williamson,Tiffany Y. Chen,Andrew J. Schaumberg,Faisal Mahmood
1986|Natural language processing to identify lupus nephritis phenotype in electronic health records|Systemic lupus erythematosus (SLE) is a rare autoimmune disorder characterized by an unpredictable course of flares and remission with diverse manifestations. Lupus nephritis, one of the major disease manifestations of SLE for organ damage and mortality, is a key component of lupus classification criteria. Accurately identifying lupus nephritis in electronic health records (EHRs) would therefore benefit large cohort observational studies and clinical trials where characterization of the patient population is critical for recruitment, study design, and analysis. Lupus nephritis can be recognized through procedure codes and structured data, such as laboratory tests. However, other critical information documenting lupus nephritis, such as histologic reports from kidney biopsies and prior medical history narratives, require sophisticated text processing to mine information from pathology reports and clinical notes. In this study, we developed algorithms to identify lupus nephritis with and without natural language processing (NLP) using EHR data. We developed four algorithms: a rule-based algorithm using only structured data (baseline algorithm) and three algorithms using different NLP models. The three NLP models are based on regularized logistic regression and use different sets of features including positive mention of concept unique identifiers (CUIs), number of appearances of CUIs, and a mixture of three components respectively. The baseline algorithm and the best performed NLP algorithm were external validated on a dataset from Vanderbilt University Medical Center (VUMC). Our best performing NLP model incorporating features from both structured data, regular expression concepts, and mapped CUIs improved F measure in both the NMEDW (0.41 vs 0.79) and VUMC (0.62 vs 0.96) datasets compared to the baseline lupus nephritis algorithm.|http://arxiv.org/abs/2112.10821v1|Yu Deng,Jennifer A. Pacheco,Anh Chung,Chengsheng Mao,Joshua C. Smith,Juan Zhao,Wei-Qi Wei,April Barnado,Chunhua Weng,Cong Liu,Adam Cordon,Jingzhi Yu,Yacob Tedla,Abel Kho,Rosalind Ramsey-Goldman,Theresa Walunas,Yuan Luo
1987|Repetition and reproduction of preclinical medical studies: taking a leaf from the plant sciences with consideration of generalised systematic errors|Reproduction of pre-clinical results has a high failure rate. The fundamental methodology including replication ("protocol") for hypothesis testing/validation to a state allowing inference, varies within medical and plant sciences with little justification. Here, five protocols are distinguished which deal differently with systematic/random errors and vary considerably in result veracity. Aim: to compare prevalence of protocols (defined in text). Medical/plant science articles from 2017/2019 were surveyed: 713 random articles assessed for eligibility for counts: first (with p-values): 1) non-replicated; 2) global; 3) triple-result protocols; second: 4) replication-error protocol; 5) meta-analyses. Inclusion criteria: human/plant/fungal studies with categorical groups. Exclusion criteria: phased clinical trials, pilot studies, cases, reviews, technology, rare subjects, -omic studies. Abbreviated PICOS question: which protocol was evident for a main result with categorically distinct group difference(s) ? Electronic sources: Journal Citation Reports 2017/2019, Google. Triplication prevalence differed dramatically between sciences (both years p<10-16; cluster-adjusted chi-squared tests): From 320 studies (80/science/year): in 2017, 53 (66%, 95% confidence interval (C.I.) 56%:77%) and in 2019, 48 (60%, C.I. 49%:71%) plant studies had triple-result or triplicated global protocols, compared with, in both years, 4 (5%, C.I. 0.19%:9.8%) medical studies. Plant sciences had a higher prevalence of protocols more likely to counter generalised systematic errors (the most likely cause of false positives) and random error than non-replicated protocols, without suffering from serious flaws found with random-Institutes protocols. It is suggested that a triple-result (organised-reproduction) protocol, with Institute consortia, is likely to solve most problems connected with the replicability crisis.|http://arxiv.org/abs/2201.10960v1|Jeremy S. C. Clark,Anna Salacka,Agnieszka Boron,Thierry van de Wetering,Konrad Podsiadlo,Kamila Rydzewska,Krzysztof Safranow,Kazimierz Ciechanowski,Leszek Domanski,Andrzej Ciechanowicz
1988|Triangulating Instrumental Variable, confounder adjustment and Difference-in-Difference methods for comparative effectiveness research in observational data|Observational studies can play a useful role in assessing the comparative effectiveness of competing treatments. In a clinical trial the randomization of participants to treatment and control groups generally results in well-balanced groups with respect to possible confounders, which makes the analysis straightforward. However, when analysing observational data, the potential for unmeasured confounding makes comparing treatment effects much more challenging. Causal inference methods such as Instrumental Variable and Prior Even Rate Ratio approaches make it possible to circumvent the need to adjust for confounding factors that have not been measured in the data or measured with error. Direct confounder adjustment via multivariable regression and Propensity score matching also have considerable utility. Each method relies on a different set of assumptions and leverages different aspects of the data. In this paper, we describe the assumptions of each method and assess the impact of violating these assumptions in a simulation study. We propose the prior outcome augmented Instrumental Variable method that leverages data from before and after treatment initiation, and is robust to the violation of key assumptions. Finally, we propose the use of a heterogeneity statistic to decide if two or more estimates are statistically similar, taking into account their correlation. We illustrate our causal framework to assess the risk of genital infection in patients prescribed Sodium-glucose co-transporter-2 inhibitors versus Dipeptidyl peptidase-4 inhibitors as second-line treatment for Type 2 Diabets using observational data from the Clinical Practice Research Datalink.|http://arxiv.org/abs/2202.09164v2|Laura Gdemann,John M. Dennis,Andrew P. McGovern,Lauren R. Rodgers,Beverley M. Shields,William Henley,Jack Bowden
1989|Enhancing Cancer Prediction in Challenging Screen-Detected Incident Lung Nodules Using Time-Series Deep Learning|Lung cancer is the leading cause of cancer-related mortality worldwide. Lung cancer screening (LCS) using annual low-dose computed tomography (CT) scanning has been proven to significantly reduce lung cancer mortality by detecting cancerous lung nodules at an earlier stage. Improving risk stratification of malignancy risk in lung nodules can be enhanced using machine/deep learning algorithms. However most existing algorithms: a) have primarily assessed single time-point CT data alone thereby failing to utilize the inherent advantages contained within longitudinal imaging datasets; b) have not integrated into computer models pertinent clinical data that might inform risk prediction; c) have not assessed algorithm performance on the spectrum of nodules that are most challenging for radiologists to interpret and where assistance from analytic tools would be most beneficial.   Here we show the performance of our time-series deep learning model (DeepCAD-NLM-L) which integrates multi-model information across three longitudinal data domains: nodule-specific, lung-specific, and clinical demographic data. We compared our time-series deep learning model to a) radiologist performance on CTs from the National Lung Screening Trial enriched with the most challenging nodules for diagnosis; b) a nodule management algorithm from a North London LCS study (SUMMIT). Our model demonstrated comparable and complementary performance to radiologists when interpreting challenging lung nodules and showed improved performance (AUC=88\%) against models utilizing single time-point data only. The results emphasise the importance of time-series, multi-modal analysis when interpreting malignancy risk in LCS.|http://arxiv.org/abs/2203.16606v1|Shahab Aslani,Pavan Alluri,Eyjolfur Gudmundsson,Edward Chandy,John McCabe,Anand Devaraj,Carolyn Horst,Sam M Janes,Rahul Chakkara,Arjun Nair,Daniel C Alexander,SUMMIT consortium,Joseph Jacob
1990|A Scalable Workflow to Build Machine Learning Classifiers with Clinician-in-the-Loop to Identify Patients in Specific Diseases|Clinicians may rely on medical coding systems such as International Classification of Diseases (ICD) to identify patients with diseases from Electronic Health Records (EHRs). However, due to the lack of detail and specificity as well as a probability of miscoding, recent studies suggest the ICD codes often cannot characterise patients accurately for specific diseases in real clinical practice, and as a result, using them to find patients for studies or trials can result in high failure rates and missing out on uncoded patients. Manual inspection of all patients at scale is not feasible as it is highly costly and slow.   This paper proposes a scalable workflow which leverages both structured data and unstructured textual notes from EHRs with techniques including NLP, AutoML and Clinician-in-the-Loop mechanism to build machine learning classifiers to identify patients at scale with given diseases, especially those who might currently be miscoded or missed by ICD codes.   Case studies in the MIMIC-III dataset were conducted where the proposed workflow demonstrates a higher classification performance in terms of F1 scores compared to simply using ICD codes on gold testing subset to identify patients with Ovarian Cancer (0.901 vs 0.814), Lung Cancer (0.859 vs 0.828), Cancer Cachexia (0.862 vs 0.650), and Lupus Nephritis (0.959 vs 0.855). Also, the proposed workflow that leverages unstructured notes consistently outperforms the baseline that uses structured data only with an increase of F1 (Ovarian Cancer 0.901 vs 0.719, Lung Cancer 0.859 vs 0.787, Cancer Cachexia 0.862 vs 0.838 and Lupus Nephritis 0.959 vs 0.785). Experiments on the large testing set also demonstrate the proposed workflow can find more patients who are miscoded or missed by ICD codes. Moreover, interpretability studies are also conducted to clinically validate the top impact features of the classifiers.|http://arxiv.org/abs/2205.08891v1|Jingqing Zhang,Atri Sharma,Luis Bolanos,Tong Li,Ashwani Tanwar,Vibhor Gupta,Yike Guo
1991|Siamese Encoder-based Spatial-Temporal Mixer for Growth Trend Prediction of Lung Nodules on CT Scans|In the management of lung nodules, we are desirable to predict nodule evolution in terms of its diameter variation on Computed Tomography (CT) scans and then provide a follow-up recommendation according to the predicted result of the growing trend of the nodule. In order to improve the performance of growth trend prediction for lung nodules, it is vital to compare the changes of the same nodule in consecutive CT scans. Motivated by this, we screened out 4,666 subjects with more than two consecutive CT scans from the National Lung Screening Trial (NLST) dataset to organize a temporal dataset called NLSTt. In specific, we first detect and pair regions of interest (ROIs) covering the same nodule based on registered CT scans. After that, we predict the texture category and diameter size of the nodules through models. Last, we annotate the evolution class of each nodule according to its changes in diameter. Based on the built NLSTt dataset, we propose a siamese encoder to simultaneously exploit the discriminative features of 3D ROIs detected from consecutive CT scans. Then we novelly design a spatial-temporal mixer (STM) to leverage the interval changes of the same nodule in sequential 3D ROIs and capture spatial dependencies of nodule regions and the current 3D ROI. According to the clinical diagnosis routine, we employ hierarchical loss to pay more attention to growing nodules. The extensive experiments on our organized dataset demonstrate the advantage of our proposed method. We also conduct experiments on an in-house dataset to evaluate the clinical utility of our method by comparing it against skilled clinicians.|http://arxiv.org/abs/2206.03049v1|Jiansheng Fang,Jingwen Wang,Anwei Li,Yuguang Yan,Yonghe Hou,Chao Song,Hongbo Liu,Jiang Liu
1992|Value of Information Analysis for External Validation of Risk Prediction Models|Background: Before being used to inform patient care, a risk prediction model needs to be validated in a representative sample from the target population. The finite size of the validation sample entails that there is uncertainty with respect to estimates of model performance. We apply value-of-information methodology as a framework to quantify the consequence of such uncertainty in terms of NB. Methods: We define the Expected Value of Perfect Information (EVPI) for model validation as the expected loss in NB due to not confidently knowing which of the alternative decisions confers the highest NB at a given risk threshold. We propose methods for EVPI calculations based on Bayesian or ordinary bootstrapping of NBs, as well as an asymptotic approach supported by the central limit theorem. We conducted brief simulation studies to compare the performance of these methods, and used subsets of data from an international clinical trial for predicting mortality after myocardial infarction as a case study. Results: The three computation methods generated similar EVPI values in simulation studies. In the case study, at the pre-specified threshold of 0.02, the best decision with current information would be to use the model, with an expected incremental NB of 0.0020 over treating all. At this threshold, EVPI was 0.0005 (a relative EVPI of 25%). When scaled to the annual number of heart attacks in the US, this corresponds to a loss of 400 true positives, or extra 19,600 false positives (unnecessary treatments) per year, indicating the value of further model validation. As expected, the validation EVPI generally declined with larger samples. Conclusion: Value-of-information methods can be applied to the NB calculated during external validation of clinical prediction models to provide a decision-theoretic perspective to the consequences of uncertainty.|http://arxiv.org/abs/2208.03343v1|Mohsen Sadatsafavi,Tae Yoon Lee,Laure Wynants,Andrew Vickers,Paul Gustafson
1993|Multi-level Adversarial Spatio-temporal Learning for Footstep Pressure based FoG Detection|Freezing of gait (FoG) is one of the most common symptoms of Parkinson's disease, which is a neurodegenerative disorder of the central nervous system impacting millions of people around the world. To address the pressing need to improve the quality of treatment for FoG, devising a computer-aided detection and quantification tool for FoG has been increasingly important. As a non-invasive technique for collecting motion patterns, the footstep pressure sequences obtained from pressure sensitive gait mats provide a great opportunity for evaluating FoG in the clinic and potentially in the home environment. In this study, FoG detection is formulated as a sequential modelling task and a novel deep learning architecture, namely Adversarial Spatio-temporal Network (ASTN), is proposed to learn FoG patterns across multiple levels. A novel adversarial training scheme is introduced with a multi-level subject discriminator to obtain subject-independent FoG representations, which helps to reduce the over-fitting risk due to the high inter-subject variance. As a result, robust FoG detection can be achieved for unseen subjects. The proposed scheme also sheds light on improving subject-level clinical studies from other scenarios as it can be integrated with many existing deep architectures. To the best of our knowledge, this is one of the first studies of footstep pressure-based FoG detection and the approach of utilizing ASTN is the first deep neural network architecture in pursuit of subject-independent representations. Experimental results on 393 trials collected from 21 subjects demonstrate encouraging performance of the proposed ASTN for FoG detection with an AUC 0.85.|http://arxiv.org/abs/2209.10770v1|Kun Hu,Shaohui Mei,Wei Wang,Kaylena A. Ehgoetz Martens,Liang Wang,Simon J. G. Lewis,David D. Feng,Zhiyong Wang
1994|Early in vivo Radiation Damage Quantification for Pediatric Craniospinal Irradiation Using Longitudinal MRI for Intensity Modulated Proton Therapy|Purpose: Proton vertebral body sparing craniospinal irradiation (VBS CSI) treats the thecal sac while avoiding the anterior vertebral bodies in effort to reduce myelosuppression and growth inhibition. However, robust treatment planning needs to compensate proton range uncertainty, contributing unwanted doses within the vertebral bodies. This work aims to develop an early in vivo radiation damage quantification method using longitudinal magnetic resonance (MR) scans to quantify dose effect during fractionated CSI. Materials and methods: Ten pediatric patients were enrolled in a prospective clinical trial of proton VBS CSI receiving 23.4-36 Gy. Monte Carlo robust planning was used with spinal clinical target volumes defined as the thecal sac and neural foramina. T1/T2-weighted MR scans were acquired before, during, and after treatments to detect transition from hematopoietic to less metabolically active fatty marrow. MR signal intensity histograms at each time point were analyzed and fitted by multi-Gaussian models to quantify radiation damages. Results: Fatty marrow filtration was observed on MR images as early as the fifth fraction of treatment. Maximum radiation-induced marrow damage occurred 40-50 days from the treatment start, followed by marrow regeneration. The mean damage ratios were 0.23, 0.41, 0.59, and 0.54 corresponding to 10, 20, 40, and 60 days from the treatment start. Conclusions: We demonstrated a non-invasive method to identify early vertebral marrow damage based on radiation-induced fatty marrow replacement. The proposed method can be potentially used to quantify the quality of CSI vertebral sparing to preserve metabolically active hematopoietic bone marrow.|http://arxiv.org/abs/2210.15557v2|Chih-Wei Chang,Matt Goette,Nadja Kadom,Yinan Wang,Jacob Wynne,Tonghe Wang,Tian Liu,Natia Esiashvili,Jun Zhou,Bree R. Eaton,Xiaofeng Yang
1995|Foresight -- Generative Pretrained Transformer (GPT) for Modelling of Patient Timelines using EHRs|Background: Electronic Health Records hold detailed longitudinal information about each patient's health status and general clinical history, a large portion of which is stored within the unstructured text. Existing approaches focus mostly on structured data and a subset of single-domain outcomes. We explore how temporal modelling of patients from free text and structured data, using deep generative transformers can be used to forecast a wide range of future disorders, substances, procedures or findings. Methods: We present Foresight, a novel transformer-based pipeline that uses named entity recognition and linking tools to convert document text into structured, coded concepts, followed by providing probabilistic forecasts for future medical events such as disorders, substances, procedures and findings. We processed the entire free-text portion from three different hospital datasets totalling 811336 patients covering both physical and mental health. Findings: On tests in two UK hospitals (King's College Hospital, South London and Maudsley) and the US MIMIC-III dataset precision@10 0.68, 0.76 and 0.88 was achieved for forecasting the next disorder in a patient timeline, while precision@10 of 0.80, 0.81 and 0.91 was achieved for forecasting the next biomedical concept. Foresight was also validated on 34 synthetic patient timelines by five clinicians and achieved relevancy of 97% for the top forecasted candidate disorder. As a generative model, it can forecast follow-on biomedical concepts for as many steps as required. Interpretation: Foresight is a general-purpose model for biomedical concept modelling that can be used for real-world risk forecasting, virtual trials and clinical research to study the progression of disorders, simulate interventions and counterfactuals, and educational purposes.|http://arxiv.org/abs/2212.08072v2|Zeljko Kraljevic,Dan Bean,Anthony Shek,Rebecca Bendayan,Harry Hemingway,Joshua Au Yeung,Alexander Deng,Alfie Baston,Jack Ross,Esther Idowu,James T Teo,Richard J Dobson
1996|Proton FLASH irradiation platform for small animal setup at Chang Gung Memorial Hospital|Background : Proton flash therapy is an emergency research topic in radiation therapy since the Varian announced the promising results from the first in human clinical trial of Flash therapy recently. However, it still needs a lot of researches on this topic, not only to understand the mechanism of the radiobiological effects but also to develop an appropriate dose monitoring system. Purpose : In this study we setup an experimental station for small animal proton Flash irradiation in a clinical machine. The dose monitoring system is able to provide real-time irradiation dose and irradiation time structure.   Methods : The dose monitoring system includes homebrewed transmission ionization chamber (TIC), plastic scintillator based beam position monitor, and Poor Man Faraday Cup (FC). Both TIC and FC are equipped with a homebrewed fast reading current integral electronics device. The imaging guidance system comprises a moveable CT, laser, as well as attaching a bead on the body surface of the mouse can accurately guide the testing small animal in position.   Results : The dose monitoring system can provide the time structure of delivered dose rate within 1 ms time resolution. Experimental testing results show that the highest dose in one pulse of 230 MeV proton that can be delivered to the target is about 20 Gy during 199 ms pulse period at 100 Gy/s dose rate.   Conclusion : A proton research irradiation platform dedicated for studying small animal Flash biological effects has been established at Chang Gung Memorial Hospital. The final setup data represent a reference for the beam users to plan the experiments as well as for the improvement of the facility.|http://arxiv.org/abs/2301.01875v1|Tung-Yuan Hsiao,Lu-Kai Wang,Tzung-Yuang Chen,Ching-Fang Yu,Pan,Cheng-Ya,Chun-Chieh Wang,Chien-Yu Lin,I-Chun Cho,Huan Niu,Chien-Hsu Chen
1997|Penalized Deep Partially Linear Cox Models with Application to CT Scans of Lung Cancer Patients|Lung cancer is a leading cause of cancer mortality globally, highlighting the importance of understanding its mortality risks to design effective patient-centered therapies. The National Lung Screening Trial (NLST) employed computed tomography texture analysis, which provides objective measurements of texture patterns on CT scans, to quantify the mortality risks of lung cancer patients. Partially linear Cox models have gained popularity for survival analysis by dissecting the hazard function into parametric and nonparametric components, allowing for the effective incorporation of both well-established risk factors (such as age and clinical variables) and emerging risk factors (e.g., image features) within a unified framework. However, when the dimension of parametric components exceeds the sample size, the task of model fitting becomes formidable, while nonparametric modeling grapples with the curse of dimensionality. We propose a novel Penalized Deep Partially Linear Cox Model (Penalized DPLC), which incorporates the SCAD penalty to select important texture features and employs a deep neural network to estimate the nonparametric component of the model. We prove the convergence and asymptotic properties of the estimator and compare it to other methods through extensive simulation studies, evaluating its performance in risk prediction and feature selection. The proposed method is applied to the NLST study dataset to uncover the effects of key clinical and imaging risk factors on patients' survival. Our findings provide valuable insights into the relationship between these factors and survival outcomes.|http://arxiv.org/abs/2303.05341v3|Yuming Sun,Jian Kang,Chinmay Haridas,Nicholas R. Mayne,Alexandra L. Potter,Chi-Fu Jeffrey Yang,David C. Christiani,Yi Li
1998|A Causal Roadmap for Generating High-Quality Real-World Evidence|Increasing emphasis on the use of real-world evidence (RWE) to support clinical policy and regulatory decision-making has led to a proliferation of guidance, advice, and frameworks from regulatory agencies, academia, professional societies, and industry. A broad spectrum of studies use real-world data (RWD) to produce RWE, ranging from randomized controlled trials with outcomes assessed using RWD to fully observational studies. Yet many RWE study proposals lack sufficient detail to evaluate adequacy, and many analyses of RWD suffer from implausible assumptions, other methodological flaws, or inappropriate interpretations. The Causal Roadmap is an explicit, itemized, iterative process that guides investigators to pre-specify analytic study designs; it addresses a wide range of guidance within a single framework. By requiring transparent evaluation of causal assumptions and facilitating objective comparisons of design and analysis choices based on pre-specified criteria, the Roadmap can help investigators to evaluate the quality of evidence that a given study is likely to produce, specify a study to generate high-quality RWE, and communicate effectively with regulatory agencies and other stakeholders. This paper aims to disseminate and extend the Causal Roadmap framework for use by clinical and translational researchers, with companion papers demonstrating application of the Causal Roadmap for specific use cases.|http://arxiv.org/abs/2305.06850v1|Lauren E Dang,Susan Gruber,Hana Lee,Issa Dahabreh,Elizabeth A Stuart,Brian D Williamson,Richard Wyss,Ivn Daz,Debashis Ghosh,Emre Kcman,Demissie Alemayehu,Katherine L Hoffman,Carla Y Vossen,Raymond A Huml,Henrik Ravn,Kajsa Kvist,Richard Pratley,Mei-Chiung Shih,Gene Pennello,David Martin,Salina P Waddy,Charles E Barr,Mouna Akacha,John B Buse,Mark van der Laan,Maya Petersen
1999|A Prototype Scintillator Real-Time Beam Monitor for Ultra-high Dose Rate Radiotherapy|FLASH Radiotherapy (RT) is a potentially new cancer radiotherapy technique where an entire therapeutic dose is delivered in about 0.1 s and at ~1000 times higher dose rate than in conventional RT. For clinical trials to be conducted safely, precise and fast beam monitoring that can generate an out-of-tolerance beam interrupt is required. A FLASH Beam Scintillator Monitor (FBSM) is being developed based in part on a novel proprietary inorganic hybrid scintillator material. The FBSM provides large area coverage, low mass profile, linear response over a broad dynamic range, radiation tolerance, and real-time analysis IEC-compliant fast beam-interrupt signal. This paper includes the design concept and test results from a prototype device in radiation beams that include heavy ions, FLASH level dose per pulse electron beams, anda hospital radiotherapy clinic with electron beams. Results include image quality, response linearity, radiation hardness, spatial resolution, and real-time data processing. The scintillator showed a small -0.02%/kGy signal decrease after a 212 kGy cumulative dose resulting from continuous exposure for 15 minutes at a FLASH compatible dose rate of 234 Gy/s. These tests established the linear response of the FBSM with respect to dose per pulse. Comparison with commercial Gafchromic film indicates that the FBSM produces a high resolution 2D beam image and can reproduce a nearly identical beam profile. At 20 kfps or 50 microsec/frame, the real-time FPGA based computation and analysis of beam position, beam shape, and beam dose takes < 1 microsec.|http://arxiv.org/abs/2305.15306v3|Daniel S. Levin,Peter S. Friedman,Claudio Ferretti,Nicholas Ristow,Monica Tecchio,Dale W. Litzenberg,Vladimir Bashkirov,Reinhard Schulte
