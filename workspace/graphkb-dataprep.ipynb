{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRIS GraphRAG Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a demo of using IRIS Vector Search capabilities for a graphrag application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is used to get all the requirements. The jupyter image should already have these downloaded, but running this cell just to be safe is advised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain_community arxiv tiktoken langchainhub pymilvus langchain langgraph tavily-python sentence-transformers langchain-milvus langchain-ollama langchain-huggingface beautifulsoup4 langchain-experimental neo4j json-repair langchain-openai langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just some basic setup for the langchain application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "import os\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "max_papers=20\n",
    "data_path=\"/home/jevyan/workspace/data/\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set langchain variables\n",
    "set_debug(False)\n",
    "set_verbose(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should set your OPENAI KEY to be used for the llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<your_token_here>\"\n",
    "\n",
    "gpt4omini = \"gpt-4o-mini\"\n",
    "\n",
    "model = gpt4omini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our project we are using immunology and clinical trials papers from arxiv x. This data has already been converted into graphs and exported as CSV files. Below our example you will find the code we used to load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell connects to our IRIS container, which has the Vector Search Code. If you are running this locally, please enter the information for you own IRIS server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris module\n",
    "import iris\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# change these variables to reflect your connection\n",
    "hostname = \"iris\"\n",
    "port = 1972\n",
    "namespace = \"IRISAPP\"\n",
    "username = \"SuperUser\"\n",
    "password = \"SYS\"\n",
    "\n",
    "# connect\n",
    "connection = iris.connect(\"{:}:{:}/{:}\".format(hostname, port, namespace), username, password)\n",
    "irispy = iris.createIRIS(connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is used to load the data into IRIS, and create embeddings for the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsfile = '/home/irisowner/dev/CSV/Documents.csv'\n",
    "relationsfile = '/home/irisowner/dev/CSV/Relations2.csv'\n",
    "entitiesfile = '/home/irisowner/dev/CSV/Entities.csv'\n",
    "\n",
    "# Load data\n",
    "irispy.classMethodValue(\"GraphKB.Documents\",\"LoadData\",docsfile)\n",
    "irispy.classMethodValue(\"GraphKB.Entity\",\"LoadData\",entitiesfile)\n",
    "irispy.classMethodValue(\"GraphKB.Relations\",\"LoadData\",relationsfile)\n",
    "\n",
    "# Create embeddings\n",
    "irispy.classMethodValue(\"GraphKB.DocumentsEmbeddings\",\"DataToEmbeddings\")\n",
    "irispy.classMethodValue(\"GraphKB.EntityEmbeddings\",\"DataToEmbeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here are the functions which will perform the GraphRAG. Note that these are relatively simple implementations, and can be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import ast\n",
    "import os, sys\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "def extract_query_entities(query):\n",
    "\n",
    "  prompt_text = '''Based on the following example, extract entities from the user provided queries.\n",
    "                Below are a number of example queries and their extracted entities. Provide only the entities.\n",
    "                'How many wars was George Washington involved in' -> ['War', 'George Washington'].\\n\n",
    "                'What are the relationships between the employees' -> ['relationships','employees].\\n\n",
    "\n",
    "                For the following query, extract entities as in the above example.\\n query: {content}'''\n",
    "\n",
    "  llm = ChatOpenAI(temperature=0, model_name=model)\n",
    "  prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "  chain = prompt | llm | StrOutputParser()\n",
    "  response = chain.invoke({\"content\": query})\n",
    "  return ast.literal_eval(response)\n",
    "\n",
    "def global_query(query, items=50,vector_search=10, batch_size = 10):\n",
    "    with HiddenPrints():\n",
    "        docs = irispy.classMethodValue(\"GraphKB.Query\",\"Search\",query,items/2,items/2)\n",
    "        docs = docs.split('\\n\\r\\n')\n",
    "    \n",
    "    answers = []\n",
    "    \n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch = docs[i:i+batch_size]\n",
    "        response = llm_answer_for_batch(batch, query)\n",
    "        answers.append(response)\n",
    "\n",
    "    return llm_answer_summarize(query, answers)\n",
    "\n",
    "def ask_query(query, items = 10, method='local'):\n",
    "    with HiddenPrints():\n",
    "        docs = [irispy.classMethodValue(\"GraphKB.Query\",\"Search\",query,items/2,items/2)]\n",
    "        \n",
    "    response = llm_answer_for_batch(docs, query, False)\n",
    "    return response\n",
    "\n",
    "\n",
    "def llm_answer_summarize(query, answers):\n",
    "    llm = ChatOpenAI(temperature=0, model_name=model)\n",
    "    prompt_text = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following answers to a query derived from analyzing batches of documents. Please compile these answers into one overall answer. If you don't know the answer, just say that you don't know. \n",
    "    Question: {question}  \n",
    "    Previous Answers: {answers}\n",
    "    Answer: \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\"question\": query, 'answers':answers})\n",
    "    return response\n",
    "    \n",
    "    \n",
    "    \n",
    "def llm_answer_for_batch(batch, query, cutoff=True):\n",
    "    llm = ChatOpenAI(temperature=0, model_name=model)\n",
    "    prompt_text = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context from a graph database to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    \"\"\" + ((\"Use three sentences maximum and keep the answer concise:\") if cutoff else \" \") + \"\"\"\n",
    "    Question: {question}  \n",
    "    Graph Context: {graph_context}\n",
    "    Answer: \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\"question\": query, 'graph_context':batch})\n",
    "    return response\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Here we show an example of running the GraphRAG algorithm on our loaded data. First, we use the ask_query method, which will retrieve the inputted number of relevant documents and perform RAG with them. The most relevant documents identified using Vector Search, as well as traversing the created graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most significant papers in immunology summarized from the provided abstracts include:\n",
      "\n",
      "1. **Longitudinal Evaluation of T and B Cell Immunity to SARS-CoV-2**: This paper presents a detailed longitudinal study of immune responses in a single individual over a year post-vaccination, providing insights into vaccine-elicited immunity and suggesting directions for future studies.\n",
      "\n",
      "2. **Vaccine Development and Immune Response Markers**: This research discusses the challenges in developing effective vaccines for high-burden diseases like HIV and proposes a nonparametric methodology for estimating the impact of immune response markers on infection probabilities, enhancing the evaluation of vaccine efficacy.\n",
      "\n",
      "3. **HIV Treatment Monitoring Guidelines**: The paper critiques WHO guidelines for monitoring HIV treatment effectiveness, proposing a diagnostic algorithm that optimizes the use of viral load testing based on clinical and immunological markers to reduce misdiagnosis rates.\n",
      "\n",
      "4. **Natural Language Processing in Clinical Trials**: This study introduces a framework (CT-BERT) for extracting relevant information from clinical trial documents, demonstrating its superiority over existing methods in identifying eligibility criteria.\n",
      "\n",
      "5. **Automated Medical Coding for Clinical Trials**: The paper presents ALIGN, a novel system for automated medical coding that improves data interoperability and reusability in clinical trials, significantly enhancing the accuracy of coding tasks.\n",
      "\n",
      "6. **Generalized Phase 1-2 Trial Designs**: This research proposes a new trial design that incorporates biological outcomes to improve dose selection in clinical trials, potentially leading to better long-term therapeutic success.\n",
      "\n",
      "7. **Structured Treatment Interruptions in HIV Therapy**: This paper analyzes the dynamics of HIV treatment interruptions and proposes a hypothesis explaining virologic failure, emphasizing the need for patient-specific treatment regimens based on immunologic and virologic parameters.\n",
      "\n",
      "These papers collectively contribute to advancements in vaccine development, treatment monitoring, clinical trial methodologies, and personalized medicine in immunology.\n"
     ]
    }
   ],
   "source": [
    "print(ask_query(\"Summarize the most significant papers in immunology\", items = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "You may notice that if we decide to search for many items (maybe 100), these may be too large for the LLM context, and thus we won't be able to get a good result.\n",
    "\n",
    "The 'global_query' method thus takes advantage of another GraphRAG concept. This query will batch files together, answer the question for each batch, and then summarize the batch answers into one final answer. This means that we can now ask for as many items as we think may be necessary, and won't run into an context issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next directions in HIV treatment involve several key areas of focus. Firstly, there is an emphasis on optimizing clinical trial designs and implementing selective viral load testing to enhance diagnosis and treatment monitoring. Personalized treatment regimens based on immunologic and virologic parameters, particularly in structured treatment interruptions, are also being prioritized. \n",
      "\n",
      "Additionally, the development of effective preventive vaccines remains a critical goal, despite slow progress. Utilizing immune response markers as surrogates for clinical endpoints can aid in vaccine development and evaluation. Furthermore, advances in methodologies, such as Bayesian models and automated coding systems, are expected to improve clinical trial designs and data integration for HIV research. \n",
      "\n",
      "Finally, machine learning and data analysis are being leveraged to enhance trial outcomes and create patient-specific treatment strategies. Overall, these directions aim to improve both treatment and prevention of HIV.\n"
     ]
    }
   ],
   "source": [
    "print(global_query(\"What are the next directions in HIV treatment\", items = 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ends the example. Feel free to play around with the above cell, asking more questions about the dataset we are using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "Below is the code we used to download the arxiv data, create graphs from the data, and then export the graphs into csv files that can be imported into IRIS. The below code can be modified to make use of other types of data and documents.\n",
    "\n",
    "(Note -> The GraphKB classes are currently hard-coded to accept the format of data we provide. We did not have time to write a more generalized implementation, but this should prove to be relatively simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    " Uncomment and replace with your own data if desired\n",
    "search_query = \"immunology OR 'clinical trials' OR 'neuroscience'\"\n",
    "max_results = max_papers\n",
    "\n",
    "# Fetch papers from arXiv\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=search_query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for result in client.results(search):\n",
    "    docs.append(\n",
    "        {\"title\": result.title, \"summary\": result.summary, \"url\": result.entry_id, \"authors\": result.authors}\n",
    "    )\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.create_documents(\n",
    "    [doc[\"summary\"]+\" \"+doc[\"title\"]+\"\"+str(doc[\"authors\"]) for doc in docs], metadatas=docs\n",
    ")\n",
    "\n",
    "print(f\"Number of papers: {len(docs)}\")\n",
    "print(f\"Number of chunks: {len(doc_splits)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we save the raw data files as a csv, which will then be loaded into an IRIS table in future steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file into the datapath\n",
    "\n",
    "filename=data_path+\"docs\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|title|abstract|url|authors\",file=file)\n",
    "    s=\"|,\"\n",
    "    for i,doc in enumerate(docs):\n",
    "        abstract=doc['summary'].replace(\"\\n\",' ')\n",
    "        title=doc['title']\n",
    "        try:\n",
    "            print(f\"{i}|{title}|{abstract}|{doc['url']}\",end=\"\",file=file)\n",
    "        except UnicodeEncodeError:\n",
    "            err=1\n",
    "        a=0\n",
    "        for author in doc[\"authors\"]:\n",
    "            auth=str(author).replace('\\u0107','').replace('\\u0131','').replace('\\u0142','').replace('\\u016b','').replace('\\u010d','')\n",
    "            auth=auth.replace('\\u0111','').replace('\\u015f','')\n",
    "            try:\n",
    "                print(f\"{s[a]}{auth}\",end=\"\",file=file)\n",
    "                a=1\n",
    "            except UnicodeEncodeError:\n",
    "                err=2\n",
    "        print(file=file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphRAG Setup\n",
    "\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "graph_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "graph_transformer = LLMGraphTransformer(\n",
    "    llm=graph_llm,\n",
    "    allowed_nodes=[\"Paper\", \"Author\", \"Topic\"],\n",
    "    node_properties=[\"title\", \"summary\", \"url\", \"author\"],\n",
    "    allowed_relationships=[\"AUTHORED\", \"DISCUSSES\", \"RELATED_TO\"],\n",
    ")\n",
    "\n",
    "graph_documents = graph_transformer.convert_to_graph_documents(doc_splits)\n",
    "\n",
    "print(f\"Graph documents: {len(graph_documents)}\")\n",
    "print(f\"Nodes from 1st graph doc:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships from 1st graph doc:{graph_documents[0].relationships}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename=data_path+\"entities\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|entityid|type\",file=file)\n",
    "    for i, doc in enumerate(graph_documents):\n",
    "        for node in doc.nodes:\n",
    "            try:\n",
    "                print(f\"{i}|{node.id}|{node.type}\",file=file)\n",
    "            except UnicodeEncodeError:\n",
    "                err=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=data_path+\"relations\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|source|sourcetype|target|targettype|type\",file=file)\n",
    "    for i, doc in enumerate(graph_documents):\n",
    "        for rel in doc.relationships:\n",
    "            try:\n",
    "                print(f\"{i}|{rel.source.id}|{rel.source.type}|{rel.target.id}|{rel.target.type}|{rel.type}\",file=file)\n",
    "            except UnicodeEncodeError:\n",
    "                err=4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
