{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRIS GraphRAG Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a demo of using IRIS Vector Search capabilities for a graphrag application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is used to get all the requirements. The jupyter image should already have these downloaded, but running this cell just to be safe is advised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain_community arxiv tiktoken langchainhub pymilvus langchain langgraph tavily-python sentence-transformers langchain-milvus langchain-ollama langchain-huggingface beautifulsoup4 langchain-experimental neo4j json-repair langchain-openai langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just some basic setup for the langchain application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "import os\n",
    "\n",
    "max_papers=20\n",
    "data_path=\"/home/jevyan/workspace/data/\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set langchain variables\n",
    "set_debug(False)\n",
    "set_verbose(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should set your OPENAI KEY to be used for the llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<insert_token_here>\n",
    "\n",
    "gpt4omini = \"gpt-4o-mini\"\n",
    "\n",
    "model = gpt4omini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our project we are using immunology and clinical trials papers from arxiv x. Since the data is loaded already we commented these cells out, but you can add your own data extraction here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "''' Uncomment and replace with your own data if desired\n",
    "search_query = \"immunology OR 'clinical trials' OR 'neuroscience'\"\n",
    "max_results = max_papers\n",
    "\n",
    "# Fetch papers from arXiv\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=search_query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for result in client.results(search):\n",
    "    docs.append(\n",
    "        {\"title\": result.title, \"summary\": result.summary, \"url\": result.entry_id, \"authors\": result.authors}\n",
    "    )\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.create_documents(\n",
    "    [doc[\"summary\"]+\" \"+doc[\"title\"]+\"\"+str(doc[\"authors\"]) for doc in docs], metadatas=docs\n",
    ")\n",
    "\n",
    "print(f\"Number of papers: {len(docs)}\")\n",
    "print(f\"Number of chunks: {len(doc_splits)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=data_path+\"docs\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|title|abstract|url|authors\",file=file)\n",
    "    s=\"|,\"\n",
    "    for i,doc in enumerate(docs):\n",
    "        abstract=doc['summary'].replace(\"\\n\",' ')\n",
    "        title=doc['title']\n",
    "        try:\n",
    "            print(f\"{i}|{title}|{abstract}|{doc['url']}\",end=\"\",file=file)\n",
    "        except UnicodeEncodeError:\n",
    "            err=1\n",
    "        a=0\n",
    "        for author in doc[\"authors\"]:\n",
    "            auth=str(author).replace('\\u0107','').replace('\\u0131','').replace('\\u0142','').replace('\\u016b','').replace('\\u010d','')\n",
    "            auth=auth.replace('\\u0111','').replace('\\u015f','')\n",
    "            try:\n",
    "                print(f\"{s[a]}{auth}\",end=\"\",file=file)\n",
    "                a=1\n",
    "            except UnicodeEncodeError:\n",
    "                err=2\n",
    "        print(file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphRAG Setup\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "graph_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "graph_transformer = LLMGraphTransformer(\n",
    "    llm=graph_llm,\n",
    "    allowed_nodes=[\"Paper\", \"Author\", \"Topic\"],\n",
    "    node_properties=[\"title\", \"summary\", \"url\", \"author\"],\n",
    "    allowed_relationships=[\"AUTHORED\", \"DISCUSSES\", \"RELATED_TO\"],\n",
    ")\n",
    "\n",
    "graph_documents = graph_transformer.convert_to_graph_documents(doc_splits)\n",
    "\n",
    "print(f\"Graph documents: {len(graph_documents)}\")\n",
    "print(f\"Nodes from 1st graph doc:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships from 1st graph doc:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=data_path+\"entities\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|entityid|type\",file=file)\n",
    "    for i, doc in enumerate(graph_documents):\n",
    "        for node in doc.nodes:\n",
    "            try:\n",
    "                print(f\"{i}|{node.id}|{node.type}\",file=file)\n",
    "            except UnicodeEncodeError:\n",
    "                err=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=data_path+\"relations\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|source|sourcetype|target|targettype|type\",file=file)\n",
    "    for i, doc in enumerate(graph_documents):\n",
    "        for rel in doc.relationships:\n",
    "            try:\n",
    "                print(f\"{i}|{rel.source.id}|{rel.source.type}|{rel.target.id}|{rel.target.type}|{rel.type}\",file=file)\n",
    "            except UnicodeEncodeError:\n",
    "                err=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris module\n",
    "import iris\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these variables to reflect your connection\n",
    "hostname = \"iris\"\n",
    "port = 1972\n",
    "namespace = \"IRISAPP\"\n",
    "username = \"SuperUser\"\n",
    "password = \"SYS\"\n",
    "\n",
    "# connect\n",
    "connection = iris.connect(\"{:}:{:}/{:}\".format(hostname, port, namespace), username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irispy = iris.createIRIS(connection)\n",
    "irispy.classMethodValue(\"classname\",\"methodname\",args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import ast\n",
    "\n",
    "def extract_query_entities(query):\n",
    "\n",
    "  prompt_text = '''Based on the following example, extract entities from the user provided queries.\n",
    "                Below are a number of example queries and their extracted entities. Provide only the entities.\n",
    "                'How many wars was George Washington involved in' -> ['War', 'George Washington'].\\n\n",
    "                'What are the relationships between the employees' -> ['relationships','employees].\\n\n",
    "\n",
    "                For the following query, extract entities as in the above example.\\n query: {content}'''\n",
    "\n",
    "  llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "  prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "  chain = prompt | llm | StrOutputParser()\n",
    "  response = chain.invoke({\"content\": query})\n",
    "  return ast.literal_eval(response)\n",
    "\n",
    "entities = extract_query_entities(\"What are the most common Phase I trials?\")\n",
    "print(entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
