{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_papers=20\n",
    "data_path=\"c:\\\\u\\\\isc\\\\learning\\\\hack\\\\2025ai\\\\data\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: python.exe: command not found\n"
     ]
    }
   ],
   "source": [
    "! python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain_community arxiv tiktoken langchainhub pymilvus langchain langgraph tavily-python sentence-transformers langchain-milvus langchain-ollama langchain-huggingface beautifulsoup4 langchain-experimental neo4j json-repair langchain-openai langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "set_debug(False)\n",
    "set_verbose(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-svcacct-LiasX3dHur6fYCwfqB2eiUvhIvW-8guitWxm1fiCjxYRU9OqUnkhUryirfvSjX7CwgcstvT3BlbkFJuFBDCLF4aW0VOEnv_nYQ7aERSt83UsBaJv5r3804TjTxh7ms54O3uB-gx6FNJpZH_Y2KEA\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "gpt4omini = \"gpt-4o-mini\"\n",
    "\n",
    "model = gpt4omini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "search_query = \"immunology OR 'clinical trials' OR 'neuroscience'\"\n",
    "max_results = max_papers\n",
    "\n",
    "# Fetch papers from arXiv\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=search_query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "docs = []\n",
    "for result in client.results(search):\n",
    "    docs.append(\n",
    "        {\"title\": result.title, \"summary\": result.summary, \"url\": result.entry_id, \"authors\": result.authors}\n",
    "    )\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.create_documents(\n",
    "    [doc[\"summary\"]+\" \"+doc[\"title\"]+\"\"+str(doc[\"authors\"]) for doc in docs], metadatas=docs\n",
    ")\n",
    "\n",
    "print(f\"Number of papers: {len(docs)}\")\n",
    "print(f\"Number of chunks: {len(doc_splits)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=data_path+\"docs\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|title|abstract|url|authors\",file=file)\n",
    "    s=\"|,\"\n",
    "    for i,doc in enumerate(docs):\n",
    "        abstract=doc['summary'].replace(\"\\n\",' ')\n",
    "        title=doc['title']\n",
    "        try:\n",
    "            print(f\"{i}|{title}|{abstract}|{doc['url']}\",end=\"\",file=file)\n",
    "        except UnicodeEncodeError:\n",
    "            err=1\n",
    "        a=0\n",
    "        for author in doc[\"authors\"]:\n",
    "            auth=str(author).replace('\\u0107','').replace('\\u0131','').replace('\\u0142','').replace('\\u016b','').replace('\\u010d','')\n",
    "            auth=auth.replace('\\u0111','').replace('\\u015f','')\n",
    "            try:\n",
    "                print(f\"{s[a]}{auth}\",end=\"\",file=file)\n",
    "                a=1\n",
    "            except UnicodeEncodeError:\n",
    "                err=2\n",
    "        print(file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphRAG Setup\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "graph_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "graph_transformer = LLMGraphTransformer(\n",
    "    llm=graph_llm,\n",
    "    allowed_nodes=[\"Paper\", \"Author\", \"Topic\"],\n",
    "    node_properties=[\"title\", \"summary\", \"url\", \"author\"],\n",
    "    allowed_relationships=[\"AUTHORED\", \"DISCUSSES\", \"RELATED_TO\"],\n",
    ")\n",
    "\n",
    "graph_documents = graph_transformer.convert_to_graph_documents(doc_splits)\n",
    "\n",
    "print(f\"Graph documents: {len(graph_documents)}\")\n",
    "print(f\"Nodes from 1st graph doc:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships from 1st graph doc:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=data_path+\"entities\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|entityid|type\",file=file)\n",
    "    for i, doc in enumerate(graph_documents):\n",
    "        for node in doc.nodes:\n",
    "            try:\n",
    "                print(f\"{i}|{node.id}|{node.type}\",file=file)\n",
    "            except UnicodeEncodeError:\n",
    "                err=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=data_path+\"relations\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|source|sourcetype|target|targettype|type\",file=file)\n",
    "    for i, doc in enumerate(graph_documents):\n",
    "        for rel in doc.relationships:\n",
    "            try:\n",
    "                print(f\"{i}|{rel.source.id}|{rel.source.type}|{rel.target.id}|{rel.target.type}|{rel.type}\",file=file)\n",
    "            except UnicodeEncodeError:\n",
    "                err=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris module\n",
    "import iris\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these variables to reflect your connection\n",
    "hostname = \"iris\"\n",
    "port = 1972\n",
    "namespace = \"IRISAPP\"\n",
    "username = \"SuperUser\"\n",
    "password = \"SYS\"\n",
    "\n",
    "# connect\n",
    "connection = iris.connect(\"{:}:{:}/{:}\".format(hostname, port, namespace), username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irispy = iris.createIRIS(connection)\n",
    "irispy.classMethodValue(\"classname\",\"methodname\",args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
