{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRIS GraphRAG Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a demo of using IRIS Vector Search capabilities for a graphrag application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is used to get all the requirements. The jupyter image should already have these downloaded, but running this cell just to be safe is advised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /opt/conda/lib/python3.10/site-packages (0.3.16)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: arxiv in /opt/conda/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.8.0)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: langchainhub in /opt/conda/lib/python3.10/site-packages (0.1.21)\n",
      "Requirement already satisfied: pymilvus in /opt/conda/lib/python3.10/site-packages (2.5.4)\n",
      "Collecting pymilvus\n",
      "  Downloading pymilvus-2.5.5-py3-none-any.whl (223 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.3.17)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.20-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: langgraph in /opt/conda/lib/python3.10/site-packages (0.2.69)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.3.5-py3-none-any.whl (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.5/131.5 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tavily-python in /opt/conda/lib/python3.10/site-packages (0.5.0)\n",
      "Collecting tavily-python\n",
      "  Downloading tavily_python-0.5.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: langchain-milvus in /opt/conda/lib/python3.10/site-packages (0.1.8)\n",
      "Requirement already satisfied: langchain-ollama in /opt/conda/lib/python3.10/site-packages (0.2.3)\n",
      "Requirement already satisfied: langchain-huggingface in /opt/conda/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (4.13.1)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.0/186.0 kB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: langchain-experimental in /opt/conda/lib/python3.10/site-packages (0.3.4)\n",
      "Requirement already satisfied: neo4j in /opt/conda/lib/python3.10/site-packages (5.27.0)\n",
      "Collecting neo4j\n",
      "  Downloading neo4j-5.28.1-py3-none-any.whl (312 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.3/312.3 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: json-repair in /opt/conda/lib/python3.10/site-packages (0.35.0)\n",
      "Collecting json-repair\n",
      "  Downloading json_repair-0.39.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: langchain-openai in /opt/conda/lib/python3.10/site-packages (0.3.3)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.8-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flask\n",
      "  Downloading flask-3.1.0-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting setuptools==69.0.3\n",
      "  Downloading setuptools-69.0.3-py3-none-any.whl (819 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.5/819.5 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (1.4.44)\n",
      "Collecting langchain-core<1.0.0,>=0.3.41\n",
      "  Downloading langchain_core-0.3.43-py3-none-any.whl (415 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (0.3.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from langchain_community) (2.7.1)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /opt/conda/lib/python3.10/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /opt/conda/lib/python3.10/site-packages (from langchainhub) (2.32.0.20241016)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchainhub) (24.2)\n",
      "Requirement already satisfied: ujson>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pymilvus) (5.10.0)\n",
      "Requirement already satisfied: milvus-lite>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from pymilvus) (2.4.11)\n",
      "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /opt/conda/lib/python3.10/site-packages (from pymilvus) (1.67.1)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from pymilvus) (1.0.1)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /opt/conda/lib/python3.10/site-packages (from pymilvus) (1.5.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /opt/conda/lib/python3.10/site-packages (from pymilvus) (5.29.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.6\n",
      "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /opt/conda/lib/python3.10/site-packages (from langgraph) (2.0.10)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /opt/conda/lib/python3.10/site-packages (from langgraph) (0.1.51)\n",
      "Collecting langgraph-prebuilt<0.2,>=0.1.1\n",
      "  Downloading langgraph_prebuilt-0.1.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from tavily-python) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.48.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.3.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in /opt/conda/lib/python3.10/site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from langchain-huggingface) (0.21.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from neo4j) (2022.6)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /opt/conda/lib/python3.10/site-packages (from langchain-openai) (1.61.0)\n",
      "Collecting itsdangerous>=2.2\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting Werkzeug>=3.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting blinker>=1.9\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Collecting click>=8.1.3\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from flask) (3.1.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: sgmllib3k in /opt/conda/lib/python3.10/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask) (2.1.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (1.33)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/conda/lib/python3.10/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->tavily-python) (3.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->tavily-python) (1.0.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->tavily-python) (2022.9.24)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->tavily-python) (3.6.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain_community) (2.1.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (2.0.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.2.4->pymilvus) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Installing collected packages: Werkzeug, setuptools, neo4j, json-repair, itsdangerous, click, blinker, beautifulsoup4, tiktoken, flask, tavily-python, pymilvus, langchain-core, langchain-text-splitters, langchain-openai, langgraph-prebuilt, langchain, langgraph, langchain_community\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.8.0\n",
      "    Uninstalling setuptools-75.8.0:\n",
      "      Successfully uninstalled setuptools-75.8.0\n",
      "  Attempting uninstall: neo4j\n",
      "    Found existing installation: neo4j 5.27.0\n",
      "    Uninstalling neo4j-5.27.0:\n",
      "      Successfully uninstalled neo4j-5.27.0\n",
      "  Attempting uninstall: json-repair\n",
      "    Found existing installation: json_repair 0.35.0\n",
      "    Uninstalling json_repair-0.35.0:\n",
      "      Successfully uninstalled json_repair-0.35.0\n",
      "  Attempting uninstall: blinker\n",
      "    Found existing installation: blinker 1.5\n",
      "    Uninstalling blinker-1.5:\n",
      "      Successfully uninstalled blinker-1.5\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.13.1\n",
      "    Uninstalling beautifulsoup4-4.13.1:\n",
      "      Successfully uninstalled beautifulsoup4-4.13.1\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.8.0\n",
      "    Uninstalling tiktoken-0.8.0:\n",
      "      Successfully uninstalled tiktoken-0.8.0\n",
      "  Attempting uninstall: tavily-python\n",
      "    Found existing installation: tavily-python 0.5.0\n",
      "    Uninstalling tavily-python-0.5.0:\n",
      "      Successfully uninstalled tavily-python-0.5.0\n",
      "  Attempting uninstall: pymilvus\n",
      "    Found existing installation: pymilvus 2.5.4\n",
      "    Uninstalling pymilvus-2.5.4:\n",
      "      Successfully uninstalled pymilvus-2.5.4\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.33\n",
      "    Uninstalling langchain-core-0.3.33:\n",
      "      Successfully uninstalled langchain-core-0.3.33\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.5\n",
      "    Uninstalling langchain-text-splitters-0.3.5:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.5\n",
      "  Attempting uninstall: langchain-openai\n",
      "    Found existing installation: langchain-openai 0.3.3\n",
      "    Uninstalling langchain-openai-0.3.3:\n",
      "      Successfully uninstalled langchain-openai-0.3.3\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.17\n",
      "    Uninstalling langchain-0.3.17:\n",
      "      Successfully uninstalled langchain-0.3.17\n",
      "  Attempting uninstall: langgraph\n",
      "    Found existing installation: langgraph 0.2.69\n",
      "    Uninstalling langgraph-0.2.69:\n",
      "      Successfully uninstalled langgraph-0.2.69\n",
      "  Attempting uninstall: langchain_community\n",
      "    Found existing installation: langchain-community 0.3.16\n",
      "    Uninstalling langchain-community-0.3.16:\n",
      "      Successfully uninstalled langchain-community-0.3.16\n",
      "Successfully installed Werkzeug-3.1.3 beautifulsoup4-4.13.3 blinker-1.9.0 click-8.1.8 flask-3.1.0 itsdangerous-2.2.0 json-repair-0.39.1 langchain-0.3.20 langchain-core-0.3.43 langchain-openai-0.3.8 langchain-text-splitters-0.3.6 langchain_community-0.3.19 langgraph-0.3.5 langgraph-prebuilt-0.1.2 neo4j-5.28.1 pymilvus-2.5.5 setuptools-69.0.3 tavily-python-0.5.1 tiktoken-0.9.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -U langchain_community arxiv tiktoken langchainhub pymilvus langchain langgraph tavily-python sentence-transformers langchain-milvus langchain-ollama langchain-huggingface beautifulsoup4 langchain-experimental neo4j json-repair langchain-openai langchain-ollama flask setuptools==69.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just some basic setup for the langchain application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.globals import set_verbose, set_debug\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "max_papers=30\n",
    "\n",
    "data_path=\"/home/jovyan/workspace/data/\"\n",
    "\n",
    "#load_dotenv()\n",
    "\n",
    "# Set langchain variables\n",
    "set_debug(False)\n",
    "set_verbose(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should set your OPENAI KEY to be used for the llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-N84wsFAuOZNbYfq853q6D1XJCibzRfnjYk5txC700vT3BlbkFJRktCqqeKcDyKYvoxr07rEbcZ4D_o9IqkBHZ9ECRm8A\"\n",
    "\n",
    "gpt4omini = \"gpt-4o-mini\"\n",
    "\n",
    "model = gpt4omini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our project we are using immunology and clinical trials papers from arxiv x. This data has already been converted into graphs and exported as CSV files. Below our example you will find the code we used to load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell connects to our IRIS container, which has the Vector Search Code. If you are running this locally, please enter the information for you own IRIS server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris module\n",
    "import iris\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# change these variables to reflect your connection\n",
    "hostname = \"iris\"\n",
    "port = 1972\n",
    "namespace = \"IRISAPP\"\n",
    "username = \"SuperUser\"\n",
    "password = \"SYS\"\n",
    "\n",
    "# connect\n",
    "connection = iris.connect(\"{:}:{:}/{:}\".format(hostname, port, namespace), username, password)\n",
    "irispy = iris.createIRIS(connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is used to load the data into IRIS, and create embeddings for the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result Set idx: 0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsfile = '/home/jovyan/workspace/CSV/Documents.csv'\n",
    "relationsfile = '/home/jovyan/workspace/CSV/Relations2.csv'\n",
    "entitiesfile = '/home/jovyan/workspace/CSV/Entities.csv'\n",
    "\n",
    "\n",
    "# Load data\n",
    "irispy.classMethodValue(\"GraphKB.Documents\",\"LoadData\",docsfile)\n",
    "irispy.classMethodValue(\"GraphKB.Entity\",\"LoadData\",entitiesfile)\n",
    "irispy.classMethodValue(\"GraphKB.Relations\",\"LoadData\",relationsfile)\n",
    "\n",
    "# Create embeddings\n",
    "#irispy.classMethodValue(\"GraphKB.DocumentsEmbeddings\",\"DataToEmbeddings\")  ???????\n",
    "irispy.classMethodValue(\"GraphKB.EntityEmbeddings\",\"DataToEmbeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here are the functions which will perform the GraphRAG. Note that these are relatively simple implementations, and can be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import ast\n",
    "\n",
    "import os, sys\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "def extract_query_entities(query):\n",
    "\n",
    "  prompt_text = '''Based on the following example, extract entities from the user provided queries.\n",
    "                Below are a number of example queries and their extracted entities. Provide only the entities.\n",
    "                'How many wars was George Washington involved in' -> ['War', 'George Washington'].\\n\n",
    "                'What are the relationships between the employees' -> ['relationships','employees].\\n\n",
    "\n",
    "                For the following query, extract entities as in the above example.\\n query: {content}'''\n",
    "\n",
    "  llm = ChatOpenAI(temperature=0, model_name=model)\n",
    "  prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "  chain = prompt | llm | StrOutputParser()\n",
    "  response = chain.invoke({\"content\": query})\n",
    "  return ast.literal_eval(response)\n",
    "\n",
    "def global_query(query, items=50,vector_search=10, batch_size = 10):\n",
    "    with HiddenPrints():\n",
    "        docs = irispy.classMethodValue(\"GraphKB.Query\",\"Search\",query,items/2,items/2)\n",
    "        docs = docs.split('\\n\\r\\n')\n",
    "    \n",
    "    answers = []\n",
    "    \n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch = docs[i:i+batch_size]\n",
    "        response = llm_answer_for_batch(batch, query)\n",
    "        answers.append(response)\n",
    "\n",
    "    return llm_answer_summarize(query, answers)\n",
    "\n",
    "def ask_query(query, items = 10, method='local'):\n",
    "    with HiddenPrints():\n",
    "        docs = [irispy.classMethodValue(\"GraphKB.Query\",\"Search\",query,items/2,items/2)]\n",
    "        \n",
    "    response = llm_answer_for_batch(docs, query, False)\n",
    "    return response\n",
    "\n",
    "\n",
    "def llm_answer_summarize(query, answers):\n",
    "    llm = ChatOpenAI(temperature=0, model_name=model)\n",
    "    prompt_text = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following answers to a query derived from analyzing batches of documents. Please compile these answers into one overall answer. If you don't know the answer, just say that you don't know. \n",
    "    Question: {question}  \n",
    "    Previous Answers: {answers}\n",
    "    Answer: \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\"question\": query, 'answers':answers})\n",
    "    return response\n",
    "    \n",
    "    \n",
    "    \n",
    "def llm_answer_for_batch(batch, query, cutoff=True):\n",
    "    llm = ChatOpenAI(temperature=0, model_name=model)\n",
    "    prompt_text = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context from a graph database to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    \"\"\" + ((\"Use three sentences maximum and keep the answer concise:\") if cutoff else \" \") + \"\"\"\n",
    "    Question: {question}  \n",
    "    Graph Context: {graph_context}\n",
    "    Answer: \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\"question\": query, 'graph_context':batch})\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 981, in launch_instance\n",
      "    app.initialize(argv)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 110, in inner\n",
      "    return method(app, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 666, in initialize\n",
      "    self.init_sockets()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 307, in init_sockets\n",
      "    self.shell_port = self._bind_socket(self.shell_socket, self.shell_port)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 244, in _bind_socket\n",
      "    return self._try_bind_socket(s, port)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 220, in _try_bind_socket\n",
      "    s.bind(\"tcp://%s:%i\" % (self.ip, port))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/zmq/sugar/socket.py\", line 232, in bind\n",
      "    super().bind(addr)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 568, in zmq.backend.cython.socket.Socket.bind\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 28, in zmq.backend.cython.checkrc._check_rc\n",
      "zmq.error.ZMQError: Address already in use\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from flask import Flask, render_template, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Route for the homepage (question form)\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def home():\n",
    "    answer = None\n",
    "    if request.method == \"POST\":\n",
    "        question = request.form.get(\"question\")\n",
    "        if question:\n",
    "            # Call the ask_query function to get the answer to the question\n",
    "            answer = ask_query(question)\n",
    "    return render_template(\"index.html\", answer=answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True,port=5001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Here we show an example of running the GraphRAG algorithm on our loaded data. First, we use the ask_query method, which will retrieve the inputted number of relevant documents and perform RAG with them. The most relevant documents identified using Vector Search, as well as traversing the created graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most significant papers in immunology summarized from the provided abstracts are:\n",
      "\n",
      "1. **Longitudinal Evaluation of T and B Cell Immunity to SARS-CoV-2**: This paper presents a detailed longitudinal study of T and B cell immunity in a single subject over a year post-vaccination. It emphasizes the insights gained from personal biological samples collected during the Moderna Phase III clinical trial, contributing to the understanding of vaccine-elicited immunity.\n",
      "\n",
      "2. **Vaccine Development and Immune Response Markers**: This research discusses the challenges in developing effective vaccines for high-burden diseases like HIV. It proposes a nonparametric methodology for estimating the impact of immune response markers on infection probabilities, enhancing the evaluation of immune responses in clinical trials.\n",
      "\n",
      "3. **Diagnostic Algorithm for HIV Treatment Monitoring**: This paper critiques the WHO guidelines for monitoring HIV treatment effectiveness, proposing a new diagnostic algorithm that optimizes the use of viral load testing based on clinical and immunological markers, aiming to reduce misdiagnosis rates in resource-limited settings.\n",
      "\n",
      "4. **Natural Language Processing in Clinical Trials**: This study introduces CT-BERT, a framework for extracting relevant information from clinical trial documents using NLP techniques. It demonstrates improved performance in identifying eligibility criteria compared to existing methods.\n",
      "\n",
      "5. **Automated Medical Coding for Clinical Trials**: The paper presents ALIGN, a novel system for automated medical coding that addresses interoperability challenges in clinical trial data. It showcases high accuracy in coding medication and medical history terms, significantly enhancing data integration and reusability in immunology research.\n",
      "\n",
      "These papers collectively highlight advancements in vaccine research, diagnostic methodologies, and the application of technology in immunology.\n"
     ]
    }
   ],
   "source": [
    "print(ask_query(\"Summarize the most significant papers in immunology\", items = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "You may notice that if we decide to search for many items (maybe 100), these may be too large for the LLM context, and thus we won't be able to get a good result.\n",
    "\n",
    "The 'global_query' method thus takes advantage of another GraphRAG concept. This query will batch files together, answer the question for each batch, and then summarize the batch answers into one final answer. This means that we can now ask for as many items as we think may be necessary, and won't run into an context issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next directions in HIV treatment involve several key areas of focus. Firstly, there is an emphasis on optimizing clinical trial designs through advanced data analysis and the use of synthetic clinical trial data to enhance patient outcomes. This includes developing diagnostic algorithms that selectively utilize viral load testing to monitor treatment effectiveness, particularly in resource-limited settings. \n",
      "\n",
      "Additionally, there is ongoing research into personalized treatment regimens that take into account immunologic and virologic parameters, which may improve the efficacy of structured treatment interruptions. Another important direction is the development of effective preventive vaccines, which, despite slow progress, hold significant potential for public health impact. Furthermore, leveraging historical clinical trial data and advanced automated coding systems is expected to enhance data interoperability, thereby accelerating research and drug development. Collectively, these approaches aim to improve the efficiency and effectiveness of HIV treatment strategies.\n"
     ]
    }
   ],
   "source": [
    "print(global_query(\"What are the next directions in HIV treatment\", items = 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ends the example. Feel free to play around with the above cell, asking more questions about the dataset we are using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "Below is the code we used to download the arxiv data, create graphs from the data, and then export the graphs into csv files that can be imported into IRIS. The below code can be modified to make use of other types of data and documents.\n",
    "\n",
    "(Note -> The GraphKB classes are currently hard-coded to accept the format of data we provide. We did not have time to write a more generalized implementation, but this should prove to be relatively simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 1:\n",
      "Title: Bayesian Models and Decision Algorithms for Complex Early Phase Clinical Trials\n",
      "Summary: An early phase clinical trial is the first step in evaluating the effects in\n",
      "humans of a potential new anti-disease agent or combination of agents. Usually\n",
      "called \"phase I\" or \"phase I/II\" trials, these experiments typically have the\n",
      "nominal scientific goal of determining an acceptable dose, most often based on\n",
      "adverse event probabilities. This arose from a tradition of phase I trials to\n",
      "evaluate cytotoxic agents for treating cancer, although some methods may be\n",
      "applied in other medical settings, such as treatment of stroke or immunological\n",
      "diseases. Most modern statistical designs for early phase trials include\n",
      "model-based, outcome-adaptive decision rules that choose doses for successive\n",
      "patient cohorts based on data from previous patients in the trial. Such designs\n",
      "have seen limited use in clinical practice, however, due to their complexity,\n",
      "the requirement of intensive, computer-based data monitoring, and the medical\n",
      "community's resistance to change. Still, many actual applications of\n",
      "model-based outcome-adaptive designs have been remarkably successful in terms\n",
      "of both patient benefit and scientific outcome. In this paper I will review\n",
      "several Bayesian early phase trial designs that were tailored to accommodate\n",
      "specific complexities of the treatment regime and patient outcomes in\n",
      "particular clinical settings.\n",
      "URL: http://arxiv.org/abs/1011.6494v1\n",
      "Authors: Peter F. Thall\n",
      "--------------------------------------------------\n",
      "Paper 2:\n",
      "Title: Deep Historical Borrowing Framework to Prospectively and Simultaneously Synthesize Control Information in Confirmatory Clinical Trials with Multiple Endpoints\n",
      "Summary: In current clinical trial development, historical information is receiving\n",
      "more attention as it provides utility beyond sample size calculation.\n",
      "Meta-analytic-predictive (MAP) priors and robust MAP priors have been proposed\n",
      "for prospectively borrowing historical data on a single endpoint. To\n",
      "simultaneously synthesize control information from multiple endpoints in\n",
      "confirmatory clinical trials, we propose to approximate posterior probabilities\n",
      "from a Bayesian hierarchical model and estimate critical values by deep\n",
      "learning to construct pre-specified strategies for hypothesis testing. This\n",
      "feature is important to ensure study integrity by establishing prospective\n",
      "decision functions before the trial conduct. Simulations are performed to show\n",
      "that our method properly controls family-wise error rate (FWER) and preserves\n",
      "power as compared with a typical practice of choosing constant critical values\n",
      "given a subset of null space. Satisfactory performance under prior-data\n",
      "conflict is also demonstrated. We further illustrate our method using a case\n",
      "study in Immunology.\n",
      "URL: http://arxiv.org/abs/2008.12774v2\n",
      "Authors: Tianyu Zhan, Yiwang Zhou, Ziqian Geng, Yihua Gu, Jian Kang, Li Wang, Xiaohong Huang, Elizabeth H. Slate\n",
      "--------------------------------------------------\n",
      "Paper 3:\n",
      "Title: Parametric Resonance May Explain Virologic Failure to HIV Treatment Interruptions\n",
      "Summary: Pilot studies of structured treatment interruptions (STI) in HIV therapy have\n",
      "shown that patients can maintain low viral loads whilst benefiting from reduced\n",
      "treatment toxicity. However, a recent STI clinical trial reported a high degree\n",
      "of virologic failure. Here we present a novel hypothesis that could explain\n",
      "virologic failure to STI and provides new insights of great clinical relevance.\n",
      "We analyze a classic mathematical model of HIV within-host viral dynamics and\n",
      "find that nonlinear parametric resonance occurs when STI are added to the\n",
      "model; resonance is observed as virologic failure. We use the model to simulate\n",
      "clinical trial data and to calculate patient-specific resonant spectra. We gain\n",
      "two important insights. Firstly, within an STI trial, we determine that\n",
      "patients who begin with similar viral loads can be expected to show extremely\n",
      "different virologic responses as a result of resonance. Thus, high\n",
      "heterogeneity of patient response within a STI clinical trial is to be\n",
      "expected. Secondly and more importantly, we determine that virologic failure is\n",
      "not simply due to STI or patient characteristics; rather it is the result of a\n",
      "complex dynamic interaction between STI and patient viral dynamics. Hence, our\n",
      "analyses demonstrate that no universal regimen with periodic interruptions will\n",
      "be effective for all patients. On the basis of our results, we suggest that\n",
      "immunologic and virologic parameters should be used to design patient-specific\n",
      "STI regimens.\n",
      "URL: http://arxiv.org/abs/q-bio/0504031v1\n",
      "Authors: Romulus Breban, Sally Blower\n",
      "--------------------------------------------------\n",
      "Number of papers: 30\n",
      "Number of chunks: 30\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import tarfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "#Uncomment and replace with your own data if desired\n",
    "search_query = \"immunology OR 'clinical trials' OR 'neuroscience'\"\n",
    "max_papers=30\n",
    "max_results = max_papers\n",
    "max_papers=30\n",
    "\n",
    "# Fetch papers from arXiv\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=search_query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for result in client.results(search):\n",
    "    docs.append(\n",
    "        {\"title\": result.title, \"summary\": result.summary, \"url\": result.entry_id, \"authors\": result.authors}\n",
    "    )\n",
    "docs_to_print = docs[:3]\n",
    "\n",
    "\n",
    "# Print the details of each paper in the docs list\n",
    "for i, doc in enumerate(docs_to_print, start=1):\n",
    "    authors_str = \", \".join([str(author) for author in doc['authors']])  # Convert authors to strings\n",
    "    print(f\"Paper {i}:\")\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Summary: {doc['summary']}\")\n",
    "    print(f\"URL: {doc['url']}\")\n",
    "    print(f\"Authors: {authors_str}\")  # Join the authors as a string\n",
    "    print(\"-\" * 50)  # Divider to separate the papers\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.create_documents(\n",
    "    [doc[\"summary\"]+\" \"+doc[\"title\"]+\"\"+str(doc[\"authors\"]) for doc in docs], metadatas=docs\n",
    ")\n",
    "\n",
    "print(f\"Number of papers: {len(docs)}\")\n",
    "print(f\"Number of chunks: {len(doc_splits)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we save the raw data files as a csv, which will then be loaded into an IRIS table in future steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file into the datapath\n",
    "data_path=\"/home/jovyan/workspace/data/\"\n",
    "filename=data_path+\"docs\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|title|abstract|url|authors\",file=file)\n",
    "    s=\"|,\"\n",
    "    for i,doc in enumerate(docs):\n",
    "        abstract=doc['summary'].replace(\"\\n\",' ')\n",
    "        title=doc['title']\n",
    "        try:\n",
    "            print(f\"{i}|{title}|{abstract}|{doc['url']}\",end=\"\",file=file)\n",
    "        except UnicodeEncodeError:\n",
    "            err=1\n",
    "        a=0\n",
    "        for author in doc[\"authors\"]:\n",
    "            auth=str(author).replace('\\u0107','').replace('\\u0131','').replace('\\u0142','').replace('\\u016b','').replace('\\u010d','')\n",
    "            auth=auth.replace('\\u0111','').replace('\\u015f','')\n",
    "            try:\n",
    "                print(f\"{s[a]}{auth}\",end=\"\",file=file)\n",
    "                a=1\n",
    "            except UnicodeEncodeError:\n",
    "                err=2\n",
    "        print(file=file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph documents: 30\n",
      "Nodes from 1st graph doc:[Node(id='Early Phase Clinical Trial', type='Topic', properties={'summary': 'An early phase clinical trial is the first step in evaluating the effects in humans of a potential new anti-disease agent or combination of agents.'}), Node(id='Phase I Trials', type='Topic', properties={'summary': \"Usually called 'phase I' or 'phase I/II' trials, these experiments typically have the nominal scientific goal of determining an acceptable dose, most often based on adverse event probabilities.\"}), Node(id='Cytotoxic Agents', type='Topic', properties={'summary': 'Phase I trials traditionally evaluate cytotoxic agents for treating cancer.'}), Node(id='Bayesian Early Phase Trial Designs', type='Topic', properties={'summary': 'The paper reviews several Bayesian early phase trial designs tailored to accommodate specific complexities of the treatment regime and patient outcomes in particular clinical settings.'}), Node(id='Peter F. Thall', type='Author', properties={'author': 'Peter F. Thall'})]\n",
      "Relationships from 1st graph doc:[Relationship(source=Node(id='Early Phase Clinical Trial', type='Topic', properties={}), target=Node(id='Phase I Trials', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Phase I Trials', type='Topic', properties={}), target=Node(id='Cytotoxic Agents', type='Topic', properties={}), type='RELATED_TO', properties={}), Relationship(source=Node(id='Bayesian Early Phase Trial Designs', type='Topic', properties={}), target=Node(id='Peter F. Thall', type='Author', properties={}), type='AUTHORED', properties={})]\n"
     ]
    }
   ],
   "source": [
    "# GraphRAG Setup\n",
    "\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "graph_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "graph_transformer = LLMGraphTransformer(\n",
    "    llm=graph_llm,\n",
    "    allowed_nodes=[\"Paper\", \"Author\", \"Topic\"],\n",
    "    node_properties=[\"title\", \"summary\", \"url\", \"author\"],\n",
    "    allowed_relationships=[\"AUTHORED\", \"DISCUSSES\", \"RELATED_TO\"],\n",
    ")\n",
    "\n",
    "graph_documents = graph_transformer.convert_to_graph_documents(doc_splits)\n",
    "\n",
    "print(f\"Graph documents: {len(graph_documents)}\")\n",
    "print(f\"Nodes from 1st graph doc:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships from 1st graph doc:{graph_documents[0].relationships}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename=data_path+\"entities\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|entityid|type\",file=file)\n",
    "    for i, doc in enumerate(graph_documents):\n",
    "        for node in doc.nodes:\n",
    "            try:\n",
    "                print(f\"{i}|{node.id}|{node.type}\",file=file)\n",
    "            except UnicodeEncodeError:\n",
    "                err=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=data_path+\"relations\"+str(max_papers)+\".csv\"\n",
    "with open(filename,\"w\") as file:\n",
    "    print(\"docid|source|sourcetype|target|targettype|type\",file=file)\n",
    "    for i, doc in enumerate(graph_documents):\n",
    "        for rel in doc.relationships:\n",
    "            try:\n",
    "                print(f\"{i}|{rel.source.id}|{rel.source.type}|{rel.target.id}|{rel.target.type}|{rel.type}\",file=file)\n",
    "            except UnicodeEncodeError:\n",
    "                err=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
